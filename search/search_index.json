{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Gabarit - Templates Data Science","text":"<p>Gabarit provides you with a set of python templates (a.k.a. frameworks) for your Data Science projects. It allows you to generate a code base that includes many features to speed up the production and testing of your AI models. You just have to focus on the core of Data Science.</p>"},{"location":"#philosophy","title":"Philosophy","text":"<p>As a team, we strive to help Data Scientists across the board (and ourselves!) build awesome IA projects by speeding up the development process. This repository contains several frameworks allowing any data scientist, IA enthousiast (or developper of any kind, really) to kickstart an IA project from scratch.  </p> <p>We hate it when a project is left in the infamous POC shadow valley where nice ideas and clever models are forgotten, thus we tried to pack as much production-ready features as we could in these frameworks.  </p> <p>As Hadley Wickhman would say: \"you can't do data science in a GUI\". We are strong believers that during a data science or IA project, you need to be able to fine tune every nooks and crannies to make the best out of your data.  </p> <p>Therefore, these frameworks act as project templates that you can use to generate a code base from nothing (except for a project name). Doing so would allow your fresh and exciting new project to begin with loads of features on which you wouldn't want to focus this early :</p> <ul> <li>Built-in models: from the ever useful TF/IDF + SVM to the more recent transformers</li> <li>Model-agnostic save/load/reload : perfect to embed your model behind a web service</li> <li>Generic training/predict scripts to work with your data as soon as possible</li> <li>DVC &amp; MLFlow integration (you have to configure it to point to your own infrastructures)</li> <li>Streamlit demo tool</li> <li>... and so much more !</li> </ul>"},{"location":"#frameworks","title":"Frameworks","text":"<p>Gabarit contains the following frameworks :</p>"},{"location":"#nlp","title":"NLP","text":"<p>To tackle classification use cases on textual data</p> <ul> <li>Relies on the Words'n fun module for the preprocessing requirements</li> <li>Supports :<ul> <li> Mono Class / Mono Label classification</li> <li> Multi Classes / Mono Label classification</li> <li> Mono Class / Multi Labels classification</li> </ul> </li> </ul>"},{"location":"#numeric","title":"Numeric","text":"<p>To tackle classification and regression use cases on numerical data</p> <ul> <li>Supports :<ul> <li> Regression</li> <li> Multi Classes / Mono Label classification</li> <li> Mono Class / Multi Labels classification</li> </ul> </li> </ul>"},{"location":"#computer-vision","title":"Computer Vision","text":"<p>To tackle classification use cases on images</p> <ul> <li>Supports<ul> <li> Mono Class / Mono Label classification</li> <li> Multi Classes / Mono Label classification</li> <li> Area of interest detection</li> </ul> </li> </ul>"},{"location":"#api","title":"API","text":"<p>Provides a FastAPI for exposing your model to the world</p> <ul> <li>Supports<ul> <li> Gabarit model created with one of the previous package</li> <li> Any model of your own</li> </ul> </li> </ul> <p>These frameworks have been developped to manage different topics but share a common structure and a common philosophy. Once a project made using a framework is in production, any other project can be sent into production following the same process. Using the API template, you can expose framework made models in no time !</p>"},{"location":"#getting-started","title":"Getting started","text":""},{"location":"#installation","title":"Installation","text":"<p>Gabarit supports python &gt;= 3.7. To install it, run the command : </p> <pre><code>pip install gabarit\n</code></pre> <p>This will install <code>gabarit</code> package and all frameworks.</p>"},{"location":"#kickstart-a-new-project","title":"Kickstart a new project","text":"<p>To create a new project from a template, use gabarit entry points : </p> <ul> <li><code>generate_nlp_project</code></li> <li><code>generate_num_project</code></li> <li><code>generate_vision_project</code></li> <li><code>generate_api_project</code></li> </ul> <p>Example : <code>generate_nlp_project -n my_awesome_package -p my_new_project_path -c my_configuration.ini --upload my_instructions.md --dvc dvc_config</code></p> <p>They take several parameters as input :</p> <ul> <li><code>-n</code> or <code>--name</code> : Name of the package/project (lowercase, no whitespace)</li> <li><code>-p</code> or <code>--path</code> : Path (Absolute or relative) where to create the main directory of the project</li> <li><code>-c</code> or <code>--config</code> : Path (Absolute or relative) to a .ini configuration file.     A default configuration file is given alongside each project. (<code>default_config.ini</code>).     It usually contains stuff like default encoding, default separator for .csv files, pip proxy settings, etc.</li> <li><code>--upload</code> or <code>--upload_intructions</code> : Path (Absolute or relative) to a file that contains a list of instructions to upload a trained model to your favorite storage solution.</li> <li><code>--dvc</code> or <code>--dvc_config</code> : Path (Absolute or relative) to a DVC configuration file. If not provided, DVC won't be used.</li> </ul>"},{"location":"#setup-your-new-project","title":"Setup your new project","text":"<ul> <li> <p>(Optionnal) We strongly advise to create a python virtual env</p> <ul> <li><code>pip install virtualenv</code></li> <li><code>python -m venv my_awesome_venv</code></li> <li><code>cd my_awesome_venv/Scripts/ &amp;&amp; activate</code> (windows) or <code>source my_awesome_venv/bin/activate</code> (linux)</li> </ul> </li> <li> <p>Requirements : <code>pip install --no-cache-dir -r requirements.txt</code></p> </li> <li> <p>Setup the project (in develop mode) : <code>python setup.py develop</code></p> </li> </ul> <p>If the <code>make</code> tool is available, you can use the features provided in <code>Makefile</code>:</p> <ul> <li><code>create-virtualenv</code></li> <li><code>init-local-env</code></li> </ul>"},{"location":"#generate-this-documentation-locally","title":"Generate this documentation locally","text":"<p>To generate this documentation locally first clone the gabarit repository : </p> <pre><code>git clone https://github.com/OSS-Pole-Emploi/gabarit.git\ncd gabarit\n</code></pre> <p>Then install mkdocs dependencies :</p> <pre><code>pip install \\\n'mkdocs&gt;=1.4,&lt;2' \\\n'mkdocs-gen-files&gt;=0.4,&lt;1' \\\n'mkdocs-literate-nav&gt;=0.6,&lt;1' \\\n'mkdocs-material&gt;=9.0,&lt;10' \\\n'mkdocs-section-index&gt;=0.3,&lt;1' \\\n'mkdocstrings[python]&gt;=0.8,&lt;1'\n</code></pre> <p>Finally serve the doc locally :  <pre><code>mkdocs serve\n</code></pre></p> <p>Note</p> <p>Generating package references can be long. You can locally disable package references generation by setting an environment variable : <code>export DOC_NO_REF=true</code></p>"},{"location":"#security-warning","title":"Security warning","text":"<p>Gabarit relies on a number of open source packages and therefore may carry on their potential security vulnerabilities. Our philosophy is to be as transparent as possible, which is why we are actively monitoring the dependabot analysis. In order to limit these vulnerabilities, we regularly upgrade these packages as soon as we can. Notice that some packages (namely torch and tensorflow) might lag a few versions behind the actual up to date version due to compatibility issues with CUDA and our own infrastructure.</p> <p>However, we remind you to be vigilant about the security vulnerabilities of the code and models that you will produce with these frameworks. It is your responsibility to ensure that the final product matches the security standards of your organization.</p>"},{"location":"#ethics","title":"Ethics","text":"<p>P\u00f4le emploi intends to include the development and use of artificial intelligence algorithms and solutions in a sustainable and ethical approach. As such, P\u00f4le emploi has adopted an ethical charter, resulting from collaborative and consultative work. The objective is to guarantee a framework of trust, respectful of the values of P\u00f4le emploi, and to minimize the risks associated with the deployment of these technologies.</p> <p>The pdf file is located in pole-emploi.org :</p> <p>PDF - Ethics charter - P\u00f4le emploi</p>"},{"location":"#contacts","title":"Contacts","text":"<p>If you have any question/enquiry feel free to drop us a mail :</p> <p> Contact</p> <p>Maintenance team :</p> <ul> <li>Alexandre GAREL - Data Scientist</li> <li>Nicolas GREFFARD - Data Scientist</li> <li>Gautier SOLARD - Data Scientist</li> <li>Nicolas TOUZOT - Product Owner</li> </ul>"},{"location":"/home/runner/work/gabarit/gabarit/docs/reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>template_api<ul> <li>application</li> <li>core<ul> <li>config</li> <li>event_handlers</li> <li>logtools</li> </ul> </li> <li>model<ul> <li>model_base</li> <li>model_gabarit</li> </ul> </li> <li>routers<ul> <li>functional</li> <li>schemas<ul> <li>functional</li> <li>technical</li> <li>utils</li> </ul> </li> <li>technical</li> </ul> </li> </ul> </li> <li>template_nlp<ul> <li>models_training<ul> <li>hf_metrics<ul> <li>accuracy</li> <li>f1</li> <li>precision</li> <li>recall</li> </ul> </li> <li>model_aggregation</li> <li>model_class</li> <li>model_huggingface</li> <li>models_sklearn<ul> <li>model_pipeline</li> <li>model_tfidf_gbt</li> <li>model_tfidf_lgbm</li> <li>model_tfidf_sgdc</li> <li>model_tfidf_svm</li> </ul> </li> <li>models_tensorflow<ul> <li>model_embedding_cnn</li> <li>model_embedding_lstm</li> <li>model_embedding_lstm_attention</li> <li>model_embedding_lstm_gru_gpu</li> <li>model_embedding_lstm_structured_attention</li> <li>model_keras</li> <li>model_tfidf_dense</li> <li>utils_deep_keras</li> </ul> </li> <li>utils_models</li> </ul> </li> <li>monitoring<ul> <li>mlflow_logger</li> <li>model_explainer</li> </ul> </li> <li>preprocessing<ul> <li>preprocess</li> </ul> </li> <li>utils</li> </ul> </li> <li>template_num<ul> <li>models_training<ul> <li>classifiers<ul> <li>model_aggregation_classifier</li> <li>model_classifier</li> <li>model_xgboost_classifier</li> <li>models_sklearn<ul> <li>model_gbt_classifier</li> <li>model_knn_classifier</li> <li>model_lgbm_classifier</li> <li>model_logistic_regression_classifier</li> <li>model_rf_classifier</li> <li>model_ridge_classifier</li> <li>model_sgd_classifier</li> <li>model_svm_classifier</li> </ul> </li> <li>models_tensorflow<ul> <li>model_dense_classifier</li> </ul> </li> </ul> </li> <li>model_class</li> <li>model_keras</li> <li>model_pipeline</li> <li>regressors<ul> <li>model_aggregation_regressor</li> <li>model_regressor</li> <li>model_xgboost_regressor</li> <li>models_sklearn<ul> <li>model_bayesian_ridge_regressor</li> <li>model_elasticnet_regressor</li> <li>model_gbt_regressor</li> <li>model_kernel_ridge_regressor</li> <li>model_knn_regressor</li> <li>model_lgbm_regressor</li> <li>model_pls_regressor</li> <li>model_rf_regressor</li> <li>model_sgd_regressor</li> <li>model_svr_regressor</li> </ul> </li> <li>models_tensorflow<ul> <li>model_dense_regressor</li> </ul> </li> </ul> </li> <li>utils_deep_keras</li> <li>utils_models</li> </ul> </li> <li>monitoring<ul> <li>mlflow_logger</li> <li>model_explainer</li> </ul> </li> <li>preprocessing<ul> <li>column_preprocessors</li> <li>outlier_detection</li> <li>preprocess</li> </ul> </li> <li>utils</li> </ul> </li> <li>template_vision<ul> <li>models_training<ul> <li>classifiers<ul> <li>model_classifier</li> <li>model_cnn_classifier</li> <li>model_transfer_learning_classifier</li> </ul> </li> <li>model_class</li> <li>model_keras</li> <li>object_detectors<ul> <li>model_detectron_faster_rcnn</li> <li>model_keras_faster_rcnn</li> <li>model_object_detector</li> <li>utils_faster_rcnn</li> <li>utils_object_detectors</li> </ul> </li> <li>utils_deep_keras</li> <li>utils_models</li> </ul> </li> <li>monitoring<ul> <li>mlflow_logger</li> <li>model_explainer</li> </ul> </li> <li>preprocessing<ul> <li>manage_white_borders</li> <li>preprocess</li> </ul> </li> <li>utils</li> </ul> </li> </ul>"},{"location":"frameworks/","title":"Frameworks","text":""},{"location":"frameworks/#general-principles","title":"General principles","text":""},{"location":"frameworks/#templates-structure","title":"Templates structure","text":"<ul> <li> <p>Data must be kept in a directory named <code>project_name-data/</code> located at the root folder of the project (i.e. where <code>setup.py</code> is).</p> </li> <li> <p>Any data mapping or lookup can be kept under <code>project_name-data/sources/</code>. Warning : we're talking small files (&lt; 1 Mo). Larger files should be managed through DVC (or git lfs for that matter).</p> </li> <li> <p>Embedding files or equivalent should also be kept under <code>project_name-data/</code>.</p> </li> <li> <p>Transformers models (e.g. Flaubert) should be kept under <code>project_name-transformers/</code> at the root directory of the project.</p> </li> <li> <p>Trained models that you built and trained are automatically saved under <code>project_name-models/</code>.</p> </li> <li> <p>Sklearn preprocessing pipelines (mainly from the numerical framework) are automatically stored within <code>project_name-pipelines/</code>.</p> </li> <li> <p>The Computer Vision template has some more subdirectories in the <code>project_name-data/</code> folder:</p> <ul> <li> <p><code>cache_keras</code>: subfolder that replaces the default keras' cache folder. Used with transfer learning classifiers.</p> </li> <li> <p><code>transfer_learning_weights</code>: subfolder that holds networks weights to be used with custom Faster RCNN implementation.</p> </li> <li> <p><code>detectron2_conf_files</code>: subfolder that holds all necessary configuration files to be used with the detectron2 models.</p> </li> </ul> </li> <li> <p>The <code>tests/</code> directory contains numerous unit tests allowing to automatically validate the intended behaviour of the different features. It is of utter importance to keep them up to date depending on your own developments to ensure that everything is working fine. Feel free to check already existing test files if you need some directions. Note that to launch a specific test case you just have to run : <code>python test_file.py</code>; for instance: <code>python tests/test_model_tfidf_dense.py</code>.</p> </li> <li> <p>Numbered files contained in <code>project_name-scripts/</code> (e.g. <code>2_training.py</code>) hint the main steps of the project. They are indicative but we strongly advise to use them as it can speed up the development steps. It orchestrates the main features of this project: utils functions, preprocessing pipelines and model classes.</p> </li> <li> <p>The <code>preprocess.py</code> file contains the different preprocessing pipeline available by default by the package/project. More specifically, it contains a dictionnary of the pipelines. It will be used to create working datasets (for instance training set, valid test and test set).</p> </li> <li> <p>Beware that the first row of each generated csv file after running a preprocessing will contain the name of the preprocessing pipeline applied such that it can be reused in the future. Hence, this row (e.g. <code>#preprocess_P1</code>) is a metadata and it has to be skipped while parsing the csv file. Our templates provide a function (<code>utils.read_csv</code>) that does it automatically (it also returns the metadata).</p> </li> <li> <p>The modelling part is built as follow :</p> <ul> <li> <p>ModelClass : main class that manages how data / models are saved and how performance metrics are computed</p> </li> <li> <p>ModelPipeline : inherits from ModelClass, manages sklearn pipeline models</p> </li> <li> <p>ModelKeras : inherits from ModelClass, manages Keras/Tensorflow models</p> </li> <li> <p>ModelPyTorch : inherits from ModelClass, manages PyTorch models</p> </li> <li> <p>ModelXXX : built-in implementation of standard models used in the industry, inherits from one of the above classes when appropriate</p> </li> </ul> </li> </ul>"},{"location":"frameworks/#main-steps-of-a-given-project","title":"Main steps of a given project","text":"<p>The intended flow of a project driven by one of these framework is the following:</p> <ul> <li> <p>0 \u2013 Utility files</p> <ul> <li>Split train/valid/test, sampling, embedding download, etc...</li> </ul> </li> <li> <p>1 \u2013 Preprocessing</p> </li> <li> <p>2 \u2013 Model training</p> <ul> <li>You can tune the parameters within the script or update the model class depending on your needs</li> </ul> </li> <li> <p>3 \u2013 Predictions on a dataset</p> </li> <li> <p>4 \u2013 Play with a streamlit demonstrator to showcase your models</p> </li> </ul>"},{"location":"frameworks/#data-formats","title":"Data formats","text":"<p>Input data are supposed to be <code>.csv</code> files and the separator and encoding are to be provided during the generation of the project. It is obviously possible to use another datatype but a transformation step to <code>.csv</code> will be required to use the scripts provided by default.</p> <p>Concerning the prediction target, please refer to <code>2_training.py</code>. Usually we expect One Hot Encoded format for multi-labels use cases. For single-label use cases, a single column (string for classification, float for regression) is expected.</p>"},{"location":"frameworks/#features","title":"Features","text":"<p>Projects generated through the frameworks provide several main features:</p>"},{"location":"frameworks/#model-saving-and-reloading","title":"Model saving and reloading","text":"<p>When a new model is instanciated, a directory is created within <code>project_name-models/</code>. It is named after the model type and its date of creation. Each model class exposes a <code>save</code> function that allow to save everything necessary to load it back:</p> <ul> <li>Configuration file</li> <li>Serialized object (.pkl)</li> <li>\"standalone\" model</li> <li>If Deep Learning : the network weights</li> <li>etc.</li> </ul> <p>Thus any model can be loaded through the <code>utils_models.load_model</code> function. The \"standalone\" mode ensures that the model can be loaded even after its code has been modified. Indeed, the .pkl file could be out of sync with the model class (it it was modified after the model had been saved). In this specific case, you can use <code>0_reload_model.py</code>.</p>"},{"location":"frameworks/#third-party-ai-modules","title":"Third party AI modules","text":"<p>To this day, 3 main AI modules are used:</p> <ul> <li> <p>Scikit Learn</p> </li> <li> <p>TensorFlow (Keras)</p> </li> <li> <p>PyTorch (PyTorch Lightning)</p> </li> </ul> <p>Do no hesitate to extend this list as is the case for LighGBM for instance.</p>"},{"location":"frameworks/#dvc","title":"DVC","text":"<p>A new project can automatically be set up to run in sync with DVC if you supply the necessary configuration during project generation. We strongly advise to use DVC or similar (git lfs could do the trick) to keep both your code and your datasets synchronized to be able to re-train a model in the same conditions sometime down the line. Please refrain to upload large datasets (&gt;1mo) directly on your version control system. Once setup, dvc configuration is available within <code>.dvc/</code></p>"},{"location":"frameworks/#mlflow","title":"MLFlow","text":"<p>A new project can automatically be set up to work alongside a MLFlow instance. If you supply a MLFlow host url during project generation, training metrics will be automatically be send to your MLFlow server. Refer to <code>2_training.py</code> and <code>monitoring/model_logger.py</code> for further informations about this mechanism.</p>"},{"location":"frameworks/#streamlit-demonstrator","title":"Streamlit demonstrator","text":"<p>A generic demonstrator is automatically created when you generate a new project with the frameworks. It relies on Streamlit to expose a handy front-end to showcase your work. The demonstrator script can be easily modified to fit your specific needs. </p>"},{"location":"frameworks/#exploratory-data-analysis-eda","title":"Exploratory Data Analysis (EDA)","text":"<p>Some frameworks provide a generic exploratory data analysis notebook to quickly grasp your datasets (<code>project_name-exploration/EDA/</code>). Feel free to have a go with it before starting heavy modelling work; EDA is an extraordinary opportunity to get to know your data which will greatly help you further down the line.</p>"},{"location":"frameworks/#misc","title":"Misc.","text":"<p>Some additionnal features :</p> <ul> <li>Basic hyper-parameter search is provided within <code>2_training.py</code></li> <li>You can use Tensorflow checkpoints to restart the training of a model without having to start from scratch</li> <li>A custom made Learning Rate Scheduler for Tensorflow is also provided</li> <li>Etc... feel free to explore the generated classes to learn more about what you can do !</li> </ul>"},{"location":"frameworks/#industrialization","title":"Industrialization","text":""},{"location":"frameworks/#principles","title":"Principles","text":"<p>Industrialization of a project generated from one of the framework roughly follows the same pattern. Once you have trained a model which is a release candidate :</p> <ul> <li> <p>Push the actual serialized model to your artifact repository (for instance artifactory or nexus)</p> <ul> <li>Instructions about how to technically push the model are usually specified within the model directory</li> </ul> </li> <li> <p>Push the python module (the project you generated with a framework) to your artifact repository (it could be pypi or any system able to host a python repository)</p> <ul> <li> <p>First you have to build a wheel of the project <code>.whl</code> : <code>python setup.py sdist bdist_wheel</code></p> </li> <li> <p>Then you have to push it to your repository, for instance by using twine : <code>twine upload --username {USER} --password {PWD} --repository-url https://{repository_url} dist/*.whl</code></p> </li> </ul> <p>Note</p> <p>we strongly advise to embed these steps within a Continuous Integration Pipeline and ensuring that all your unit tests are OK (you can use nose to run your test suite : <code>pip install nose nose-cov &amp;&amp; nosetests tests/</code>)</p> <ul> <li> <p>Beware, function <code>utils_models.predict</code> has to be adapted to your project needs (e.g. if some specific computations are required before or after the actual inference).</p> <ul> <li>This is the function that has to be called by the web service that will serve your model. Using <code>utils_models.predict</code> instead of the actual predict method of the model class ensure that your service can stay model agnostic: if one day you decide to change your design, to use another model; the service won't be impacted.</li> </ul> </li> </ul> <p>Warning</p> <p>some libraries (such as torch, detectron2, etc.) may not be hosted on PyPI. You'll need to add an extra <code>--find-links</code> option to your pip installation.</p> <p>If you don't have access to the internet, you'll need to setup a proxy which will host all the needed libraries. You can then use <code>--trusted-host</code> and <code>--index-url</code> options.</p> </li> <li> <p>You can use our API Framework to expose your model, see API section</p> </li> </ul>"},{"location":"frameworks/#update-your-model","title":"Update your model","text":"<p>If you want to update the model exposed by the API, you just have to push a new version of the serialized model to your repository and update your service (typically only the model version). If the actual code base of the model (for instance in the predict method) was updated, you would also have to publish a new version of the python module.  </p>"},{"location":"frameworks/#unit-tests","title":"Unit tests","text":"<p>Numerous unit tests are provided by the framework. Don't forget to adapt them when you modify the code. If you wish to add features, it is obviously advised to add new unit tests.</p>"},{"location":"frameworks/#misc_1","title":"Misc.","text":"<ul> <li>To this day, each framework is tested and integrated on our own continuous integration pipeline.</li> <li>If a GPU is available, some models will automatically try to use it during training and inference</li> </ul>"},{"location":"frameworks/#update-a-project-with-the-latest-gabarit-version","title":"Update a project with the latest Gabarit version","text":"<p>It can be tricky to update a project to a newer version of Gabarit as you probably made changes into the code and don't want them to be removed. As our philosophy is to give you code and let you adapt it for your specific usage, we can't control everything.  </p> <p>However, we still provide an operating procedure that must keep your changes while updating the project to the latest Gabarit version :</p> <ol> <li> <p>Create a new branch from your latest commit C0</p> </li> <li> <p>Find the Gabarit's version last used to generate your project</p> </li> <li> <p>Generate a project ON TOP of your code using this version</p> <ul> <li>Commit the changes (commit C1)</li> </ul> </li> <li> <p>Create a patch : <code>git diff HEAD XXXXXX &gt; local_diff.patch</code> where XXXXXX is the SHA-1 of the latest commit C0</p> <ul> <li>This patch holds every changes you made since you last generated the project, except for new files</li> <li>Note that we don't really care for new files as they are not removed with Gabarit new generation</li> </ul> </li> <li> <p>Generate a project ON TOP of your code, but this time with the latest Gabarit version. Commit the changes (commit C2).</p> <ul> <li>The <code>.gitignore</code> file might change, be careful NOT TO COMMIT files that are \"unignored\".</li> </ul> </li> <li> <p>Apply the patch : <code>git am -3 &lt; local_diff.patch</code></p> <p>RENAMED / MOVED / DELETED FILES</p> <p>this won't work for renamed / moved / deleted files :</p> <ul> <li>You'll have to manage them manually</li> <li>You need to remove files that are no longer in the new Gabarit version BEFORE applying the patch.</li> <li>The patch will then probably crash. You will have to fix it manually.</li> </ul> <ol> <li>You will probably have conflict, resolve them</li> <li>Add files and commit changes (commit C3)</li> <li>You might need to run <code>git am --skip</code> as we only had a single patch to apply</li> </ol> </li> <li> <p>Squash the last commits (you should have 3 commits)</p> <ul> <li><code>git reset --soft HEAD~3</code></li> <li><code>git commit -m \"my_message\"</code></li> </ul> </li> <li> <p>CHECK IF EVERYTHING SEEMS OK</p> </li> <li> <p>Merge your branch &amp; push :) </p> </li> </ol> <p>Be aware that some of your defined functions might need to be updated as the newer Gabarit version might have some breaking changes.</p> <p></p>"},{"location":"frameworks/API/","title":"API Framework","text":""},{"location":"frameworks/API/#project-structure","title":"Project structure","text":"<p>Here is the structure of a project generated with <code>generate_api_project</code> command : </p> <pre><code>.\n\u251c\u2500 template_api                 # your application package\n\u2502   \u251c\u2500 core                     # global config and utilities\n\u2502   \u2502    \u251c\u2500 __init__.py\n\u2502   \u2502    \u251c\u2500 config.py\n\u2502   \u2502    \u251c\u2500 event_handlers.py   # load your model to your app at startup\n\u2502   \u2502    \u2514\u2500 logtools.py\n\u2502   \u2502\n\u2502   \u251c\u2500 model                    # model classes\n\u2502   \u2502    \u251c\u2500 __init__.py\n\u2502   \u2502    \u251c\u2500 model_base.py\n\u2502   \u2502    \u2514\u2500 model_gabarit.py\n\u2502   \u2502\n\u2502   \u251c\u2500 routers                  # applications routes\n\u2502   \u2502    \u251c\u2500 schemas\n\u2502   \u2502    \u251c\u2500 __init__.py\n\u2502   \u2502    \u251c\u2500 functional.py\n\u2502   \u2502    \u251c\u2500 technical.py\n\u2502   \u2502    \u2514\u2500 utils.py\n\u2502   \u2502\n\u2502   \u251c\u2500 __init__.py\n\u2502   \u2514\u2500 application.py\n\u2502\n\u251c\u2500 tests\n\u2502   \u2514\u2500 ...\n.\n.\n.\n\u251c\u2500 .env                         # environement variables for settings\n\u251c\u2500 makefile\n\u251c\u2500 Dockerfile.svc\n\u251c\u2500 pyproject.toml               # your package dependencies and infos\n\u251c\u2500 setup.py\n\u251c\u2500 launch.sh                    # start your application\n\u2514\u2500 README.md\n</code></pre>"},{"location":"frameworks/API/#quickstart","title":"Quickstart","text":"<p>Gabarit has generated a <code>template_api</code> python package that contains all your FastAPI application logic.</p> <p>It contains three main sub-packages (cf. project structure) :</p> <ul> <li> <p><code>core</code> package for configuration and loading your model into your application</p> </li> <li> <p><code>model</code> package for defining how to download your model, to load it and make predictions</p> </li> <li> <p><code>routers</code> package for defining your API routes and how they work</p> </li> </ul> <p>Have a look at your <code>.env</code> file to see the default settings : <pre><code>APP_NAME=\"template_api\"\nAPI_ENTRYPOINT=\"/template_api/rs/v1\"\nMODEL_PATH=\"template_api-models/model.pkl\"\n</code></pre></p>"},{"location":"frameworks/API/#create-a-virtualenv-and-install-your-package","title":"Create a virtualenv and install your package","text":"<p>With make : <pre><code>make run create-virtualenv\nsource .venv/bin/activate\n\nmake init-local-env\n</code></pre></p> <p>Without make : <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install -e .[dev]\n</code></pre></p>"},{"location":"frameworks/API/#start-your-application","title":"Start your application","text":"<p>To start your FastAPI application activate your virtual environment and then use the script <code>launch.sh</code> or the <code>run</code> command of the makefile :</p> <pre><code>chmod +x launch.sh\nmake run\n</code></pre> <p>This will start a FastAPI thanks to uvicorn that listen on port 5000. Visit http://localhost:5000/docs to see the automatic interactive API documentation (provided by FastAPI and Swagger UI)</p>"},{"location":"frameworks/API/#how-it-works","title":"How it works","text":""},{"location":"frameworks/API/#model-class","title":"Model class","text":"<p>Your application use a <code>Model</code> object to make predictions. You will find a base <code>Model</code> class in <code>template_api.model.model_base</code> :</p> <pre><code>class Model:\ndef __init__(self):\nself._model: Any = None\nself._model_conf: dict = None\nself._model_explainer = None\nself._loaded: bool = False\ndef is_model_loaded(self) -&gt; bool:\n\"\"\"return the state of the model\"\"\"\nreturn self._loaded\ndef loading(self, **kwargs):\n\"\"\"load the model\"\"\"\nself._model, self._model_conf = self._load_model(**kwargs)\nself._loaded = True\ndef predict(self, *args, **kwargs) -&gt; Any:\n\"\"\"Make a prediction thanks to the model\"\"\"\nreturn self._model.predict(*args, **kwargs)\ndef explain_as_json(self, *args, **kwargs) -&gt; Union[dict, list]:\n\"\"\"Compute explanations about a prediction and return a JSON serializable object\"\"\"\nreturn self._model_explainer.explain_instance_as_json(*args, **kwargs)\ndef explain_as_html(self, *args, **kwargs) -&gt; str:\n\"\"\"Compute explanations about a prediction and return an HTML report\"\"\"\nreturn self._model_explainer.explain_instance_as_html(*args, **kwargs)\ndef _load_model(self, **kwargs) -&gt; Tuple[Any, dict]:\n\"\"\"Load a model from a file\n        Returns:\n            Tuple[Any, dict]: A tuple containing the model and a dict of metadata about it.\n        \"\"\"\n...\n@staticmethod\ndef download_model(**kwargs) -&gt; bool:\n\"\"\"You should implement a download method to automatically download your model\"\"\"\n...\n</code></pre> <p>As you can see, a <code>Model</code> object has four main attributes :</p> <ul> <li> <p><code>_model</code> containing your gabarit, scikit-learn or whatever model object</p> </li> <li> <p><code>_model_conf</code> which is a python dict with metadata about your model</p> </li> <li> <p><code>_model_explainer</code> containing your model explainer</p> </li> <li> <p><code>_loaded</code> which is set to <code>True</code> after <code>_load_model</code> has been called</p> </li> </ul> <p>The <code>Model</code> class also define a <code>download_model</code> method that will be used to download your model. By default it does nothing and returns <code>True</code>.</p> <p>You will also find a <code>ModelGabarit</code> class in <code>template_api.model.model_gabarit</code> that is suited to a model constructed thanks to a Gabarit template.</p> <p>It is a great example of how to adapt the base <code>Model</code> class to your use case.</p>"},{"location":"frameworks/API/#load-your-model-at-startup","title":"Load your model at startup","text":"<p>Your model is loaded into your application at startup thanks to <code>template_api.core.event_handlers</code> :</p> <pre><code>from typing import Callable\nfrom fastapi import FastAPI\nfrom ..model.model_base import Model\ndef _startup_model(app: FastAPI) -&gt; None:\n\"\"\"Create and Load model\"\"\"\nmodel = Model()\nmodel.loading()\napp.state.model = model\ndef start_app_handler(app: FastAPI) -&gt; Callable:\n\"\"\"Startup handler: invoke init actions\"\"\"\ndef startup() -&gt; None:\nlogger.info(\"Startup Handler: Load model.\")\n_startup_model(app)\nreturn startup\n</code></pre> <p>To change the model used by your application, change the model imported here.</p>"},{"location":"frameworks/API/#functional-and-technical-routers","title":"Functional and technical routers","text":"<p>Routers are split into two categories by default : technical and functional ones.</p> <ul> <li>Technical routers are used for technical purpose such as verifying liveness or getting   infos about your application</li> <li>Functional ones are used to implement your business logic such as model predictions   or model explicability</li> </ul> <p>Since gabarit could not know what data your model is expecting, the default <code>/predict</code> route from <code>template_api.routers.functional</code> use a starlette Request object instead of pydantic.</p> <p>For a cleaner way to handle requests and reponses you should use pydantic as stated in FastAPI documentation</p> <p>You can use routes from template_api.routers.technical as examples of how to create requests and responses schemas thanks to pydantic or have a look at the FastAPI documentation.</p>"},{"location":"frameworks/API/#dockerfile","title":"Dockerfile","text":"<p>A minimal <code>Dockerfile.svc</code> is provided by the template. You should have a look a it, especially if you have to download your model in your containers.</p>"},{"location":"frameworks/NLP/","title":"NLP Framework","text":""},{"location":"frameworks/NLP/#project-structure","title":"Project structure","text":"<p>Here is the structure of a project generated with <code>generate_nlp_project</code> command : </p> <pre><code>.\n\u251c\u2500 template_nlp                 # your application package\n\u2502    \u251c\u2500 models_training         # global config and utilities\n\u2502    \u2502    \u251c\u2500 models_sklearn     # package containing some predefined scikit-learn models\n\u2502    \u2502    \u251c\u2500 models_tensorflow  # package containing some predefined tensorflow models\n\u2502    \u2502    \u251c\u2500 model_class.py     # module containing Model base class\n\u2502    \u2502    \u251c\u2500 ...\n\u2502    \u2502    \u2514\u2500 utils_models.py    # module containing utility functions\n\u2502    \u2502\n\u2502    \u251c\u2500 monitoring              # package containing monitoring utilities (mlflow, model explicability)\n\u2502    \u2502\n\u2502    \u251c\u2500 preprocessing           # package containing preprocessing logic\n\u2502    \u2502\n\u2502    \u251c\u2500 __init__.py\n\u2502    \u2514\u2500 utils.py\n\u2502 \u251c\u2500 template_nlp-data            # Folder where to store your data\n\u251c\u2500 template_nlp-exploration     # Folder where to store your exploratory notebooks\n\u251c\u2500 template_nlp-models          # Folder containing trained models\n\u251c\u2500 template_nlp-scripts         # Folder containing script for preprocessing, training, etc.\n\u251c\u2500 template_nlp-tutorials       # Folder containing a tutorial notebook\n.\n.\n.\n\u251c\u2500 makefile\n\u251c\u2500 setup.py\n\u2514\u2500 README.md\n</code></pre> <p>Warning</p> <p>If you used a custom preprocessing function <code>funcA</code> with <code>FunctionTransformer</code>, be aware that the pickled pipeline  may not return wanted results if you later modify <code>funcA</code> definition. </p> <p>Please check gabarit/issues/63</p>"},{"location":"frameworks/NUM/","title":"NUM Framework","text":""},{"location":"frameworks/NUM/#project-structure","title":"Project structure","text":"<p>Here is the structure of a project generated with <code>generate_num_project</code> command : </p> <pre><code>.\n\u251c\u2500 template_num                       # your application package\n\u2502    \u251c\u2500 models_training               # global config and utilities\n\u2502    \u2502    \u251c\u2500 classifiers\n\u2502    \u2502    \u2502    \u251c\u2500 models_sklearn      # package containing some predefined scikit-learn classifiers\n\u2502    \u2502    \u2502    \u2514\u2500 models_tensorflow   # package containing some predefined tensorflow classifiers\n\u2502    \u2502    \u251c\u2500 regressors      \u2502    \u2502    \u2502    \u251c\u2500 models_sklearn      # package containing some predefined scikit-learn regressors\n\u2502    \u2502    \u2502    \u2514\u2500 models_tensorflow   # package containing some predefined tensorflow regressors\n\u2502    \u2502    \u251c\u2500 ...\n\u2502    \u2502    \u251c\u2500 model_class.py           # module containing base Model class\n\u2502    \u2502    \u2514\u2500 utils_models.py          # module containing utility functions\n\u2502    \u2502\n\u2502    \u251c\u2500 monitoring                    # package containing monitoring utilities (mlflow, model explicability)\n\u2502    \u2502\n\u2502    \u251c\u2500 preprocessing                 # package containing preprocessing logic\n\u2502    \u2502\n\u2502    \u251c\u2500 __init__.py\n\u2502    \u2514\u2500 utils.py\n\u2502\n\u251c\u2500 template_num-data                  # Folder where to store your data\n\u251c\u2500 template_num-exploration           # Folder where to store your exploratory notebooks\n\u251c\u2500 template_num-models                # Folder containing trained models\n\u251c\u2500 template_num-pipelines             # Folder containing fitted pipelines are stored\n\u251c\u2500 template_num-scripts               # Folder containing script for preprocessing, training, etc.\n\u251c\u2500 template_num-tutorials             # Folder containing a tutorial notebook\n.\n.\n.\n\u251c\u2500 makefile\n\u251c\u2500 setup.py\n\u2514\u2500 README.md\n</code></pre>"},{"location":"frameworks/NUM/#numeric-framewrok-specificities","title":"Numeric framewrok specificities","text":"<ul> <li> <p>Preprocessing has to be computed in a two step fashion to avoid bias:</p> </li> <li> <p>Fit your transformations on the training data (<code>1_preprocess_data.py</code>)</p> </li> <li> <p>Transform your validation/test sets (<code>2_apply_existing_pipeline.py</code>)</p> </li> <li> <p>Preprocessing pipelines are stored in the <code>project_name-pipelines</code> folder</p> </li> <li> <p>They are then stored as a .pkl object in the model folders (so that these can be used during inference)</p> </li> </ul> <p>Warning</p> <p>If you used a custom preprocessing function <code>funcA</code> with <code>FunctionTransformer</code>, be aware that the pickled pipeline  may not return wanted results if you later modify <code>funcA</code> definition. </p> <p>Please check gabarit/issues/63</p>"},{"location":"frameworks/VISION/","title":"VISION Framework","text":""},{"location":"frameworks/VISION/#project-structure","title":"Project structure","text":"<p>Here is the structure of a project generated with <code>generate_vision_project</code> command : </p> <pre><code>.\n\u251c\u2500 template_vision              # your application package\n\u2502    \u251c\u2500 models_training         # global config and utilities\n\u2502    \u2502    \u2514\u2500 classifiers        # package containing some predefined classifiers\n\u2502    \u2502    \u251c\u2500 object_detectors   # package containing some predefined object detectors\n\u2502    \u2502    \u251c\u2500 ...\n\u2502    \u2502    \u251c\u2500 model_class.py     # module containing base Model class\n\u2502    \u2502    \u2514\u2500 utils_models.py    # module containing utility functions\n\u2502    \u2502\n\u2502    \u251c\u2500 monitoring              # package containing monitoring utilities (mlflow, model explicability)\n\u2502    \u2502\n\u2502    \u251c\u2500 preprocessing           # package containing preprocessing logic\n\u2502    \u2502\n\u2502    \u251c\u2500 __init__.py\n\u2502    \u2514\u2500 utils.py\n\u2502\n\u251c\u2500 template_vision-data         # Folder where to store your data\n\u251c\u2500 template_vision-exploration  # Folder where to store your exploratory notebooks\n\u251c\u2500 template_vision-models       # Folder containing trained models\n\u251c\u2500 template_vision-scripts      # Folder containing script for preprocessing, training, etc.\n\u251c\u2500 template_vision-tutorials    # Folder containing a tutorial notebook\n.\n.\n.\n\u251c\u2500 makefile\n\u251c\u2500 setup.py\n\u2514\u2500 README.md\n</code></pre>"},{"location":"frameworks/VISION/#computer-vision-framewrok-specificities","title":"Computer vision framewrok specificities","text":"<ul> <li> <p>The expected input data format is different than in the other frameworks.</p> </li> <li> <p>For image classification, 3 differents formats can be used :</p> <ol> <li>A root folder with a subfolder per class (containing all the images associated with this class)</li> <li>A unique folder containing every image where each image name is prefixed with its class</li> <li>A folder containing all the images and a .csv metadata file containing the image/class matching</li> </ol> </li> <li> <p>For object detection, you must provide a .csv metadata file containing the bounding boxes for each image</p> </li> </ul>"},{"location":"reference/template_api/","title":"Template api","text":""},{"location":"reference/template_api/application/","title":"Application","text":""},{"location":"reference/template_api/application/#template_api.application.declare_application","title":"<code>declare_application()</code>","text":"<p>Create the FastAPI application</p> <p>See https://fastapi.tiangolo.com/tutorial/first-steps/ to learn how to customize your FastAPI application</p> Source code in <code>template_api/application.py</code> <pre><code>def declare_application() -&gt; FastAPI:\n\"\"\"Create the FastAPI application\n    See https://fastapi.tiangolo.com/tutorial/first-steps/ to learn how to\n    customize your FastAPI application\n    \"\"\"\napp = FastAPI(\ntitle=f\"REST API form {settings.app_name}\",\ndescription=f\"Use {settings.app_name} thanks to FastAPI\",\n)\n# Load the model on startup\napp.add_event_handler(\"startup\", start_app_handler(app))\napp.add_event_handler(\"shutdown\", stop_app_handler(app))\n# Add PrometheusMiddleware\napp.add_middleware(PrometheusMiddleware)\napp.add_route(\"/metrics\", metrics)\n# CORS middleware that allows all origins to avoid CORS problems\n# see https://fastapi.tiangolo.com/tutorial/cors/#use-corsmiddleware\napp.add_middleware(\nCORSMiddleware,\nallow_origins=[\"*\"],\nallow_credentials=True,\nallow_methods=[\"*\"],\nallow_headers=[\"*\"],\n)\n#\napp.include_router(main_routeur, prefix=settings.api_entrypoint)\nreturn app\n</code></pre>"},{"location":"reference/template_api/core/","title":"Core","text":"<p>Core package contains global configurations and utilities</p>"},{"location":"reference/template_api/core/config/","title":"Config","text":"<p>Config global settings</p> <p>This module handle global app configuration</p>"},{"location":"reference/template_api/core/event_handlers/","title":"Event handlers","text":"<p>Startup and Stop handlers for FastAPI application</p> <p>This module define event handlers and n particular startup and stop handlers that are used to instantiate your model when the API first start.</p> <p>To use your own model instead of the base model, create a module in template_api.model such as model_awesome.py and import it as Model instead of the one used here.</p>"},{"location":"reference/template_api/core/event_handlers/#template_api.core.event_handlers.start_app_handler","title":"<code>start_app_handler(app)</code>","text":"<p>Startup handler: invoke init actions</p> Source code in <code>template_api/core/event_handlers.py</code> <pre><code>def start_app_handler(app: FastAPI) -&gt; Callable:\n\"\"\"Startup handler: invoke init actions\"\"\"\ndef startup() -&gt; None:\nlogger.info(\"Startup Handler: Load model.\")\n_startup_model(app)\nreturn startup\n</code></pre>"},{"location":"reference/template_api/core/event_handlers/#template_api.core.event_handlers.stop_app_handler","title":"<code>stop_app_handler(app)</code>","text":"<p>Stop handler: invoke shutdown actions</p> Source code in <code>template_api/core/event_handlers.py</code> <pre><code>def stop_app_handler(app: FastAPI) -&gt; Callable:\n\"\"\"Stop handler: invoke shutdown actions\"\"\"\ndef shutdown() -&gt; None:\nlogger.info(\"Shutdown handler : Clean model.\")\n_shutdown_model(app)\nreturn shutdown\n</code></pre>"},{"location":"reference/template_api/core/logtools/","title":"Logtools","text":"<p>Logs utilities</p> <p>This module is used to define log pattern, add log filters, etc.</p>"},{"location":"reference/template_api/model/","title":"Model","text":"<p>Model package contain model-related logic</p>"},{"location":"reference/template_api/model/model_base/","title":"Model base","text":"<p>This module contains the base Model class</p> <p>Model is the base model class. It contains a loading and downloading methods that are used by default to download your model into your Docker container and load it into your application.</p> <p>To use a custom model class in your application, create a new module such as model_awesome.py in this package and write a custom class that overwrite _load_model, download_model or predict depending on your needs.</p>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model","title":"<code>Model</code>","text":"<p>Parent model class.</p> <p>This class is given as an exemple, you should probably adapt it to your project. This class loads the model from a .pkl file. The model must have a predict function.</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>class Model:\n\"\"\"Parent model class.\n    This class is given as an exemple, you should probably adapt it to your project.\n    This class loads the model from a .pkl file. The model must have a predict function.\n    \"\"\"\ndef __init__(self):\n'''Init. model class'''\nself._model = None\nself._model_conf = None\nself._model_explainer = None\nself._loaded = False\ndef is_model_loaded(self):\n\"\"\"return the state of the model\"\"\"\nreturn self._loaded\ndef loading(self, **kwargs):\n\"\"\"load the model\"\"\"\nself._load_model(**kwargs)\nself._loaded = True\ndef predict(self, *args, **kwargs):\n\"\"\"Make a prediction thanks to the model\"\"\"\nreturn self._model.predict(*args, **kwargs)\ndef explain_as_json(self, *args, **kwargs) -&gt; Union[dict, list]:\n\"\"\"Compute explanations about a prediction and return a JSON serializable object\"\"\"\nreturn self._model_explainer.explain_instance_as_json(*args, **kwargs)\ndef explain_as_html(self, *args, **kwargs) -&gt; str:\n\"\"\"Compute explanations about a prediction and return an HTML report\"\"\"\nreturn self._model_explainer.explain_instance_as_html(*args, **kwargs)\ndef _load_model(self, **kwargs) -&gt; None:\n\"\"\"Load a model from a file\n        Returns:\n            Tuple[Any, dict]: A tuple containing the model and a dict of metadata about it.\n        \"\"\"\nsettings = ModelSettings(**kwargs)\nlogger.info(f\"Loading the model from {settings.model_path}\")\nwith settings.model_path.open(\"rb\") as f:\nself._model = pickle.load(f)\nself._model_conf = {\n\"model_path\": settings.model_path.name,\n\"model_name\": settings.model_path.stem,\n}\nlogger.info(f\"Model loaded\")\n@staticmethod\ndef download_model(**kwargs) -&gt; bool:\n\"\"\"You should implement a download method to automatically download your model\"\"\"\nlogger.info(\"The function download_model is empty. Implement it to automatically download your model.\")\nreturn True\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.__init__","title":"<code>__init__()</code>","text":"<p>Init. model class</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def __init__(self):\n'''Init. model class'''\nself._model = None\nself._model_conf = None\nself._model_explainer = None\nself._loaded = False\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.download_model","title":"<code>download_model(**kwargs)</code>  <code>staticmethod</code>","text":"<p>You should implement a download method to automatically download your model</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>@staticmethod\ndef download_model(**kwargs) -&gt; bool:\n\"\"\"You should implement a download method to automatically download your model\"\"\"\nlogger.info(\"The function download_model is empty. Implement it to automatically download your model.\")\nreturn True\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.explain_as_html","title":"<code>explain_as_html(*args, **kwargs)</code>","text":"<p>Compute explanations about a prediction and return an HTML report</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def explain_as_html(self, *args, **kwargs) -&gt; str:\n\"\"\"Compute explanations about a prediction and return an HTML report\"\"\"\nreturn self._model_explainer.explain_instance_as_html(*args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.explain_as_json","title":"<code>explain_as_json(*args, **kwargs)</code>","text":"<p>Compute explanations about a prediction and return a JSON serializable object</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def explain_as_json(self, *args, **kwargs) -&gt; Union[dict, list]:\n\"\"\"Compute explanations about a prediction and return a JSON serializable object\"\"\"\nreturn self._model_explainer.explain_instance_as_json(*args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.is_model_loaded","title":"<code>is_model_loaded()</code>","text":"<p>return the state of the model</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def is_model_loaded(self):\n\"\"\"return the state of the model\"\"\"\nreturn self._loaded\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.loading","title":"<code>loading(**kwargs)</code>","text":"<p>load the model</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def loading(self, **kwargs):\n\"\"\"load the model\"\"\"\nself._load_model(**kwargs)\nself._loaded = True\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.Model.predict","title":"<code>predict(*args, **kwargs)</code>","text":"<p>Make a prediction thanks to the model</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>def predict(self, *args, **kwargs):\n\"\"\"Make a prediction thanks to the model\"\"\"\nreturn self._model.predict(*args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_base/#template_api.model.model_base.ModelSettings","title":"<code>ModelSettings</code>","text":"<p>         Bases: <code>BaseSettings</code></p> <p>Download settings</p> <p>This class is used for settings management purpose, have a look at the pydantic documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/</p> <p>By default, it looks for environment variables (case insensitive) to set the settings if a variable is not found, it looks for a file name .env in your working directory where you can declare the values of the variables and finally it sets the values to the default ones you can see above.</p> Source code in <code>template_api/model/model_base.py</code> <pre><code>class ModelSettings(BaseSettings):\n\"\"\"Download settings\n    This class is used for settings management purpose, have a look at the pydantic\n    documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/\n    By default, it looks for environment variables (case insensitive) to set the settings\n    if a variable is not found, it looks for a file name .env in your working directory\n    where you can declare the values of the variables and finally it sets the values\n    to the default ones you can see above.\n    \"\"\"\nmodel_path: Path = DEFAULT_MODEL_PATH\nclass Config:\nenv_file = \".env\"\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/","title":"Model gabarit","text":"<p>This module contains a ModelGabarit class you can use for your gabarit generated projects</p> ModelGabarit overwrite some methods of the base Model class <ul> <li>download_model method to download a model from a JFrog Artifactory repository ;</li> <li>_load_model method to use the gabarit_package.models_training.utils_models.load_model function from a typical gabarit project ;</li> <li>predict method to use the the gabarit_package.models_training.utils_models.predict function from a typical gabarit project.</li> </ul>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit","title":"<code>ModelGabarit</code>","text":"<p>         Bases: <code>Model</code></p> <p>Model class for a Gabarit generated project</p> <ul> <li>download_model has been redefined to download a model from artifactory based on the settings : ARTIFACTORY_MODEL_URL, ARTIFACTORY_USER, ARTIFACTORY_PASSWORD</li> <li>_load_model has been redefined to use utils_models.load_model</li> <li>predict has been redefined to use utils_models.predict</li> </ul> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>class ModelGabarit(Model):\n\"\"\"Model class for a Gabarit generated project\n    - download_model has been redefined to download a model from artifactory based on\n    the settings : ARTIFACTORY_MODEL_URL, ARTIFACTORY_USER, ARTIFACTORY_PASSWORD\n    - _load_model has been redefined to use utils_models.load_model\n    - predict has been redefined to use utils_models.predict\n    \"\"\"\ndef __init__(self, *args, **kwargs):\n\"\"\"Object initialization\n        By default, it initialize the attributes _model, _model_config and _loaded\n        see the parent __init__ method in template_api.model.model_base.Model\n        \"\"\"\nsuper().__init__(*args, **kwargs)\ndef predict(self, content: Any, *args, **kwargs) -&gt; Any:\n\"\"\"Make a prediction by calling utils_models.predict with the loaded model\"\"\"\nif isinstance(content, list) or isinstance(content, dict):\ncontent = pd.DataFrame(content)\nreturn utils_models.predict(content, model=self._model, model_conf=self._model_conf, **kwargs)\ndef explain_as_json(self, content: Any, *args, **kwargs) -&gt; Union[dict, list]:\n\"\"\"Compute explanations about a prediction and return a JSON serializable object\"\"\"\nif isinstance(content, list) or isinstance(content, dict):\ncontent = pd.DataFrame(content)\nreturn self._model_explainer.explain_instance_as_json(content, *args, **kwargs)\ndef explain_as_html(self, content: Any, *args, **kwargs) -&gt; str:\n\"\"\"Compute explanations about a prediction and return an HTML report\"\"\"\nif isinstance(content, list) or isinstance(content, dict):\ncontent = pd.DataFrame(content)\nreturn self._model_explainer.explain_instance_as_html(content, *args, **kwargs)\ndef _load_model(self, **kwargs) -&gt; None:\n\"\"\"Load a model in a gabarit fashion\"\"\"\nsettings = ModelSettings(**kwargs)\n# Replace get_data_path method from gabarit.utils to use template_api data directory\nif hasattr(utils_gabarit, \"get_data_path\"):\nutils_gabarit.get_data_path = lambda: str(settings.data_dir.resolve())\n# Using is_path=True allow to specify a path instead of a folder relative\n# to gabarit_package.utils.DIR_PATH\nmodel, model_conf = utils_models.load_model(model_dir=settings.model_path, is_path=True)\n# Set attributes\nself._model = model\nself._model_conf = model_conf\n# Create a model explainer\nself._model_explainer = Explainer(model=model, model_conf=model_conf)\n@staticmethod\ndef download_model(**kwargs) -&gt; bool:\n\"\"\"Download the model from an JFrog Artifactory repository\"\"\"\nsettings = ModelSettings(**kwargs)\nmodel_path = settings.model_path\n# If the model already exists there is no need to download it\nif not settings.redownload and model_path.is_dir() and not any(model_path.iterdir()):\nlogger.info(f\"The model is already dowloaded : {model_path} already exists\")\nreturn True\n# Create models directory if it doesn not exists\nmodels_dir = settings.models_dir\nmodels_dir.mkdir(parents=True, exist_ok=True)\n# Download model from artifactory\ntry:\nfrom artifactory import ArtifactoryPath\nexcept ImportError:\nraise ImportError(\"Module artifactory not found. Please install it : `pip install dohq-artifactory`\")\nmodel_artifactory_path = ArtifactoryPath(\nsettings.artifactory_model_url,\nauth=(settings.artifactory_user, settings.artifactory_password),\nverify=False,\n)\nwith tempfile.TemporaryDirectory(dir=models_dir) as tmpdir:\nmodel_archive_path = Path(tmpdir) / model_artifactory_path.name\n# Download model\nlogger.info(f\"Downloading the model to : {model_path}\")\nwith model_archive_path.open(\"wb\") as out:\nmodel_artifactory_path.writeto(out)\n# Unzip model\nshutil.unpack_archive(model_archive_path, model_path)\nlogger.info(f\"Model downloaded\")\nlogger.info(f\"Model archive removed\")\nreturn True\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Object initialization By default, it initialize the attributes _model, _model_config and _loaded</p> <p>see the parent init method in template_api.model.model_base.Model</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>def __init__(self, *args, **kwargs):\n\"\"\"Object initialization\n    By default, it initialize the attributes _model, _model_config and _loaded\n    see the parent __init__ method in template_api.model.model_base.Model\n    \"\"\"\nsuper().__init__(*args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit.download_model","title":"<code>download_model(**kwargs)</code>  <code>staticmethod</code>","text":"<p>Download the model from an JFrog Artifactory repository</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>@staticmethod\ndef download_model(**kwargs) -&gt; bool:\n\"\"\"Download the model from an JFrog Artifactory repository\"\"\"\nsettings = ModelSettings(**kwargs)\nmodel_path = settings.model_path\n# If the model already exists there is no need to download it\nif not settings.redownload and model_path.is_dir() and not any(model_path.iterdir()):\nlogger.info(f\"The model is already dowloaded : {model_path} already exists\")\nreturn True\n# Create models directory if it doesn not exists\nmodels_dir = settings.models_dir\nmodels_dir.mkdir(parents=True, exist_ok=True)\n# Download model from artifactory\ntry:\nfrom artifactory import ArtifactoryPath\nexcept ImportError:\nraise ImportError(\"Module artifactory not found. Please install it : `pip install dohq-artifactory`\")\nmodel_artifactory_path = ArtifactoryPath(\nsettings.artifactory_model_url,\nauth=(settings.artifactory_user, settings.artifactory_password),\nverify=False,\n)\nwith tempfile.TemporaryDirectory(dir=models_dir) as tmpdir:\nmodel_archive_path = Path(tmpdir) / model_artifactory_path.name\n# Download model\nlogger.info(f\"Downloading the model to : {model_path}\")\nwith model_archive_path.open(\"wb\") as out:\nmodel_artifactory_path.writeto(out)\n# Unzip model\nshutil.unpack_archive(model_archive_path, model_path)\nlogger.info(f\"Model downloaded\")\nlogger.info(f\"Model archive removed\")\nreturn True\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit.explain_as_html","title":"<code>explain_as_html(content, *args, **kwargs)</code>","text":"<p>Compute explanations about a prediction and return an HTML report</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>def explain_as_html(self, content: Any, *args, **kwargs) -&gt; str:\n\"\"\"Compute explanations about a prediction and return an HTML report\"\"\"\nif isinstance(content, list) or isinstance(content, dict):\ncontent = pd.DataFrame(content)\nreturn self._model_explainer.explain_instance_as_html(content, *args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit.explain_as_json","title":"<code>explain_as_json(content, *args, **kwargs)</code>","text":"<p>Compute explanations about a prediction and return a JSON serializable object</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>def explain_as_json(self, content: Any, *args, **kwargs) -&gt; Union[dict, list]:\n\"\"\"Compute explanations about a prediction and return a JSON serializable object\"\"\"\nif isinstance(content, list) or isinstance(content, dict):\ncontent = pd.DataFrame(content)\nreturn self._model_explainer.explain_instance_as_json(content, *args, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelGabarit.predict","title":"<code>predict(content, *args, **kwargs)</code>","text":"<p>Make a prediction by calling utils_models.predict with the loaded model</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>def predict(self, content: Any, *args, **kwargs) -&gt; Any:\n\"\"\"Make a prediction by calling utils_models.predict with the loaded model\"\"\"\nif isinstance(content, list) or isinstance(content, dict):\ncontent = pd.DataFrame(content)\nreturn utils_models.predict(content, model=self._model, model_conf=self._model_conf, **kwargs)\n</code></pre>"},{"location":"reference/template_api/model/model_gabarit/#template_api.model.model_gabarit.ModelSettings","title":"<code>ModelSettings</code>","text":"<p>         Bases: <code>BaseSettings</code></p> <p>Download settings</p> <p>This class is used for settings management purpose, have a look at the pydantic documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/</p> <p>By default, it looks for environment variables (case insensitive) to set the settings if a variable is not found, it looks for a file name .env in your working directory where you can declare the values of the variables and finally it sets the values to the default ones you can see above.</p> Source code in <code>template_api/model/model_gabarit.py</code> <pre><code>class ModelSettings(BaseSettings):\n\"\"\"Download settings\n    This class is used for settings management purpose, have a look at the pydantic\n    documentation for more details : https://pydantic-docs.helpmanual.io/usage/settings/\n    By default, it looks for environment variables (case insensitive) to set the settings\n    if a variable is not found, it looks for a file name .env in your working directory\n    where you can declare the values of the variables and finally it sets the values\n    to the default ones you can see above.\n    \"\"\"\ndata_dir: Path = DEFAULT_DATA_DIR\nmodels_dir: Path = DEFAULT_MODELS_DIR\nmodel_path: Path = DEFAULT_MODELS_DIR / \"model\"\nartifactory_model_url: str = \"\"\nartifactory_user: str = \"\"\nartifactory_password: str = \"\"\nredownload: bool = False\nclass Config:\nenv_file = \".env\"\n</code></pre>"},{"location":"reference/template_api/routers/","title":"Routers","text":"<p>Main router of the REST API</p>"},{"location":"reference/template_api/routers/functional/","title":"Functional","text":""},{"location":"reference/template_api/routers/functional/#template_api.routers.functional.explain","title":"<code>explain(request)</code>  <code>async</code>","text":"<p>Explain route that expose a model explainer in charge of model explicability</p> <p>This function is using starlette Request object instead of pydantic since we can not know what data your model is expecting. See https://fastapi.tiangolo.com/advanced/using-request-directly/ for more infos.</p> <p>We also use the custom starlette JSONResponse class (PredictionResponse) instead of pydantic for the same reasons</p> <p>For a cleaner way to handle requests and reponses you should use pydantic as stated in FastAPI doc : https://fastapi.tiangolo.com/tutorial/body/#create-your-data-model</p> <p>You can use routes from example_api_num.routers.technical as examples of how to create requests and responses schemas thanks to pydantic or have a look at the FastAPI documentation : https://fastapi.tiangolo.com/tutorial/response-model/</p> <p>If there is not explainer or the explainer does not implement explain_as_json or explain_as_html we return a 501 HTTP error : https://developer.mozilla.org/fr/docs/Web/HTTP/Status/501</p> Source code in <code>template_api/routers/functional.py</code> <pre><code>@router.post(\"/explain\")\nasync def explain(request: Request):\n\"\"\"Explain route that expose a model explainer in charge of model explicability\n    This function is using starlette Request object instead of pydantic since we can not\n    know what data your model is expecting.\n    See https://fastapi.tiangolo.com/advanced/using-request-directly/ for more infos.\n    We also use the custom starlette JSONResponse class (PredictionResponse)\n    instead of pydantic for the same reasons\n    For a cleaner way to handle requests and reponses you should use pydantic as stated in FastAPI\n    doc : https://fastapi.tiangolo.com/tutorial/body/#create-your-data-model\n    You can use routes from example_api_num.routers.technical as examples of how to create requests and\n    responses schemas thanks to pydantic or have a look at the FastAPI documentation :\n    https://fastapi.tiangolo.com/tutorial/response-model/\n    If there is not explainer or the explainer does not implement explain_as_json or explain_as_html\n    we return a 501 HTTP error : https://developer.mozilla.org/fr/docs/Web/HTTP/Status/501\n    \"\"\"\nmodel: Model = request.app.state.model\nbody = await request.body()\nbody = json.loads(body) if body else {}\n# JSON repsonse (when Accept: application/json in the request)\nif request.headers.get(\"Accept\") == \"application/json\":\ntry:\nexplanation_json = model.explain_as_json(**body)\nexcept (AttributeError, NotImplementedError):\nerror_msg = {\n\"error\": {\n\"code\": 501,\n\"message\": \"No explainer capable of handling explicability\"\n}\n}\nreturn Response(\ncontent=json.dumps(error_msg),\nstatus_code=501,\nmedia_type='application/json',\n)\nelse:\nreturn NumpyJSONResponse(explanation_json)\n# HTML repsonse (otherwise)\nelse:\ntry:\nexplanation_html = model.explain_as_html(**body)\nexcept (AttributeError, NotImplementedError):\nreturn Response(\ncontent=\"No explainer capable of handling explicability\",\nstatus_code=501,\nmedia_type='text/plain',\n)\nelse:\nreturn HTMLResponse(explanation_html)\n</code></pre>"},{"location":"reference/template_api/routers/functional/#template_api.routers.functional.predict","title":"<code>predict(request)</code>  <code>async</code>","text":"<p>Predict route that exposes your model</p> <p>This function is using starlette Request object instead of pydantic since we can not know what data your model is expecting. See https://fastapi.tiangolo.com/advanced/using-request-directly/ for more infos.</p> <p>We also use a custom starlette JSONResponse class (NumpyJSONResponse) instead of pydantic for the same reasons</p> <p>For a cleaner way to handle requests and reponses you should use pydantic as stated in FastAPI doc : https://fastapi.tiangolo.com/tutorial/body/#create-your-data-model</p> <p>You can use routes from template_api.routers.technical as examples of how to create requests and responses schemas thanks to pydantic or have a look at the FastAPI documentation : https://fastapi.tiangolo.com/tutorial/response-model/</p> Source code in <code>template_api/routers/functional.py</code> <pre><code>@router.post(\"/predict\")\nasync def predict(request: Request):\n\"\"\"Predict route that exposes your model\n    This function is using starlette Request object instead of pydantic since we can not\n    know what data your model is expecting.\n    See https://fastapi.tiangolo.com/advanced/using-request-directly/ for more infos.\n    We also use a custom starlette JSONResponse class (NumpyJSONResponse)\n    instead of pydantic for the same reasons\n    For a cleaner way to handle requests and reponses you should use pydantic as stated in FastAPI\n    doc : https://fastapi.tiangolo.com/tutorial/body/#create-your-data-model\n    You can use routes from template_api.routers.technical as examples of how to create requests and\n    responses schemas thanks to pydantic or have a look at the FastAPI documentation :\n    https://fastapi.tiangolo.com/tutorial/response-model/\n    \"\"\"\nmodel: Model = request.app.state.model\nbody = await request.body()\nbody = json.loads(body) if body else {}\nprediction = model.predict(**body)\nreturn NumpyJSONResponse(prediction)\n</code></pre>"},{"location":"reference/template_api/routers/technical/","title":"Technical","text":""},{"location":"reference/template_api/routers/technical/#template_api.routers.technical.get_liveness","title":"<code>get_liveness()</code>  <code>async</code>","text":"<p>Liveness probe for k8s</p> Source code in <code>template_api/routers/technical.py</code> <pre><code>@router.get(\n\"/liveness\",\nresponse_model=ReponseLiveness,\nname=\"liveness\",\ntags=[\"technical\"],\n)\nasync def get_liveness() -&gt; ReponseLiveness:\n\"\"\"Liveness probe for k8s\"\"\"\nliveness_msg = ReponseLiveness(alive=\"ok\")\nreturn liveness_msg\n</code></pre>"},{"location":"reference/template_api/routers/technical/#template_api.routers.technical.get_readiness","title":"<code>get_readiness(request)</code>  <code>async</code>","text":"<p>Readiness probe for k8s</p> Source code in <code>template_api/routers/technical.py</code> <pre><code>@router.get(\n\"/readiness\",\nresponse_model=ReponseReadiness,\nname=\"readiness\",\ntags=[\"technical\"],\n)\nasync def get_readiness(request: Request) -&gt; ReponseReadiness:\n\"\"\"Readiness probe for k8s\"\"\"\nmodel: Model = (\nrequest.app.state.model if hasattr(request.app.state, \"model\") else None\n)\nif model and model.is_model_loaded():\nreturn ReponseReadiness(ready=\"ok\")\nelse:\nreturn ReponseReadiness(ready=\"ko\")\n</code></pre>"},{"location":"reference/template_api/routers/technical/#template_api.routers.technical.info","title":"<code>info(request)</code>  <code>async</code>","text":"<p>Rest resource for info</p> Source code in <code>template_api/routers/technical.py</code> <pre><code>@router.get(\n\"/info\",\nresponse_model=ReponseInformation,\nname=\"information\",\ntags=[\"technical\"],\n)\nasync def info(request: Request) -&gt; ReponseInformation:\n\"\"\"Rest resource for info\"\"\"\nmodel: Model = request.app.state.model\nreturn ReponseInformation(\napplication=settings.app_name,\nversion=settings.app_version,\nmodel_name=model._model_conf.get(\"model_name\", \"?\"),\nmodel_version=model._model_conf.get(\"package_version\", \"?\"),\n)\n</code></pre>"},{"location":"reference/template_api/routers/schemas/","title":"Schemas","text":""},{"location":"reference/template_api/routers/schemas/functional/","title":"Functional","text":"<p>Functional schemas</p>"},{"location":"reference/template_api/routers/schemas/technical/","title":"Technical","text":"<p>Technical schemas</p>"},{"location":"reference/template_api/routers/schemas/technical/#template_api.routers.schemas.technical.ReponseInformation","title":"<code>ReponseInformation</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Return object for info resource</p> Source code in <code>template_api/routers/schemas/technical.py</code> <pre><code>class ReponseInformation(BaseModel):\n\"\"\"Return object for info resource\"\"\"\napplication: str = Field(None, title=\"Application name\")\nversion: str = Field(None, title=\"Application version\")\nmodel_name: str = Field(None, title=\"Model name\")\nmodel_version: str = Field(None, title=\"Model version\")\n</code></pre>"},{"location":"reference/template_api/routers/schemas/technical/#template_api.routers.schemas.technical.ReponseLiveness","title":"<code>ReponseLiveness</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Return object for liveness probe</p> Source code in <code>template_api/routers/schemas/technical.py</code> <pre><code>class ReponseLiveness(BaseModel):\n\"\"\"Return object for liveness probe\"\"\"\nalive: str = Field(None, title=\"Message\")\n</code></pre>"},{"location":"reference/template_api/routers/schemas/technical/#template_api.routers.schemas.technical.ReponseReadiness","title":"<code>ReponseReadiness</code>","text":"<p>         Bases: <code>BaseModel</code></p> <p>Return object for readiness probe</p> Source code in <code>template_api/routers/schemas/technical.py</code> <pre><code>class ReponseReadiness(BaseModel):\n\"\"\"Return object for readiness probe\"\"\"\nready: str = Field(None, title=\"Message\")\n</code></pre>"},{"location":"reference/template_api/routers/schemas/utils/","title":"Utils","text":""},{"location":"reference/template_api/routers/schemas/utils/#template_api.routers.schemas.utils.NumpyArrayEncoder","title":"<code>NumpyArrayEncoder</code>","text":"<p>         Bases: <code>json.JSONEncoder</code></p> <p>JSONEncoder to store python dict or list containing numpy arrays</p> Source code in <code>template_api/routers/schemas/utils.py</code> <pre><code>class NumpyArrayEncoder(json.JSONEncoder):\n\"\"\"JSONEncoder to store python dict or list containing numpy arrays\"\"\"\ndef default(self, obj: Any) -&gt; Any:\n\"\"\"Transform numpy arrays into JSON serializable object such as list\n        see : https://docs.python.org/3/library/json.html#json.JSONEncoder.default\n        \"\"\"\n# numpy.ndarray have dtype, astype and tolist attribute and methods that we want\n# to use to convert their element into JSON serializable objects\nif hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\"):\nif np.issubdtype(obj.dtype, np.integer):\nreturn obj.astype(int).tolist()\nelif np.issubdtype(obj.dtype, np.number):\nreturn obj.astype(float).tolist()\nelse:\nreturn obj.tolist()\n# sets are not json serializable\nelif isinstance(obj, set):\nreturn list(obj)\nreturn json.JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"reference/template_api/routers/schemas/utils/#template_api.routers.schemas.utils.NumpyArrayEncoder.default","title":"<code>default(obj)</code>","text":"<p>Transform numpy arrays into JSON serializable object such as list see : https://docs.python.org/3/library/json.html#json.JSONEncoder.default</p> Source code in <code>template_api/routers/schemas/utils.py</code> <pre><code>def default(self, obj: Any) -&gt; Any:\n\"\"\"Transform numpy arrays into JSON serializable object such as list\n    see : https://docs.python.org/3/library/json.html#json.JSONEncoder.default\n    \"\"\"\n# numpy.ndarray have dtype, astype and tolist attribute and methods that we want\n# to use to convert their element into JSON serializable objects\nif hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\"):\nif np.issubdtype(obj.dtype, np.integer):\nreturn obj.astype(int).tolist()\nelif np.issubdtype(obj.dtype, np.number):\nreturn obj.astype(float).tolist()\nelse:\nreturn obj.tolist()\n# sets are not json serializable\nelif isinstance(obj, set):\nreturn list(obj)\nreturn json.JSONEncoder.default(self, obj)\n</code></pre>"},{"location":"reference/template_nlp/","title":"Template nlp","text":""},{"location":"reference/template_nlp/utils/","title":"Utils","text":""},{"location":"reference/template_nlp/utils/#template_nlp.utils.NpEncoder","title":"<code>NpEncoder</code>","text":"<p>         Bases: <code>json.JSONEncoder</code></p> <p>JSON encoder to manage numpy objects</p> Source code in <code>template_nlp/utils.py</code> <pre><code>class NpEncoder(json.JSONEncoder):\n'''JSON encoder to manage numpy objects'''\ndef default(self, obj) -&gt; Any:\nif is_ndarray_convertable(obj):\nreturn ndarray_to_builtin_object(obj)\nelif isinstance(obj, set):\nreturn list(obj)\nelse:\nreturn super(NpEncoder, self).default(obj)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.data_agnostic_str_to_list","title":"<code>data_agnostic_str_to_list(function)</code>","text":"<p>Decorator to transform a string into a list of one element, and retrieve first element of the function returns. Idea: be able to do <code>predict(my_string)</code> Otherwise, we would have to do <code>prediction = predict([my_string])[0]</code></p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>func</code> <p>Function to decorate</p> required <p>Returns:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The decorated function</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def data_agnostic_str_to_list(function: Callable) -&gt; Callable:\n'''Decorator to transform a string into a list of one element,\n    and retrieve first element of the function returns.\n    Idea: be able to do `predict(my_string)`\n    Otherwise, we would have to do `prediction = predict([my_string])[0]`\n    Args:\n        function (func): Function to decorate\n    Returns:\n        function: The decorated function\n    '''\n# Get wrapper\ndef wrapper(self, x, *args, **kwargs):\n'''Wrapper'''\nif type(x) == str:\n# Cast str into a single element list\nmy_list = [x]\n# Get function result\nresults = function(self, my_list, *args, **kwargs)\n# Cast back to single element\nfinal_result = results[0]\nelse:\nfinal_result = function(self, x, *args, **kwargs)\n# Return\nreturn final_result\nreturn wrapper\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.display_shape","title":"<code>display_shape(df)</code>","text":"<p>Displays the number of line and of column of a table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Table to parse</p> required Source code in <code>template_nlp/utils.py</code> <pre><code>def display_shape(df: pd.DataFrame) -&gt; None:\n'''Displays the number of line and of column of a table.\n    Args:\n        df (pd.DataFrame): Table to parse\n    '''\n# Display\nlogger.info(f\"Number of lines : {df.shape[0]}. Number of columns : {df.shape[1]}.\")\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.find_folder_path","title":"<code>find_folder_path(folder_name, base_folder=None)</code>","text":"<p>Find a folder in a base folder and its subfolders. If base_folder is None, considers folder_name as a path and check it exists</p> <p>i.e., with the following structure : - C:/     - base_folder/         - folderA/             - folderB/         - folderC/ find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC find_folder_path(folderB, None) raises an error</p> <p>Parameters:</p> Name Type Description Default <code>folder_name</code> <code>str</code> <p>name of the folder to find. If base_folder is None, consider a path instead.</p> required Kwargs <p>base_folder (str): path of the base folder. If None, consider folder_name as a path.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>if we can't find folder_name in base_folder</p> <code>FileNotFoundError</code> <p>if folder_name is not a valid path (case where base_folder is None)</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>path to the wanted folder</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def find_folder_path(folder_name: str, base_folder: Union[str, None] = None) -&gt; str:\n'''Find a folder in a base folder and its subfolders.\n    If base_folder is None, considers folder_name as a path and check it exists\n    i.e., with the following structure :\n    - C:/\n        - base_folder/\n            - folderA/\n                - folderB/\n            - folderC/\n    find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA\n    find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB\n    find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC\n    find_folder_path(folderB, None) raises an error\n    Args:\n        folder_name (str): name of the folder to find. If base_folder is None, consider a path instead.\n    Kwargs:\n        base_folder (str): path of the base folder. If None, consider folder_name as a path.\n    Raises:\n        FileNotFoundError: if we can't find folder_name in base_folder\n        FileNotFoundError: if folder_name is not a valid path (case where base_folder is None)\n    Returns:\n        str: path to the wanted folder\n    '''\nif base_folder is not None:\nfolder_path = None\nfor path, subdirs, files in os.walk(base_folder):\nfor name in subdirs:\nif name == folder_name:\nfolder_path = os.path.join(path, name)\nif folder_path is None:\nraise FileNotFoundError(f\"Can't find folder {folder_name} inside {base_folder} and its subfolders\")\nelse:\nfolder_path = folder_name\nif not os.path.exists(folder_path):\nraise FileNotFoundError(f\"Can't find folder {folder_path} (considered as a path)\")\nreturn folder_path\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_chunk_limits","title":"<code>get_chunk_limits(x, chunksize=10000)</code>","text":"<p>Gets chunk limits from a pandas series or dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>pd.Series or pd.DataFrame</code> <p>Documents to consider</p> required Kwargs <p>chunksize (int): The chunk size</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the chunk size is negative</p> <p>Returns:</p> Type Description <code>List[Tuple[int]]</code> <p>list: the chunk limits Source code in <code>template_nlp/utils.py</code> <pre><code>def get_chunk_limits(x: Union[pd.DataFrame, pd.Series], chunksize: int = 10000) -&gt; List[Tuple[int]]:\n'''Gets chunk limits from a pandas series or dataframe.\n    Args:\n        x (pd.Series or pd.DataFrame): Documents to consider\n    Kwargs:\n        chunksize (int): The chunk size\n    Raises:\n        ValueError: If the chunk size is negative\n    Returns:\n        list&lt;tuple&gt;: the chunk limits\n    '''\nif chunksize &lt; 0:\nraise ValueError('The object chunksize must not be negative.')\n# Processs\nif chunksize == 0 or chunksize &gt;= x.shape[0]:\nchunks_limits = [(0, x.shape[0])]\nelse:\nchunks_limits = [(i * chunksize, min((i + 1) * chunksize, x.shape[0]))\nfor i in range(1 + ((x.shape[0] - 1) // chunksize))]\nreturn chunks_limits  # type: ignore\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_data_path","title":"<code>get_data_path()</code>","text":"<p>Returns the path to the data folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the data folder</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def get_data_path() -&gt; str:\n'''Returns the path to the data folder\n    Returns:\n        str: Path of the data folder\n    '''\nif DIR_PATH is None:\ndir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_nlp-data')\nelse:\ndir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_nlp-data')\nif not os.path.isdir(dir_path):\nos.mkdir(dir_path)\nreturn os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_models_path","title":"<code>get_models_path()</code>","text":"<p>Returns the path to the models folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the models folder</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def get_models_path() -&gt; str:\n'''Returns the path to the models folder\n    Returns:\n        str: Path of the models folder\n    '''\nif DIR_PATH is None:\ndir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_nlp-models')\nelse:\ndir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_nlp-models')\nif not os.path.isdir(dir_path):\nos.mkdir(dir_path)\nreturn os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_new_column_name","title":"<code>get_new_column_name(column_list, wanted_name)</code>","text":"<p>Gets a new column name from a list of existing ones &amp; a wanted name</p> <p>If the wanted name does not exists, return it. Otherwise get a new column prefixed by the wanted name.</p> <p>Parameters:</p> Name Type Description Default <code>column_list</code> <code>list</code> <p>List of existing columns</p> required <code>wanted_name</code> <code>str</code> <p>Wanted name</p> required Source code in <code>template_nlp/utils.py</code> <pre><code>def get_new_column_name(column_list: list, wanted_name: str) -&gt; str:\n'''Gets a new column name from a list of existing ones &amp; a wanted name\n    If the wanted name does not exists, return it.\n    Otherwise get a new column prefixed by the wanted name.\n    Args:\n        column_list (list): List of existing columns\n        wanted_name (str): Wanted name\n    '''\nif wanted_name not in column_list:\nreturn wanted_name\nelse:\nnew_name = f'{wanted_name}_{str(uuid.uuid4())[:8]}'\n# It should not happen, but we still check if new_name is available (bad luck ?)\nreturn get_new_column_name(column_list, new_name)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_package_version","title":"<code>get_package_version()</code>","text":"<p>Returns the current version of the package</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>version of the package</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def get_package_version() -&gt; str:\n'''Returns the current version of the package\n    Returns:\n        str: version of the package\n    '''\nversion = pkg_resources.get_distribution('template_nlp').version\nreturn version\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_ressources_path","title":"<code>get_ressources_path()</code>","text":"<p>Returns the path to the ressources folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the ressources folder</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def get_ressources_path() -&gt; str:\n'''Returns the path to the ressources folder\n    Returns:\n        str: Path of the ressources folder\n    '''\ndir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_nlp-ressources')\nif not os.path.isdir(dir_path):\nos.mkdir(dir_path)\nreturn os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.get_transformers_path","title":"<code>get_transformers_path()</code>","text":"<p>Returns the path to the transformers folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the transformers folder</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def get_transformers_path() -&gt; str:\n'''Returns the path to the transformers folder\n    Returns:\n        str: Path of the transformers folder\n    '''\nif DIR_PATH is None:\ndir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_nlp-transformers')\nelse:\ndir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_nlp-transformers')\nif not os.path.isdir(dir_path):\nos.mkdir(dir_path)\nreturn os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.is_ndarray_convertable","title":"<code>is_ndarray_convertable(obj)</code>","text":"<p>Returns True if the object is covertable to a builtin type in the same way a np.ndarray is</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>an object to test</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the object is covertable to a list as a np.ndarray is</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def is_ndarray_convertable(obj: Any) -&gt; bool:\n'''Returns True if the object is covertable to a builtin type in the same way a np.ndarray is\n    Args:\n        obj (Any): an object to test\n    Returns:\n        bool: True if the object is covertable to a list as a np.ndarray is\n    '''\nreturn hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\")\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.ndarray_to_builtin_object","title":"<code>ndarray_to_builtin_object(obj)</code>","text":"<p>Transform a numpy.ndarray like object to a builtin type like int, float or list</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>An object</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raise a ValueError when obj is not ndarray convertable</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The object converted to a builtin type like int, float or list</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def ndarray_to_builtin_object(obj: Any) -&gt; Any:\n'''Transform a numpy.ndarray like object to a builtin type like int, float or list\n    Args:\n        obj (Any): An object\n    Raises:\n        ValueError: Raise a ValueError when obj is not ndarray convertable\n    Returns:\n        Any: The object converted to a builtin type like int, float or list\n    '''\nif is_ndarray_convertable(obj):\nif np.issubdtype(obj.dtype, np.integer):\nreturn obj.astype(int).tolist()\nelif np.issubdtype(obj.dtype, np.number):\nreturn obj.astype(float).tolist()\nelse:\nreturn obj.tolist()\nelse:\nraise ValueError(f\"{obj} is not ndarray convertable\")\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.read_csv","title":"<code>read_csv(file_path, sep=';', encoding='utf-8', dtype=str, **kwargs)</code>","text":"<p>Reads a .csv file and parses the first line.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the .csv file containing the data</p> required Kwargs <p>sep (str): Separator of the data file encoding (str): Encoding of the data file kwargs: Pandas' kwargs</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file_path object does not point to an existing file</p> <p>Returns:</p> Name Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: Data</p> <code>str</code> <code>Union[str, None]</code> <p>First line of the .csv (None if not beginning with #) and with no line break</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def read_csv(file_path: str, sep: str = ';', encoding: str = 'utf-8', dtype: type = str, **kwargs) -&gt; Tuple[pd.DataFrame, Union[str, None]]:\n'''Reads a .csv file and parses the first line.\n    Args:\n        file_path (str): Path to the .csv file containing the data\n    Kwargs:\n        sep (str): Separator of the data file\n        encoding (str): Encoding of the data file\n        kwargs: Pandas' kwargs\n    Raises:\n        FileNotFoundError: If the file_path object does not point to an existing file\n    Returns:\n        pd.DataFrame: Data\n        str: First line of the .csv (None if not beginning with #) and with no line break\n    '''\nif not os.path.isfile(file_path):\nraise FileNotFoundError(f\"The file {file_path} does not exist\")\n# We get the first line\nwith open(file_path, 'r', encoding=encoding) as f:\nfirst_line = f.readline()\n# We check if the first line contains metadata\nhas_metada = True if first_line.startswith('#') else False\n# We load the dataset\nif has_metada:\ndf = pd.read_csv(file_path, sep=sep, encoding=encoding, dtype=dtype, skiprows=1, **kwargs).fillna('')\nelse:\ndf = pd.read_csv(file_path, sep=sep, encoding=encoding, dtype=dtype, **kwargs).fillna('')\n# If no metadata, return only the dataframe\nif not has_metada:\nreturn df, None\n# Else process the first_line\nelse:\n# Deletion of the line break\nif first_line is not None and first_line.endswith('\\n'):\nfirst_line = first_line[:-1]\n# Deletion of the return carriage\nif first_line is not None and first_line.endswith('\\r'):\nfirst_line = first_line[:-1]\n# Return\nreturn df, first_line\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.to_csv","title":"<code>to_csv(df, file_path, first_line=None, sep=';', encoding='utf-8', **kwargs)</code>","text":"<p>Writes a .csv and manages the first line.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Data to write</p> required <code>file_path</code> <code>str</code> <p>Path to the file to create</p> required Kwargs <p>first_line (str): First line to write (without line break which is done in this function) sep (str): Separator for the data file encoding (str): Encoding of the data file kwargs: pandas' kwargs</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def to_csv(df: pd.DataFrame, file_path: str, first_line: Union[str, None] = None, sep: str = ';',\nencoding: str = 'utf-8', **kwargs) -&gt; None:\n'''Writes a .csv and manages the first line.\n    Args:\n        df (pd.DataFrame): Data to write\n        file_path (str): Path to the file to create\n    Kwargs:\n        first_line (str): First line to write (without line break which is done in this function)\n        sep (str): Separator for the data file\n        encoding (str): Encoding of the data file\n        kwargs: pandas' kwargs\n    '''\n# We get the first line\nwith open(file_path, 'w', encoding=encoding) as f:\nif first_line is not None:\nf.write(first_line + '\\n')  # We add the first line if metadata are present\ndf.to_csv(f, sep=sep, encoding=encoding, index=None, **kwargs)\n</code></pre>"},{"location":"reference/template_nlp/utils/#template_nlp.utils.trained_needed","title":"<code>trained_needed(function)</code>","text":"<p>Decorator to ensure that a model has been trained.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>func</code> <p>Function to decorate</p> required <p>Returns:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The decorated function</p> Source code in <code>template_nlp/utils.py</code> <pre><code>def trained_needed(function: Callable) -&gt; Callable:\n'''Decorator to ensure that a model has been trained.\n    Args:\n        function (func): Function to decorate\n    Returns:\n        function: The decorated function\n    '''\n# Get wrapper\ndef wrapper(self, *args, **kwargs):\n'''Wrapper'''\nif not self.trained:\nraise AttributeError(f\"The function {function.__name__} can't be called as long as the model hasn't been fitted\")\nelse:\nreturn function(self, *args, **kwargs)\nreturn wrapper\n</code></pre>"},{"location":"reference/template_nlp/models_training/","title":"Models training","text":""},{"location":"reference/template_nlp/models_training/model_aggregation/","title":"Model aggregation","text":""},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation","title":"<code>ModelAggregation</code>","text":"<p>         Bases: <code>ModelClass</code></p> <p>Model for aggregating several instances of ModelClass</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>class ModelAggregation(ModelClass):\n'''Model for aggregating several instances of ModelClass'''\n_default_name = 'model_aggregation'\n_dict_aggregation_function = {'majority_vote': {'aggregation_function': majority_vote, 'using_proba': False, 'multi_label': False},\n'proba_argmax': {'aggregation_function': proba_argmax, 'using_proba': True, 'multi_label': False},\n'all_predictions': {'aggregation_function': all_predictions, 'using_proba': False, 'multi_label': True},\n'vote_labels': {'aggregation_function': vote_labels, 'using_proba': False, 'multi_label': True}}\ndef __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'majority_vote',\nusing_proba: bool = False, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n        This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg)\n        from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes.\n        However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.\n        Kwargs:\n            list_models (list) : The list of models to be aggregated (can be None if reloading from standalones)\n            aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg)\n            using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).\n        Raises:\n            ValueError: All the aggregated sub_models have not the same multi_label attributes\n            ValueError: The multi_label attributes of the aggregated models are inconsistent with multi_label\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Set attributes\nself.using_proba = using_proba\nself.aggregation_function = aggregation_function\n# Manage submodels\nself.sub_models = list_models  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n# Check for multi-labels inconsistencies\nset_multi_label = {sub_model['model'].multi_label for sub_model in self.sub_models}\nif len(set_multi_label) &gt; 1:\nraise ValueError(f\"All the aggregated sub_models do not have the same multi_label attribute\")\nif len(set_multi_label.union({self.multi_label})) &gt; 1:\nraise ValueError(f\"The multi_label attributes of the aggregated models are inconsistent with the provided multi label attribute ({self.multi_label}).\")\n# Set trained &amp; classes info from submodels\nself.trained, self.list_classes, self.dict_classes = self._check_trained()\n# Set nb_fit to 1 if already trained\nif self.trained:\nself.nb_fit = 1\n@property\ndef aggregation_function(self):\n'''Getter for aggregation_function'''\nreturn self._aggregation_function\n@aggregation_function.setter\ndef aggregation_function(self, agg_function: Union[Callable, str]):\n'''Setter for aggregation_function\n        If a string, try to match a predefined function\n        Raises:\n            ValueError: If the object aggregation_function is a str but not found in the dictionary of predefined aggregation functions\n            ValueError: If the object aggregation_function is incompatible with multi_label\n        '''\n# Retrieve aggregation function from dict if a string\nif isinstance(agg_function, str):\n# Get infos\nif agg_function not in self._dict_aggregation_function.keys():\nraise ValueError(f\"The aggregation_function ({agg_function}) is not a valid option (must be chosen in {self._dict_aggregation_function.keys()})\")\nusing_proba = self._dict_aggregation_function[agg_function]['using_proba']\nmulti_label = self._dict_aggregation_function[agg_function]['multi_label']\nagg_function = self._dict_aggregation_function[agg_function]['aggregation_function']  # type: ignore\n# Apply checks\nif self.using_proba != using_proba:\nself.logger.warning(f\"using_proba {self.using_proba} is incompatible with the selected aggregation function '{agg_function}'. We force using_proba to {using_proba}.\")\nself.using_proba = using_proba  # type: ignore\nif self.multi_label != multi_label:\nraise ValueError(f\"multi_label {self.multi_label} is incompatible with the selected aggregation function '{agg_function}'.\")\nself._aggregation_function = agg_function\n@aggregation_function.deleter\ndef aggregation_function(self):\n'''Deleter for aggregation_function'''\nself._aggregation_function = None\n@property\ndef sub_models(self):\n'''Getter for sub_models'''\nreturn self._sub_models\n@sub_models.setter\ndef sub_models(self, list_models: Union[list, None] = None):\n'''Setter for sub_models\n        Kwargs:\n            list_models (list) : The list of models to be aggregated\n        '''\nlist_models = [] if list_models is None else list_models\nsub_models = []  # Init list of models\nfor model in list_models:\n# If a string (a model name), reload it\nif isinstance(model, str):\nreal_model, _ = utils_models.load_model(model)\ndict_model = {'name': model, 'model': real_model}\nelse:\ndict_model = {'name': os.path.split(model.model_dir)[-1], 'model': model}\nsub_models.append(dict_model.copy())\nself._sub_models = sub_models.copy()\n@sub_models.deleter\ndef sub_models(self):\n'''Deleter for sub_models'''\nself._sub_models = None\ndef _check_trained(self) -&gt; Tuple[bool, list, dict]:\n'''Checks and sets various attributes related to the fitting of underlying models\n        Returns:\n            bool: is the aggregation model is considered fitted\n            list: list of classes\n            dict: dict of classes\n        '''\n# Check fitted\nmodels_trained = {sub_model['model'].trained for sub_model in self.sub_models}\nif len(models_trained) &gt; 0 and all(models_trained):\n# All models trained\ntrained = True\n# Set list_classes\nlist_classes = list({label for sub_model in self.sub_models for label in sub_model['model'].list_classes})\nlist_classes.sort()\n# Set dict_classes based on self.list_classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# No model or not fitted\nelse:\ntrained, list_classes, dict_classes = False, [], {}\nreturn trained, list_classes, dict_classes\ndef fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Fits the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models\n            y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models\n            with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models\n        '''\n# Fit each model\nfor sub_model in self.sub_models:\nmodel = sub_model['model']\nif not model.trained:\nmodel.fit(x_train, y_train, x_valid=x_valid, y_valid=y_valid, with_shuffle=True, **kwargs)\n# Set nb_fit to 1 if not already trained\nif not self.trained:\nself.nb_fit = 1\n# Update attributes\nself.trained, self.list_classes, self.dict_classes = self._check_trained()\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Prediction\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n            return_proba (bool): If the function should return the probabilities instead of the classes\n        Returns:\n            np.ndarray: array of shape = [n_samples]\n        '''\n# We decide whether to rely on each model's probas or their predictions\nif return_proba:\nreturn self.predict_proba(x_test)\nelse:\n# Get what we want (probas or preds) and use the aggregation function\nif self.using_proba:\npreds_or_probas = self._predict_probas_sub_models(x_test, **kwargs)\nelse:\npreds_or_probas = self._predict_sub_models(x_test, **kwargs)\nreturn np.array([self.aggregation_function(array, list_classes=self.list_classes) for array in preds_or_probas])  # type: ignore\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Returns:\n            np.ndarray: array of shape = [n_samples, n_classes]\n        '''\nprobas_sub_models = self._predict_probas_sub_models(x_test, **kwargs)\n# The probas of all models are averaged\nreturn np.sum(probas_sub_models, axis=1) / probas_sub_models.shape[1]\n@utils.trained_needed\ndef _predict_probas_sub_models(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Recover the probabilities of each model being aggregated\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Returns:\n            np.ndarray: array of shape = [n_samples, nb_model, nb_classes]\n        '''\narray_probas = np.array([self._predict_full_list_classes(sub_model['model'], x_test, return_proba=True) for sub_model in self.sub_models])\narray_probas = np.transpose(array_probas, (1, 0, 2))\nreturn array_probas\n@utils.trained_needed\ndef _predict_sub_models(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Recover the predictions of each model being aggregated\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Returns:\n            np.ndarray: not multi_label : array of shape = [n_samples, nb_model]\n                        multi_label : array of shape = [n_samples, nb_model, n_classes]\n        '''\nif self.multi_label:\narray_predict = np.array([self._predict_full_list_classes(sub_model['model'], x_test, return_proba=False) for sub_model in self.sub_models])\narray_predict = np.transpose(array_predict, (1, 0, 2))\nelse:\narray_predict = np.array([sub_model['model'].predict(x_test) for sub_model in self.sub_models])\narray_predict = np.transpose(array_predict, (1, 0))\nreturn array_predict\ndef _predict_full_list_classes(self, model: Type[ModelClass], x_test, return_proba: bool = False) -&gt; np.ndarray:\n'''For multi_label: adds missing columns in the prediction of model (class missing in their list_classes)\n        Or, if return_proba, adds a proba of zero to the missing classes in their list_classes\n        Args:\n            model (ModelClass): Model to use\n            x_test (?): Array-like or sparse matrix of shape = [n_samples, n_features]\n            return_proba (bool): If the function should return the probabilities instead of the classes\n        Returns:\n            np.ndarray: The array with the missing columns added\n        '''\n# Get predictions or probas\npreds_or_probas = model.predict(x_test, return_proba=return_proba)\n# Manage each cases. Reorder predictions or probas according to aggregation model list_classes\n# Multi label, proba = True\n# Multi label, proba = False\n# Mono label, proba = True\nif model.multi_label or return_proba:\ndf_all = pd.DataFrame(np.zeros((len(preds_or_probas), len(self.list_classes))), columns=self.list_classes)  # type: ignore\ndf_model = pd.DataFrame(preds_or_probas, columns=model.list_classes)\nfor col in model.list_classes:\ndf_all[col] = df_model[col]\nreturn df_all.to_numpy()\n# Mono label, proba = False\nelse:\nreturn preds_or_probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\nif json_data is None:\njson_data = {}\n# Specific aggregation - save some wanted entries\ntrain_keys = ['filename', 'filename_valid', 'preprocess_str']\ndefault_json_data = {key: json_data.get(key, None) for key in train_keys}\ndefault_json_data['aggregator_dir'] = self.model_dir\n# Save each trained and unsaved model\nfor sub_model in self.sub_models:\npath_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\nif os.path.exists(path_config):\nwith open(path_config, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\ntrained = configs.get('trained', False)\nif not trained:\nsub_model['model'].save(default_json_data)\nelse:\nsub_model['model'].save(default_json_data)\n# Add some specific information\njson_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\njson_data['using_proba'] = self.using_proba\n# Save aggregation_function if not None &amp; level_save &gt; LOW\nif (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\naggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n# Save as pickle\nwith open(aggregation_function_path, 'wb') as f:\npickle.dump(self.aggregation_function, f)\n# Save\nmodels_list = [sub_model['name'] for sub_model in self.sub_models]\naggregation_function = self.aggregation_function\ndelattr(self, \"sub_models\")\ndelattr(self, \"aggregation_function\")\nsuper().save(json_data=json_data)\nsetattr(self, \"aggregation_function\", aggregation_function)\nsetattr(self, \"sub_models\", models_list)  # Setter needs list of models, not sub_models itself\n# Add message in model_upload_instructions.md\nmd_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\nline = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it .  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\nself.prepend_line(md_path, line)\ndef prepend_line(self, file_name: str, line: str) -&gt; None:\n''' Insert given string as a new line at the beginning of a file\n        Kwargs:\n            file_name (str): Path to file\n            line (str): line to insert\n        '''\nwith open(file_name, 'r+') as f:\nlines = f.readlines()\nlines.insert(0, line)\nf.seek(0)\nf.writelines(lines)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model aggregation from its configuration and \"standalones\" files\n           Reloads the sub_models from their files\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            aggregation_function_path (str): Path to aggregation_function_path\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If aggregation_function_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object aggregation_function_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\naggregation_function_path = kwargs.get('aggregation_function_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif aggregation_function_path is None:\nraise ValueError(\"The argument aggregation_function_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(aggregation_function_path):\nraise FileNotFoundError(f\"The file {aggregation_function_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Reload aggregation_function_path\nwith open(aggregation_function_path, 'rb') as f:\nself.aggregation_function = pickle.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\nself.sub_models = configs.get('list_models_name', [])  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx,}, ...]\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label',\n'level_save', 'using_proba']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.aggregation_function","title":"<code>aggregation_function</code>  <code>deletable</code> <code>writable</code> <code>property</code>","text":"<p>Getter for aggregation_function</p>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.sub_models","title":"<code>sub_models</code>  <code>deletable</code> <code>writable</code> <code>property</code>","text":"<p>Getter for sub_models</p>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.__init__","title":"<code>__init__(list_models=None, aggregation_function='majority_vote', using_proba=False, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments) This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg) from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes. However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.</p> Kwargs <p>list_models (list) : The list of models to be aggregated (can be None if reloading from standalones) aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg) using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>All the aggregated sub_models have not the same multi_label attributes</p> <code>ValueError</code> <p>The multi_label attributes of the aggregated models are inconsistent with multi_label</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'majority_vote',\nusing_proba: bool = False, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n    This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg)\n    from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes.\n    However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.\n    Kwargs:\n        list_models (list) : The list of models to be aggregated (can be None if reloading from standalones)\n        aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg)\n        using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).\n    Raises:\n        ValueError: All the aggregated sub_models have not the same multi_label attributes\n        ValueError: The multi_label attributes of the aggregated models are inconsistent with multi_label\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Set attributes\nself.using_proba = using_proba\nself.aggregation_function = aggregation_function\n# Manage submodels\nself.sub_models = list_models  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n# Check for multi-labels inconsistencies\nset_multi_label = {sub_model['model'].multi_label for sub_model in self.sub_models}\nif len(set_multi_label) &gt; 1:\nraise ValueError(f\"All the aggregated sub_models do not have the same multi_label attribute\")\nif len(set_multi_label.union({self.multi_label})) &gt; 1:\nraise ValueError(f\"The multi_label attributes of the aggregated models are inconsistent with the provided multi label attribute ({self.multi_label}).\")\n# Set trained &amp; classes info from submodels\nself.trained, self.list_classes, self.dict_classes = self._check_trained()\n# Set nb_fit to 1 if already trained\nif self.trained:\nself.nb_fit = 1\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required Kwargs <p>x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Fits the model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models\n        y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models\n        with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models\n    '''\n# Fit each model\nfor sub_model in self.sub_models:\nmodel = sub_model['model']\nif not model.trained:\nmodel.fit(x_train, y_train, x_valid=x_valid, y_valid=y_valid, with_shuffle=True, **kwargs)\n# Set nb_fit to 1 if not already trained\nif not self.trained:\nself.nb_fit = 1\n# Update attributes\nself.trained, self.list_classes, self.dict_classes = self._check_trained()\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Prediction</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <code>return_proba</code> <code>bool</code> <p>If the function should return the probabilities instead of the classes</p> <code>False</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: array of shape = [n_samples]</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Prediction\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        return_proba (bool): If the function should return the probabilities instead of the classes\n    Returns:\n        np.ndarray: array of shape = [n_samples]\n    '''\n# We decide whether to rely on each model's probas or their predictions\nif return_proba:\nreturn self.predict_proba(x_test)\nelse:\n# Get what we want (probas or preds) and use the aggregation function\nif self.using_proba:\npreds_or_probas = self._predict_probas_sub_models(x_test, **kwargs)\nelse:\npreds_or_probas = self._predict_sub_models(x_test, **kwargs)\nreturn np.array([self.aggregation_function(array, list_classes=self.list_classes) for array in preds_or_probas])  # type: ignore\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: array of shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n    Returns:\n        np.ndarray: array of shape = [n_samples, n_classes]\n    '''\nprobas_sub_models = self._predict_probas_sub_models(x_test, **kwargs)\n# The probas of all models are averaged\nreturn np.sum(probas_sub_models, axis=1) / probas_sub_models.shape[1]\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.prepend_line","title":"<code>prepend_line(file_name, line)</code>","text":"<p>Insert given string as a new line at the beginning of a file</p> Kwargs <p>file_name (str): Path to file line (str): line to insert</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def prepend_line(self, file_name: str, line: str) -&gt; None:\n''' Insert given string as a new line at the beginning of a file\n    Kwargs:\n        file_name (str): Path to file\n        line (str): line to insert\n    '''\nwith open(file_name, 'r+') as f:\nlines = f.readlines()\nlines.insert(0, line)\nf.seek(0)\nf.writelines(lines)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model aggregation from its configuration and \"standalones\" files    Reloads the sub_models from their files</p> Kwargs <p>configuration_path (str): Path to configuration file aggregation_function_path (str): Path to aggregation_function_path</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If aggregation_function_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object aggregation_function_path is not an existing file</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model aggregation from its configuration and \"standalones\" files\n       Reloads the sub_models from their files\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        aggregation_function_path (str): Path to aggregation_function_path\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If aggregation_function_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object aggregation_function_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\naggregation_function_path = kwargs.get('aggregation_function_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif aggregation_function_path is None:\nraise ValueError(\"The argument aggregation_function_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(aggregation_function_path):\nraise FileNotFoundError(f\"The file {aggregation_function_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Reload aggregation_function_path\nwith open(aggregation_function_path, 'rb') as f:\nself.aggregation_function = pickle.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\nself.sub_models = configs.get('list_models_name', [])  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx,}, ...]\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label',\n'level_save', 'using_proba']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.ModelAggregation.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\nif json_data is None:\njson_data = {}\n# Specific aggregation - save some wanted entries\ntrain_keys = ['filename', 'filename_valid', 'preprocess_str']\ndefault_json_data = {key: json_data.get(key, None) for key in train_keys}\ndefault_json_data['aggregator_dir'] = self.model_dir\n# Save each trained and unsaved model\nfor sub_model in self.sub_models:\npath_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\nif os.path.exists(path_config):\nwith open(path_config, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\ntrained = configs.get('trained', False)\nif not trained:\nsub_model['model'].save(default_json_data)\nelse:\nsub_model['model'].save(default_json_data)\n# Add some specific information\njson_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\njson_data['using_proba'] = self.using_proba\n# Save aggregation_function if not None &amp; level_save &gt; LOW\nif (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\naggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n# Save as pickle\nwith open(aggregation_function_path, 'wb') as f:\npickle.dump(self.aggregation_function, f)\n# Save\nmodels_list = [sub_model['name'] for sub_model in self.sub_models]\naggregation_function = self.aggregation_function\ndelattr(self, \"sub_models\")\ndelattr(self, \"aggregation_function\")\nsuper().save(json_data=json_data)\nsetattr(self, \"aggregation_function\", aggregation_function)\nsetattr(self, \"sub_models\", models_list)  # Setter needs list of models, not sub_models itself\n# Add message in model_upload_instructions.md\nmd_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\nline = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it .  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\nself.prepend_line(md_path, line)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.all_predictions","title":"<code>all_predictions(predictions, **kwargs)</code>","text":"<p>Calculates the sum of the arrays along axis 0 casts it to bool and then to int. Expects a numpy array containing only zeroes and ones. When used as an aggregation function, keeps all the prediction of each model (multi-labels)</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray) </code> <p>Array of shape : (n_models, n_classes)</p> required Return <p>np.ndarray: The prediction</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def all_predictions(predictions: np.ndarray, **kwargs) -&gt; np.ndarray:\n'''Calculates the sum of the arrays along axis 0 casts it to bool and then to int.\n    Expects a numpy array containing only zeroes and ones.\n    When used as an aggregation function, keeps all the prediction of each model (multi-labels)\n    Args:\n        predictions (np.ndarray) : Array of shape : (n_models, n_classes)\n    Return:\n        np.ndarray: The prediction\n    '''\nreturn np.sum(predictions, axis=0, dtype=bool).astype(int)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.majority_vote","title":"<code>majority_vote(predictions, **kwargs)</code>","text":"<p>Gives the class corresponding to the most present prediction in the given predictions. In case of a tie, gives the prediction of the first model involved in the tie</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray</code> <p>The array containing the predictions of each model (shape (n_models))</p> required <p>Returns:</p> Type Description <p>The prediction</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def majority_vote(predictions: np.ndarray, **kwargs):\n'''Gives the class corresponding to the most present prediction in the given predictions.\n    In case of a tie, gives the prediction of the first model involved in the tie\n    Args:\n        predictions (np.ndarray): The array containing the predictions of each model (shape (n_models))\n    Returns:\n        The prediction\n    '''\nlabels, counts = np.unique(predictions, return_counts=True)\nvotes = [(label, count) for label, count in zip(labels, counts)]\nvotes = sorted(votes, key=lambda x: x[1], reverse=True)\npossible_classes = {vote[0] for vote in votes if vote[1]==votes[0][1]}\nreturn [prediction for prediction in predictions if prediction in possible_classes][0]\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.proba_argmax","title":"<code>proba_argmax(proba, list_classes, **kwargs)</code>","text":"<p>Gives the class corresponding to the argmax of the average of the given probabilities</p> <p>Parameters:</p> Name Type Description Default <code>proba</code> <code>np.ndarray</code> <p>The probabilities of each model for each class, array of shape (nb_models, nb_classes)</p> required <code>list_classes</code> <code>list</code> <p>List of classes</p> required <p>Returns:</p> Type Description <p>The prediction</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def proba_argmax(proba: np.ndarray, list_classes: list, **kwargs):\n'''Gives the class corresponding to the argmax of the average of the given probabilities\n    Args:\n        proba (np.ndarray): The probabilities of each model for each class, array of shape (nb_models, nb_classes)\n        list_classes (list): List of classes\n    Returns:\n        The prediction\n    '''\nproba_average = np.sum(proba, axis=0) / proba.shape[0]\nindex_class = np.argmax(proba_average)\nreturn list_classes[index_class]\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_aggregation/#template_nlp.models_training.model_aggregation.vote_labels","title":"<code>vote_labels(predictions, **kwargs)</code>","text":"<p>Gives the result of majority_vote applied on the second axis. When used as an aggregation_function, for each class, performs a majority vote for the aggregated models. It gives a multi-labels result</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray</code> <p>array of shape : (n_models, n_classes)</p> required Return <p>np.ndarray: prediction</p> Source code in <code>template_nlp/models_training/model_aggregation.py</code> <pre><code>def vote_labels(predictions: np.ndarray, **kwargs) -&gt; np.ndarray:\n'''Gives the result of majority_vote applied on the second axis.\n    When used as an aggregation_function, for each class, performs a majority vote for the aggregated models.\n    It gives a multi-labels result\n    Args:\n        predictions (np.ndarray): array of shape : (n_models, n_classes)\n    Return:\n        np.ndarray: prediction\n    '''\nreturn np.apply_along_axis(majority_vote, 0, predictions)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/","title":"Model class","text":""},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass","title":"<code>ModelClass</code>","text":"<p>Parent class for the models</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>class ModelClass:\n'''Parent class for the models'''\n_default_name = 'none'\n# Not implemented :\n# -&gt; fit\n# -&gt; predict\n# -&gt; predict_proba\ndef __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None, x_col: Union[str, int, None] = None,\ny_col: Union[str, int, list, None] = None, level_save: str = 'HIGH', multi_label: bool = False, **kwargs) -&gt; None:\n'''Initialization of the parent class.\n        Kwargs:\n            model_dir (str): Folder where to save the model\n                If None, creates a directory based on the model's name and the date (most common usage)\n            model_name (str): The name of the model\n            x_col (str | int): Name of the columns used for the training - x\n            y_col (str | int | list if multi-labels): Name of the model's target column(s) - y\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOW + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n            multi_label (bool): If the classification is multi-labels\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n            NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n        '''\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model name\nself.model_name = self._default_name if model_name is None else model_name\n# Names of the columns used\nself.x_col = x_col\nself.y_col = y_col\n# Model folder\nif model_dir is None:\nself.model_dir = self._get_model_dir()\nelse:\nif not os.path.exists(model_dir):\nos.makedirs(model_dir)\nif not os.path.isdir(model_dir):\nraise NotADirectoryError(f\"{model_dir} is not a valid directory\")\nself.model_dir = os.path.abspath(model_dir)\n# List of classes to consider (set on fit)\nself.list_classes: Optional[List[Any]] = None\nself.dict_classes: Optional[Dict[Any, Any]] = None\n# Multi-labels ?\nself.multi_label: bool = multi_label\n# Other options\nself.level_save = level_save\n# is trained ?\nself.trained = False\nself.nb_fit = 0\n# Configuration dict. to be logged. Set on save.\nself.json_dict: Dict[Any, Any] = {}\ndef fit(self, x_train, y_train, **kwargs) -&gt; None:\n'''Trains the model\n        Args:\n            x_train (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_features]\n        '''\nraise NotImplementedError(\"'fit' needs to be overridden\")\n@utils.data_agnostic_str_to_list\ndef predict(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predictions on the test set\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nraise NotImplementedError(\"'predict' needs to be overridden\")\n@utils.data_agnostic_str_to_list\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nraise NotImplementedError(\"'predict_proba' needs to be overridden\")\n@utils.trained_needed\ndef predict_with_proba(self, x_test) -&gt; Tuple[np.ndarray, np.ndarray]:\n'''Predicts on the test set with probabilities\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            predicted_class (np.ndarray): The predicted classes, shape = [n_samples, n_classes]\n            predicted_proba (np.ndarray): The predicted probabilities for each class, shape = [n_samples, n_classes]\n        '''\n# Process\npredicted_proba = self.predict(x_test, return_proba=True)\npredicted_class = self.get_classes_from_proba(predicted_proba)\nreturn predicted_class, predicted_proba\n@utils.trained_needed\ndef get_predict_position(self, x_test, y_true) -&gt; np.ndarray:\n'''Gets the order of predictions of y_true.\n        Positions start at 1 (not 0)\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n            y_true (?): Array-like, shape = [n_samples, n_features]\n        Raises:\n            ValueError: Not available in multi-labels case\n        Returns:\n            np.ndarray: Array, shape = [n_samples]\n        '''\nif self.multi_label:\nraise ValueError(\"The method 'get_predict_position'is unavailable in the multi-labels case\")\n# Process\n# Cast en pd.Series\ny_true = pd.Series(y_true)\n# Get predicted proba\npredicted_proba = self.predict(x_test, return_proba=True)\n# Get position\norder = predicted_proba.argsort()\nranks = len(self.list_classes) - order.argsort()  # type: ignore\ndf_probas = pd.DataFrame(ranks, columns=self.list_classes)  # type: ignore\npredict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\nreturn predict_positions\ndef get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n'''Gets the classes from probabilities\n        Args:\n            predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n        Returns:\n            predicted_class (np.ndarray): Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise\n        '''\nif not self.multi_label:\npredicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\nelse:\n# If multi-labels, returns a list of 0 and 1\npredicted_class = np.rint(predicted_proba)  # 1 if x &gt; 0.5 else 0\nreturn predicted_class\ndef get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n'''Gets the Top n predictions from probabilities\n        Args:\n            predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n        kwargs:\n            n (int): Number of classes to return\n        Raises:\n            ValueError: If the number of classes to return is greater than the number of classes of the model\n        Returns:\n            list: top n predictions\n            list: top n probabilities\n        '''\n# TODO: Make this method available with multi-labels tasks\nif self.multi_label:\nraise ValueError(\"The method 'get_top_n_from_proba' is unavailable with multi-labels tasks\")\nif self.list_classes is not None and n &gt; len(self.list_classes):  # type: ignore\nraise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n# Process\nidx = predicted_proba.argsort()[:, -n:][:, ::-1]\ntop_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\ntop_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))  # type: ignore\nreturn top_n, top_n_proba\ndef inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Gets a list of classes from the predictions\n        Args:\n            y (?): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n                   OR 1D array shape = [n_classes] (only one prediction)\n        Raises:\n            ValueError: If the size of y does not correspond to the number of classes of the model\n        Returns:\n            List of tuple if multi-labels and several predictions\n            Tuple if multi-labels and one prediction\n            List of classes if mono-label\n        '''\n# If multi-label, get classes in tuple\nif self.multi_label:\n# Cast to np array\nif not isinstance(y, np.ndarray):\ny = np.array(y)\nif y.shape[-1] != len(self.list_classes):  # We consider \"-1\" in order to take care of the case where y is 1D\nraise ValueError(f\"The size of y ({y.shape[-1]}) does not correspond\"\nf\" to the number of classes of the model : ({len(self.list_classes)})\")\n# Manage 1D array (only one pred)\nif len(y.shape) == 1:\n# TODO : shoudln't we return a list here ?\nreturn tuple(np.array(self.list_classes).compress(y))\n# Several preds\nelse:\nreturn [tuple(np.array(self.list_classes).compress(indicators)) for indicators in y]\n# If mono-label, just cast in list if y is np array\nelse:\nreturn list(y) if isinstance(y, np.ndarray) else y\ndef get_and_save_metrics(self, y_true, y_pred, x=None, series_to_add: Union[List[pd.Series], None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_features]\n            y_pred (?): Array-like, shape = [n_samples, n_features]\n        Kwargs:\n            x (?): Input data - Array-like, shape = [n_samples]\n            series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing the statistics\n        '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif not self.multi_label:\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Save a predictionn file if wanted\nif self.level_save == 'HIGH':\n# Inverse transform\ny_true_df = list(self.inverse_transform(y_true))\ny_pred_df = list(self.inverse_transform(y_pred))\n# Concat in a dataframe\nif x is not None:\ndf = pd.DataFrame({'x': list(x), 'y_true': y_true_df, 'y_pred': y_pred_df})\nelse:\ndf = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n# Add a matched column\ndf.loc[:, 'matched'] = df[['y_true', 'y_pred']].apply(lambda x: 1 if x.y_true == x.y_pred else 0, axis=1)\n# Add some more columns\nif series_to_add is not None:\nfor ser in series_to_add:\ndf[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex\n# Save predictions\nfile_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\ndf.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nif self.multi_label:\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\nfalses = len(y_true) - trues\nacc_tot = trues / len(y_true)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nsupport = list(pd.DataFrame(y_true).sum().values)\nsupport = [_ / sum(support) for _ in support] + [1.0]\nelse:\n# We use 'weighted' even in the mono-label case since there can be several classes !\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.0] * len(self.list_classes) + [1.0]  # type: ignore\nfor i, cl in enumerate(self.list_classes):  # type: ignore\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# Global Statistics\nself.logger.info('-- * * * * * * * * * * * * * * --')\nself.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\nself.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\nself.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\nself.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\nself.logger.info('--------------------------------')\n# Metrics file\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Add metrics depending on mono/multi labels &amp; manage confusion matrices\nlabels = self.list_classes\nlog_stats = len(labels) &lt; 50  # type: ignore\nif self.multi_label:\n# Details per category\nmcm = multilabel_confusion_matrix(y_true, y_pred)\nfor i, label in enumerate(labels):  # type: ignore\nc_mat = mcm[i]\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat, label, log_info=log_stats), ignore_index=True)\n# Plot individual confusion matrix if level_save &gt; LOW\nif self.level_save in ['MEDIUM', 'HIGH']:\nnone_class = 'not_' + label\ntmp_label = re.sub(r',|:|\\s', '_', label)\nself._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\nnormalized=False, subdir=type_data)\nself._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\nnormalized=True, subdir=type_data)\nelse:\n# Plot confusion matrices if level_save &gt; LOW\nif self.level_save in ['MEDIUM', 'HIGH']:\nif len(labels) &gt; 50:\nself.logger.warning(\nf\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n\"Heavy chances of slowness/display bugs/crashes...\\n\"\n\"SKIP the plots\"\n)\nelse:\n# Global stats\nc_mat = confusion_matrix(y_true, y_pred, labels=labels)\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)  # type: ignore\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)  # type: ignore\n# Get stats per class\nfor label in labels:  # type: ignore\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Save .csv\nfile_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\ndf_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n# Save accuracy\nacc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\nwith open(acc_path, 'w'):\npass\nreturn df_stats\ndef get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on mono-label predictions\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_features]\n            y_pred (?): Array-like, shape = [n_samples, n_features]\n        Raises:\n            ValueError: If not in mono-label mode\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\nif self.multi_label:\nraise ValueError(\"The method get_metrics_simple_monolabel only works for the mono-label case\")\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.] * len(self.list_classes) + [1.0]\nfor i, cl in enumerate(self.list_classes):\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# DataFrame metrics\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Get stats per class\nlabels = self.list_classes\nfor label in labels:\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Return dataframe\nreturn df_stats\ndef get_metrics_simple_multilabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on multi-label predictions\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_features]\n            y_pred (?): Array-like, shape = [n_samples, n_features]\n        Raises:\n            ValueError: If not with multi-labels tasks\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\nif not self.multi_label:\nraise ValueError(\"The method get_metrics_simple_multilabel only works for multi-labels cases\")\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\nfalses = len(y_true) - trues\nacc_tot = trues / len(y_true)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nsupport = list(pd.DataFrame(y_true).sum().values)\nsupport = [_ / sum(support) for _ in support] + [1.0]\n# DataFrame metrics\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Add metrics\nlabels = self.list_classes\n# Details per category\nmcm = multilabel_confusion_matrix(y_true, y_pred)\nfor i, label in enumerate(labels):\nc_mat = mcm[i]\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Return dataframe\nreturn df_stats\ndef _update_info_from_c_mat(self, c_mat: np.ndarray, label: str, log_info: bool = True) -&gt; dict:\n'''Updates a dataframe for the method get_and_save_metrics, given a confusion matrix\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            label (str): Label to use\n        Kwargs:\n            log_info (bool): If the statistics must be logged\n        Returns:\n            dict: Dictionary with the information for the update of the dataframe\n        '''\n# Extract all needed info from c_mat\ntrue_negative = c_mat[0][0]\ntrue_positive = c_mat[1][1]\nfalse_negative = c_mat[1][0]\nfalse_positive = c_mat[0][1]\ncondition_positive = false_negative + true_positive\ncondition_negative = false_positive + true_negative\npredicted_positive = false_positive + true_positive\npredicted_negative = false_negative + true_negative\ntrues_cat = true_negative + true_positive\nfalses_cat = false_negative + false_positive\naccuracy = (true_negative + true_positive) / (true_negative + true_positive + false_negative + false_positive)\nprecision = 0 if predicted_positive == 0 else true_positive / predicted_positive\nrecall = 0 if condition_positive == 0 else true_positive / condition_positive\nf1 = 0 if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n# Display some info\nif log_info:\nself.logger.info(\nf\"F1-score: {round(f1, 5)}  \\t Precision: {round(100 * precision, 2)}% \\t\"\nf\"Recall: {round(100 * recall, 2)}% \\t Trues: {trues_cat} \\t Falses: {falses_cat} \\t\\t --- {label} \"\n)\n# Return result\nreturn {\n'Label': f'{label}',\n'F1-Score': f1,\n'Accuracy': accuracy,\n'Precision': precision,\n'Recall': recall,\n'Trues': trues_cat,\n'Falses': falses_cat,\n'True positive': true_positive,\n'True negative': true_negative,\n'False positive': false_positive,\n'False negative': false_negative,\n'Condition positive': condition_positive,\n'Condition negative': condition_negative,\n'Predicted positive': predicted_positive,\n'Predicted negative': predicted_negative,\n}\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Manage paths\npkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\nconf_path = os.path.join(self.model_dir, \"configurations.json\")\n# Save the model if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nwith open(pkl_path, 'wb') as f:\npickle.dump(self, f)\n# Save configuration JSON\njson_dict = {\n'maintainers': 'Agence DataServices',\n'gabarit_version': '1.2.5-dev-local',\n'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n'package_version': utils.get_package_version(),\n'model_name': self.model_name,\n'model_dir': self.model_dir,\n'trained': self.trained,\n'nb_fit': self.nb_fit,\n'list_classes': self.list_classes,\n'dict_classes': self.dict_classes,\n'x_col': self.x_col,\n'y_col': self.y_col,\n'multi_label': self.multi_label,\n'level_save': self.level_save,\n'librairie': None,\n}\n# Merge json_data if not None\nif json_data is not None:\n# Priority given to json_data !\njson_dict = {**json_dict, **json_data}\n# Add conf to attributes\nself.json_dict = json_dict\n# Save conf\nwith open(conf_path, 'w', encoding='utf-8') as json_file:\njson.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n# Now, save a proprietes file for the model upload\nself._save_upload_properties(json_dict)\ndef _save_upload_properties(self, json_dict: Union[dict, None] = None) -&gt; None:\n'''Prepares a configuration file for a future export (e.g on an artifactory)\n        Kwargs:\n            json_dict: Configurations to save\n        '''\nif json_dict is None:\njson_dict = {}\n# Manage paths\nproprietes_path = os.path.join(self.model_dir, \"proprietes.json\")\nvanilla_model_upload_instructions = os.path.join(utils.get_ressources_path(), 'model_upload_instructions.md')\nspecific_model_upload_instructions = os.path.join(self.model_dir, \"model_upload_instructions.md\")\n# First, we define a list of \"allowed\" properties\nallowed_properties = [\"maintainers\", \"gabarit_version\", \"date\", \"package_version\", \"model_name\", \"list_classes\",\n\"librairie\", \"fit_time\"]\n# Now we filter these properties\nfinal_dict = {k: v for k, v in json_dict.items() if k in allowed_properties}\n# Save\nwith open(proprietes_path, 'w', encoding='utf-8') as f:\njson.dump(final_dict, f, indent=4, cls=utils.NpEncoder)\n# Add instructions to upload a model to a storage solution (e.g. Artifactory)\nwith open(vanilla_model_upload_instructions, 'r', encoding='utf-8') as f:\ncontent = f.read()\n# TODO: to be improved\nnew_content = content.replace('model_dir_path_identifier', os.path.abspath(self.model_dir))\nwith open(specific_model_upload_instructions, 'w', encoding='utf-8') as f:\nf.write(new_content)\ndef _plot_confusion_matrix(self, c_mat: np.ndarray, labels: list, type_data: str = '',\nnormalized: bool = False, subdir: Union[str, None] = None) -&gt; None:\n'''Plots a confusion matrix\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            labels (list): Labels to plot\n        Kwargs:\n            type_data (str): Type of dataset (validation, test, ...)\n            normalized (bool): If the confusion matrix should be normalized\n            subdir (str): Sub-directory for writing the plot\n        '''\n# Get title\nif normalized:\ntitle = f\"Normalized confusion matrix{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\nelse:\ntitle = f\"Confusion matrix, without normalization{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\n# Init. plot\nwidth = round(10 + 0.5 * len(c_mat))\nheight = round(4 / 5 * width)\nfig, ax = plt.subplots(figsize=(width, height))\n# Plot\nif normalized:\nc_mat = c_mat.astype('float') / c_mat.sum(axis=1)[:, np.newaxis]\nsns.heatmap(c_mat, annot=True, fmt=\".2f\", cmap=plt.cm.Blues, ax=ax)\nelse:\nsns.heatmap(c_mat, annot=True, fmt=\"d\", cmap=plt.cm.Blues, ax=ax)\n# labels, title and ticks\nax.set_xlabel('Predicted classes', fontsize=height * 2)\nax.set_ylabel('Real classes', fontsize=height * 2)\nax.set_title(title, fontsize=width * 2)\nax.xaxis.set_ticklabels(labels)\nax.yaxis.set_ticklabels(labels)\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\nplt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\nplt.tight_layout()\n# Save\nplots_path = os.path.join(self.model_dir, 'plots')\nif subdir is not None:  # Ajout subdir\nplots_path = os.path.join(plots_path, subdir)\nfile_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}confusion_matrix{'_normalized' if normalized else ''}.png\"\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\nplt.savefig(os.path.join(plots_path, file_name))\n# Close figures\nplt.close('all')\ndef _get_model_dir(self) -&gt; str:\n'''Gets a folder where to save the model\n        Returns:\n            str: Path to the folder\n        '''\nmodels_dir = utils.get_models_path()\nsubfolder = os.path.join(models_dir, self.model_name)\nfolder_name = datetime.now().strftime(f\"{self.model_name}_%Y_%m_%d-%H_%M_%S\")\nmodel_dir = os.path.join(subfolder, folder_name)\nif os.path.isdir(model_dir):\ntime.sleep(1)  # Wait 1 second so that the 'date' changes...\nreturn self._get_model_dir()  # Get new directory name\nelse:\nos.makedirs(model_dir)\nreturn model_dir\ndef display_if_gpu_activated(self) -&gt; None:\n'''Displays if a GPU is being used'''\nif self._is_gpu_activated():\nself.logger.info(\"GPU activated\")\ndef _is_gpu_activated(self) -&gt; bool:\n'''Checks if we use a GPU\n        Returns:\n            bool: whether GPU is available or not\n        '''\n# By default, no GPU\nreturn False\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.__init__","title":"<code>__init__(model_dir=None, model_name=None, x_col=None, y_col=None, level_save='HIGH', multi_label=False, **kwargs)</code>","text":"<p>Initialization of the parent class.</p> Kwargs <p>model_dir (str): Folder where to save the model     If None, creates a directory based on the model's name and the date (most common usage) model_name (str): The name of the model x_col (str | int): Name of the columns used for the training - x y_col (str | int | list if multi-labels): Name of the model's target column(s) - y level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOW + hdf5 + pkl + plots     HIGH: MEDIUM + predictions multi_label (bool): If the classification is multi-labels</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])</p> <code>NotADirectoryError</code> <p>If a provided model directory is not a directory (i.e. it's a file)</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None, x_col: Union[str, int, None] = None,\ny_col: Union[str, int, list, None] = None, level_save: str = 'HIGH', multi_label: bool = False, **kwargs) -&gt; None:\n'''Initialization of the parent class.\n    Kwargs:\n        model_dir (str): Folder where to save the model\n            If None, creates a directory based on the model's name and the date (most common usage)\n        model_name (str): The name of the model\n        x_col (str | int): Name of the columns used for the training - x\n        y_col (str | int | list if multi-labels): Name of the model's target column(s) - y\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOW + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n        multi_label (bool): If the classification is multi-labels\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n    '''\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model name\nself.model_name = self._default_name if model_name is None else model_name\n# Names of the columns used\nself.x_col = x_col\nself.y_col = y_col\n# Model folder\nif model_dir is None:\nself.model_dir = self._get_model_dir()\nelse:\nif not os.path.exists(model_dir):\nos.makedirs(model_dir)\nif not os.path.isdir(model_dir):\nraise NotADirectoryError(f\"{model_dir} is not a valid directory\")\nself.model_dir = os.path.abspath(model_dir)\n# List of classes to consider (set on fit)\nself.list_classes: Optional[List[Any]] = None\nself.dict_classes: Optional[Dict[Any, Any]] = None\n# Multi-labels ?\nself.multi_label: bool = multi_label\n# Other options\nself.level_save = level_save\n# is trained ?\nself.trained = False\nself.nb_fit = 0\n# Configuration dict. to be logged. Set on save.\nself.json_dict: Dict[Any, Any] = {}\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.display_if_gpu_activated","title":"<code>display_if_gpu_activated()</code>","text":"<p>Displays if a GPU is being used</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def display_if_gpu_activated(self) -&gt; None:\n'''Displays if a GPU is being used'''\nif self._is_gpu_activated():\nself.logger.info(\"GPU activated\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.fit","title":"<code>fit(x_train, y_train, **kwargs)</code>","text":"<p>Trains the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def fit(self, x_train, y_train, **kwargs) -&gt; None:\n'''Trains the model\n    Args:\n        x_train (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_features]\n    '''\nraise NotImplementedError(\"'fit' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, x=None, series_to_add=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required Kwargs <p>x (?): Input data - Array-like, shape = [n_samples] series_to_add (list): List of pd.Series to add to the dataframe type_data (str): Type of dataset (validation, test, ...) <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing the statistics</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, x=None, series_to_add: Union[List[pd.Series], None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_features]\n        y_pred (?): Array-like, shape = [n_samples, n_features]\n    Kwargs:\n        x (?): Input data - Array-like, shape = [n_samples]\n        series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing the statistics\n    '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif not self.multi_label:\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Save a predictionn file if wanted\nif self.level_save == 'HIGH':\n# Inverse transform\ny_true_df = list(self.inverse_transform(y_true))\ny_pred_df = list(self.inverse_transform(y_pred))\n# Concat in a dataframe\nif x is not None:\ndf = pd.DataFrame({'x': list(x), 'y_true': y_true_df, 'y_pred': y_pred_df})\nelse:\ndf = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n# Add a matched column\ndf.loc[:, 'matched'] = df[['y_true', 'y_pred']].apply(lambda x: 1 if x.y_true == x.y_pred else 0, axis=1)\n# Add some more columns\nif series_to_add is not None:\nfor ser in series_to_add:\ndf[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex\n# Save predictions\nfile_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\ndf.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nif self.multi_label:\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\nfalses = len(y_true) - trues\nacc_tot = trues / len(y_true)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nsupport = list(pd.DataFrame(y_true).sum().values)\nsupport = [_ / sum(support) for _ in support] + [1.0]\nelse:\n# We use 'weighted' even in the mono-label case since there can be several classes !\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.0] * len(self.list_classes) + [1.0]  # type: ignore\nfor i, cl in enumerate(self.list_classes):  # type: ignore\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# Global Statistics\nself.logger.info('-- * * * * * * * * * * * * * * --')\nself.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\nself.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\nself.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\nself.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\nself.logger.info('--------------------------------')\n# Metrics file\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Add metrics depending on mono/multi labels &amp; manage confusion matrices\nlabels = self.list_classes\nlog_stats = len(labels) &lt; 50  # type: ignore\nif self.multi_label:\n# Details per category\nmcm = multilabel_confusion_matrix(y_true, y_pred)\nfor i, label in enumerate(labels):  # type: ignore\nc_mat = mcm[i]\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat, label, log_info=log_stats), ignore_index=True)\n# Plot individual confusion matrix if level_save &gt; LOW\nif self.level_save in ['MEDIUM', 'HIGH']:\nnone_class = 'not_' + label\ntmp_label = re.sub(r',|:|\\s', '_', label)\nself._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\nnormalized=False, subdir=type_data)\nself._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\nnormalized=True, subdir=type_data)\nelse:\n# Plot confusion matrices if level_save &gt; LOW\nif self.level_save in ['MEDIUM', 'HIGH']:\nif len(labels) &gt; 50:\nself.logger.warning(\nf\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n\"Heavy chances of slowness/display bugs/crashes...\\n\"\n\"SKIP the plots\"\n)\nelse:\n# Global stats\nc_mat = confusion_matrix(y_true, y_pred, labels=labels)\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)  # type: ignore\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)  # type: ignore\n# Get stats per class\nfor label in labels:  # type: ignore\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Save .csv\nfile_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\ndf_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n# Save accuracy\nacc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\nwith open(acc_path, 'w'):\npass\nreturn df_stats\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_classes_from_proba","title":"<code>get_classes_from_proba(predicted_proba)</code>","text":"<p>Gets the classes from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>np.ndarray</code> <p>The probabilities predicted by the model, shape = [n_samples, n_classes]</p> required <p>Returns:</p> Name Type Description <code>predicted_class</code> <code>np.ndarray</code> <p>Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n'''Gets the classes from probabilities\n    Args:\n        predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n    Returns:\n        predicted_class (np.ndarray): Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise\n    '''\nif not self.multi_label:\npredicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\nelse:\n# If multi-labels, returns a list of 0 and 1\npredicted_class = np.rint(predicted_proba)  # 1 if x &gt; 0.5 else 0\nreturn predicted_class\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_metrics_simple_monolabel","title":"<code>get_metrics_simple_monolabel(y_true, y_pred)</code>","text":"<p>Gets metrics on mono-label predictions Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If not in mono-label mode</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on mono-label predictions\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_features]\n        y_pred (?): Array-like, shape = [n_samples, n_features]\n    Raises:\n        ValueError: If not in mono-label mode\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\nif self.multi_label:\nraise ValueError(\"The method get_metrics_simple_monolabel only works for the mono-label case\")\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.] * len(self.list_classes) + [1.0]\nfor i, cl in enumerate(self.list_classes):\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# DataFrame metrics\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Get stats per class\nlabels = self.list_classes\nfor label in labels:\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Return dataframe\nreturn df_stats\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_metrics_simple_multilabel","title":"<code>get_metrics_simple_multilabel(y_true, y_pred)</code>","text":"<p>Gets metrics on multi-label predictions Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If not with multi-labels tasks</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def get_metrics_simple_multilabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on multi-label predictions\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_features]\n        y_pred (?): Array-like, shape = [n_samples, n_features]\n    Raises:\n        ValueError: If not with multi-labels tasks\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\nif not self.multi_label:\nraise ValueError(\"The method get_metrics_simple_multilabel only works for multi-labels cases\")\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\nfalses = len(y_true) - trues\nacc_tot = trues / len(y_true)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nsupport = list(pd.DataFrame(y_true).sum().values)\nsupport = [_ / sum(support) for _ in support] + [1.0]\n# DataFrame metrics\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Add metrics\nlabels = self.list_classes\n# Details per category\nmcm = multilabel_confusion_matrix(y_true, y_pred)\nfor i, label in enumerate(labels):\nc_mat = mcm[i]\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Return dataframe\nreturn df_stats\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_predict_position","title":"<code>get_predict_position(x_test, y_true)</code>","text":"<p>Gets the order of predictions of y_true. Positions start at 1 (not 0)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Not available in multi-labels case</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: Array, shape = [n_samples]</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@utils.trained_needed\ndef get_predict_position(self, x_test, y_true) -&gt; np.ndarray:\n'''Gets the order of predictions of y_true.\n    Positions start at 1 (not 0)\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        y_true (?): Array-like, shape = [n_samples, n_features]\n    Raises:\n        ValueError: Not available in multi-labels case\n    Returns:\n        np.ndarray: Array, shape = [n_samples]\n    '''\nif self.multi_label:\nraise ValueError(\"The method 'get_predict_position'is unavailable in the multi-labels case\")\n# Process\n# Cast en pd.Series\ny_true = pd.Series(y_true)\n# Get predicted proba\npredicted_proba = self.predict(x_test, return_proba=True)\n# Get position\norder = predicted_proba.argsort()\nranks = len(self.list_classes) - order.argsort()  # type: ignore\ndf_probas = pd.DataFrame(ranks, columns=self.list_classes)  # type: ignore\npredict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\nreturn predict_positions\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.get_top_n_from_proba","title":"<code>get_top_n_from_proba(predicted_proba, n=5)</code>","text":"<p>Gets the Top n predictions from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>np.ndarray</code> <p>The probabilities predicted by the model, shape = [n_samples, n_classes]</p> required kwargs <p>n (int): Number of classes to return</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of classes to return is greater than the number of classes of the model</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>top n predictions</p> <code>list</code> <code>list</code> <p>top n probabilities</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n'''Gets the Top n predictions from probabilities\n    Args:\n        predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n    kwargs:\n        n (int): Number of classes to return\n    Raises:\n        ValueError: If the number of classes to return is greater than the number of classes of the model\n    Returns:\n        list: top n predictions\n        list: top n probabilities\n    '''\n# TODO: Make this method available with multi-labels tasks\nif self.multi_label:\nraise ValueError(\"The method 'get_top_n_from_proba' is unavailable with multi-labels tasks\")\nif self.list_classes is not None and n &gt; len(self.list_classes):  # type: ignore\nraise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n# Process\nidx = predicted_proba.argsort()[:, -n:][:, ::-1]\ntop_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\ntop_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))  # type: ignore\nreturn top_n, top_n_proba\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets a list of classes from the predictions</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>?</code> <p>Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s    OR 1D array shape = [n_classes] (only one prediction)</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the size of y does not correspond to the number of classes of the model</p> <p>Returns:</p> Type Description <code>Union[list, tuple]</code> <p>List of tuple if multi-labels and several predictions</p> <code>Union[list, tuple]</code> <p>Tuple if multi-labels and one prediction</p> <code>Union[list, tuple]</code> <p>List of classes if mono-label</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Gets a list of classes from the predictions\n    Args:\n        y (?): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n               OR 1D array shape = [n_classes] (only one prediction)\n    Raises:\n        ValueError: If the size of y does not correspond to the number of classes of the model\n    Returns:\n        List of tuple if multi-labels and several predictions\n        Tuple if multi-labels and one prediction\n        List of classes if mono-label\n    '''\n# If multi-label, get classes in tuple\nif self.multi_label:\n# Cast to np array\nif not isinstance(y, np.ndarray):\ny = np.array(y)\nif y.shape[-1] != len(self.list_classes):  # We consider \"-1\" in order to take care of the case where y is 1D\nraise ValueError(f\"The size of y ({y.shape[-1]}) does not correspond\"\nf\" to the number of classes of the model : ({len(self.list_classes)})\")\n# Manage 1D array (only one pred)\nif len(y.shape) == 1:\n# TODO : shoudln't we return a list here ?\nreturn tuple(np.array(self.list_classes).compress(y))\n# Several preds\nelse:\nreturn [tuple(np.array(self.list_classes).compress(indicators)) for indicators in y]\n# If mono-label, just cast in list if y is np array\nelse:\nreturn list(y) if isinstance(y, np.ndarray) else y\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.predict","title":"<code>predict(x_test, **kwargs)</code>","text":"<p>Predictions on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@utils.data_agnostic_str_to_list\ndef predict(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predictions on the test set\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\nraise NotImplementedError(\"'predict' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@utils.data_agnostic_str_to_list\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\nraise NotImplementedError(\"'predict_proba' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.predict_with_proba","title":"<code>predict_with_proba(x_test)</code>","text":"<p>Predicts on the test set with probabilities</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:</p> Name Type Description <code>predicted_class</code> <code>np.ndarray</code> <p>The predicted classes, shape = [n_samples, n_classes]</p> <code>predicted_proba</code> <code>np.ndarray</code> <p>The predicted probabilities for each class, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>@utils.trained_needed\ndef predict_with_proba(self, x_test) -&gt; Tuple[np.ndarray, np.ndarray]:\n'''Predicts on the test set with probabilities\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        predicted_class (np.ndarray): The predicted classes, shape = [n_samples, n_classes]\n        predicted_proba (np.ndarray): The predicted probabilities for each class, shape = [n_samples, n_classes]\n    '''\n# Process\npredicted_proba = self.predict(x_test, return_proba=True)\npredicted_class = self.get_classes_from_proba(predicted_proba)\nreturn predicted_class, predicted_proba\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_class/#template_nlp.models_training.model_class.ModelClass.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/model_class.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Manage paths\npkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\nconf_path = os.path.join(self.model_dir, \"configurations.json\")\n# Save the model if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nwith open(pkl_path, 'wb') as f:\npickle.dump(self, f)\n# Save configuration JSON\njson_dict = {\n'maintainers': 'Agence DataServices',\n'gabarit_version': '1.2.5-dev-local',\n'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n'package_version': utils.get_package_version(),\n'model_name': self.model_name,\n'model_dir': self.model_dir,\n'trained': self.trained,\n'nb_fit': self.nb_fit,\n'list_classes': self.list_classes,\n'dict_classes': self.dict_classes,\n'x_col': self.x_col,\n'y_col': self.y_col,\n'multi_label': self.multi_label,\n'level_save': self.level_save,\n'librairie': None,\n}\n# Merge json_data if not None\nif json_data is not None:\n# Priority given to json_data !\njson_dict = {**json_dict, **json_data}\n# Add conf to attributes\nself.json_dict = json_dict\n# Save conf\nwith open(conf_path, 'w', encoding='utf-8') as json_file:\njson.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n# Now, save a proprietes file for the model upload\nself._save_upload_properties(json_dict)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/","title":"Model huggingface","text":""},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace","title":"<code>ModelHuggingFace</code>","text":"<p>         Bases: <code>ModelClass</code></p> <p>Generic model for Huggingface NN</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>class ModelHuggingFace(ModelClass):\n'''Generic model for Huggingface NN'''\n_default_name = 'model_huggingface'\n# TODO: perhaps it would be smarter to have this class behaving as the abstract class for all the model types\n# implemented on the HF hub and to create model specific subclasses.\n# =&gt; might change it as use cases grow\ndef __init__(self, batch_size: int = 8, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\ntransformer_name: str = 'Geotrend/distilbert-base-fr-cased', transformer_params: Union[dict, None] = None,\ntrainer_params: Union[dict, None] = None, model_max_length: int = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n        Kwargs:\n            batch_size (int): Batch size\n            epochs (int): Number of epochs\n            validation_split (float): Percentage for the validation set split\n                Only used if no input validation set when fitting\n            patience (int): Early stopping patience\n            transformer_name (str) : The name of the transformer backbone to use\n            transformer_params (dict): Parameters used by the Transformer model.\n                The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n                This parameter was initially added in order to do an hyperparameters search\n            trainer_params (dict): A set of parameters to be use by the Trainer. It is recommended to use the default params (leave this empty).\n        '''\n# TODO: learning rate should be an attribute !\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Param. model\nself.batch_size = batch_size\nself.epochs = epochs\nself.validation_split = validation_split\nself.patience = patience\nself.transformer_name = transformer_name\nself.model_max_length = model_max_length\n# transformer_params has no use as of 14/12/2022\n# we still leave it for compatibility with keras models and future usage\nself.transformer_params = transformer_params\n# Trainer params\nif trainer_params is None:\ntrainer_params = {\n'output_dir': self.model_dir,\n'learning_rate': 2e-5,\n'per_device_train_batch_size': self.batch_size,\n'per_device_eval_batch_size': self.batch_size,\n'num_train_epochs': self.epochs,\n'weight_decay': 0.0,\n'evaluation_strategy': 'epoch',\n'save_strategy': 'epoch',\n'logging_strategy': 'epoch',\n'save_total_limit': 1,\n'load_best_model_at_end': True\n}\n# TODO: maybe we should keep the default dict &amp; only add/replace keys in provided dict ?\nself.trainer_params = trainer_params\n# Model set on fit or on reload\nself.model: Any = None\nself.pipe: Any = None  # Set on first predict\n# Tokenizer set on fit or on reload\nself.tokenizer: Any = None\ndef fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Fits the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n            y_valid (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            with_shuffle (bool): If x, y must be shuffled before fitting\n                Experimental: We must verify if it works as intended depending on the formats of x and y\n                This should be used if y is not shuffled as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            AssertionError: If different classes when comparing an already fitted model and a new dataset\n        '''\n##############################################\n# Manage retrain\n##############################################\n# If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n# And we save the old conf\nif self.trained:\n# Get src files to save\nsrc_files = [os.path.join(self.model_dir, \"configurations.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\nsrc_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Change model dir\nself.model_dir = self._get_model_dir()\n# Get dst files\ndst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\ndst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Copies\nfor src, dst in zip(src_files, dst_files):\ntry:\nshutil.copyfile(src, dst)\nexcept Exception as e:\nself.logger.error(f\"Impossible to copy {src} to {dst}\")\nself.logger.error(\"We still continue ...\")\nself.logger.error(repr(e))\n##############################################\n# Prepare x_train, x_valid, y_train &amp; y_valid\n# Also extract list of classes\n##############################################\n# If not multilabel, transform y_train as dummies (should already be the case for multi-labels)\nif not self.multi_label:\n# If len(array.shape)==2, we flatten the array if the second dimension is useless\nif isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\ny_train = np.ravel(y_train)\nif isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\ny_valid = np.ravel(y_valid)\n# Transformation dummies\ny_train_dummies = pd.get_dummies(y_train)\ny_valid_dummies = pd.get_dummies(y_valid) if y_valid is not None else None\n# Important : get_dummies reorder the columns in alphabetical order\n# Thus, there is no problem if we fit again on a new dataframe with shuffled data\nlist_classes = list(y_train_dummies.columns)\n# FIX: valid test might miss some classes, hence we need to add them back to y_valid_dummies\nif y_valid_dummies is not None and y_train_dummies.shape[1] != y_valid_dummies.shape[1]:\nfor cl in list_classes:\n# Add missing columns\nif cl not in y_valid_dummies.columns:\ny_valid_dummies[cl] = 0\ny_valid_dummies = y_valid_dummies[list_classes]  # Reorder\n# Else keep it as it is\nelse:\ny_train_dummies = y_train\ny_valid_dummies = y_valid\nif hasattr(y_train_dummies, 'columns'):\nlist_classes = list(y_train_dummies.columns)\nelse:\nself.logger.warning(\n\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n)\n# We still create a list of classes in order to be compatible with other functions\nlist_classes = [str(_) for _ in range(pd.DataFrame(y_train_dummies).shape[1])]\n# Set dict_classes based on list classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# Validate classes if already trained, else set them\nif self.trained:\nassert self.list_classes == list_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nassert self.dict_classes == dict_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nelse:\nself.list_classes = list_classes\nself.dict_classes = dict_classes\n# Shuffle x, y if wanted\n# It is advised as validation_split from keras does not shufle the data\n# Hence we might have classes in the validation data that we never met in the training data\nif with_shuffle:\np = np.random.permutation(len(x_train))\nx_train = np.array(x_train)[p]\ny_train_dummies = np.array(y_train_dummies)[p]\n# Else still transform to numpy array\nelse:\nx_train = np.array(x_train)\ny_train_dummies = np.array(y_train_dummies)\n# Also get y_valid_dummies as numpy\ny_valid_dummies = np.array(y_valid_dummies)\n# If no valid set, split train set according to validation_split\nif y_valid is None:\nself.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\np = np.random.permutation(len(x_train))\ncutoff = int(len(p) * self.validation_split)\nx_valid = x_train[p[0:cutoff]]\nx_train = x_train[p[cutoff:]]\ny_valid_dummies = y_train_dummies[p[0:cutoff]]\ny_train_dummies = y_train_dummies[p[cutoff:]]\n##############################################\n# Get model &amp; prepare datasets\n##############################################\n# Get model (if already fitted, _get_model returns instance model)\nself.model = self._get_model(num_labels=y_train_dummies.shape[1])\n# Get tokenizer (if already fitted, _get_tokenizer returns instance tokenizer)\nself.tokenizer = self._get_tokenizer()\n# Preprocess datasets\ntrain_dataset = self._prepare_x_train(x_train, y_train_dummies)\nvalid_dataset = self._prepare_x_valid(x_valid, y_valid_dummies)\n##############################################\n# Fit\n##############################################\n# Fit\ntry:\n# TODO: remove the checkpoints !\n# Prepare trainer\ntrainer = Trainer(\nmodel=self.model,\nargs=TrainingArguments(**self.trainer_params),\ntrain_dataset=train_dataset,\neval_dataset=valid_dataset,\ntokenizer=self.tokenizer,  # Only use for padding, dataset are already preprocessed. Pby not needed as we define a collator.\ndata_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),  # Pad batches\ncompute_metrics=self._compute_metrics_mono_label if not self.multi_label else self._compute_metrics_multi_label,\noptimizers=self._get_optimizers(),\n)\n# Add callbacks\ntrainer.add_callback(MetricsTrainCallback(trainer))\ntrainer.add_callback(EarlyStoppingCallback(early_stopping_patience=self.patience))\n# Fit\ntrainer.train()\n# Save model &amp; tokenizer\nhf_model_dir = os.path.join(self.model_dir, 'hf_model')\nhf_tokenizer_dir = os.path.join(self.model_dir, 'hf_tokenizer')\ntrainer.model.save_pretrained(save_directory=hf_model_dir)\nself.tokenizer.save_pretrained(save_directory=hf_tokenizer_dir)\n# Remove checkpoint dir if save total limit is set to 1 (no need to keep this as we resave the model)\nif self.trainer_params.get('save_total_limit', None) == 1:\ncheckpoint_dirs = [_ for _ in os.listdir(self.model_dir) if _.startswith('checkpoint-')]\nif len(checkpoint_dirs) == 0:\nself.logger.warning(\"Can't find a checkpoint dir to be removed.\")\nelse:\nfor checkpoint_dir in checkpoint_dirs:\nshutil.rmtree(os.path.join(self.model_dir, checkpoint_dir))\nexcept (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, Exception) as e:\nself.logger.error(repr(e))\nraise RuntimeError(\"Error during model training\")\n# Print accuracy &amp; loss if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\n# Plot accuracy\nfit_history = trainer.state.log_history\nself._plot_metrics_and_loss(fit_history)\n# Reload best model ?\n# Default trainer has load_best_model_at_end = True\n# Hence we consider the best model is already reloaded\n# Set trained\nself.trained = True\nself.nb_fit += 1\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples]\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Predict probas\npredicted_proba = self.predict_proba(x_test)\n# We return the probabilities if wanted\nif return_proba:\nreturn predicted_proba\n# Finally, we get the classes predictions\nreturn self.get_classes_from_proba(predicted_proba)\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Does not work with np array nor pandas Series\nif type(x_test) in [np.ndarray, pd.Series]:\nx_test = x_test.tolist()\n# Prepare predict\nif self.model.training:\nself.model.eval()\nif self.pipe is None:\n# Set model on gpu if available\nself.model = self.model.to('cuda') if self._is_gpu_activated() else self.model.to('cpu')\ndevice = 0 if self._is_gpu_activated() else -1\nself.pipe = TextClassificationPipeline(model=self.model, tokenizer=self.tokenizer, return_all_scores=True, device=device)\n# Predict\n# As we are using the pipeline, we do not need to prepare x_test (done inside the pipeline)\n# However, we still need to set the tokenizer params (truncate &amp; padding)\ntokenizer_kwargs = {'padding': False, 'truncation': True}\nresults = np.array(self.pipe(x_test, **tokenizer_kwargs))\npredicted_proba = np.array([[x['score'] for x in x] for x in results])\nreturn predicted_proba\ndef _prepare_x_train(self, x_train, y_train_dummies) -&gt; Dataset:\n'''Prepares the input data for the model - train\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (datasets.Dataset): Prepared dataset\n        '''\n# TMP FIX : https://github.com/OSS-Pole-Emploi/gabarit/issues/98\n# We can't call this function if the tokenizer is not set. We will pby change this object to a property.\n# This isn't really a problem as this function should not be called outside the class &amp; tokenizer is set in the fit function.\nif self.tokenizer is None:\nself.tokenizer = self._get_tokenizer()\n# Check np format (should be the case if using fit)\nif not type(x_train) == np.ndarray:\nx_train = np.array(x_train)\nif not type(y_train_dummies) == np.ndarray:\ny_train_dummies = np.array(y_train_dummies)\n# It seems that HF does not manage dummies targets for non multilabel\nif not self.multi_label:\nlabels = np.argmax(y_train_dummies, axis=-1).astype(int).tolist()\nelse:\nlabels = y_train_dummies.astype(np.float32).tolist()\nreturn Dataset.from_dict({'text': x_train.tolist(), 'label': labels}).map(self._tokenize_function, batched=True)\ndef _prepare_x_valid(self, x_valid, y_valid_dummies) -&gt; Dataset:\n'''Prepares the input data for the model - valid\n        Args:\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (datasets.Dataset): Prepared dataset\n        '''\n# Same as train (we don't fit any tokenizer)\nreturn self._prepare_x_train(x_valid, y_valid_dummies)\ndef _prepare_x_test(self, x_test) -&gt; Dataset:\n'''Prepares the input data for the model - test\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (datasets.Dataset): Prepared dataset\n        '''\n# Check np format\nif not type(x_test) == np.ndarray:\nx_test = np.array(x_test)\n# /!\\ We don't use it as we are using a TextClassificationPipeline\n# yet we are leaving this here in case we need it later\nreturn Dataset.from_dict({'text': x_test.tolist()}).map(self._tokenize_function, batched=True)\ndef _tokenize_function(self, examples: Batch) -&gt; BatchEncoding:\n'''Tokenizes input data\n        Args:\n            examples (Batch): input data (Dataset Batch)\n        Returns:\n            BatchEncoding: tokenized data\n        '''\n# Padding to False as we will use a Trainer and a DataCollatorWithPadding that will manage padding for us (better limit the memory impact)\n# We leave max_length to None -&gt; backup on model max length\n# https://stackoverflow.com/questions/74657367/how-do-i-know-which-parameters-to-use-with-a-pretrained-tokenizer\nreturn self.tokenizer(examples[\"text\"], padding=False, truncation=True)\ndef _get_model(self, model_path: str = None, num_labels: int = None) -&gt; Any:\n'''Gets a model structure - returns the instance model instead if already defined\n        Returns:\n            (Any): a HF model\n        '''\n# Return model if already set\nif self.model is not None:\nreturn self.model\nmodel = AutoModelForSequenceClassification.from_pretrained(\nself.transformer_name if model_path is None else model_path,\nnum_labels=len(self.list_classes) if num_labels is None else num_labels,\nproblem_type=\"multi_label_classification\" if self.multi_label else \"single_label_classification\",\ncache_dir=HF_CACHE_DIR)\n# Set model on gpu if available\nmodel = model.to('cuda') if self._is_gpu_activated() else model.to('cpu')\nreturn model\ndef _get_tokenizer(self, model_path: str = None) -&gt; PreTrainedTokenizer:\n'''Gets a tokenizer\n        Returns:\n            (PreTrainedTokenizer): a HF tokenizer\n        '''\n# Return tokenizer if already set\nif self.tokenizer is not None:\nreturn self.tokenizer\ntokenizer = AutoTokenizer.from_pretrained(self.transformer_name if model_path is None else model_path,\ncache_dir=HF_CACHE_DIR)\nif self.model_max_length:\ntokenizer.model_max_length = self.model_max_length\n# If the model name is not in tokenizer.max_model_input_sizes it is likely that the attribute model_max_length is not well\n# initialized. If it is set to VERY_LARGE_INTEGER we warn the user that there is a risk of errors with long sequences\nelif self.transformer_name not in tokenizer.max_model_input_sizes and tokenizer.model_max_length == VERY_LARGE_INTEGER:\nself.logger.warning(f\"The model name '{self.transformer_name}' is not present in tokenizer.max_model_input_sizes : '{tokenizer.max_model_input_sizes}' \"\nf\"and tokenizer.model_max_length is set to VERY_LARGE_INTEGER. You may encounter errors with long sequences. \"\nf\"see. https://huggingface.co/transformers/v4.0.1/main_classes/tokenizer.html?highlight=very_large_integer#transformers.PreTrainedTokenizer\")\nreturn tokenizer\ndef _get_optimizers(self) -&gt; Tuple[Any, Any]:\n'''Fonction to define the Trainer optimizers\n           -&gt; per default return (None, None), i.e. default optimizers (cf HF Trainer doc)\n        Returns:\n            Tuple (Optimizer, LambdaLR): An optimizer/scheduler couple\n        '''\n# e.g.\n# Here, your custom Optimizer / scheduler couple\n# (check https://huggingface.co/docs/transformers/v4.24.0/en/main_classes/optimizer_schedules)\nreturn (None, None)\ndef _compute_metrics_mono_label(self, eval_pred: EvalPrediction) -&gt; dict:\n'''Computes some metrics for mono label cases\n        Args:\n            eval_pred: predicted &amp; ground truth values to be considered\n        Returns:\n            dict: dictionnary with computed metrics\n        '''\n# Load metrics\nmetric_accuracy = load_metric(hf_metrics.accuracy.__file__)\nmetric_precision = load_metric(hf_metrics.precision.__file__)\nmetric_recall = load_metric(hf_metrics.recall.__file__)\nmetric_f1 = load_metric(hf_metrics.f1.__file__)\n# Get predictions\nlogits, labels = eval_pred\npredictions = np.argmax(logits, axis=-1)\n# Compute metrics\naccuracy = metric_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\nprecision = metric_precision.compute(predictions=predictions, references=labels, average='weighted')[\"precision\"]\nrecall = metric_recall.compute(predictions=predictions, references=labels, average='weighted')[\"recall\"]\nf1 = metric_f1.compute(predictions=predictions, references=labels, average='weighted')[\"f1\"]\n# Return dict of metrics\nreturn {'accuracy': accuracy, 'weighted_precision': precision, 'weighted_recall': recall, 'weighted_f1': f1}\ndef _compute_metrics_multi_label(self, eval_pred: EvalPrediction) -&gt; dict:\n'''Computes some metrics for mono label cases\n        Args:\n            eval_pred: predicted &amp; ground truth values to be considered\n        Returns:\n            dict: dictionnary with computed metrics\n        '''\n# Sigmoid activation (multi_label)\nsigmoid = torch.nn.Sigmoid()\n# Get probas\nlogits, labels = eval_pred\nprobas = sigmoid(torch.Tensor(logits))\n# Get predictions (probas &gt;= 0.5)\npredictions = np.zeros(probas.shape)\npredictions[np.where(probas &gt;= 0.5)] = 1\n# Compute metrics (we can't use HF metrics, it sucks)\naccuracy = accuracy_score(y_true=labels, y_pred=predictions)  # Must be exact match on all labels\nf1 = f1_score(y_true=labels, y_pred=predictions, average='weighted')\nprecision = precision_score(y_true=labels, y_pred=predictions, average='weighted')\nrecall = recall_score(y_true=labels, y_pred=predictions, average='weighted')\n# return as dictionary\nreturn {'accuracy': accuracy, 'weighted_precision': precision, 'weighted_recall': recall, 'weighted_f1': f1}\ndef _plot_metrics_and_loss(self, fit_history) -&gt; None:\n'''Plots TrainOutput, for legacy and compatibility purpose\n        Arguments:\n            fit_history (list) : fit history - actually list of logs\n        '''\n# Manage dir\nplots_path = os.path.join(self.model_dir, 'plots')\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\n# Rework fit_history to better match Keras fit history\nfit_history_dict: Dict[str, list] = {}\nfor log in fit_history:\nfor key, value in log.items():\nif key not in fit_history_dict.keys():\nfit_history_dict[key] = [value]\nelse:\nfit_history_dict[key] += [value]\n# Get a dictionnary of possible metrics/loss plots\nmetrics_dir = {\n'loss': ['Loss', 'loss'],\n'accuracy': ['Accuracy', 'accuracy'],\n'weighted_f1': ['Weighted F1-score', 'weighted_f1_score'],\n'weighted_precision': ['Weighted Precision', 'weighted_precision'],\n'weighted_recall': ['Weighted Recall', 'weighted_recall'],\n}\n# Plot each available metric\nfor metric in metrics_dir.keys():\nif any([f'{dataset}_{metric}' in fit_history_dict.keys() for dataset in ['train_metrics', 'eval']]):\ntitle = metrics_dir[metric][0]\nfilename = metrics_dir[metric][1]\nplt.figure(figsize=(10, 8))\nlegend = []\nfor dataset in ['train_metrics', 'eval']:\nif f'{dataset}_{metric}' in fit_history_dict.keys():\nplt.plot(fit_history_dict[f'{dataset}_{metric}'])\nlegend += ['Train'] if dataset == 'train_metrics' else ['Validation']\nplt.title(f\"Model {title}\")\nplt.ylabel(title)\nplt.xlabel('Epoch')\nplt.legend(legend, loc='upper left')\n# Save\nfilename == f\"{filename}.jpeg\"\nplt.savefig(os.path.join(plots_path, filename))\n# Close figures\nplt.close('all')\n@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'huggingface'\njson_data['batch_size'] = self.batch_size\njson_data['epochs'] = self.epochs\njson_data['validation_split'] = self.validation_split\njson_data['patience'] = self.patience\njson_data['transformer_name'] = self.transformer_name\njson_data['transformer_params'] = self.transformer_params\njson_data['trainer_params'] = self.trainer_params\njson_data['model_max_length'] = self.model_max_length\n# Add model structure if not none\nif self.model is not None:\njson_data['hf_model'] = self.model.__repr__()\nif '_get_model' not in json_data.keys():\njson_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\nif '_get_tokenizer' not in json_data.keys():\njson_data['_get_tokenizer'] = pickle.source.getsourcelines(self._get_tokenizer)[0]\n# Save strategy :\n# - HuggingFace model &amp; tokenizer are already saved in the fit() function\n# - We don't want them in the .pkl as they are heavy &amp; already saved\n# - Also get rid of the pipe (takes too much disk space for nothing),\n#   will be reloaded automatically at first call to predict functions\nhf_model = self.model\nhf_tokenizer = self.tokenizer\nself.model = None\nself.tokenizer = None\nif self.pipe is not None:\nself.pipe = None\nsuper().save(json_data=json_data)\nself.model = hf_model\nself.tokenizer = hf_tokenizer\ndef reload_model(self, model_path: str) -&gt; Any:\n'''Loads a HF model from a directory\n        Args:\n            model_path (str): Path to the directory containing both the model.bin and its conf\n        Returns:\n            ?: HF model\n        '''\n# Loading the model\nhf_model = self._get_model(model_path)\n# Set trained to true if not already true\n# TODO: check if we really need this\nif not self.trained:\nself.trained = True\nself.nb_fit = 1\n# Return\nreturn hf_model\ndef reload_tokenizer(self, tokenizer_path: str) -&gt; Any:\n'''Loads a HF model from a directory\n        Args:\n            tokenizer_path (str): Path to the directory containing the tokenizer files\n        Returns:\n            ?: HF tokenizer\n        '''\n# Loading the tokenizer\nhf_tokenizer = self._get_tokenizer(tokenizer_path)\n# Return\nreturn hf_tokenizer\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hf_model_dir (str): Path to HuggingFace model directory\n            hf_tokenizer_dir (str): Path to HuggingFace tokenizer directory\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hf_model_dir is None\n            ValueError: If hf_tokenizer_dir is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hf_model_dir is not an existing file\n            FileNotFoundError: If the object hf_tokenizer_dir is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhf_model_dir_path = kwargs.get('hf_model_dir_path', None)\nhf_tokenizer_dir_path = kwargs.get('hf_tokenizer_dir_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hf_model_dir_path is None:\nraise ValueError(\"The argument hf_model_dir_path can't be None\")\nif hf_tokenizer_dir_path is None:\nraise ValueError(\"The argument hf_tokenizer_dir_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hf_model_dir_path):\nraise FileNotFoundError(f\"The file {hf_model_dir_path} does not exist\")\nif not os.path.exists(hf_tokenizer_dir_path):\nraise FileNotFoundError(f\"The file {hf_tokenizer_dir_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label',\n'level_save', 'batch_size', 'epochs', 'validation_split', 'patience',\n'transformer_name', 'transformer_params', 'trainer_params', 'model_max_length']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model &amp; tokenizer\nself.model = self._get_model(hf_model_dir_path)\nself.tokenizer = self._get_tokenizer(hf_tokenizer_dir_path)\n# Save hf folders in new folder\nnew_hf_model_dir_path = os.path.join(self.model_dir, 'hf_model')\nnew_hf_tokenizer_dir_path = os.path.join(self.model_dir, 'hf_tokenizer')\nshutil.copytree(hf_model_dir_path, new_hf_model_dir_path)\nshutil.copytree(hf_tokenizer_dir_path, new_hf_tokenizer_dir_path)\ndef _is_gpu_activated(self) -&gt; bool:\n'''Checks if a GPU is used\n        Returns:\n            bool: whether GPU is available or not\n        '''\n# Check for available GPU devices\nreturn torch.cuda.is_available()\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.__init__","title":"<code>__init__(batch_size=8, epochs=99, validation_split=0.2, patience=5, transformer_name='Geotrend/distilbert-base-fr-cased', transformer_params=None, trainer_params=None, model_max_length=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>batch_size (int): Batch size epochs (int): Number of epochs validation_split (float): Percentage for the validation set split     Only used if no input validation set when fitting patience (int): Early stopping patience transformer_name (str) : The name of the transformer backbone to use transformer_params (dict): Parameters used by the Transformer model.     The purpose of this dictionary is for the user to use it as they wants in the _get_model function     This parameter was initially added in order to do an hyperparameters search trainer_params (dict): A set of parameters to be use by the Trainer. It is recommended to use the default params (leave this empty).</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>def __init__(self, batch_size: int = 8, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\ntransformer_name: str = 'Geotrend/distilbert-base-fr-cased', transformer_params: Union[dict, None] = None,\ntrainer_params: Union[dict, None] = None, model_max_length: int = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n    Kwargs:\n        batch_size (int): Batch size\n        epochs (int): Number of epochs\n        validation_split (float): Percentage for the validation set split\n            Only used if no input validation set when fitting\n        patience (int): Early stopping patience\n        transformer_name (str) : The name of the transformer backbone to use\n        transformer_params (dict): Parameters used by the Transformer model.\n            The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n            This parameter was initially added in order to do an hyperparameters search\n        trainer_params (dict): A set of parameters to be use by the Trainer. It is recommended to use the default params (leave this empty).\n    '''\n# TODO: learning rate should be an attribute !\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Param. model\nself.batch_size = batch_size\nself.epochs = epochs\nself.validation_split = validation_split\nself.patience = patience\nself.transformer_name = transformer_name\nself.model_max_length = model_max_length\n# transformer_params has no use as of 14/12/2022\n# we still leave it for compatibility with keras models and future usage\nself.transformer_params = transformer_params\n# Trainer params\nif trainer_params is None:\ntrainer_params = {\n'output_dir': self.model_dir,\n'learning_rate': 2e-5,\n'per_device_train_batch_size': self.batch_size,\n'per_device_eval_batch_size': self.batch_size,\n'num_train_epochs': self.epochs,\n'weight_decay': 0.0,\n'evaluation_strategy': 'epoch',\n'save_strategy': 'epoch',\n'logging_strategy': 'epoch',\n'save_total_limit': 1,\n'load_best_model_at_end': True\n}\n# TODO: maybe we should keep the default dict &amp; only add/replace keys in provided dict ?\nself.trainer_params = trainer_params\n# Model set on fit or on reload\nself.model: Any = None\nself.pipe: Any = None  # Set on first predict\n# Tokenizer set on fit or on reload\nself.tokenizer: Any = None\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <code>x_valid</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> <code>None</code> <code>y_valid</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> <code>None</code> Kwargs <p>with_shuffle (bool): If x, y must be shuffled before fitting     Experimental: We must verify if it works as intended depending on the formats of x and y     This should be used if y is not shuffled as the split_validation takes the lines in order.     Thus, the validation set might get classes which are not in the train set ...</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If different classes when comparing an already fitted model and a new dataset</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Fits the model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n        x_valid (?): Array-like, shape = [n_samples, n_features]\n        y_valid (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        with_shuffle (bool): If x, y must be shuffled before fitting\n            Experimental: We must verify if it works as intended depending on the formats of x and y\n            This should be used if y is not shuffled as the split_validation takes the lines in order.\n            Thus, the validation set might get classes which are not in the train set ...\n    Raises:\n        AssertionError: If different classes when comparing an already fitted model and a new dataset\n    '''\n##############################################\n# Manage retrain\n##############################################\n# If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n# And we save the old conf\nif self.trained:\n# Get src files to save\nsrc_files = [os.path.join(self.model_dir, \"configurations.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\nsrc_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Change model dir\nself.model_dir = self._get_model_dir()\n# Get dst files\ndst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\ndst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Copies\nfor src, dst in zip(src_files, dst_files):\ntry:\nshutil.copyfile(src, dst)\nexcept Exception as e:\nself.logger.error(f\"Impossible to copy {src} to {dst}\")\nself.logger.error(\"We still continue ...\")\nself.logger.error(repr(e))\n##############################################\n# Prepare x_train, x_valid, y_train &amp; y_valid\n# Also extract list of classes\n##############################################\n# If not multilabel, transform y_train as dummies (should already be the case for multi-labels)\nif not self.multi_label:\n# If len(array.shape)==2, we flatten the array if the second dimension is useless\nif isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\ny_train = np.ravel(y_train)\nif isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\ny_valid = np.ravel(y_valid)\n# Transformation dummies\ny_train_dummies = pd.get_dummies(y_train)\ny_valid_dummies = pd.get_dummies(y_valid) if y_valid is not None else None\n# Important : get_dummies reorder the columns in alphabetical order\n# Thus, there is no problem if we fit again on a new dataframe with shuffled data\nlist_classes = list(y_train_dummies.columns)\n# FIX: valid test might miss some classes, hence we need to add them back to y_valid_dummies\nif y_valid_dummies is not None and y_train_dummies.shape[1] != y_valid_dummies.shape[1]:\nfor cl in list_classes:\n# Add missing columns\nif cl not in y_valid_dummies.columns:\ny_valid_dummies[cl] = 0\ny_valid_dummies = y_valid_dummies[list_classes]  # Reorder\n# Else keep it as it is\nelse:\ny_train_dummies = y_train\ny_valid_dummies = y_valid\nif hasattr(y_train_dummies, 'columns'):\nlist_classes = list(y_train_dummies.columns)\nelse:\nself.logger.warning(\n\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n)\n# We still create a list of classes in order to be compatible with other functions\nlist_classes = [str(_) for _ in range(pd.DataFrame(y_train_dummies).shape[1])]\n# Set dict_classes based on list classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# Validate classes if already trained, else set them\nif self.trained:\nassert self.list_classes == list_classes, \\\n            \"Error: the new dataset does not match with the already fitted model\"\nassert self.dict_classes == dict_classes, \\\n            \"Error: the new dataset does not match with the already fitted model\"\nelse:\nself.list_classes = list_classes\nself.dict_classes = dict_classes\n# Shuffle x, y if wanted\n# It is advised as validation_split from keras does not shufle the data\n# Hence we might have classes in the validation data that we never met in the training data\nif with_shuffle:\np = np.random.permutation(len(x_train))\nx_train = np.array(x_train)[p]\ny_train_dummies = np.array(y_train_dummies)[p]\n# Else still transform to numpy array\nelse:\nx_train = np.array(x_train)\ny_train_dummies = np.array(y_train_dummies)\n# Also get y_valid_dummies as numpy\ny_valid_dummies = np.array(y_valid_dummies)\n# If no valid set, split train set according to validation_split\nif y_valid is None:\nself.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\np = np.random.permutation(len(x_train))\ncutoff = int(len(p) * self.validation_split)\nx_valid = x_train[p[0:cutoff]]\nx_train = x_train[p[cutoff:]]\ny_valid_dummies = y_train_dummies[p[0:cutoff]]\ny_train_dummies = y_train_dummies[p[cutoff:]]\n##############################################\n# Get model &amp; prepare datasets\n##############################################\n# Get model (if already fitted, _get_model returns instance model)\nself.model = self._get_model(num_labels=y_train_dummies.shape[1])\n# Get tokenizer (if already fitted, _get_tokenizer returns instance tokenizer)\nself.tokenizer = self._get_tokenizer()\n# Preprocess datasets\ntrain_dataset = self._prepare_x_train(x_train, y_train_dummies)\nvalid_dataset = self._prepare_x_valid(x_valid, y_valid_dummies)\n##############################################\n# Fit\n##############################################\n# Fit\ntry:\n# TODO: remove the checkpoints !\n# Prepare trainer\ntrainer = Trainer(\nmodel=self.model,\nargs=TrainingArguments(**self.trainer_params),\ntrain_dataset=train_dataset,\neval_dataset=valid_dataset,\ntokenizer=self.tokenizer,  # Only use for padding, dataset are already preprocessed. Pby not needed as we define a collator.\ndata_collator=DataCollatorWithPadding(tokenizer=self.tokenizer),  # Pad batches\ncompute_metrics=self._compute_metrics_mono_label if not self.multi_label else self._compute_metrics_multi_label,\noptimizers=self._get_optimizers(),\n)\n# Add callbacks\ntrainer.add_callback(MetricsTrainCallback(trainer))\ntrainer.add_callback(EarlyStoppingCallback(early_stopping_patience=self.patience))\n# Fit\ntrainer.train()\n# Save model &amp; tokenizer\nhf_model_dir = os.path.join(self.model_dir, 'hf_model')\nhf_tokenizer_dir = os.path.join(self.model_dir, 'hf_tokenizer')\ntrainer.model.save_pretrained(save_directory=hf_model_dir)\nself.tokenizer.save_pretrained(save_directory=hf_tokenizer_dir)\n# Remove checkpoint dir if save total limit is set to 1 (no need to keep this as we resave the model)\nif self.trainer_params.get('save_total_limit', None) == 1:\ncheckpoint_dirs = [_ for _ in os.listdir(self.model_dir) if _.startswith('checkpoint-')]\nif len(checkpoint_dirs) == 0:\nself.logger.warning(\"Can't find a checkpoint dir to be removed.\")\nelse:\nfor checkpoint_dir in checkpoint_dirs:\nshutil.rmtree(os.path.join(self.model_dir, checkpoint_dir))\nexcept (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, Exception) as e:\nself.logger.error(repr(e))\nraise RuntimeError(\"Error during model training\")\n# Print accuracy &amp; loss if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\n# Plot accuracy\nfit_history = trainer.state.log_history\nself._plot_metrics_and_loss(fit_history)\n# Reload best model ?\n# Default trainer has load_best_model_at_end = True\n# Hence we consider the best model is already reloaded\n# Set trained\nself.trained = True\nself.nb_fit += 1\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples]</p> required Kwargs <p>return_proba (bool): If the function should return the probabilities instead of the classes</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples]\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Predict probas\npredicted_proba = self.predict_proba(x_test)\n# We return the probabilities if wanted\nif return_proba:\nreturn predicted_proba\n# Finally, we get the classes predictions\nreturn self.get_classes_from_proba(predicted_proba)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Does not work with np array nor pandas Series\nif type(x_test) in [np.ndarray, pd.Series]:\nx_test = x_test.tolist()\n# Prepare predict\nif self.model.training:\nself.model.eval()\nif self.pipe is None:\n# Set model on gpu if available\nself.model = self.model.to('cuda') if self._is_gpu_activated() else self.model.to('cpu')\ndevice = 0 if self._is_gpu_activated() else -1\nself.pipe = TextClassificationPipeline(model=self.model, tokenizer=self.tokenizer, return_all_scores=True, device=device)\n# Predict\n# As we are using the pipeline, we do not need to prepare x_test (done inside the pipeline)\n# However, we still need to set the tokenizer params (truncate &amp; padding)\ntokenizer_kwargs = {'padding': False, 'truncation': True}\nresults = np.array(self.pipe(x_test, **tokenizer_kwargs))\npredicted_proba = np.array([[x['score'] for x in x] for x in results])\nreturn predicted_proba\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hf_model_dir (str): Path to HuggingFace model directory hf_tokenizer_dir (str): Path to HuggingFace tokenizer directory</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hf_model_dir is None</p> <code>ValueError</code> <p>If hf_tokenizer_dir is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hf_model_dir is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hf_tokenizer_dir is not an existing file</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hf_model_dir (str): Path to HuggingFace model directory\n        hf_tokenizer_dir (str): Path to HuggingFace tokenizer directory\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hf_model_dir is None\n        ValueError: If hf_tokenizer_dir is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hf_model_dir is not an existing file\n        FileNotFoundError: If the object hf_tokenizer_dir is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhf_model_dir_path = kwargs.get('hf_model_dir_path', None)\nhf_tokenizer_dir_path = kwargs.get('hf_tokenizer_dir_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hf_model_dir_path is None:\nraise ValueError(\"The argument hf_model_dir_path can't be None\")\nif hf_tokenizer_dir_path is None:\nraise ValueError(\"The argument hf_tokenizer_dir_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hf_model_dir_path):\nraise FileNotFoundError(f\"The file {hf_model_dir_path} does not exist\")\nif not os.path.exists(hf_tokenizer_dir_path):\nraise FileNotFoundError(f\"The file {hf_tokenizer_dir_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label',\n'level_save', 'batch_size', 'epochs', 'validation_split', 'patience',\n'transformer_name', 'transformer_params', 'trainer_params', 'model_max_length']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model &amp; tokenizer\nself.model = self._get_model(hf_model_dir_path)\nself.tokenizer = self._get_tokenizer(hf_tokenizer_dir_path)\n# Save hf folders in new folder\nnew_hf_model_dir_path = os.path.join(self.model_dir, 'hf_model')\nnew_hf_tokenizer_dir_path = os.path.join(self.model_dir, 'hf_tokenizer')\nshutil.copytree(hf_model_dir_path, new_hf_model_dir_path)\nshutil.copytree(hf_tokenizer_dir_path, new_hf_tokenizer_dir_path)\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.reload_model","title":"<code>reload_model(model_path)</code>","text":"<p>Loads a HF model from a directory</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>str</code> <p>Path to the directory containing both the model.bin and its conf</p> required <p>Returns:</p> Type Description <code>Any</code> <p>?: HF model</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>def reload_model(self, model_path: str) -&gt; Any:\n'''Loads a HF model from a directory\n    Args:\n        model_path (str): Path to the directory containing both the model.bin and its conf\n    Returns:\n        ?: HF model\n    '''\n# Loading the model\nhf_model = self._get_model(model_path)\n# Set trained to true if not already true\n# TODO: check if we really need this\nif not self.trained:\nself.trained = True\nself.nb_fit = 1\n# Return\nreturn hf_model\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.reload_tokenizer","title":"<code>reload_tokenizer(tokenizer_path)</code>","text":"<p>Loads a HF model from a directory</p> <p>Parameters:</p> Name Type Description Default <code>tokenizer_path</code> <code>str</code> <p>Path to the directory containing the tokenizer files</p> required <p>Returns:</p> Type Description <code>Any</code> <p>?: HF tokenizer</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>def reload_tokenizer(self, tokenizer_path: str) -&gt; Any:\n'''Loads a HF model from a directory\n    Args:\n        tokenizer_path (str): Path to the directory containing the tokenizer files\n    Returns:\n        ?: HF tokenizer\n    '''\n# Loading the tokenizer\nhf_tokenizer = self._get_tokenizer(tokenizer_path)\n# Return\nreturn hf_tokenizer\n</code></pre>"},{"location":"reference/template_nlp/models_training/model_huggingface/#template_nlp.models_training.model_huggingface.ModelHuggingFace.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/model_huggingface.py</code> <pre><code>@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'huggingface'\njson_data['batch_size'] = self.batch_size\njson_data['epochs'] = self.epochs\njson_data['validation_split'] = self.validation_split\njson_data['patience'] = self.patience\njson_data['transformer_name'] = self.transformer_name\njson_data['transformer_params'] = self.transformer_params\njson_data['trainer_params'] = self.trainer_params\njson_data['model_max_length'] = self.model_max_length\n# Add model structure if not none\nif self.model is not None:\njson_data['hf_model'] = self.model.__repr__()\nif '_get_model' not in json_data.keys():\njson_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\nif '_get_tokenizer' not in json_data.keys():\njson_data['_get_tokenizer'] = pickle.source.getsourcelines(self._get_tokenizer)[0]\n# Save strategy :\n# - HuggingFace model &amp; tokenizer are already saved in the fit() function\n# - We don't want them in the .pkl as they are heavy &amp; already saved\n# - Also get rid of the pipe (takes too much disk space for nothing),\n#   will be reloaded automatically at first call to predict functions\nhf_model = self.model\nhf_tokenizer = self.tokenizer\nself.model = None\nself.tokenizer = None\nif self.pipe is not None:\nself.pipe = None\nsuper().save(json_data=json_data)\nself.model = hf_model\nself.tokenizer = hf_tokenizer\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/","title":"Utils models","text":""},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.display_train_test_shape","title":"<code>display_train_test_shape(df_train, df_test, df_shape=None)</code>","text":"<p>Displays the size of a train/test split</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>pd.DataFrame</code> <p>Train dataset</p> required <code>df_test</code> <code>pd.DataFrame</code> <p>Test dataset</p> required Kwargs <p>df_shape (int): Size of the initial dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object df_shape is not positive</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def display_train_test_shape(df_train: pd.DataFrame, df_test: pd.DataFrame, df_shape: Union[int, None] = None) -&gt; None:\n'''Displays the size of a train/test split\n    Args:\n        df_train (pd.DataFrame): Train dataset\n        df_test (pd.DataFrame): Test dataset\n    Kwargs:\n        df_shape (int): Size of the initial dataset\n    Raises:\n        ValueError: If the object df_shape is not positive\n    '''\nif df_shape is not None and df_shape &lt; 1:\nraise ValueError(\"The object df_shape must be positive\")\n# Process\nif df_shape is None:\ndf_shape = df_train.shape[0] + df_test.shape[0]\nlogger.info(f\"There are {df_train.shape[0]} lines in the train dataset and {df_test.shape[0]} in the test dataset.\")\nlogger.info(f\"{round(100 * df_train.shape[0] / df_shape, 2)}% of data are in the train set\")\nlogger.info(f\"{round(100 * df_test.shape[0] / df_shape, 2)}% of data are in the test set\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.get_embedding","title":"<code>get_embedding(embedding_name='cc.fr.300.pkl')</code>","text":"<p>Loads an embedding previously saved as a .pkl file</p> <p>Parameters:</p> Name Type Description Default <code>embedding_name</code> <code>str</code> <p>Name of the embedding file (actually a path relative to template_nlp-data)</p> <code>'cc.fr.300.pkl'</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the embedding file does not exist in template_nlp-data</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, np.ndarray]</code> <p>Loaded embedding</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def get_embedding(embedding_name: str = \"cc.fr.300.pkl\") -&gt; Dict[str, np.ndarray]:\n'''Loads an embedding previously saved as a .pkl file\n    Args:\n        embedding_name (str): Name of the embedding file (actually a path relative to template_nlp-data)\n    Raises:\n        FileNotFoundError: If the embedding file does not exist in template_nlp-data\n    Returns:\n        dict: Loaded embedding\n    '''\nlogger.info(\"Reloading an embedding ...\")\n# Manage path\ndata_path = utils.get_data_path()\nembedding_path = os.path.join(data_path, embedding_name)\nif not os.path.exists(embedding_path):\nlogger.error(f\"The provided embedding file ({embedding_path}) does not exist.\")\nlogger.error(\"If you want to create one, you have to :\")\nlogger.error(\"    1. Download a fasttext embedding (e.g. French -&gt; https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.vec.gz\")\nlogger.error(\"    2. Use the 0_get_embedding_dict.py to generate the pkl. file\")\nraise FileNotFoundError()\n# Load embedding indexes\nwith open(embedding_path, 'rb') as f:\nembedding_indexes = pickle.load(f)\n# Return\nreturn embedding_indexes\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.hierarchical_split","title":"<code>hierarchical_split(df, col, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets - Hierarchical strategy</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str or int</code> <p>column on which to do the hierarchical split</p> required Kwargs <p>test_size (float): Proportion representing the size of the expected test set seed (int): Random seed</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object test_size is not between 0 and 1</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>Train dataframe</p> <code>DataFrame</code> <code>pd.DataFrame</code> <p>Test dataframe</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def hierarchical_split(df: pd.DataFrame, col: Union[str, int], test_size: float = 0.25, seed: int = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n'''Splits a DataFrame into train and test sets - Hierarchical strategy\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str or int): column on which to do the hierarchical split\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): Random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\nif not 0 &lt;= test_size &lt;= 1:\nraise ValueError('The object test_size must be between 0 and 1')\n# Hierarchical split\nlogger.info(\"Hierarchical split\")\nmodalities_train, _ = train_test_split(df[col].unique(), test_size=test_size, random_state=seed)\ntrain_rows = df[col].isin(modalities_train)\ndf_train = df[train_rows]\ndf_test = df[~train_rows]\n# Display\ndisplay_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n# Return\nreturn df_train, df_test\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.load_model","title":"<code>load_model(model_dir, is_path=False)</code>","text":"<p>Loads a model from a path</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19)</p> required Kwargs <p>is_path (bool): If folder path instead of name (permits to load model from elsewhere)</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>?: Model</p> <code>dict</code> <code>dict</code> <p>Model configurations</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def load_model(model_dir: str, is_path: bool = False) -&gt; Tuple[Any, dict]:\n'''Loads a model from a path\n    Args:\n        model_dir (str): Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19)\n    Kwargs:\n        is_path (bool): If folder path instead of name (permits to load model from elsewhere)\n    Returns:\n        ?: Model\n        dict: Model configurations\n    '''\n# Find model path\nbase_folder = None if is_path else utils.get_models_path()\nmodel_path = utils.find_folder_path(model_dir, base_folder)\n# Get configs\nconfiguration_path = os.path.join(model_path, 'configurations.json')\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys() and configs['dict_classes'] is not None:\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n# Load model\npkl_path = os.path.join(model_path, f\"{configs['model_name']}.pkl\")\nwith open(pkl_path, 'rb') as f:\nmodel = pickle.load(f)\n# Change model_dir if diff\nif model_path != model.model_dir:\nmodel.model_dir = model_path\nconfigs['model_dir'] = model_path\n# Load specifics\nhdf5_path = os.path.join(model_path, 'best.hdf5')\nhf_model_dir = os.path.join(model_path, 'hf_model')\nhf_tokenizer_dir = os.path.join(model_path, 'hf_tokenizer')\n# TODO : we should probably have a single function `load_self` and let the model manage its reload\n# Check for keras model\nif os.path.exists(hdf5_path):\nmodel.model = model.reload_model(hdf5_path)\n# Check for huggingface model\nif os.path.exists(hf_model_dir):\nmodel.model = model.reload_model(hf_model_dir)\nmodel.tokenizer = model.reload_tokenizer(hf_tokenizer_dir)\n# Display if GPU is being used\nmodel.display_if_gpu_activated()\n# Return model &amp; configs\nreturn model, configs\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.normal_split","title":"<code>normal_split(df, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe containing the data</p> required Kwargs <p>test_size (float): Proportion representing the size of the expected test set seed (int): random seed</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object test_size is not between 0 and 1</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>Train dataframe</p> <code>DataFrame</code> <code>pd.DataFrame</code> <p>Test dataframe</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def normal_split(df: pd.DataFrame, test_size: float = 0.25, seed: Union[int, None] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n'''Splits a DataFrame into train and test sets\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\nif not 0 &lt;= test_size &lt;= 1:\nraise ValueError('The object test_size must be between 0 and 1')\n# Normal split\nlogger.info(\"Normal split\")\ndf_train, df_test = train_test_split(df, test_size=test_size, random_state=seed)\n# Display\ndisplay_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n# Return\nreturn df_train, df_test\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.predict","title":"<code>predict(content, model, model_conf, **kwargs)</code>","text":"<p>Gets predictions of a model on a content</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, list]</code> <p>New content to be predicted</p> required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <code>model_conf</code> <code>dict</code> <p>Model configurations</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>a list of strings (resp. tuples) in case of mono-label (resp. multi-labels) classification predictions</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def predict(content: Union[str, list], model, model_conf: dict, **kwargs) -&gt; list:\n'''Gets predictions of a model on a content\n    Args:\n        content (Union[str, list]): New content to be predicted\n        model (ModelClass): Model to use\n        model_conf (dict): Model configurations\n    Returns:\n        list: a list of strings (resp. tuples) in case of mono-label (resp. multi-labels) classification predictions\n    '''\nif isinstance(content, str):\ncontent = [content]\n# Get preprocessor\nif 'preprocess_str' in model_conf.keys():\npreprocess_str = model_conf['preprocess_str']\nelse:\npreprocess_str = 'no_preprocess'\npreprocessor = preprocess.get_preprocessor(preprocess_str)\n# Preprocess\ncontent = preprocessor(content)\n# Get prediction (some models need an iterable)\npredictions = model.predict(content)\n# Return predictions with inverse transform when relevant\nreturn model.inverse_transform(predictions)\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.predict_with_proba","title":"<code>predict_with_proba(content, model, model_conf)</code>","text":"<p>Gets predictions of a model on a content, with probabilities</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Union[str, list]</code> <p>New content to be predicted</p> required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <code>model_conf</code> <code>dict</code> <p>Model configurations</p> required <p>Returns:</p> Type Description <code>Union[Tuple[List[str], List[float]], Tuple[List[tuple], List[tuple]]]</code> <p>MONO-LABEL CLASSIFICATION: List[str]: predictions List[float]: probabilities</p> <code>Union[Tuple[List[str], List[float]], Tuple[List[tuple], List[tuple]]]</code> <p>MULTI-LABELS CLASSIFICATION: List[tuple]: predictions List[tuple]: probabilities</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def predict_with_proba(content: Union[str, list], model, model_conf: dict) -&gt; Union[Tuple[List[str], List[float]], Tuple[List[tuple], List[tuple]]]:\n'''Gets predictions of a model on a content, with probabilities\n    Args:\n        content (Union[str, list]): New content to be predicted\n        model (ModelClass): Model to use\n        model_conf (dict): Model configurations\n    Returns:\n        MONO-LABEL CLASSIFICATION:\n            List[str]: predictions\n            List[float]: probabilities\n        MULTI-LABELS CLASSIFICATION:\n            List[tuple]: predictions\n            List[tuple]: probabilities\n    '''\nif isinstance(content, str):\ncontent = [content]\n# Get preprocessor\nif 'preprocess_str' in model_conf.keys():\npreprocess_str = model_conf['preprocess_str']\nelse:\npreprocess_str = 'no_preprocess'\npreprocessor = preprocess.get_preprocessor(preprocess_str)\n# Preprocess\ncontent = preprocessor(content)\n# Get prediction (some models need an iterable)\n# predictions is a ndarray of shape (n_samples, n_classes)\n# probas is a ndarray of shape (n_samples, n_classes)\npredictions, probas = model.predict_with_proba(content)\n# Rework format :\nif model.multi_label:\nmodel_labels = np.array(model.list_classes)\nall_preds = [tuple(np.compress(content_pred, model_labels)) for content_pred in predictions]\nall_probs = [tuple(np.compress(content_pred, content_prob)) for content_pred, content_prob in zip(predictions, probas)]\nelse:\nall_preds = model.inverse_transform(predictions)\nall_probs = probas.max(axis=1)\n# Return prediction &amp; proba\nreturn all_preds, all_probs\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.preprocess_model_multilabel","title":"<code>preprocess_model_multilabel(df, y_col, classes=None)</code>","text":"<p>Prepares a dataframe for a multi-labels classification</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Training dataset This dataset must be preprocessed. Example:     # Group by &amp; apply tuple to y_col     x_cols = [col for col in list(df.columns) if col != y_col]     df = pd.DataFrame(df.groupby(x_cols)[y_col].apply(tuple))</p> required <code>y_col</code> <code>str or int</code> <p>Name of the column to be used for training - y</p> required Kwargs <p>classes (list): List of classes to consider</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>Dataframe for training</p> <code>list</code> <code>list</code> <p>List of 'y' columns</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def preprocess_model_multilabel(df: pd.DataFrame, y_col: Union[str, int], classes: Union[list, None] = None) -&gt; Tuple[pd.DataFrame, list]:\n'''Prepares a dataframe for a multi-labels classification\n    Args:\n        df (pd.DataFrame): Training dataset\n            This dataset must be preprocessed.\n            Example:\n                # Group by &amp; apply tuple to y_col\n                x_cols = [col for col in list(df.columns) if col != y_col]\n                df = pd.DataFrame(df.groupby(x_cols)[y_col].apply(tuple))\n        y_col (str or int): Name of the column to be used for training - y\n    Kwargs:\n        classes (list): List of classes to consider\n    Returns:\n        DataFrame: Dataframe for training\n        list: List of 'y' columns\n    '''\n# TODO: add possibility to have sparse output\nlogger.info(\"Preprocess dataframe for multi-labels model\")\n# Process\nlogger.info(\"Preparing dataset for multi-labels format. Might take several minutes.\")\n# /!\\ The reset_index is compulsory in order to have the same indexes between df, and MLB transformed values\ndf = df.reset_index(drop=True)\n# Apply MLB\nmlb = MultiLabelBinarizer(classes=classes)\ndf = df.assign(**pd.DataFrame(mlb.fit_transform(df[y_col]), columns=mlb.classes_))\n# Return dataframe &amp; y_cols (i.e. classes)\nreturn df, list(mlb.classes_)\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.remove_small_classes","title":"<code>remove_small_classes(df, col, min_rows=2)</code>","text":"<p>Deletes the classes with small numbers of elements</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str | int</code> <p>Columns containing the classes</p> required Kwargs <p>min_rows (int): Minimal number of lines in the training set (default: 2)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object min_rows is not positive</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: New dataset</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def remove_small_classes(df: pd.DataFrame, col: Union[str, int], min_rows: int = 2) -&gt; pd.DataFrame:\n'''Deletes the classes with small numbers of elements\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str | int): Columns containing the classes\n    Kwargs:\n        min_rows (int): Minimal number of lines in the training set (default: 2)\n    Raises:\n        ValueError: If the object min_rows is not positive\n    Returns:\n        pd.DataFrame: New dataset\n    '''\nif min_rows &lt; 1:\nraise ValueError(\"The object min_rows must be positive\")\n# Looking for classes with less than min_rows lines\nv_count = df[col].value_counts()\nclasses_to_remove = list(v_count[v_count &lt; min_rows].index.values)\nfor cl in classes_to_remove:\nlogger.warning(f\"/!\\\\ /!\\\\ /!\\\\ Class {cl} has less than {min_rows} lines in the training set.\")\nlogger.warning(\"/!\\\\ /!\\\\ /!\\\\ This class is automatically removed from the dataset.\")\nreturn df[~df[col].isin(classes_to_remove)]\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.search_hp_cv","title":"<code>search_hp_cv(model_cls, model_params, hp_params, scoring_fn, kwargs_fit, n_splits=5)</code>","text":"<p>Searches for hyperparameters</p> <p>Parameters:</p> Name Type Description Default <code>model_cls</code> <code>?</code> <p>Class of models on which to do a hyperparameters search</p> required <code>model_params</code> <code>dict</code> <p>Set of \"fixed\" parameters of the model (e.g. x_col, y_col). Must contain 'multi_label'.</p> required <code>hp_params</code> <code>dict</code> <p>Set of \"variable\" parameters on which to do a hyperparameters search</p> required <code>scoring_fn</code> <code>str or func</code> <p>Scoring function to maximize This function must take as input a dictionary containing metrics e.g. {'F1-Score': 0.85, 'Accuracy': 0.57, 'Precision': 0.64, 'Recall': 0.90}</p> required <code>kwargs_fit</code> <code>dict</code> <p>Set of kwargs to input in the fit function Must contain 'x_train' and 'y_train'</p> required Kwargs <p>n_splits (int): Number of folds to use</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If scoring_fn is not a known string</p> <code>ValueError</code> <p>If multi_label is not a key in model_params</p> <code>ValueError</code> <p>If x_train is not a key in kwargs_fit</p> <code>ValueError</code> <p>If y_train is not a key in kwargs_fit</p> <code>ValueError</code> <p>If model_params and hp_params share some keys</p> <code>ValueError</code> <p>If hp_params values are not the same length</p> <code>ValueError</code> <p>If the number of crossvalidation split is less or equal to 1</p> <p>Returns:</p> Name Type Description <code>ModelClass</code> <p>best model to be \"fitted\" on the dataset</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def search_hp_cv(model_cls, model_params: dict, hp_params: dict, scoring_fn: Union[str, Callable],\nkwargs_fit: dict, n_splits: int = 5):\n'''Searches for hyperparameters\n    Args:\n        model_cls (?): Class of models on which to do a hyperparameters search\n        model_params (dict): Set of \"fixed\" parameters of the model (e.g. x_col, y_col).\n            Must contain 'multi_label'.\n        hp_params (dict): Set of \"variable\" parameters on which to do a hyperparameters search\n        scoring_fn (str or func): Scoring function to maximize\n            This function must take as input a dictionary containing metrics\n            e.g. {'F1-Score': 0.85, 'Accuracy': 0.57, 'Precision': 0.64, 'Recall': 0.90}\n        kwargs_fit (dict): Set of kwargs to input in the fit function\n            Must contain 'x_train' and 'y_train'\n    Kwargs:\n        n_splits (int): Number of folds to use\n    Raises:\n        ValueError: If scoring_fn is not a known string\n        ValueError: If multi_label is not a key in model_params\n        ValueError: If x_train is not a key in kwargs_fit\n        ValueError: If y_train is not a key in kwargs_fit\n        ValueError: If model_params and hp_params share some keys\n        ValueError: If hp_params values are not the same length\n        ValueError: If the number of crossvalidation split is less or equal to 1\n    Returns:\n        ModelClass: best model to be \"fitted\" on the dataset\n    '''\nlist_known_scoring = ['accuracy', 'f1', 'precision', 'recall']\n#################\n# Manage errors\n#################\nif isinstance(scoring_fn, str) and scoring_fn not in list_known_scoring:\nraise ValueError(f\"The input {scoring_fn} is not a known value for scoring_fn (known values : {list_known_scoring})\")\nif 'multi_label' not in model_params.keys():\nraise ValueError(\"The key 'multi_label' must be present in the dictionary model_params\")\nif 'x_train' not in kwargs_fit.keys():\nraise ValueError(\"The key 'x_train' must be present in the dictionary kwargs_fit\")\nif 'y_train' not in kwargs_fit.keys():\nraise ValueError(\"The key 'y_train' must be present in the dictionary kwargs_fit\")\nif any([k in hp_params.keys() for k in model_params.keys()]):\n# A key can't be \"fixed\" and \"variable\"\nraise ValueError(\"The dictionaries model_params and hp_params share at least one key\")\nif len(set([len(_) for _ in hp_params.values()])) != 1:\nraise ValueError(\"The values of hp_params must have the same length\")\nif n_splits &lt;= 1:\nraise ValueError(f\"The number of crossvalidation splits ({n_splits}) must be more than 1\")\n#################\n# Manage scoring\n#################\n# Get scoring functions\nif scoring_fn == 'accuracy':\nscoring_fn = lambda x: x['Accuracy']\nelif scoring_fn == 'f1':\nscoring_fn = lambda x: x['F1-Score']\nelif scoring_fn == 'precision':\nscoring_fn = lambda x: x['Precision']\nelif scoring_fn == 'recall':\nscoring_fn = lambda x: x['Recall']\n#################\n# Manage x_train &amp; y_train format\n#################\nif not isinstance(kwargs_fit['x_train'], (pd.DataFrame, pd.Series)):\nkwargs_fit['x_train'] = pd.Series(kwargs_fit['x_train'].copy())\nif not isinstance(kwargs_fit['y_train'], (pd.DataFrame, pd.Series)):\nkwargs_fit['y_train'] = pd.Series(kwargs_fit['y_train'].copy())\n#################\n# Process\n#################\n# Loop on hyperparameters\nnb_search = len(list(hp_params.values())[0])\nlogger.info(\"Beginning of hyperparameters search\")\nlogger.info(f\"Number of model fits : {nb_search} (search number) x {n_splits} (CV splits number) = {nb_search * n_splits}\")\n# DataFrame for stocking metrics :\nmetrics_df = pd.DataFrame(columns=['index_params', 'index_fold', 'Score', 'Accuracy', 'F1-Score', 'Precision', 'Recall'])\nfor i in range(nb_search):\n# Display informations\nlogger.info(f\"Search n\u00b0{i + 1}\")\ntmp_hp_params = {k: v[i] for k, v in hp_params.items()}\nlogger.info(\"Tested hyperparameters : \")\nlogger.info(pprint.pformat(tmp_hp_params))\n# Get folds (shuffle recommended since the classes could be ordered)\nif model_params['multi_label']:\nk_fold = KFold(n_splits=n_splits, shuffle=True)  # Can't stratify on multi-labels\nelse:\nk_fold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n# Process each fold\nfor j, (train_index, valid_index) in enumerate(k_fold.split(kwargs_fit['x_train'], kwargs_fit['y_train'])):\nlogger.info(f\"Search n\u00b0{i + 1}/{nb_search} - fit n\u00b0{j + 1}/{n_splits}\")\n# get tmp x, y\nx_train, x_valid = kwargs_fit['x_train'].iloc[train_index], kwargs_fit['x_train'].iloc[valid_index]\ny_train, y_valid = kwargs_fit['y_train'].iloc[train_index], kwargs_fit['y_train'].iloc[valid_index]\n# Get tmp model\n# Manage model_dir\ntmp_model_dir = os.path.join(utils.get_models_path(), datetime.now().strftime(\"tmp_%Y_%m_%d-%H_%M_%S\"))\n# The next line prioritize the last dictionary\n# We force a temporary folder and a low save level (we only want the metrics)\nmodel_tmp = model_cls(**{**model_params, **tmp_hp_params, **{'model_dir': tmp_model_dir, 'level_save': 'LOW'}})\n# Setting log level to ERROR\nmodel_tmp.logger.setLevel(logging.ERROR)\n# Let's fit ! (priority to the last dictionary)\nmodel_tmp.fit(**{**kwargs_fit, **{'x_train': x_train, 'y_train': y_train, 'x_valid': x_valid, 'y_valid': y_valid}})\n# Let's predict !\ny_pred = model_tmp.predict(x_valid)\n# Get metrics !\nmetrics_func = model_tmp.get_metrics_simple_multilabel if model_tmp.multi_label else model_tmp.get_metrics_simple_monolabel\nmetrics_tmp = metrics_func(y_valid, y_pred)\nmetrics_tmp = metrics_tmp[metrics_tmp.Label == \"All\"].copy()  # Add .copy() to avoid pandas settingwithcopy\nmetrics_tmp[\"Score\"] = scoring_fn(metrics_tmp.iloc[0].to_dict())  # type: ignore\nmetrics_tmp[\"index_params\"] = i\nmetrics_tmp[\"index_fold\"] = j\nmetrics_tmp = metrics_tmp[metrics_df.columns]  # Keeping only the necessary columns\nmetrics_df = pd.concat([metrics_df, metrics_tmp], ignore_index=True)\n# Delete the temporary model : the final model must be refitted on the whole dataset\ndel model_tmp\ngc.collect()\nshutil.rmtree(tmp_model_dir)\n# Display score\nlogger.info(f\"Score for search n\u00b0{i + 1}: {metrics_df[metrics_df['index_params'] == i]['Score'].mean()}\")\n# Metric agregation for all the folds\nmetrics_df = metrics_df.join(metrics_df[['index_params', 'Score']].groupby('index_params').mean().rename({'Score': 'mean_score'}, axis=1), on='index_params', how='left')\n# Select the set of parameters with the best mean score (on the folds)\nbest_index = metrics_df[metrics_df.mean_score == metrics_df.mean_score.max()][\"index_params\"].values[0]\nbest_params = {k: v[best_index] for k, v in hp_params.items()}\nlogger.info(f\"Best results for the set of parameter n\u00b0{best_index + 1}: {pprint.pformat(best_params)}\")\n# Instanciation of a model with the best parameters\nbest_model = model_cls(**{**model_params, **best_params})\n# Save the metrics report of the hyperparameters search and the tested parameters\ncsv_path = os.path.join(best_model.model_dir, \"hyper_params_results.csv\")\nmetrics_df.to_csv(csv_path, sep=';', index=False, encoding='utf-8')\njson_data = {\n'model_params': model_params,\n'scoring_fn': pickle.source.getsourcelines(scoring_fn)[0],\n'n_splits': n_splits,\n'hp_params_set': {i: {k: v[i] for k, v in hp_params.items()} for i in range(nb_search)},\n}\njson_path = os.path.join(best_model.model_dir, \"hyper_params_tested.json\")\nwith open(json_path, 'w', encoding='utf-8') as f:\njson.dump(json_data, f, indent=4, cls=utils.NpEncoder)\n# TODO: We are forced to reset the logging level which is linked to the class\nbest_model.logger.setLevel(logging.getLogger('template_nlp').getEffectiveLevel())\n# Return model to be fitted\nreturn best_model\n</code></pre>"},{"location":"reference/template_nlp/models_training/utils_models/#template_nlp.models_training.utils_models.stratified_split","title":"<code>stratified_split(df, col, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets - Stratified strategy</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str or int</code> <p>column on which to do the stratified split</p> required Kwargs <p>test_size (float): Proportion representing the size of the expected test set seed (int): Random seed</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object test_size is not between 0 and 1</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>Train dataframe</p> <code>DataFrame</code> <code>pd.DataFrame</code> <p>Test dataframe</p> Source code in <code>template_nlp/models_training/utils_models.py</code> <pre><code>def stratified_split(df: pd.DataFrame, col: Union[str, int], test_size: float = 0.25, seed: int = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n'''Splits a DataFrame into train and test sets - Stratified strategy\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str or int): column on which to do the stratified split\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): Random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\nif not 0 &lt;= test_size &lt;= 1:\nraise ValueError('The object test_size must be between 0 and 1')\n# Stratified split\nlogger.info(\"Stratified split\")\ndf = remove_small_classes(df, col, min_rows=math.ceil(1 / test_size))  # minimum lines number per category to split\ndf_train, df_test = train_test_split(df, stratify=df[col], test_size=test_size, random_state=seed)\n# Display\ndisplay_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n# Return\nreturn df_train, df_test\n</code></pre>"},{"location":"reference/template_nlp/models_training/hf_metrics/","title":"Hf metrics","text":""},{"location":"reference/template_nlp/models_training/hf_metrics/accuracy/","title":"Accuracy","text":"<p>Accuracy metric.</p>"},{"location":"reference/template_nlp/models_training/hf_metrics/f1/","title":"F1","text":"<p>F1 metric.</p>"},{"location":"reference/template_nlp/models_training/hf_metrics/precision/","title":"Precision","text":"<p>Precision metric.</p>"},{"location":"reference/template_nlp/models_training/hf_metrics/recall/","title":"Recall","text":"<p>Recall metric.</p>"},{"location":"reference/template_nlp/models_training/models_sklearn/","title":"Models sklearn","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/","title":"Model pipeline","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline","title":"<code>ModelPipeline</code>","text":"<p>         Bases: <code>ModelClass</code></p> <p>Generic model for sklearn pipeline</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>class ModelPipeline(ModelClass):\n'''Generic model for sklearn pipeline'''\n_default_name = 'model_pipeline'\n# Not implemented :\n# -&gt; reload\ndef __init__(self, pipeline: Union[Pipeline, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n        Kwargs:\n            pipeline (Pipeline): Pipeline to use\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model (to implement for children class)\nself.pipeline = pipeline\ndef fit(self, x_train, y_train, **kwargs) -&gt; None:\n'''Trains the model\n           **kwargs permits compatibility with Keras model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Raises:\n            RuntimeError: If the model is already fitted\n        '''\nif self.trained:\nself.logger.error(\"We can't train again a pipeline sklearn model\")\nself.logger.error(\"Please train a new model\")\nraise RuntimeError(\"We can't train again a pipeline sklearn model\")\n# We \"only\" check if no multi-classes multi-labels (which can't be managed by most SKLEARN pipelines)\nif self.multi_label:\ndf_tmp = pd.DataFrame(y_train)\nfor col in df_tmp:\nuniques = df_tmp[col].unique()\nif len(uniques) &gt; 2:\nself.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\nself.logger.warning(\"Most sklearn pipelines can't manage multi-classes/multi-labels\")\nself.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\n# We \"let\" the process crash by itself\nbreak\n# Fit pipeline\nself.pipeline.fit(x_train, y_train)\n# Set list classes\nif not self.multi_label:\nself.list_classes = list(self.pipeline.classes_)\n# TODO : check pipeline.classes_ for multi-labels\nelse:\nif hasattr(y_train, 'columns'):\nself.list_classes = list(y_train.columns)\nelse:\nself.logger.warning(\n\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n)\n# We still create a list of classes in order to be compatible with other functions\nself.list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n# Set dict_classes based on list classes\nself.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n# Set trained\nself.trained = True\nself.nb_fit += 1\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nif not return_proba:\nreturn np.array(self.pipeline.predict(x_test))\nelse:\nreturn self.predict_proba(x_test)\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nprobas = np.array(self.pipeline.predict_proba(x_test))\n# Very specific fix: in some cases, with OvR, strategy, all estimators return 0, which generates a division per 0 when normalizing\n# Hence, we replace NaNs with 1 / nb_classes\nif not self.multi_label:\nprobas = np.nan_to_num(probas, nan=1/len(self.list_classes))\n# If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n# Same thing for some base models\n# Correction in case where we detect a shape of length &gt; 2 (ie. equals to 3)\n# Reminder : we do not manage multi-labels/multi-classes\nif len(probas.shape) &gt; 2:\nprobas = np.swapaxes(probas[:, :, 1], 0, 1)\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'scikit-learn'\n# Add each pipeline steps' conf\nif self.pipeline is not None:\nfor step in self.pipeline.steps:\nname = step[0]\nconfs = step[1].get_params()\n# Get rid of some non serializable conf\nfor special_conf in ['dtype', 'base_estimator', 'estimator', 'estimator__base_estimator',\n'estimator__estimator', 'estimator__estimator__base_estimator']:\nif special_conf in confs.keys():\nconfs[special_conf] = str(confs[special_conf])\njson_data[f'{name}_confs'] = confs\n# Save\nsuper().save(json_data=json_data)\n# Save model standalone if wanted &amp; pipeline is not None &amp; level_save &gt; 'LOW'\nif self.pipeline is not None and self.level_save in ['MEDIUM', 'HIGH']:\npkl_path = os.path.join(self.model_dir, \"sklearn_pipeline_standalone.pkl\")\n# Save model\nwith open(pkl_path, 'wb') as f:\npickle.dump(self.pipeline, f)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Needs to be overridden /!\\\\ -\n        '''\nraise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline.__init__","title":"<code>__init__(pipeline=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>pipeline (Pipeline): Pipeline to use</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>def __init__(self, pipeline: Union[Pipeline, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n    Kwargs:\n        pipeline (Pipeline): Pipeline to use\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model (to implement for children class)\nself.pipeline = pipeline\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline.fit","title":"<code>fit(x_train, y_train, **kwargs)</code>","text":"<p>Trains the model    **kwargs permits compatibility with Keras model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model is already fitted</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>def fit(self, x_train, y_train, **kwargs) -&gt; None:\n'''Trains the model\n       **kwargs permits compatibility with Keras model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Raises:\n        RuntimeError: If the model is already fitted\n    '''\nif self.trained:\nself.logger.error(\"We can't train again a pipeline sklearn model\")\nself.logger.error(\"Please train a new model\")\nraise RuntimeError(\"We can't train again a pipeline sklearn model\")\n# We \"only\" check if no multi-classes multi-labels (which can't be managed by most SKLEARN pipelines)\nif self.multi_label:\ndf_tmp = pd.DataFrame(y_train)\nfor col in df_tmp:\nuniques = df_tmp[col].unique()\nif len(uniques) &gt; 2:\nself.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\nself.logger.warning(\"Most sklearn pipelines can't manage multi-classes/multi-labels\")\nself.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\n# We \"let\" the process crash by itself\nbreak\n# Fit pipeline\nself.pipeline.fit(x_train, y_train)\n# Set list classes\nif not self.multi_label:\nself.list_classes = list(self.pipeline.classes_)\n# TODO : check pipeline.classes_ for multi-labels\nelse:\nif hasattr(y_train, 'columns'):\nself.list_classes = list(y_train.columns)\nelse:\nself.logger.warning(\n\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n)\n# We still create a list of classes in order to be compatible with other functions\nself.list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n# Set dict_classes based on list classes\nself.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n# Set trained\nself.trained = True\nself.nb_fit += 1\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required Kwargs <p>return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\nif not return_proba:\nreturn np.array(self.pipeline.predict(x_test))\nelse:\nreturn self.predict_proba(x_test)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\nprobas = np.array(self.pipeline.predict_proba(x_test))\n# Very specific fix: in some cases, with OvR, strategy, all estimators return 0, which generates a division per 0 when normalizing\n# Hence, we replace NaNs with 1 / nb_classes\nif not self.multi_label:\nprobas = np.nan_to_num(probas, nan=1/len(self.list_classes))\n# If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n# Same thing for some base models\n# Correction in case where we detect a shape of length &gt; 2 (ie. equals to 3)\n# Reminder : we do not manage multi-labels/multi-classes\nif len(probas.shape) &gt; 2:\nprobas = np.swapaxes(probas[:, :, 1], 0, 1)\nreturn probas\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Needs to be overridden /! -</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Needs to be overridden /!\\\\ -\n    '''\nraise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_pipeline/#template_nlp.models_training.models_sklearn.model_pipeline.ModelPipeline.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_sklearn/model_pipeline.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'scikit-learn'\n# Add each pipeline steps' conf\nif self.pipeline is not None:\nfor step in self.pipeline.steps:\nname = step[0]\nconfs = step[1].get_params()\n# Get rid of some non serializable conf\nfor special_conf in ['dtype', 'base_estimator', 'estimator', 'estimator__base_estimator',\n'estimator__estimator', 'estimator__estimator__base_estimator']:\nif special_conf in confs.keys():\nconfs[special_conf] = str(confs[special_conf])\njson_data[f'{name}_confs'] = confs\n# Save\nsuper().save(json_data=json_data)\n# Save model standalone if wanted &amp; pipeline is not None &amp; level_save &gt; 'LOW'\nif self.pipeline is not None and self.level_save in ['MEDIUM', 'HIGH']:\npkl_path = os.path.join(self.model_dir, \"sklearn_pipeline_standalone.pkl\")\n# Save model\nwith open(pkl_path, 'wb') as f:\npickle.dump(self.pipeline, f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_gbt/","title":"Model tfidf gbt","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_gbt/#template_nlp.models_training.models_sklearn.model_tfidf_gbt.ModelTfidfGbt","title":"<code>ModelTfidfGbt</code>","text":"<p>         Bases: <code>ModelPipeline</code></p> <p>Model for predictions via TF-IDF + GBT</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_gbt.py</code> <pre><code>class ModelTfidfGbt(ModelPipeline):\n'''Model for predictions via TF-IDF + GBT'''\n_default_name = 'model_tfidf_gbt'\ndef __init__(self, tfidf_params: Union[dict, None] = None, gbt_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n        Kwargs:\n            tfidf_params (dict) : Parameters for the tfidf\n            gbt_params (dict) : parameters for the gbt\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif tfidf_params is None:\ntfidf_params = {}\nself.tfidf = TfidfVectorizer(**tfidf_params)\nif gbt_params is None:\ngbt_params = {}\nself.gbt = GradientBoostingClassifier(**gbt_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', OneVsRestClassifier(self.gbt))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', OneVsOneClassifier(self.gbt))])\nelse:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', self.gbt)])\n# Manage multi-labels -&gt; add a MultiOutputClassifier\n# The GBT does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', MultiOutputClassifier(self.gbt))])\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Probabilities prediction on the test dataset\n            'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Use super() of Pipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test, **kwargs)\n# We return 1 if predicted, otherwise 0\nelse:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\n# No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.tfidf = self.pipeline['tfidf']\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.gbt = self.pipeline['gbt']\nelse:\nself.gbt = self.pipeline['gbt'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_gbt/#template_nlp.models_training.models_sklearn.model_tfidf_gbt.ModelTfidfGbt.__init__","title":"<code>__init__(tfidf_params=None, gbt_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)</p> Kwargs <p>tfidf_params (dict) : Parameters for the tfidf gbt_params (dict) : parameters for the gbt multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_gbt.py</code> <pre><code>def __init__(self, tfidf_params: Union[dict, None] = None, gbt_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n    Kwargs:\n        tfidf_params (dict) : Parameters for the tfidf\n        gbt_params (dict) : parameters for the gbt\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif tfidf_params is None:\ntfidf_params = {}\nself.tfidf = TfidfVectorizer(**tfidf_params)\nif gbt_params is None:\ngbt_params = {}\nself.gbt = GradientBoostingClassifier(**gbt_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', OneVsRestClassifier(self.gbt))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', OneVsOneClassifier(self.gbt))])\nelse:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', self.gbt)])\n# Manage multi-labels -&gt; add a MultiOutputClassifier\n# The GBT does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('gbt', MultiOutputClassifier(self.gbt))])\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_gbt/#template_nlp.models_training.models_sklearn.model_tfidf_gbt.ModelTfidfGbt.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Probabilities prediction on the test dataset     'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_gbt.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Probabilities prediction on the test dataset\n        'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Use super() of Pipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test, **kwargs)\n# We return 1 if predicted, otherwise 0\nelse:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_gbt/#template_nlp.models_training.models_sklearn.model_tfidf_gbt.ModelTfidfGbt.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_gbt.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.tfidf = self.pipeline['tfidf']\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.gbt = self.pipeline['gbt']\nelse:\nself.gbt = self.pipeline['gbt'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_gbt/#template_nlp.models_training.models_sklearn.model_tfidf_gbt.ModelTfidfGbt.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_gbt.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\n# No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_lgbm/","title":"Model tfidf lgbm","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_lgbm/#template_nlp.models_training.models_sklearn.model_tfidf_lgbm.ModelTfidfLgbm","title":"<code>ModelTfidfLgbm</code>","text":"<p>         Bases: <code>ModelPipeline</code></p> <p>Model for predictions via TF-IDF + LGBM</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_lgbm.py</code> <pre><code>class ModelTfidfLgbm(ModelPipeline):\n'''Model for predictions via TF-IDF + LGBM'''\n_default_name = 'model_tfidf_lgbm'\ndef __init__(self, tfidf_params: Union[dict, None] = None, lgbm_params: Union[dict, None] = None,\nmulticlass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n        Kwargs:\n            tfidf_params (dict) : Parameters for the tfidf\n            lgbm_params (dict) : Parameters for the lgbm\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif tfidf_params is None:\ntfidf_params = {}\nself.tfidf = TfidfVectorizer(**tfidf_params)\nif lgbm_params is None:\nlgbm_params = {}\nself.lgbm = LGBMClassifier(**lgbm_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', OneVsRestClassifier(self.lgbm))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', OneVsOneClassifier(self.lgbm))])\nelse:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', self.lgbm)])\n# Manage multi-labels -&gt; add a MultiOutputClassifier\n# The LGBM does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', MultiOutputClassifier(self.lgbm))])\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Probabilities prediction on the test dataset\n            'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Use super() of Pipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test, **kwargs)\n# We return 1 if predicted, otherwise 0\nelse:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\n# No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)  # Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.tfidf = self.pipeline['tfidf']\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.lgbm = self.pipeline['lgbm']\nelse:\nself.lgbm = self.pipeline['lgbm'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_lgbm/#template_nlp.models_training.models_sklearn.model_tfidf_lgbm.ModelTfidfLgbm.__init__","title":"<code>__init__(tfidf_params=None, lgbm_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)</p> Kwargs <p>tfidf_params (dict) : Parameters for the tfidf lgbm_params (dict) : Parameters for the lgbm multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_lgbm.py</code> <pre><code>def __init__(self, tfidf_params: Union[dict, None] = None, lgbm_params: Union[dict, None] = None,\nmulticlass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n    Kwargs:\n        tfidf_params (dict) : Parameters for the tfidf\n        lgbm_params (dict) : Parameters for the lgbm\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif tfidf_params is None:\ntfidf_params = {}\nself.tfidf = TfidfVectorizer(**tfidf_params)\nif lgbm_params is None:\nlgbm_params = {}\nself.lgbm = LGBMClassifier(**lgbm_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', OneVsRestClassifier(self.lgbm))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', OneVsOneClassifier(self.lgbm))])\nelse:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', self.lgbm)])\n# Manage multi-labels -&gt; add a MultiOutputClassifier\n# The LGBM does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('lgbm', MultiOutputClassifier(self.lgbm))])\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_lgbm/#template_nlp.models_training.models_sklearn.model_tfidf_lgbm.ModelTfidfLgbm.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Probabilities prediction on the test dataset     'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_lgbm.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Probabilities prediction on the test dataset\n        'ovo' can't predict probabilities. By default we return 1 if it is the predicted class, 0 otherwise.\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Use super() of Pipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test, **kwargs)\n# We return 1 if predicted, otherwise 0\nelse:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_lgbm/#template_nlp.models_training.models_sklearn.model_tfidf_lgbm.ModelTfidfLgbm.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_lgbm.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)  # Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.tfidf = self.pipeline['tfidf']\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.lgbm = self.pipeline['lgbm']\nelse:\nself.lgbm = self.pipeline['lgbm'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_lgbm/#template_nlp.models_training.models_sklearn.model_tfidf_lgbm.ModelTfidfLgbm.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_lgbm.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\n# No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/","title":"Model tfidf sgdc","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/#template_nlp.models_training.models_sklearn.model_tfidf_sgdc.ModelTfidfSgdc","title":"<code>ModelTfidfSgdc</code>","text":"<p>         Bases: <code>ModelPipeline</code></p> <p>Model for predictions via TF-IDF + SGDClassier</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_sgdc.py</code> <pre><code>class ModelTfidfSgdc(ModelPipeline):\n'''Model for predictions via TF-IDF + SGDClassier'''\n_default_name = 'model_tfidf_sgdc'\ndef __init__(self, tfidf_params: Union[dict, None] = None, sgdc_params: Union[dict, None] = None,\nmulticlass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n        Kwargs:\n            tfidf_params (dict) : Parameters for the tfidf\n            sgdc_params (dict) : Parameters for the sgdc\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif tfidf_params is None:\ntfidf_params = {}\nself.tfidf = TfidfVectorizer(**tfidf_params)\nif sgdc_params is None:\nsgdc_params = {}\nself.sgdc = SGDClassifier(**sgdc_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', OneVsRestClassifier(self.sgdc))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', OneVsOneClassifier(self.sgdc))])\nelse:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', self.sgdc)])\n# Manage multi-labels -&gt; add a MultiOutputClassifier\n# The SGDClassifier does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', MultiOutputClassifier(self.sgdc))])\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        - /!\\\\ THE SGDC DOES NOT RETURN PROBABILITIES UNDER SOME CONDITIONS HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /!\\\\ -\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Can't use probabilities if loss not in ['log', 'modified_huber'] or 'ovo' and not multi-labels\nif self.sgdc.loss not in ['log', 'modified_huber'] or (self.multiclass_strategy == 'ovo' and not self.multi_label):\nif not self.multi_label:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nelse:\npreds = self.pipeline.predict(x_test)\n# Already right format, but in int !\nprobas = np.array([[float(_) for _ in x] for x in preds])\n# Otherwise, classic probabilities\nelse:\nprobas = super().predict_proba(x_test, **kwargs)\nreturn probas\ndef get_predict_position(self, x_test, y_true, **kwargs) -&gt; np.ndarray:\n'''Gets the order of predictions of y_true.\n        Positions start at 1 (not 0)\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n            y_true (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            np.ndarray: Array, shape = [n_samples]\n        '''\nif self.sgdc.loss not in ['log', 'modified_huber']:\nself.logger.warning(\"Warning, the method get_predict_position is not suitable for a SGDC model (except for the losses 'log' and 'modified_huber')\"\n\"(no probabilities, we use 1 or 0)\")\nreturn super().get_predict_position(x_test, y_true)\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\n# No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.tfidf = self.pipeline['tfidf']\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.sgdc = self.pipeline['sgdc']\nelse:\nself.sgdc = self.pipeline['sgdc'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/#template_nlp.models_training.models_sklearn.model_tfidf_sgdc.ModelTfidfSgdc.__init__","title":"<code>__init__(tfidf_params=None, sgdc_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)</p> Kwargs <p>tfidf_params (dict) : Parameters for the tfidf sgdc_params (dict) : Parameters for the sgdc multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_sgdc.py</code> <pre><code>def __init__(self, tfidf_params: Union[dict, None] = None, sgdc_params: Union[dict, None] = None,\nmulticlass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n    Kwargs:\n        tfidf_params (dict) : Parameters for the tfidf\n        sgdc_params (dict) : Parameters for the sgdc\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif tfidf_params is None:\ntfidf_params = {}\nself.tfidf = TfidfVectorizer(**tfidf_params)\nif sgdc_params is None:\nsgdc_params = {}\nself.sgdc = SGDClassifier(**sgdc_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', OneVsRestClassifier(self.sgdc))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', OneVsOneClassifier(self.sgdc))])\nelse:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', self.sgdc)])\n# Manage multi-labels -&gt; add a MultiOutputClassifier\n# The SGDClassifier does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('sgdc', MultiOutputClassifier(self.sgdc))])\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/#template_nlp.models_training.models_sklearn.model_tfidf_sgdc.ModelTfidfSgdc.get_predict_position","title":"<code>get_predict_position(x_test, y_true, **kwargs)</code>","text":"<p>Gets the order of predictions of y_true. Positions start at 1 (not 0)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: Array, shape = [n_samples]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_sgdc.py</code> <pre><code>def get_predict_position(self, x_test, y_true, **kwargs) -&gt; np.ndarray:\n'''Gets the order of predictions of y_true.\n    Positions start at 1 (not 0)\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        y_true (?): Array-like, shape = [n_samples, n_features]\n    Returns:\n        np.ndarray: Array, shape = [n_samples]\n    '''\nif self.sgdc.loss not in ['log', 'modified_huber']:\nself.logger.warning(\"Warning, the method get_predict_position is not suitable for a SGDC model (except for the losses 'log' and 'modified_huber')\"\n\"(no probabilities, we use 1 or 0)\")\nreturn super().get_predict_position(x_test, y_true)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/#template_nlp.models_training.models_sklearn.model_tfidf_sgdc.ModelTfidfSgdc.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - /! THE SGDC DOES NOT RETURN PROBABILITIES UNDER SOME CONDITIONS HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /! -</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_sgdc.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n    - /!\\\\ THE SGDC DOES NOT RETURN PROBABILITIES UNDER SOME CONDITIONS HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /!\\\\ -\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Can't use probabilities if loss not in ['log', 'modified_huber'] or 'ovo' and not multi-labels\nif self.sgdc.loss not in ['log', 'modified_huber'] or (self.multiclass_strategy == 'ovo' and not self.multi_label):\nif not self.multi_label:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nelse:\npreds = self.pipeline.predict(x_test)\n# Already right format, but in int !\nprobas = np.array([[float(_) for _ in x] for x in preds])\n# Otherwise, classic probabilities\nelse:\nprobas = super().predict_proba(x_test, **kwargs)\nreturn probas\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/#template_nlp.models_training.models_sklearn.model_tfidf_sgdc.ModelTfidfSgdc.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_sgdc.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.tfidf = self.pipeline['tfidf']\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.sgdc = self.pipeline['sgdc']\nelse:\nself.sgdc = self.pipeline['sgdc'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_sgdc/#template_nlp.models_training.models_sklearn.model_tfidf_sgdc.ModelTfidfSgdc.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_sgdc.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\n# No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/","title":"Model tfidf svm","text":""},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm","title":"<code>ModelTfidfSvm</code>","text":"<p>         Bases: <code>ModelPipeline</code></p> <p>Model for predictions via TF-IDF + SVM</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>class ModelTfidfSvm(ModelPipeline):\n'''Model for predictions via TF-IDF + SVM'''\n_default_name = 'model_tfidf_svm'\ndef __init__(self, tfidf_params: Union[dict, None] = None, svc_params: Union[dict, None] = None,\nmulticlass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n        Kwargs:\n            tfidf_params (dict) : Parameters for the tfidf\n            svc_params (dict) : Parameters for the SVC\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif tfidf_params is None:\ntfidf_params = {}\nself.tfidf = TfidfVectorizer(**tfidf_params)\nif svc_params is None:\nsvc_params = {}\nself.svc = LinearSVC(**svc_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', OneVsRestClassifier(self.svc))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', OneVsOneClassifier(self.svc))])\nelse:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', self.svc)])\n# Manage multi-labels -&gt; add a MultiOutputClassifier\n# The SVC does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', MultiOutputClassifier(self.svc))])\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        - /!\\\\ THE SVM DOES NOT RETURN PROBABILITIES, HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /!\\\\ -\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nif not self.multi_label:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nelse:\npreds = self.pipeline.predict(x_test)\n# Already right format, but in int !\nprobas = np.array([[float(_) for _ in x] for x in preds])\nreturn probas\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef decision_function(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predict confidence scores for samples\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Returns:\n            (?): Array, shape = [n_samples]\n        '''\nif self.multi_label:\nraise ValueError(\"The method 'decision_function' is not compatible with a multi-labels case.\")\nreturn self.pipeline.decision_function(x_test)\ndef get_predict_position(self, x_test, y_true, **kwargs) -&gt; np.ndarray:\n'''Gets the order of predictions of y_true.\n        Positions start at 1 (not 0)\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n            y_true (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            np.ndarray: Array, shape = [n_samples]\n        '''\nself.logger.warning(\"Warning, the method get_predict_position is not suitable for a SVM model\"\n\"(no probabilities, we use 1 or 0)\")\nreturn super().get_predict_position(x_test, y_true)\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\n# No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.tfidf = self.pipeline['tfidf']\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.svc = self.pipeline['svc']\nelse:\nself.svc = self.pipeline['svc'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm.__init__","title":"<code>__init__(tfidf_params=None, svc_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)</p> Kwargs <p>tfidf_params (dict) : Parameters for the tfidf svc_params (dict) : Parameters for the SVC multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>def __init__(self, tfidf_params: Union[dict, None] = None, svc_params: Union[dict, None] = None,\nmulticlass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline &amp; ModelClass for more arguments)\n    Kwargs:\n        tfidf_params (dict) : Parameters for the tfidf\n        svc_params (dict) : Parameters for the SVC\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif tfidf_params is None:\ntfidf_params = {}\nself.tfidf = TfidfVectorizer(**tfidf_params)\nif svc_params is None:\nsvc_params = {}\nself.svc = LinearSVC(**svc_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', OneVsRestClassifier(self.svc))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', OneVsOneClassifier(self.svc))])\nelse:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', self.svc)])\n# Manage multi-labels -&gt; add a MultiOutputClassifier\n# The SVC does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('tfidf', self.tfidf), ('svc', MultiOutputClassifier(self.svc))])\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm.decision_function","title":"<code>decision_function(x_test, **kwargs)</code>","text":"<p>Predict confidence scores for samples</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>?</code> <p>Array, shape = [n_samples]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef decision_function(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predict confidence scores for samples\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (?): Array, shape = [n_samples]\n    '''\nif self.multi_label:\nraise ValueError(\"The method 'decision_function' is not compatible with a multi-labels case.\")\nreturn self.pipeline.decision_function(x_test)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm.get_predict_position","title":"<code>get_predict_position(x_test, y_true, **kwargs)</code>","text":"<p>Gets the order of predictions of y_true. Positions start at 1 (not 0)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: Array, shape = [n_samples]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>def get_predict_position(self, x_test, y_true, **kwargs) -&gt; np.ndarray:\n'''Gets the order of predictions of y_true.\n    Positions start at 1 (not 0)\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        y_true (?): Array-like, shape = [n_samples, n_features]\n    Returns:\n        np.ndarray: Array, shape = [n_samples]\n    '''\nself.logger.warning(\"Warning, the method get_predict_position is not suitable for a SVM model\"\n\"(no probabilities, we use 1 or 0)\")\nreturn super().get_predict_position(x_test, y_true)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - /! THE SVM DOES NOT RETURN PROBABILITIES, HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /! -</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n    - /!\\\\ THE SVM DOES NOT RETURN PROBABILITIES, HERE WE SIMULATE PROBABILITIES EQUAL TO 0 OR 1 /!\\\\ -\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\nif not self.multi_label:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nelse:\npreds = self.pipeline.predict(x_test)\n# Already right format, but in int !\nprobas = np.array([[float(_) for _ in x] for x in preds])\nreturn probas\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.tfidf = self.pipeline['tfidf']\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.svc = self.pipeline['svc']\nelse:\nself.svc = self.pipeline['svc'].estimator\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_sklearn/model_tfidf_svm/#template_nlp.models_training.models_sklearn.model_tfidf_svm.ModelTfidfSvm.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_sklearn/model_tfidf_svm.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\n# No need to save the parameters of the pipeline steps, it is already done in ModelPipeline\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/","title":"Models tensorflow","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_cnn/","title":"Model embedding cnn","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_cnn/#template_nlp.models_training.models_tensorflow.model_embedding_cnn.ModelEmbeddingCnn","title":"<code>ModelEmbeddingCnn</code>","text":"<p>         Bases: <code>ModelKeras</code></p> <p>Model for predictions via embedding + CNN</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_cnn.py</code> <pre><code>class ModelEmbeddingCnn(ModelKeras):\n'''Model for predictions via embedding + CNN'''\n_default_name = 'model_embedding_cnn'\ndef __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\npadding: str = 'pre', truncating: str = 'post',\ntokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n            max_words (int): Maximum number of words for tokenization\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n            tokenizer_filters (str): Filter to use by the tokenizer\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        '''\nif padding not in ['pre', 'post']:\nraise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\nif truncating not in ['pre', 'post']:\nraise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nself.max_sequence_length = max_sequence_length\nself.max_words = max_words\nself.padding = padding\nself.truncating = truncating\n# Tokenizer set on fit\nself.tokenizer: Any = None\nself.tokenizer_filters = tokenizer_filters\ndef _prepare_x_train(self, x_train) -&gt; np.ndarray:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n# Get tokenizer &amp; fit on train\nself.tokenizer = Tokenizer(num_words=self.max_words, filters=self.tokenizer_filters)\nself.logger.info('Fitting the tokenizer')\nself.tokenizer.fit_on_texts(x_train)\nreturn self._get_sequence(x_train, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\ndef _prepare_x_test(self, x_test) -&gt; np.ndarray:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n# Get sequences on test (already fitted on train)\nreturn self._get_sequence(x_test, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\ndef _get_model(self, custom_tokenizer=None) -&gt; Any:\n'''Gets a model structure - returns the instance model instead if already defined\n        Kwargs:\n            custom_tokenizer (?): Tokenizer (if different from the one of the class). Permits to manage \"new embeddings\"\n        Returns:\n            (Model): a Keras model\n        '''\n# Return model if already set\nif self.model is not None:\nreturn self.model\n# Start by getting embedding matrix\nif custom_tokenizer is not None:\nembedding_matrix, embedding_size = self._get_embedding_matrix(custom_tokenizer)\nelse:\nembedding_matrix, embedding_size = self._get_embedding_matrix(self.tokenizer)\n# Get input dim\ninput_dim = embedding_matrix.shape[0]\n# Get model\nnum_classes = len(self.list_classes)\n# Process\nmodel = Sequential()\nmodel.add(Embedding(input_dim, embedding_size,\nweights=[embedding_matrix],\ninput_length=self.max_sequence_length,\ntrainable=False))\nmodel.add(Conv1D(128, 3, activation=None, kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization(momentum=0.9))\nmodel.add(ELU(alpha=1.0))\nmodel.add(AveragePooling1D(2))\nmodel.add(Conv1D(128, 3, activation=None, kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization(momentum=0.9))\nmodel.add(ELU(alpha=1.0))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(512, activation=None, kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization(momentum=0.9))\nmodel.add(ELU(alpha=1.0))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(512, activation=None, kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization(momentum=0.9))\nmodel.add(ELU(alpha=1.0))\nmodel.add(Dropout(0.5))\n# Last layer\nactivation = 'sigmoid' if self.multi_label else 'softmax'\nmodel.add(Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform'))\n# Compile model\nlr = self.keras_params.get('learning_rate', 0.002)\ndecay = self.keras_params.get('decay', 0.0)\nself.logger.info(f\"Learning rate: {lr}\")\nself.logger.info(f\"Decay: {decay}\")\noptimizer = Adam(lr=lr, decay=decay)\n# loss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\nloss = 'binary_crossentropy' if self.multi_label else 'categorical_crossentropy'  # utils_deep_keras.f1_loss also possible if multi-labels\nmetrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall]\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nmodel.summary()\n# Try to save model as png if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._save_model_png(model)\n# Return\nreturn model\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add specific data\njson_data['max_sequence_length'] = self.max_sequence_length\njson_data['max_words'] = self.max_words\njson_data['padding'] = self.padding\njson_data['truncating'] = self.truncating\njson_data['tokenizer_filters'] = self.tokenizer_filters\n# Save tokenizer if not None &amp; level_save &gt; LOW\nif (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n# Save as pickle\nwith open(tokenizer_path, 'wb') as f:\npickle.dump(self.tokenizer, f)\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            tokenizer_path (str): Path to tokenizer file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If tokenizer_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object tokenizer_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntokenizer_path = kwargs.get('tokenizer_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tokenizer_path is None:\nraise ValueError(\"The argument tokenizer_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tokenizer_path):\nraise FileNotFoundError(f\"The file {tokenizer_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'keras_params', 'embedding_name', 'max_sequence_length',\n'max_words', 'padding', 'truncating', 'tokenizer_filters']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tokenizer\nwith open(tokenizer_path, 'rb') as f:\nself.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_cnn/#template_nlp.models_training.models_tensorflow.model_embedding_cnn.ModelEmbeddingCnn.__init__","title":"<code>__init__(max_sequence_length=200, max_words=100000, padding='pre', truncating='post', tokenizer_filters='\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\"', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)</p> Kwargs <p>max_sequence_length (int): Maximum number of words per sequence (ie. sentences) max_words (int): Maximum number of words for tokenization padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length) tokenizer_filters (str): Filter to use by the tokenizer</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object padding is not a valid choice (['pre', 'post'])</p> <code>ValueError</code> <p>If the object truncating is not a valid choice (['pre', 'post'])</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_cnn.py</code> <pre><code>def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\npadding: str = 'pre', truncating: str = 'post',\ntokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n    Kwargs:\n        max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n        max_words (int): Maximum number of words for tokenization\n        padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n        truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        tokenizer_filters (str): Filter to use by the tokenizer\n    Raises:\n        ValueError: If the object padding is not a valid choice (['pre', 'post'])\n        ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n    '''\nif padding not in ['pre', 'post']:\nraise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\nif truncating not in ['pre', 'post']:\nraise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nself.max_sequence_length = max_sequence_length\nself.max_words = max_words\nself.padding = padding\nself.truncating = truncating\n# Tokenizer set on fit\nself.tokenizer: Any = None\nself.tokenizer_filters = tokenizer_filters\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_cnn/#template_nlp.models_training.models_tensorflow.model_embedding_cnn.ModelEmbeddingCnn.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file tokenizer_path (str): Path to tokenizer file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hdf5_path is None</p> <code>ValueError</code> <p>If tokenizer_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object tokenizer_path is not an existing file</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_cnn.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        tokenizer_path (str): Path to tokenizer file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If tokenizer_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object tokenizer_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntokenizer_path = kwargs.get('tokenizer_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tokenizer_path is None:\nraise ValueError(\"The argument tokenizer_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tokenizer_path):\nraise FileNotFoundError(f\"The file {tokenizer_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'keras_params', 'embedding_name', 'max_sequence_length',\n'max_words', 'padding', 'truncating', 'tokenizer_filters']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tokenizer\nwith open(tokenizer_path, 'rb') as f:\nself.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_cnn/#template_nlp.models_training.models_tensorflow.model_embedding_cnn.ModelEmbeddingCnn.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_cnn.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add specific data\njson_data['max_sequence_length'] = self.max_sequence_length\njson_data['max_words'] = self.max_words\njson_data['padding'] = self.padding\njson_data['truncating'] = self.truncating\njson_data['tokenizer_filters'] = self.tokenizer_filters\n# Save tokenizer if not None &amp; level_save &gt; LOW\nif (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n# Save as pickle\nwith open(tokenizer_path, 'wb') as f:\npickle.dump(self.tokenizer, f)\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm/","title":"Model embedding lstm","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm/#template_nlp.models_training.models_tensorflow.model_embedding_lstm.ModelEmbeddingLstm","title":"<code>ModelEmbeddingLstm</code>","text":"<p>         Bases: <code>ModelKeras</code></p> <p>Model for prediction via embedding + LSTM</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm.py</code> <pre><code>class ModelEmbeddingLstm(ModelKeras):\n'''Model for prediction via embedding + LSTM'''\n_default_name = 'model_embedding_lstm'\ndef __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\npadding: str = 'pre', truncating: str = 'post',\ntokenizer_filters=\"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n            max_words (int): Maximum number of words for tokenization\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n            tokenizer_filters (str): Filter to use by the tokenizer\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        '''\nif padding not in ['pre', 'post']:\nraise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\nif truncating not in ['pre', 'post']:\nraise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nself.max_sequence_length = max_sequence_length\nself.max_words = max_words\nself.padding = padding\nself.truncating = truncating\n# Tokenizer set on fit\nself.tokenizer: Any = None\nself.tokenizer_filters = tokenizer_filters\ndef _prepare_x_train(self, x_train) -&gt; np.ndarray:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n# Get tokenizer &amp; fit on train\nself.tokenizer = Tokenizer(num_words=self.max_words, filters=self.tokenizer_filters)\nself.logger.info('Fitting the tokenizer')\nself.tokenizer.fit_on_texts(x_train)\nreturn self._get_sequence(x_train, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\ndef _prepare_x_test(self, x_test) -&gt; np.ndarray:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n# Get sequences on test (already fitted on train)\nreturn self._get_sequence(x_test, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\ndef _get_model(self, custom_tokenizer=None) -&gt; Any:\n'''Gets a model structure - returns the instance model instead if already defined\n        Kwargs:\n            custom_tokenizer (?): Tokenizer (if different from the one of the class). Permits to manage \"new embeddings\"\n        Returns:\n            (Model): a Keras model\n        '''\n# Return model if already set\nif self.model is not None:\nreturn self.model\n# Start by getting embedding matrix\nif custom_tokenizer is not None:\nembedding_matrix, embedding_size = self._get_embedding_matrix(custom_tokenizer)\nelse:\nembedding_matrix, embedding_size = self._get_embedding_matrix(self.tokenizer)\n# Get input dim\ninput_dim = embedding_matrix.shape[0]\n# Get model\nnum_classes = len(self.list_classes)\n# Process\nLSTM_UNITS = 100\nDENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\nwords = Input(shape=(self.max_sequence_length,))\nx = Embedding(input_dim, embedding_size, weights=[embedding_matrix], trainable=False)(words)\nx = BatchNormalization(momentum=0.9)(x)\nx = SpatialDropout1D(0.5)(x)\nx = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\nx = SpatialDropout1D(0.5)(x)\nhidden = concatenate([\nGlobalMaxPooling1D()(x),\nGlobalAveragePooling1D()(x),\n])\nhidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n# Last layer\nactivation = 'sigmoid' if self.multi_label else 'softmax'\nout = Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform')(hidden)\n# Compile model\nmodel = Model(inputs=words, outputs=[out])\nlr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.01\ndecay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.004\nself.logger.info(f\"Learning rate: {lr}\")\nself.logger.info(f\"Decay: {decay}\")\noptimizer = Adam(lr=lr, decay=decay)\n# loss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\nloss = 'binary_crossentropy' if self.multi_label else 'categorical_crossentropy'  # utils_deep_keras.f1_loss also possible if multi-labels\nmetrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall]\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nmodel.summary()\n# Try to save model as png if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._save_model_png(model)\n# Return\nreturn model\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add specific data\njson_data['max_sequence_length'] = self.max_sequence_length\njson_data['max_words'] = self.max_words\njson_data['padding'] = self.padding\njson_data['truncating'] = self.truncating\njson_data['tokenizer_filters'] = self.tokenizer_filters\n# Save tokenizer if not None &amp; level_save &gt; LOW\nif (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n# Save as pickle\nwith open(tokenizer_path, 'wb') as f:\npickle.dump(self.tokenizer, f)\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            tokenizer_path (str): Path to tokenizer file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If tokenizer_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object tokenizer_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntokenizer_path = kwargs.get('tokenizer_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tokenizer_path is None:\nraise ValueError(\"The argument tokenizer_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tokenizer_path):\nraise FileNotFoundError(f\"The file {tokenizer_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'keras_params', 'embedding_name', 'max_sequence_length',\n'max_words', 'padding', 'truncating', 'tokenizer_filters']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tokenizer\nwith open(tokenizer_path, 'rb') as f:\nself.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm/#template_nlp.models_training.models_tensorflow.model_embedding_lstm.ModelEmbeddingLstm.__init__","title":"<code>__init__(max_sequence_length=200, max_words=100000, padding='pre', truncating='post', tokenizer_filters='\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\"', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)</p> Kwargs <p>max_sequence_length (int): Maximum number of words per sequence (ie. sentences) max_words (int): Maximum number of words for tokenization padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length) tokenizer_filters (str): Filter to use by the tokenizer</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object padding is not a valid choice (['pre', 'post'])</p> <code>ValueError</code> <p>If the object truncating is not a valid choice (['pre', 'post'])</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm.py</code> <pre><code>def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\npadding: str = 'pre', truncating: str = 'post',\ntokenizer_filters=\"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n    Kwargs:\n        max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n        max_words (int): Maximum number of words for tokenization\n        padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n        truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        tokenizer_filters (str): Filter to use by the tokenizer\n    Raises:\n        ValueError: If the object padding is not a valid choice (['pre', 'post'])\n        ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n    '''\nif padding not in ['pre', 'post']:\nraise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\nif truncating not in ['pre', 'post']:\nraise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nself.max_sequence_length = max_sequence_length\nself.max_words = max_words\nself.padding = padding\nself.truncating = truncating\n# Tokenizer set on fit\nself.tokenizer: Any = None\nself.tokenizer_filters = tokenizer_filters\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm/#template_nlp.models_training.models_tensorflow.model_embedding_lstm.ModelEmbeddingLstm.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file tokenizer_path (str): Path to tokenizer file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hdf5_path is None</p> <code>ValueError</code> <p>If tokenizer_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object tokenizer_path is not an existing file</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        tokenizer_path (str): Path to tokenizer file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If tokenizer_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object tokenizer_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntokenizer_path = kwargs.get('tokenizer_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tokenizer_path is None:\nraise ValueError(\"The argument tokenizer_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tokenizer_path):\nraise FileNotFoundError(f\"The file {tokenizer_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'keras_params', 'embedding_name', 'max_sequence_length',\n'max_words', 'padding', 'truncating', 'tokenizer_filters']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tokenizer\nwith open(tokenizer_path, 'rb') as f:\nself.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm/#template_nlp.models_training.models_tensorflow.model_embedding_lstm.ModelEmbeddingLstm.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add specific data\njson_data['max_sequence_length'] = self.max_sequence_length\njson_data['max_words'] = self.max_words\njson_data['padding'] = self.padding\njson_data['truncating'] = self.truncating\njson_data['tokenizer_filters'] = self.tokenizer_filters\n# Save tokenizer if not None &amp; level_save &gt; LOW\nif (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n# Save as pickle\nwith open(tokenizer_path, 'wb') as f:\npickle.dump(self.tokenizer, f)\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention/","title":"Model embedding lstm attention","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_attention.ModelEmbeddingLstmAttention","title":"<code>ModelEmbeddingLstmAttention</code>","text":"<p>         Bases: <code>ModelKeras</code></p> <p>Model for predictions via embedding + LSTM + Attention</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention.py</code> <pre><code>class ModelEmbeddingLstmAttention(ModelKeras):\n'''Model for predictions via embedding + LSTM + Attention'''\n_default_name = 'model_embedding_lstm_attention'\ndef __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\npadding: str = 'pre', truncating: str = 'post',\ntokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n            max_words (int): Maximum number of words for tokenization\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n            tokenizer_filters (str): Filter to use by the tokenizer\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        '''\nif padding not in ['pre', 'post']:\nraise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\nif truncating not in ['pre', 'post']:\nraise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nself.max_sequence_length = max_sequence_length\nself.max_words = max_words\nself.padding = padding\nself.truncating = truncating\n# Tokenizer set on fit\nself.tokenizer: Any = None\nself.tokenizer_filters = tokenizer_filters\ndef _prepare_x_train(self, x_train) -&gt; np.ndarray:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n# Get tokenizer &amp; fit on train\nself.tokenizer = Tokenizer(num_words=self.max_words, filters=self.tokenizer_filters)\nself.logger.info('Fitting the tokenizer')\nself.tokenizer.fit_on_texts(x_train)\nreturn self._get_sequence(x_train, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\ndef _prepare_x_test(self, x_test) -&gt; np.ndarray:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n# Get sequences on test (already fitted on train)\nreturn self._get_sequence(x_test, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\ndef _get_model(self, custom_tokenizer=None) -&gt; Any:\n'''Gets a model structure - returns the instance model instead if already defined\n        Kwargs:\n            custom_tokenizer (?): Tokenizer (if different from the one of the class). Permits to manage \"new embeddings\"\n        Returns:\n            (Model): a Keras model\n        '''\n# Return model if already set\nif self.model is not None:\nreturn self.model\n# Start by getting embedding matrix\nif custom_tokenizer is not None:\nembedding_matrix, embedding_size = self._get_embedding_matrix(custom_tokenizer)\nelse:\nembedding_matrix, embedding_size = self._get_embedding_matrix(self.tokenizer)\n# Get input dim\ninput_dim = embedding_matrix.shape[0]\n# Get model\nnum_classes = len(self.list_classes)\n# Process\nLSTM_UNITS = 100\nwords = Input(shape=(self.max_sequence_length,))\n# trainable=True to finetune the model\n# words = Input(shape=(None,))\n# x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\nx = Embedding(input_dim, embedding_size, weights=[embedding_matrix], trainable=False)(words)\nx = BatchNormalization(momentum=0.9)(x)\nx = SpatialDropout1D(0.5)(x)\n# LSTM and GRU will default to CuDNNLSTM and CuDNNGRU if all conditions are met:\n# - activation = 'tanh'\n# - recurrent_activation = 'sigmoid'\n# - recurrent_dropout = 0\n# - unroll = False\n# - use_bias = True\n# - Inputs, if masked, are strictly right-padded\n# - reset_after = True (GRU only)\n# /!\\ https://stackoverflow.com/questions/60468385/is-there-cudnnlstm-or-cudnngru-alternative-in-tensorflow-2-0\nx = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)  # returns a sequence of vectors of dimension 32\nx = Bidirectional(GRU(LSTM_UNITS, return_sequences=True))(x)  # returns a sequence of vectors of dimension 32\natt = AttentionWithContext()(x)\navg_pool1 = GlobalAveragePooling1D()(x)\nmax_pool1 = GlobalMaxPooling1D()(x)\nx = concatenate([att, avg_pool1, max_pool1])\n# Last layer\nactivation = 'sigmoid' if self.multi_label else 'softmax'\nout = Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform')(x)\n# Compile model\nmodel = Model(inputs=words, outputs=[out])\nlr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.001\ndecay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\nself.logger.info(f\"Learning rate: {lr}\")\nself.logger.info(f\"Decay: {decay}\")\noptimizer = Adam(lr=lr, decay=decay)\nloss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\n# loss = 'binary_crossentropy' if self.multi_label else 'categorical_crossentropy'  # utils_deep_keras.f1_loss also possible if multi-labels\nmetrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall]\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nmodel.summary()\n# Try to save model as png if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._save_model_png(model)\n# Return\nreturn model\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add specific data\njson_data['max_sequence_length'] = self.max_sequence_length\njson_data['max_words'] = self.max_words\njson_data['padding'] = self.padding\njson_data['truncating'] = self.truncating\njson_data['tokenizer_filters'] = self.tokenizer_filters\n# Save tokenizer if not None &amp; level_save &gt; LOW\nif (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n# Save as pickle\nwith open(tokenizer_path, 'wb') as f:\npickle.dump(self.tokenizer, f)\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            tokenizer_path (str): Path to tokenizer file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If tokenizer_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object tokenizer_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntokenizer_path = kwargs.get('tokenizer_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tokenizer_path is None:\nraise ValueError(\"The argument tokenizer_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tokenizer_path):\nraise FileNotFoundError(f\"The file {tokenizer_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'embedding_name', 'max_sequence_length', 'max_words', 'padding',\n'truncating', 'tokenizer_filters', 'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tokenizer\nwith open(tokenizer_path, 'rb') as f:\nself.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_attention.ModelEmbeddingLstmAttention.__init__","title":"<code>__init__(max_sequence_length=200, max_words=100000, padding='pre', truncating='post', tokenizer_filters='\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\"', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)</p> Kwargs <p>max_sequence_length (int): Maximum number of words per sequence (ie. sentences) max_words (int): Maximum number of words for tokenization padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length) tokenizer_filters (str): Filter to use by the tokenizer</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object padding is not a valid choice (['pre', 'post'])</p> <code>ValueError</code> <p>If the object truncating is not a valid choice (['pre', 'post'])</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention.py</code> <pre><code>def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\npadding: str = 'pre', truncating: str = 'post',\ntokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n    Kwargs:\n        max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n        max_words (int): Maximum number of words for tokenization\n        padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n        truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        tokenizer_filters (str): Filter to use by the tokenizer\n    Raises:\n        ValueError: If the object padding is not a valid choice (['pre', 'post'])\n        ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n    '''\nif padding not in ['pre', 'post']:\nraise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\nif truncating not in ['pre', 'post']:\nraise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nself.max_sequence_length = max_sequence_length\nself.max_words = max_words\nself.padding = padding\nself.truncating = truncating\n# Tokenizer set on fit\nself.tokenizer: Any = None\nself.tokenizer_filters = tokenizer_filters\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_attention.ModelEmbeddingLstmAttention.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file tokenizer_path (str): Path to tokenizer file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hdf5_path is None</p> <code>ValueError</code> <p>If tokenizer_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object tokenizer_path is not an existing file</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        tokenizer_path (str): Path to tokenizer file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If tokenizer_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object tokenizer_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntokenizer_path = kwargs.get('tokenizer_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tokenizer_path is None:\nraise ValueError(\"The argument tokenizer_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tokenizer_path):\nraise FileNotFoundError(f\"The file {tokenizer_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'embedding_name', 'max_sequence_length', 'max_words', 'padding',\n'truncating', 'tokenizer_filters', 'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tokenizer\nwith open(tokenizer_path, 'rb') as f:\nself.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_attention.ModelEmbeddingLstmAttention.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_attention.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add specific data\njson_data['max_sequence_length'] = self.max_sequence_length\njson_data['max_words'] = self.max_words\njson_data['padding'] = self.padding\njson_data['truncating'] = self.truncating\njson_data['tokenizer_filters'] = self.tokenizer_filters\n# Save tokenizer if not None &amp; level_save &gt; LOW\nif (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n# Save as pickle\nwith open(tokenizer_path, 'wb') as f:\npickle.dump(self.tokenizer, f)\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru_gpu/","title":"Model embedding lstm gru gpu","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru_gpu/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_gru_gpu.ModelEmbeddingLstmGruGpu","title":"<code>ModelEmbeddingLstmGruGpu</code>","text":"<p>         Bases: <code>ModelKeras</code></p> <p>Model for predictions via embedding + LSTM/GRU with use of a GPU</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru_gpu.py</code> <pre><code>class ModelEmbeddingLstmGruGpu(ModelKeras):\n'''Model for predictions via embedding + LSTM/GRU with use of a GPU'''\n_default_name = 'model_embedding_lstm_gru_gpu'\ndef __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\npadding: str = 'pre', truncating: str = 'post',\ntokenizer_filters=\"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n            max_words (int): Maximum number of words for tokenization\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n            tokenizer_filters (str): Filter to use by the tokenizer\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        '''\nif padding not in ['pre', 'post']:\nraise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\nif truncating not in ['pre', 'post']:\nraise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nself.max_sequence_length = max_sequence_length\nself.max_words = max_words\nself.padding = padding\nself.truncating = truncating\n# Tokenizer set on fit\nself.tokenizer: Any = None\nself.tokenizer_filters = tokenizer_filters\ndef _prepare_x_train(self, x_train) -&gt; np.ndarray:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n# Get tokenizer &amp; fit on train\nself.tokenizer = Tokenizer(num_words=self.max_words, filters=self.tokenizer_filters)\nself.logger.info('Fitting the tokenizer')\nself.tokenizer.fit_on_texts(x_train)\nreturn self._get_sequence(x_train, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\ndef _prepare_x_test(self, x_test) -&gt; np.ndarray:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n# Get sequences on test (already fitted on train)\nreturn self._get_sequence(x_test, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\ndef _get_model(self, custom_tokenizer=None) -&gt; Any:\n'''Gets a model structure - returns the instance model instead if already defined\n        Kwargs:\n            custom_tokenizer (?): Tokenizer (if different from the one of the class). Permits to manage \"new embeddings\"\n        Returns:\n            (Model): a Keras model\n        '''\n# Return model if already set\nif self.model is not None:\nreturn self.model\n# Start by getting embedding matrix\nif custom_tokenizer is not None:\nembedding_matrix, embedding_size = self._get_embedding_matrix(custom_tokenizer)\nelse:\nembedding_matrix, embedding_size = self._get_embedding_matrix(self.tokenizer)\n# Get input dim\ninput_dim = embedding_matrix.shape[0]\n# Get model\nnum_classes = len(self.list_classes)\n# Process\nLSTM_UNITS = 60\nGRU_UNITS = 120\nwords = Input(shape=(self.max_sequence_length,))\nx = Embedding(input_dim, embedding_size, weights=[embedding_matrix], trainable=False)(words)\nx = SpatialDropout1D(0.5)(x)\n# LSTM and GRU will default to CuDNNLSTM and CuDNNGRU if all conditions are met:\n# - activation = 'tanh'\n# - recurrent_activation = 'sigmoid'\n# - recurrent_dropout = 0\n# - unroll = False\n# - use_bias = True\n# - Inputs, if masked, are strictly right-padded\n# - reset_after = True (GRU only)\n# /!\\ https://stackoverflow.com/questions/60468385/is-there-cudnnlstm-or-cudnngru-alternative-in-tensorflow-2-0\nx = Bidirectional(LSTM(LSTM_UNITS, return_sequences=True))(x)\nx = BatchNormalization(momentum=0.9)(x)\nx, state_h, state_c = Bidirectional(GRU(GRU_UNITS, return_sequences=True, return_state=True))(x)\nx = BatchNormalization(momentum=0.9)(x)\nstate_h = BatchNormalization(momentum=0.9)(state_h)\nstate_c = BatchNormalization(momentum=0.9)(state_c)\npools = []\npools.append(GlobalAveragePooling1D()(x))\npools.append(state_h)\npools.append(state_c)\npools.append(GlobalMaxPooling1D()(x))\nx = Concatenate()(pools)\nx = Dense(128, activation=None, kernel_initializer=\"he_uniform\")(x)\nx = BatchNormalization(momentum=0.9)(x)\nx = ELU(alpha=1.0)(x)\n# Last layer\nactivation = 'sigmoid' if self.multi_label else 'softmax'\nout = Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform')(x)\n# Compile model\nmodel = Model(inputs=words, outputs=[out])\nlr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.01\ndecay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\nself.logger.info(f\"Learning rate: {lr}\")\nself.logger.info(f\"Decay: {decay}\")\noptimizer = Adam(lr=lr, decay=decay)\nloss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\nmetrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', 'categorical_crossentropy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall, utils_deep_keras.f1_loss]\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nmodel.summary()\n# Try to save model as png if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._save_model_png(model)\n# Return\nreturn model\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add specific data\njson_data['max_sequence_length'] = self.max_sequence_length\njson_data['max_words'] = self.max_words\njson_data['padding'] = self.padding\njson_data['truncating'] = self.truncating\njson_data['tokenizer_filters'] = self.tokenizer_filters\n# Save tokenizer if not None &amp; level_save &gt; LOW\nif (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n# Save as pickle\nwith open(tokenizer_path, 'wb') as f:\npickle.dump(self.tokenizer, f)\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            tokenizer_path (str): Path to tokenizer file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If tokenizer_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object tokenizer_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntokenizer_path = kwargs.get('tokenizer_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tokenizer_path is None:\nraise ValueError(\"The argument tokenizer_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tokenizer_path):\nraise FileNotFoundError(f\"The file {tokenizer_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'keras_params', 'embedding_name', 'max_sequence_length',\n'max_words', 'padding', 'truncating', 'tokenizer_filters']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tokenizer\nwith open(tokenizer_path, 'rb') as f:\nself.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru_gpu/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_gru_gpu.ModelEmbeddingLstmGruGpu.__init__","title":"<code>__init__(max_sequence_length=200, max_words=100000, padding='pre', truncating='post', tokenizer_filters='\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\"', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)</p> Kwargs <p>max_sequence_length (int): Maximum number of words per sequence (ie. sentences) max_words (int): Maximum number of words for tokenization padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length) tokenizer_filters (str): Filter to use by the tokenizer</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object padding is not a valid choice (['pre', 'post'])</p> <code>ValueError</code> <p>If the object truncating is not a valid choice (['pre', 'post'])</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru_gpu.py</code> <pre><code>def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\npadding: str = 'pre', truncating: str = 'post',\ntokenizer_filters=\"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n    Kwargs:\n        max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n        max_words (int): Maximum number of words for tokenization\n        padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n        truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        tokenizer_filters (str): Filter to use by the tokenizer\n    Raises:\n        ValueError: If the object padding is not a valid choice (['pre', 'post'])\n        ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n    '''\nif padding not in ['pre', 'post']:\nraise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\nif truncating not in ['pre', 'post']:\nraise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nself.max_sequence_length = max_sequence_length\nself.max_words = max_words\nself.padding = padding\nself.truncating = truncating\n# Tokenizer set on fit\nself.tokenizer: Any = None\nself.tokenizer_filters = tokenizer_filters\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru_gpu/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_gru_gpu.ModelEmbeddingLstmGruGpu.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file tokenizer_path (str): Path to tokenizer file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hdf5_path is None</p> <code>ValueError</code> <p>If tokenizer_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object tokenizer_path is not an existing file</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru_gpu.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        tokenizer_path (str): Path to tokenizer file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If tokenizer_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object tokenizer_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntokenizer_path = kwargs.get('tokenizer_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tokenizer_path is None:\nraise ValueError(\"The argument tokenizer_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tokenizer_path):\nraise FileNotFoundError(f\"The file {tokenizer_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'keras_params', 'embedding_name', 'max_sequence_length',\n'max_words', 'padding', 'truncating', 'tokenizer_filters']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tokenizer\nwith open(tokenizer_path, 'rb') as f:\nself.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru_gpu/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_gru_gpu.ModelEmbeddingLstmGruGpu.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_gru_gpu.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add specific data\njson_data['max_sequence_length'] = self.max_sequence_length\njson_data['max_words'] = self.max_words\njson_data['padding'] = self.padding\njson_data['truncating'] = self.truncating\njson_data['tokenizer_filters'] = self.tokenizer_filters\n# Save tokenizer if not None &amp; level_save &gt; LOW\nif (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n# Save as pickle\nwith open(tokenizer_path, 'wb') as f:\npickle.dump(self.tokenizer, f)\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention/","title":"Model embedding lstm structured attention","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_structured_attention.ModelEmbeddingLstmStructuredAttention","title":"<code>ModelEmbeddingLstmStructuredAttention</code>","text":"<p>         Bases: <code>ModelKeras</code></p> <p>Model for predictions via embedding + LSTM + structured attention -&gt; useful to get predictions explenation. Based on Ga\u00eblle JOUIS Thesis. Work in progress.</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention.py</code> <pre><code>class ModelEmbeddingLstmStructuredAttention(ModelKeras):\n'''Model for predictions via embedding + LSTM + structured attention -&gt; useful to get predictions explenation.\n    Based on Ga\u00eblle JOUIS Thesis.\n    Work in progress.\n    '''\n_default_name = 'model_embedding_lstm_structured_attention'\ndef __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\npadding: str = 'pre', truncating: str = 'post', oov_token: str = \"oovt\",\ntokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n            max_words (int): Maximum number of words for tokenization\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n            oov_token (str): Out Of Vocabulary token (to be used with the Tokenizer)\n            tokenizer_filters (str): Filter to use by the tokenizer\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        '''\nif padding not in ['pre', 'post']:\nraise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\nif truncating not in ['pre', 'post']:\nraise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nself.max_sequence_length = max_sequence_length\nself.max_words = max_words\nself.padding = padding\nself.truncating = truncating\nself.oov_token = oov_token\n# Tokenizer set on fit\nself.tokenizer: Any = None\nself.tokenizer_filters = tokenizer_filters\ndef _prepare_x_train(self, x_train) -&gt; np.ndarray:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n# Get tokenizer &amp; fit on train\nself.tokenizer = Tokenizer(num_words=self.max_words, filters=self.tokenizer_filters, oov_token=self.oov_token)\nself.logger.info('Fitting the tokenizer')\nself.tokenizer.fit_on_texts(x_train)\nreturn self._get_sequence(x_train, self.tokenizer, self.max_sequence_length, padding=self.padding, truncating=self.truncating)\ndef _prepare_x_test(self, x_test, max_sequence_length: int = 0) -&gt; Any:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Kwargs:\n            max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n                Default to self.max_sequence_length.\n                Useful only with explanations.\n                We don't use 'None' as it is a particular usage for _get_sequence.\n                Hence we backup on default value if max_sequence_length is 0.\n        Returns:\n            (np.ndarray): Prepared data\n        '''\nif max_sequence_length == 0:\nmax_sequence_length = self.max_sequence_length\n# Get sequences on test (already fitted on train)\nreturn self._get_sequence(x_test, self.tokenizer, max_sequence_length, padding=self.padding, truncating=self.truncating)\ndef _get_model(self, custom_tokenizer=None) -&gt; Any:\n'''Gets a model structure - returns the instance model instead if already defined\n        Kwargs:\n            custom_tokenizer (?): Tokenizer (if different from the one of the class). Permits to manage \"new embeddings\"\n        Returns:\n            (Model): a Keras model\n        '''\n# Return model if already set\nif self.model is not None:\nreturn self.model\n# Get parameters\nlstm_units = self.keras_params['lstm_units'] if 'lstm_units' in self.keras_params.keys() else 50  # u = 50 in the GIT implementation, 300 in the paper (YELP)\ndense_size = self.keras_params['dense_size'] if 'dense_size' in self.keras_params.keys() else 300  # d_a = 100 in the GIT implementation, 350 in the paper (YELP)\nattention_hops = self.keras_params['attention_hops'] if 'attention_hops' in self.keras_params.keys() else 1  # r = 10 in the GIT implementation, 30 in the paper (YELP)\nlr = self.keras_params['lr'] if 'lr' in self.keras_params.keys() else 0.01\n# Start by getting embedding matrix\nif custom_tokenizer is not None:\nembedding_matrix, embedding_size = self._get_embedding_matrix(custom_tokenizer)\nelse:\nembedding_matrix, embedding_size = self._get_embedding_matrix(self.tokenizer)\n# Get input dim\ninput_dim = embedding_matrix.shape[0]\n# Get model\nnum_classes = len(self.list_classes)\n# Process\nwords = Input(shape=(self.max_sequence_length,))\nx = Embedding(input_dim, embedding_size, weights=[embedding_matrix], trainable=False)(words)\nh = Bidirectional(LSTM(lstm_units, return_sequences=True))(x)\nx = Dense(dense_size, activation='tanh')(h)  # tanh(W_{S1}*H^T) , H^T = x (LSTM output), dim = d_a*2u\na = Dense(attention_hops, activation=utils_deep_keras.softmax_axis)(x)  # softmax(W_{s2}*X) = A\nat = tf.transpose(a, perm=[0, 2, 1], name=\"attention_layer\")  # At, used in Kaushalshetty project, output dim = (r,n)\n# Trick to name the attention layer (does not work with TensorFlow layers)\n# https://github.com/keras-team/keras/issues/6194#issuecomment-416365112\nat_identity = Lambda(lambda x: x, name=\"attention_layer\")(at)\nm = at_identity @ h  # M = AH\nx = AttentionAverage(attention_hops)(m)\n# Last layer\nactivation = 'sigmoid' if self.multi_label else 'softmax'\nout = Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform')(x)\n# Compile model\nmodel = Model(inputs=words, outputs=[out])\noptimizer = Adam(lr=lr)\n# optimizer = SGD(lr=0.06, clipnorm=0.5)  # paper: 0.06 YELP\nloss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\nmetrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', 'categorical_crossentropy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall, utils_deep_keras.f1_loss]\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nmodel.summary()\n# Try to save model as png if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._save_model_png(model)\n# Return\nreturn model\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef explain(self, x_test, attention_threshold: float = 0.15, fix_index: bool = False) -&gt; list:\n'''Predictions on test set, with all attentions weights\n        -&gt; explanations on preprocessed words\n        This function returns a list of dictionnaries:\n            {\n                index: (word, value)\n            }\n        where:\n            - index: word index in the sequence (after tokenization &amp; padding)\n            - word: the corresponding word after preprocessing\n            - value: the attention value for this word\n        Precision: if fix_index is set to True, the output indexes correspond to the word index in the preprocessed sentence (and not in the sequence)\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples]\n                WARNING : sentences must be preprocessed here\n        Kwargs:\n            attention_threshold (float): Minimum attention threshold\n            fix_index (bool): If we have to fix sequences index to get word indexes in the preprocessed sentence\n        Returns:\n            list: List of dictionnaries (one entry per sentence) with matched words (i.e. attention &gt; threshold)\n        '''\n# Cast en pd.Series\nx_test = pd.Series(x_test)\n# Prepare input\nx_test_prep = self._prepare_x_test(x_test)\n# Retrieve attention scores\nintermediate_layer_model = Model(inputs=self.model.input, outputs=self.model.get_layer('attention_layer').output)  # type: ignore\nintermediate_output = intermediate_layer_model(x_test_prep)\n# Retrieve (word, attention) tuples\nintermediate_output_reshaped = tf.squeeze(intermediate_output)\n# Manage cases where x_test has only one element\nif len(intermediate_output_reshaped.shape) == 1:\nintermediate_output_reshaped = tf.expand_dims(intermediate_output_reshaped, axis=0)\nseq_q_attention = tf.stack([x_test_prep, intermediate_output_reshaped], axis=1)\nseq_q_attention = tf.transpose(seq_q_attention, [0, 2, 1])\nseq_q_attention = seq_q_attention.numpy()\ntext_w_attention = [[(self.tokenizer.sequences_to_texts([[y[0]]]), y[1])  # type: ignore\nif y[0] != 0 else (\"&lt;PAD&gt;\", y[1]) for y in entry]\nfor entry in seq_q_attention]\n# Filter words with low attention score\nselected_words = [{index: (tup[0][0], tup[1]) for index, tup in enumerate(entry) if tup[0] != '&lt;PAD&gt;' and tup[1] &gt;= attention_threshold} for entry in text_w_attention]\n# If wanted, fix indexes\nif fix_index:\n# Process sentence per sentence\nfor i, entry in enumerate(selected_words):\n# Case 1 : padding\n# Check for padding (at least one 0)\nnb_padding = len(np.where(x_test_prep[i] == 0)[0])\npadded = True if nb_padding != 0 else False\nif self.padding == 'pre':\nif nb_padding != 0:\n# We shift the sequence (i.e. we remove the padding)\nselected_words[i] = {index - nb_padding: val for index, val in entry.items()}\nelse:\npass  # We do nothing (already in correct ordre if post padding)\n# Case 2 : truncating\nif not padded:\nif self.truncating == 'pre':\n# We must reapply get_sequences with max sequence length to retrieve removed words\nx_test_full = self._prepare_x_test([x_test[i]], max_sequence_length=None)[0]  # type: ignore\nnb_truncating = len(x_test_full) - len(x_test_prep[i])\nif nb_truncating != 0:\n# We shift the sequence to take truncation in account\nselected_words[i] = {index + nb_truncating: val for index, val in entry.items()}\nelse:\npass  # We do nothing (already in correct ordre if post truncating)\n# Returns\nreturn selected_words\ndef _pad_text(self, text: list, pad_token: str = '&lt;PAD&gt;') -&gt; list:\n'''Apply padding on a tokenized text (list)\n        Args:\n            text (list): List of tokenized words\n        Kwargs:\n            pad_token (str): Default pad token\n        Returns:\n            list: List of tokenized words, with padding / truncating management\n        '''\n# If there is too much words, we truncate the sequence\nif len(text) &gt; self.max_sequence_length:\nif self.truncating == 'post':\ntext = text[:self.max_sequence_length]\nelse:  # pre\ntext = text[-self.max_sequence_length:]\n# If there is not enough words, we pad the sequence\nelif len(text) &lt; self.max_sequence_length:\npadding_list = [pad_token for i in range(self.max_sequence_length - len(text))]\nif self.padding == 'pre':\ntext = padding_list + text\nelse:\ntext = text + padding_list\n# Return\nreturn text\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add specific data\njson_data['max_sequence_length'] = self.max_sequence_length\njson_data['max_words'] = self.max_words\njson_data['padding'] = self.padding\njson_data['truncating'] = self.truncating\njson_data['oov_token'] = self.oov_token\njson_data['tokenizer_filters'] = self.tokenizer_filters\n# Save tokenizer if not None &amp; level_save &gt; LOW\nif (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n# Save as pickle\nwith open(tokenizer_path, 'wb') as f:\npickle.dump(self.tokenizer, f)\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            tokenizer_path (str): Path to tokenizer file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If tokenizer_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object tokenizer_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntokenizer_path = kwargs.get('tokenizer_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tokenizer_path is None:\nraise ValueError(\"The argument tokenizer_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tokenizer_path):\nraise FileNotFoundError(f\"The file {tokenizer_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'embedding_name', 'max_sequence_length', 'max_words', 'padding',\n'truncating', 'oov_token', 'tokenizer_filters', 'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tokenizer\nwith open(tokenizer_path, 'rb') as f:\nself.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_structured_attention.ModelEmbeddingLstmStructuredAttention.__init__","title":"<code>__init__(max_sequence_length=200, max_words=100000, padding='pre', truncating='post', oov_token='oovt', tokenizer_filters='\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\"', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)</p> Kwargs <p>max_sequence_length (int): Maximum number of words per sequence (ie. sentences) max_words (int): Maximum number of words for tokenization padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length) oov_token (str): Out Of Vocabulary token (to be used with the Tokenizer) tokenizer_filters (str): Filter to use by the tokenizer</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object padding is not a valid choice (['pre', 'post'])</p> <code>ValueError</code> <p>If the object truncating is not a valid choice (['pre', 'post'])</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention.py</code> <pre><code>def __init__(self, max_sequence_length: int = 200, max_words: int = 100000,\npadding: str = 'pre', truncating: str = 'post', oov_token: str = \"oovt\",\ntokenizer_filters: str = \"\u2019!#$%&amp;()*+,-./:;&lt;=&gt;?@[\\\\]^_`{|}~\\t\\n\\r\\'\\\"\", **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments)\n    Kwargs:\n        max_sequence_length (int): Maximum number of words per sequence (ie. sentences)\n        max_words (int): Maximum number of words for tokenization\n        padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n        truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        oov_token (str): Out Of Vocabulary token (to be used with the Tokenizer)\n        tokenizer_filters (str): Filter to use by the tokenizer\n    Raises:\n        ValueError: If the object padding is not a valid choice (['pre', 'post'])\n        ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n    '''\nif padding not in ['pre', 'post']:\nraise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\nif truncating not in ['pre', 'post']:\nraise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nself.max_sequence_length = max_sequence_length\nself.max_words = max_words\nself.padding = padding\nself.truncating = truncating\nself.oov_token = oov_token\n# Tokenizer set on fit\nself.tokenizer: Any = None\nself.tokenizer_filters = tokenizer_filters\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_structured_attention.ModelEmbeddingLstmStructuredAttention.explain","title":"<code>explain(x_test, attention_threshold=0.15, fix_index=False)</code>","text":"<p>Predictions on test set, with all attentions weights -&gt; explanations on preprocessed words</p> This function returns a list of dictionnaries <p>{     index: (word, value) }</p> where <ul> <li>index: word index in the sequence (after tokenization &amp; padding)</li> <li>word: the corresponding word after preprocessing</li> <li>value: the attention value for this word</li> </ul> <p>Precision: if fix_index is set to True, the output indexes correspond to the word index in the preprocessed sentence (and not in the sequence)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples] WARNING : sentences must be preprocessed here</p> required Kwargs <p>attention_threshold (float): Minimum attention threshold fix_index (bool): If we have to fix sequences index to get word indexes in the preprocessed sentence</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of dictionnaries (one entry per sentence) with matched words (i.e. attention &gt; threshold)</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef explain(self, x_test, attention_threshold: float = 0.15, fix_index: bool = False) -&gt; list:\n'''Predictions on test set, with all attentions weights\n    -&gt; explanations on preprocessed words\n    This function returns a list of dictionnaries:\n        {\n            index: (word, value)\n        }\n    where:\n        - index: word index in the sequence (after tokenization &amp; padding)\n        - word: the corresponding word after preprocessing\n        - value: the attention value for this word\n    Precision: if fix_index is set to True, the output indexes correspond to the word index in the preprocessed sentence (and not in the sequence)\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples]\n            WARNING : sentences must be preprocessed here\n    Kwargs:\n        attention_threshold (float): Minimum attention threshold\n        fix_index (bool): If we have to fix sequences index to get word indexes in the preprocessed sentence\n    Returns:\n        list: List of dictionnaries (one entry per sentence) with matched words (i.e. attention &gt; threshold)\n    '''\n# Cast en pd.Series\nx_test = pd.Series(x_test)\n# Prepare input\nx_test_prep = self._prepare_x_test(x_test)\n# Retrieve attention scores\nintermediate_layer_model = Model(inputs=self.model.input, outputs=self.model.get_layer('attention_layer').output)  # type: ignore\nintermediate_output = intermediate_layer_model(x_test_prep)\n# Retrieve (word, attention) tuples\nintermediate_output_reshaped = tf.squeeze(intermediate_output)\n# Manage cases where x_test has only one element\nif len(intermediate_output_reshaped.shape) == 1:\nintermediate_output_reshaped = tf.expand_dims(intermediate_output_reshaped, axis=0)\nseq_q_attention = tf.stack([x_test_prep, intermediate_output_reshaped], axis=1)\nseq_q_attention = tf.transpose(seq_q_attention, [0, 2, 1])\nseq_q_attention = seq_q_attention.numpy()\ntext_w_attention = [[(self.tokenizer.sequences_to_texts([[y[0]]]), y[1])  # type: ignore\nif y[0] != 0 else (\"&lt;PAD&gt;\", y[1]) for y in entry]\nfor entry in seq_q_attention]\n# Filter words with low attention score\nselected_words = [{index: (tup[0][0], tup[1]) for index, tup in enumerate(entry) if tup[0] != '&lt;PAD&gt;' and tup[1] &gt;= attention_threshold} for entry in text_w_attention]\n# If wanted, fix indexes\nif fix_index:\n# Process sentence per sentence\nfor i, entry in enumerate(selected_words):\n# Case 1 : padding\n# Check for padding (at least one 0)\nnb_padding = len(np.where(x_test_prep[i] == 0)[0])\npadded = True if nb_padding != 0 else False\nif self.padding == 'pre':\nif nb_padding != 0:\n# We shift the sequence (i.e. we remove the padding)\nselected_words[i] = {index - nb_padding: val for index, val in entry.items()}\nelse:\npass  # We do nothing (already in correct ordre if post padding)\n# Case 2 : truncating\nif not padded:\nif self.truncating == 'pre':\n# We must reapply get_sequences with max sequence length to retrieve removed words\nx_test_full = self._prepare_x_test([x_test[i]], max_sequence_length=None)[0]  # type: ignore\nnb_truncating = len(x_test_full) - len(x_test_prep[i])\nif nb_truncating != 0:\n# We shift the sequence to take truncation in account\nselected_words[i] = {index + nb_truncating: val for index, val in entry.items()}\nelse:\npass  # We do nothing (already in correct ordre if post truncating)\n# Returns\nreturn selected_words\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_structured_attention.ModelEmbeddingLstmStructuredAttention.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file tokenizer_path (str): Path to tokenizer file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hdf5_path is None</p> <code>ValueError</code> <p>If tokenizer_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object tokenizer_path is not an existing file</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        tokenizer_path (str): Path to tokenizer file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If tokenizer_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object tokenizer_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntokenizer_path = kwargs.get('tokenizer_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tokenizer_path is None:\nraise ValueError(\"The argument tokenizer_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tokenizer_path):\nraise FileNotFoundError(f\"The file {tokenizer_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'embedding_name', 'max_sequence_length', 'max_words', 'padding',\n'truncating', 'oov_token', 'tokenizer_filters', 'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tokenizer\nwith open(tokenizer_path, 'rb') as f:\nself.tokenizer = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention/#template_nlp.models_training.models_tensorflow.model_embedding_lstm_structured_attention.ModelEmbeddingLstmStructuredAttention.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_embedding_lstm_structured_attention.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add specific data\njson_data['max_sequence_length'] = self.max_sequence_length\njson_data['max_words'] = self.max_words\njson_data['padding'] = self.padding\njson_data['truncating'] = self.truncating\njson_data['oov_token'] = self.oov_token\njson_data['tokenizer_filters'] = self.tokenizer_filters\n# Save tokenizer if not None &amp; level_save &gt; LOW\nif (self.tokenizer is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntokenizer_path = os.path.join(self.model_dir, \"embedding_tokenizer.pkl\")\n# Save as pickle\nwith open(tokenizer_path, 'wb') as f:\npickle.dump(self.tokenizer, f)\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/","title":"Model keras","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras","title":"<code>ModelKeras</code>","text":"<p>         Bases: <code>ModelClass</code></p> <p>Generic model for Keras NN</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>class ModelKeras(ModelClass):\n'''Generic model for Keras NN'''\n_default_name = 'model_keras'\n# Not implemented :\n# -&gt; _prepare_x_train\n# -&gt; _prepare_x_test\n# -&gt; _get_model\n# -&gt; reload_from_standalone\ndef __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\nembedding_name: str = 'cc.fr.300.pkl', keras_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n        Kwargs:\n            batch_size (int): Batch size\n            epochs (int): Number of epochs\n            validation_split (float): Percentage for the validation set split\n                Only used if no input validation set when fitting\n            patience (int): Early stopping patience\n            embedding_name (str) : The name of the embedding matrix to use\n            keras_params (dict): Parameters used by keras models.\n                e.g. learning_rate, nb_lstm_units, etc...\n                The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n                This parameter was initially added in order to do an hyperparameters search\n        '''\n# TODO: learning rate should be an attribute !\n# Init.\nsuper().__init__(**kwargs)\n# Fix tensorflow GPU\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Param. model\nself.batch_size = batch_size\nself.epochs = epochs\nself.validation_split = validation_split\nself.patience = patience\n# Model set on fit\nself.model: Any = None\n# Param embedding (can be None if no embedding)\nself.embedding_name = embedding_name\n# Keras params\nif keras_params is None:\nkeras_params = {}\nself.keras_params = keras_params.copy()\n# Keras custom objects : we get the ones specified in utils_deep_keras\nself.custom_objects = utils_deep_keras.custom_objects\ndef fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Fits the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n            y_valid (?): Array-like, shape = [n_samples, n_targets]\n            with_shuffle (bool): If x, y must be shuffled before fitting\n                Experimental: We must verify if it works as intended depending on the formats of x and y\n                This should be used if y is not shuffled as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            AssertionError: If different classes when comparing an already fitted model and a new dataset\n        '''\n##############################################\n# Manage retrain\n##############################################\n# If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n# And we save the old conf\nif self.trained:\n# Get src files to save\nsrc_files = [os.path.join(self.model_dir, \"configurations.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\nsrc_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Change model dir\nself.model_dir = self._get_model_dir()\n# Get dst files\ndst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\ndst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Copies\nfor src, dst in zip(src_files, dst_files):\ntry:\nshutil.copyfile(src, dst)\nexcept Exception as e:\nself.logger.error(f\"Impossible to copy {src} to {dst}\")\nself.logger.error(\"We still continue ...\")\nself.logger.error(repr(e))\n##############################################\n# Prepare x_train, x_valid, y_train &amp; y_valid\n# Also extract list of classes\n##############################################\n# If not multilabel, transform y_train as dummies (should already be the case for multi-labels)\nif not self.multi_label:\n# If len(array.shape)==2, we flatten the array if the second dimension is useless\nif isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\ny_train = np.ravel(y_train)\nif isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\ny_valid = np.ravel(y_valid)\n# Transformation dummies\ny_train_dummies = pd.get_dummies(y_train)\ny_valid_dummies = pd.get_dummies(y_valid) if y_valid is not None else None\n# Important : get_dummies reorder the columns in alphabetical order\n# Thus, there is no problem if we fit again on a new dataframe with shuffled data\nlist_classes = list(y_train_dummies.columns)\n# FIX: valid test might miss some classes, hence we need to add them back to y_valid_dummies\nif y_valid_dummies is not None and y_train_dummies.shape[1] != y_valid_dummies.shape[1]:\nfor cl in list_classes:\n# Add missing columns\nif cl not in y_valid_dummies.columns:\ny_valid_dummies[cl] = 0\ny_valid_dummies = y_valid_dummies[list_classes]  # Reorder\n# Else keep it as it is\nelse:\ny_train_dummies = y_train\ny_valid_dummies = y_valid\nif hasattr(y_train_dummies, 'columns'):\nlist_classes = list(y_train_dummies.columns)\nelse:\nself.logger.warning(\n\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n)\n# We still create a list of classes in order to be compatible with other functions\nlist_classes = [str(_) for _ in range(pd.DataFrame(y_train_dummies).shape[1])]\n# Set dict_classes based on list classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# Validate classes if already trained, else set them\nif self.trained:\nassert self.list_classes == list_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nassert self.dict_classes == dict_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nelse:\nself.list_classes = list_classes\nself.dict_classes = dict_classes\n# Shuffle x, y if wanted\n# It is advised as validation_split from keras does not shufle the data\n# Hence we might have classes in the validation data that we never met in the training data\nif with_shuffle:\np = np.random.permutation(len(x_train))\nx_train = np.array(x_train)[p]\ny_train_dummies = np.array(y_train_dummies)[p]\n# Else still transform to numpy array\nelse:\nx_train = np.array(x_train)\ny_train_dummies = np.array(y_train_dummies)\n# Also get y_valid_dummies as numpy\ny_valid_dummies = np.array(y_valid_dummies)\n# Prepare x_train\nx_train = self._prepare_x_train(x_train)\n# If available, also prepare x_valid &amp; get validation_data (tuple)\nvalidation_data: Optional[tuple] = None  # Def. None if y_valid is None\nif y_valid is not None:\nx_valid = self._prepare_x_test(x_valid)\nvalidation_data = (x_valid, y_valid_dummies)\nif validation_data is None:\nself.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n##############################################\n# Fit\n##############################################\n# Get model (if already fitted, _get_model returns instance model)\nself.model = self._get_model()\n# Get callbacks (early stopping &amp; checkpoint)\ncallbacks = self._get_callbacks()\n# Fit\n# We use a try...except in order to save the model if an error arises\n# after more than a minute into training\nstart_time = time.time()\ntry:\nfit_history = self.model.fit(  # type: ignore\nx_train,\ny_train_dummies,\nbatch_size=self.batch_size,\nepochs=self.epochs,\nvalidation_split=self.validation_split if validation_data is None else None,\nvalidation_data=validation_data,\ncallbacks=callbacks,\nverbose=1,\n)\nexcept (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, tf.errors.ResourceExhaustedError, tf.errors.InternalError,\ntf.errors.UnavailableError, tf.errors.UnimplementedError, tf.errors.UnknownError, Exception) as e:\n# Steps:\n# 1. Display tensorflow error\n# 2. Check if more than one minute elapsed &amp; existence best.hdf5\n# 3. Reload best model\n# 4. We consider that a fit occured (trained = True, nb_fit += 1)\n# 5. Save &amp; create a warning file\n# 6. Display error messages\n# 7. Raise an error\n# 1.\nself.logger.error(repr(e))\n# 2.\nbest_path = os.path.join(self.model_dir, 'best.hdf5')\ntime_spent = time.time() - start_time\nif time_spent &gt;= 60 and os.path.exists(best_path):\n# 3.\nself.model = load_model(best_path, custom_objects=self.custom_objects)\n# 4.\nself.trained = True\nself.nb_fit += 1\n# 5.\nself.save()\nwith open(os.path.join(self.model_dir, \"0_MODEL_INCOMPLETE\"), 'w'):\npass\nwith open(os.path.join(self.model_dir, \"1_TRAINING_NEEDS_TO_BE_RESUMED\"), 'w'):\npass\n# 6.\nself.logger.error(\"[EXPERIMENTAL] Error during model training\")\nself.logger.error(f\"[EXPERIMENTAL] The error happened after {round(time_spent, 2)}s of training\")\nself.logger.error(\"[EXPERIMENTAL] A saving of the model is done but this model won't be usable as is.\")\nself.logger.error(f\"[EXPERIMENTAL] In order to resume the training, we have to specify this model ({ntpath.basename(self.model_dir)}) in the file 2_training.py\")\nself.logger.error(\"[EXPERIMENTAL] Warning, the preprocessing is not saved in the configuration file\")\nself.logger.error(\"[EXPERIMENTAL] Warning, the best model might be corrupted in some cases\")\n# 7.\nraise RuntimeError(\"Error during model training\")\n# Print accuracy &amp; loss if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\n# Plot accuracy\nself._plot_metrics_and_loss(fit_history)\n# Reload best model\nself.model = load_model(os.path.join(self.model_dir, 'best.hdf5'), custom_objects=self.custom_objects)\n# Set trained\nself.trained = True\nself.nb_fit += 1\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples]\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Cast in pd.Series\nx_test = pd.Series(x_test)\n# Predict\npredicted_proba = self.predict_proba(x_test)\n# We return the probabilities if wanted\nif return_proba:\nreturn predicted_proba\n# Finally, we get the classes predictions\nreturn self.get_classes_from_proba(predicted_proba)\n@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, experimental_version: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n        Kwargs:\n            experimental_version (bool): If an experimental (but faster) version must be used\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Prepare input\nx_test = self._prepare_x_test(x_test)\n# Process\nif experimental_version:\nreturn self.experimental_predict_proba(x_test)\nelse:\nreturn self.model.predict(x_test, batch_size=128, verbose=1)  # type: ignore\n@utils.trained_needed\ndef experimental_predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset - Experimental function\n        Preprocessings must be done before (in predict_proba)\n        Here we only do the prediction and return the result\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n@tf.function\ndef serve(x):\nreturn self.model(x, training=False)\nreturn serve(x_test).numpy()\ndef _prepare_x_train(self, x_train) -&gt; np.ndarray:\n'''Prepares the input data for the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\nraise NotImplementedError(\"'_prepare_x_train' needs to be overridden\")\ndef _prepare_x_test(self, x_test) -&gt; np.ndarray:\n'''Prepares the input data for the model\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\nraise NotImplementedError(\"'_prepare_x_test' needs to be overridden\")\ndef _get_embedding_matrix(self, tokenizer) -&gt; Tuple[np.ndarray, int]:\n'''Get embedding matrix\n        Args:\n            tokenizer (?): Tokenizer to use (useful to test with a new matrice embedding)\n        Returns:\n            np.ndarray: Embedding matrix\n            int: Embedding size\n        '''\n# Get embedding indexes\nembedding_indexes = utils_models.get_embedding(self.embedding_name)\n# Get embedding_size\nembedding_size = len(embedding_indexes[list(embedding_indexes.keys())[0]])\n# Get embedding matrix\n# The first line of this matrix is a zero vector\n# The following lines are the projections of the words obtained by the tokenizer (same index)\n# We keep only the max tokens 'num_words'\n# https://github.com/keras-team/keras/issues/8092\nif tokenizer.num_words is None:\nword_index = {e: i for e, i in tokenizer.word_index.items()}\nelse:\nword_index = {e: i for e, i in tokenizer.word_index.items() if i &lt;= tokenizer.num_words}\n# Create embedding matrix\nembedding_matrix = np.zeros((len(word_index) + 1, embedding_size))\n# Fill it\nfor word, i in word_index.items():\nembedding_vector = embedding_indexes.get(word)\nif embedding_vector is not None:\n# words not found in embedding index will be all-zeros.\nembedding_matrix[i] = embedding_vector\nself.logger.info(f\"Size of the embedding matrix (ie. number of matches on the input) : {len(embedding_matrix)}\")\nreturn embedding_matrix, embedding_size\ndef _get_sequence(self, x_test, tokenizer, maxlen: int, padding: str = 'pre', truncating: str = 'post') -&gt; np.ndarray:\n'''Transform input of text into sequences. Needs a tokenizer.\n        Args:\n            x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n            tokenizer (?): Tokenizer to use (useful to test with a new matrice embedding)\n            maxlen (int): maximum sequence length\n        Kwargs:\n            padding (str): Padding (add zeros) at the beginning ('pre') or at the end ('post') of the sequences\n            truncating (str): Truncating the beginning ('pre') or the end ('post') of the sequences (if superior to max_sequence_length)\n        Raises:\n            ValueError: If the object padding is not a valid choice (['pre', 'post'])\n            ValueError: If the object truncating is not a valid choice (['pre', 'post'])\n        Returns:\n            (np.ndarray): Padded sequence\n        '''\nif padding not in ['pre', 'post']:\nraise ValueError(f\"The object padding ({padding}) is not a valid choice (['pre', 'post'])\")\nif truncating not in ['pre', 'post']:\nraise ValueError(f\"The object truncating ({truncating}) is not a valid choice (['pre', 'post'])\")\n# Process\nsequences = tokenizer.texts_to_sequences(x_test)\nreturn pad_sequences(sequences, maxlen=maxlen, padding=padding, truncating=truncating)\ndef _get_model(self) -&gt; Model:\n'''Gets a model structure - returns the instance model instead if already defined\n        Returns:\n            (Model): a Keras model\n        '''\nraise NotImplementedError(\"'_get_model' needs to be overridden\")\ndef _get_callbacks(self) -&gt; list:\n'''Gets model callbacks\n        Returns:\n            list: List of callbacks\n        '''\n# Get classic callbacks\ncallbacks = [EarlyStopping(monitor='val_loss', patience=self.patience, restore_best_weights=True)]\nif self.level_save in ['MEDIUM', 'HIGH']:\ncallbacks.append(\nModelCheckpoint(\nfilepath=os.path.join(self.model_dir, f'best.hdf5'), monitor='val_loss', save_best_only=True, mode='auto'\n)\n)\ncallbacks.append(CSVLogger(filename=os.path.join(self.model_dir, f'logger.csv'), separator=';', append=False))\ncallbacks.append(TerminateOnNaN())\n# Get LearningRateScheduler\nscheduler = self._get_learning_rate_scheduler()\nif scheduler is not None:\ncallbacks.append(LearningRateScheduler(scheduler))\n# Manage tensorboard\nif self.level_save in ['HIGH']:\n# Get log directory\nmodels_path = utils.get_models_path()\ntensorboard_dir = os.path.join(models_path, 'tensorboard_logs')\n# We add a prefix so that the function load_model works correctly (it looks for a sub-folder with model name)\nlog_dir = os.path.join(tensorboard_dir, f\"tensorboard_{ntpath.basename(self.model_dir)}\")\nif not os.path.exists(log_dir):\nos.makedirs(log_dir)\n# TODO: check if this class does not slow proccesses\n# -&gt; For now: comment\n# Create custom class to monitore LR changes\n# https://stackoverflow.com/questions/49127214/keras-how-to-output-learning-rate-onto-tensorboard\n# class LRTensorBoard(TensorBoard):\n#     def __init__(self, log_dir, **kwargs) -&gt; None:  # add other arguments to __init__ if you need\n#         super().__init__(log_dir=log_dir, **kwargs)\n#\n#     def on_epoch_end(self, epoch, logs=None):\n#         logs.update({'lr': K.eval(self.model.optimizer.lr)})\n#         super().on_epoch_end(epoch, logs)\n# Append tensorboard callback\n# TODO: check compatibility tensorflow 2.3\n# WARNING : https://stackoverflow.com/questions/63619763/model-training-hangs-forever-when-using-a-tensorboard-callback-with-an-lstm-laye\n# A compatibility problem TensorBoard / TensorFlow 2.3 (cuDNN implementation of LSTM/GRU) can arise\n# In this case, the training of the model can be \"blocked\" and does not respond anymore\n# This problem has arisen two times on P\u00f4le Emploi computers (windows 7 &amp; VM Ubuntu on windows 7 host)\n# No problem on Valeuriad computers (windows 10)\n# Thus, TensorBoard is deactivated by default for now\n# While awaiting a possible fix, you are responsible for checking if TensorBoard works on your computer\nself.logger.warning(\" ###################### \")\nself.logger.warning(\"TensorBoard deactivated : compatibility problem TensorBoard / TensorFlow 2.3 (cuDNN implementation of LSTM/GRU) can arise\")\nself.logger.warning(\"https://stackoverflow.com/questions/63619763/model-training-hangs-forever-when-using-a-tensorboard-callback-with-an-lstm-laye\")\nself.logger.warning(\" In order to activate if, one has to modify the method _get_callbacks of model_keras.py\")\nself.logger.warning(\" ###################### \")\n# callbacks.append(TensorBoard(log_dir=log_dir, write_grads=False, write_images=False))\n# self.logger.info(f\"To start tensorboard: python -m tensorboard.main --logdir {tensorboard_dir}\")\nreturn callbacks\ndef _get_learning_rate_scheduler(self) -&gt; Union[Callable, None]:\n'''Fonction to define a Learning Rate Scheduler\n           -&gt; if it returns None, no scheduler will be used. (def.)\n           -&gt; This function will be save directly in the model configuration file\n           -&gt; This can be overridden at runing time\n        Returns:\n            (Callable | None): A learning rate Scheduler\n        '''\n# e.g.\n# def scheduler(epoch):\n#     lim_epoch = 75\n#     if epoch &lt; lim_epoch:\n#         return 0.01\n#     else:\n#         return max(0.001, 0.01 * math.exp(0.01 * (lim_epoch - epoch)))\nscheduler = None\nreturn scheduler\ndef _plot_metrics_and_loss(self, fit_history) -&gt; None:\n'''Plots accuracy &amp; loss\n        Arguments:\n            fit_history (?) : fit history\n        '''\n# Manage dir\nplots_path = os.path.join(self.model_dir, 'plots')\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\n# Get a dictionnary of possible metrics/loss plots\nmetrics_dir = {\n'acc': ['Accuracy', 'accuracy'],\n'loss': ['Loss', 'loss'],\n'categorical_accuracy': ['Categorical accuracy', 'categorical_accuracy'],\n'f1': ['F1-score', 'f1_score'],\n'precision': ['Precision', 'precision'],\n'recall': ['Recall', 'recall'],\n}\n# Plot each available metric\nfor metric in fit_history.history.keys():\nif metric in metrics_dir.keys():\ntitle = metrics_dir[metric][0]\nfilename = metrics_dir[metric][1]\nplt.figure(figsize=(10, 8))\nplt.plot(fit_history.history[metric])\nplt.plot(fit_history.history[f'val_{metric}'])\nplt.title(f\"Model {title}\")\nplt.ylabel(title)\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n# Save\nfilename == f\"{filename}.jpeg\"\nplt.savefig(os.path.join(plots_path, filename))\n# Close figures\nplt.close('all')\ndef _save_model_png(self, model) -&gt; None:\n'''Tries to save the structure of the model in png format\n        Graphviz necessary\n        Args:\n            model (?): model to plot\n        '''\n# Check if graphiz is intalled\n# TODO : to be improved !\ngraphiz_path = 'C:/Program Files (x86)/Graphviz2.38/bin/'\nif os.path.isdir(graphiz_path):\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\nimg_path = os.path.join(self.model_dir, 'model.png')\nplot_model(model, to_file=img_path)\n@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'keras'\njson_data['batch_size'] = self.batch_size\njson_data['epochs'] = self.epochs\njson_data['validation_split'] = self.validation_split\njson_data['patience'] = self.patience\njson_data['embedding_name'] = self.embedding_name\njson_data['keras_params'] = self.keras_params\nif self.model is not None:\njson_data['keras_model'] = json.loads(self.model.to_json())\nelse:\njson_data['keras_model'] = None\n# Add _get_model code if not in json_data\nif '_get_model' not in json_data.keys():\njson_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n# Add _get_learning_rate_scheduler code if not in json_data\nif '_get_learning_rate_scheduler' not in json_data.keys():\njson_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n# Add custom_objects code if not in json_data\nif 'custom_objects' not in json_data.keys():\ncustom_objects_str = self.custom_objects.copy()\nfor key in custom_objects_str.keys():\nif callable(custom_objects_str[key]):\n# Nominal case\nif not type(custom_objects_str[key]) == functools.partial:\ncustom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n# Manage partials\nelse:\ncustom_objects_str[key] = {\n'type': 'partial',\n'args': custom_objects_str[key].args,\n'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n}\njson_data['custom_objects'] = custom_objects_str\n# Save strategy :\n# - best.hdf5 already saved in fit()\n# - We don't want it in the .pkl as it is heavy &amp; already saved\nkeras_model = self.model\nself.model = None\nsuper().save(json_data=json_data)\nself.model = keras_model\ndef reload_model(self, hdf5_path: str) -&gt; Any:\n'''Loads a Keras model from a HDF5 file\n        Args:\n            hdf5_path (str): Path to the hdf5 file\n        Returns:\n            ?: Keras model\n        '''\n# Fix tensorflow GPU if not already done (useful if we reload a model)\ntry:\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\nexcept Exception:\npass\n# We check if we already have the custom objects\nif hasattr(self, 'custom_objects') and self.custom_objects is not None:\ncustom_objects = self.custom_objects\nelse:\nself.logger.warning(\"Can't find the attribute 'custom_objects' in the model to be reloaded\")\nself.logger.warning(\"Backup on the default custom_objects of utils_deep_keras\")\ncustom_objects = utils_deep_keras.custom_objects\n# Loading of the model\nkeras_model = load_model(hdf5_path, custom_objects=custom_objects)\n# Set trained to true if not already true\nif not self.trained:\nself.trained = True\nself.nb_fit = 1\n# Return\nreturn keras_model\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Needs to be overridden /!\\\\ -\n        '''\nraise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\ndef _is_gpu_activated(self) -&gt; bool:\n'''Checks if a GPU is used\n        Returns:\n            bool: whether GPU is available or not\n        '''\n# Check for available GPU devices\nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) &gt; 0:\nreturn True\nelse:\nreturn False\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.__init__","title":"<code>__init__(batch_size=64, epochs=99, validation_split=0.2, patience=5, embedding_name='cc.fr.300.pkl', keras_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>batch_size (int): Batch size epochs (int): Number of epochs validation_split (float): Percentage for the validation set split     Only used if no input validation set when fitting patience (int): Early stopping patience embedding_name (str) : The name of the embedding matrix to use keras_params (dict): Parameters used by keras models.     e.g. learning_rate, nb_lstm_units, etc...     The purpose of this dictionary is for the user to use it as they wants in the _get_model function     This parameter was initially added in order to do an hyperparameters search</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>def __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\nembedding_name: str = 'cc.fr.300.pkl', keras_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n    Kwargs:\n        batch_size (int): Batch size\n        epochs (int): Number of epochs\n        validation_split (float): Percentage for the validation set split\n            Only used if no input validation set when fitting\n        patience (int): Early stopping patience\n        embedding_name (str) : The name of the embedding matrix to use\n        keras_params (dict): Parameters used by keras models.\n            e.g. learning_rate, nb_lstm_units, etc...\n            The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n            This parameter was initially added in order to do an hyperparameters search\n    '''\n# TODO: learning rate should be an attribute !\n# Init.\nsuper().__init__(**kwargs)\n# Fix tensorflow GPU\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Param. model\nself.batch_size = batch_size\nself.epochs = epochs\nself.validation_split = validation_split\nself.patience = patience\n# Model set on fit\nself.model: Any = None\n# Param embedding (can be None if no embedding)\nself.embedding_name = embedding_name\n# Keras params\nif keras_params is None:\nkeras_params = {}\nself.keras_params = keras_params.copy()\n# Keras custom objects : we get the ones specified in utils_deep_keras\nself.custom_objects = utils_deep_keras.custom_objects\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.experimental_predict_proba","title":"<code>experimental_predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset - Experimental function Preprocessings must be done before (in predict_proba) Here we only do the prediction and return the result</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>@utils.trained_needed\ndef experimental_predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset - Experimental function\n    Preprocessings must be done before (in predict_proba)\n    Here we only do the prediction and return the result\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n@tf.function\ndef serve(x):\nreturn self.model(x, training=False)\nreturn serve(x_test).numpy()\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required Kwargs <p>x_valid (?): Array-like, shape = [n_samples, n_features] y_valid (?): Array-like, shape = [n_samples, n_targets] with_shuffle (bool): If x, y must be shuffled before fitting     Experimental: We must verify if it works as intended depending on the formats of x and y     This should be used if y is not shuffled as the split_validation takes the lines in order.     Thus, the validation set might get classes which are not in the train set ...</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If different classes when comparing an already fitted model and a new dataset</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Fits the model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features]\n        y_valid (?): Array-like, shape = [n_samples, n_targets]\n        with_shuffle (bool): If x, y must be shuffled before fitting\n            Experimental: We must verify if it works as intended depending on the formats of x and y\n            This should be used if y is not shuffled as the split_validation takes the lines in order.\n            Thus, the validation set might get classes which are not in the train set ...\n    Raises:\n        AssertionError: If different classes when comparing an already fitted model and a new dataset\n    '''\n##############################################\n# Manage retrain\n##############################################\n# If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n# And we save the old conf\nif self.trained:\n# Get src files to save\nsrc_files = [os.path.join(self.model_dir, \"configurations.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\nsrc_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Change model dir\nself.model_dir = self._get_model_dir()\n# Get dst files\ndst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\ndst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Copies\nfor src, dst in zip(src_files, dst_files):\ntry:\nshutil.copyfile(src, dst)\nexcept Exception as e:\nself.logger.error(f\"Impossible to copy {src} to {dst}\")\nself.logger.error(\"We still continue ...\")\nself.logger.error(repr(e))\n##############################################\n# Prepare x_train, x_valid, y_train &amp; y_valid\n# Also extract list of classes\n##############################################\n# If not multilabel, transform y_train as dummies (should already be the case for multi-labels)\nif not self.multi_label:\n# If len(array.shape)==2, we flatten the array if the second dimension is useless\nif isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\ny_train = np.ravel(y_train)\nif isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\ny_valid = np.ravel(y_valid)\n# Transformation dummies\ny_train_dummies = pd.get_dummies(y_train)\ny_valid_dummies = pd.get_dummies(y_valid) if y_valid is not None else None\n# Important : get_dummies reorder the columns in alphabetical order\n# Thus, there is no problem if we fit again on a new dataframe with shuffled data\nlist_classes = list(y_train_dummies.columns)\n# FIX: valid test might miss some classes, hence we need to add them back to y_valid_dummies\nif y_valid_dummies is not None and y_train_dummies.shape[1] != y_valid_dummies.shape[1]:\nfor cl in list_classes:\n# Add missing columns\nif cl not in y_valid_dummies.columns:\ny_valid_dummies[cl] = 0\ny_valid_dummies = y_valid_dummies[list_classes]  # Reorder\n# Else keep it as it is\nelse:\ny_train_dummies = y_train\ny_valid_dummies = y_valid\nif hasattr(y_train_dummies, 'columns'):\nlist_classes = list(y_train_dummies.columns)\nelse:\nself.logger.warning(\n\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n)\n# We still create a list of classes in order to be compatible with other functions\nlist_classes = [str(_) for _ in range(pd.DataFrame(y_train_dummies).shape[1])]\n# Set dict_classes based on list classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# Validate classes if already trained, else set them\nif self.trained:\nassert self.list_classes == list_classes, \\\n            \"Error: the new dataset does not match with the already fitted model\"\nassert self.dict_classes == dict_classes, \\\n            \"Error: the new dataset does not match with the already fitted model\"\nelse:\nself.list_classes = list_classes\nself.dict_classes = dict_classes\n# Shuffle x, y if wanted\n# It is advised as validation_split from keras does not shufle the data\n# Hence we might have classes in the validation data that we never met in the training data\nif with_shuffle:\np = np.random.permutation(len(x_train))\nx_train = np.array(x_train)[p]\ny_train_dummies = np.array(y_train_dummies)[p]\n# Else still transform to numpy array\nelse:\nx_train = np.array(x_train)\ny_train_dummies = np.array(y_train_dummies)\n# Also get y_valid_dummies as numpy\ny_valid_dummies = np.array(y_valid_dummies)\n# Prepare x_train\nx_train = self._prepare_x_train(x_train)\n# If available, also prepare x_valid &amp; get validation_data (tuple)\nvalidation_data: Optional[tuple] = None  # Def. None if y_valid is None\nif y_valid is not None:\nx_valid = self._prepare_x_test(x_valid)\nvalidation_data = (x_valid, y_valid_dummies)\nif validation_data is None:\nself.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n##############################################\n# Fit\n##############################################\n# Get model (if already fitted, _get_model returns instance model)\nself.model = self._get_model()\n# Get callbacks (early stopping &amp; checkpoint)\ncallbacks = self._get_callbacks()\n# Fit\n# We use a try...except in order to save the model if an error arises\n# after more than a minute into training\nstart_time = time.time()\ntry:\nfit_history = self.model.fit(  # type: ignore\nx_train,\ny_train_dummies,\nbatch_size=self.batch_size,\nepochs=self.epochs,\nvalidation_split=self.validation_split if validation_data is None else None,\nvalidation_data=validation_data,\ncallbacks=callbacks,\nverbose=1,\n)\nexcept (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, tf.errors.ResourceExhaustedError, tf.errors.InternalError,\ntf.errors.UnavailableError, tf.errors.UnimplementedError, tf.errors.UnknownError, Exception) as e:\n# Steps:\n# 1. Display tensorflow error\n# 2. Check if more than one minute elapsed &amp; existence best.hdf5\n# 3. Reload best model\n# 4. We consider that a fit occured (trained = True, nb_fit += 1)\n# 5. Save &amp; create a warning file\n# 6. Display error messages\n# 7. Raise an error\n# 1.\nself.logger.error(repr(e))\n# 2.\nbest_path = os.path.join(self.model_dir, 'best.hdf5')\ntime_spent = time.time() - start_time\nif time_spent &gt;= 60 and os.path.exists(best_path):\n# 3.\nself.model = load_model(best_path, custom_objects=self.custom_objects)\n# 4.\nself.trained = True\nself.nb_fit += 1\n# 5.\nself.save()\nwith open(os.path.join(self.model_dir, \"0_MODEL_INCOMPLETE\"), 'w'):\npass\nwith open(os.path.join(self.model_dir, \"1_TRAINING_NEEDS_TO_BE_RESUMED\"), 'w'):\npass\n# 6.\nself.logger.error(\"[EXPERIMENTAL] Error during model training\")\nself.logger.error(f\"[EXPERIMENTAL] The error happened after {round(time_spent, 2)}s of training\")\nself.logger.error(\"[EXPERIMENTAL] A saving of the model is done but this model won't be usable as is.\")\nself.logger.error(f\"[EXPERIMENTAL] In order to resume the training, we have to specify this model ({ntpath.basename(self.model_dir)}) in the file 2_training.py\")\nself.logger.error(\"[EXPERIMENTAL] Warning, the preprocessing is not saved in the configuration file\")\nself.logger.error(\"[EXPERIMENTAL] Warning, the best model might be corrupted in some cases\")\n# 7.\nraise RuntimeError(\"Error during model training\")\n# Print accuracy &amp; loss if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\n# Plot accuracy\nself._plot_metrics_and_loss(fit_history)\n# Reload best model\nself.model = load_model(os.path.join(self.model_dir, 'best.hdf5'), custom_objects=self.custom_objects)\n# Set trained\nself.trained = True\nself.nb_fit += 1\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples]</p> required Kwargs <p>return_proba (bool): If the function should return the probabilities instead of the classes</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples]\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Cast in pd.Series\nx_test = pd.Series(x_test)\n# Predict\npredicted_proba = self.predict_proba(x_test)\n# We return the probabilities if wanted\nif return_proba:\nreturn predicted_proba\n# Finally, we get the classes predictions\nreturn self.get_classes_from_proba(predicted_proba)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.predict_proba","title":"<code>predict_proba(x_test, experimental_version=False, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>Array-like or sparse matrix, shape = [n_samples, n_features]</p> required Kwargs <p>experimental_version (bool): If an experimental (but faster) version must be used</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>@utils.data_agnostic_str_to_list\n@utils.trained_needed\ndef predict_proba(self, x_test, experimental_version: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n    Args:\n        x_test (?): Array-like or sparse matrix, shape = [n_samples, n_features]\n    Kwargs:\n        experimental_version (bool): If an experimental (but faster) version must be used\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Prepare input\nx_test = self._prepare_x_test(x_test)\n# Process\nif experimental_version:\nreturn self.experimental_predict_proba(x_test)\nelse:\nreturn self.model.predict(x_test, batch_size=128, verbose=1)  # type: ignore\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Needs to be overridden /! -</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Needs to be overridden /!\\\\ -\n    '''\nraise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.reload_model","title":"<code>reload_model(hdf5_path)</code>","text":"<p>Loads a Keras model from a HDF5 file</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to the hdf5 file</p> required <p>Returns:</p> Type Description <code>Any</code> <p>?: Keras model</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>def reload_model(self, hdf5_path: str) -&gt; Any:\n'''Loads a Keras model from a HDF5 file\n    Args:\n        hdf5_path (str): Path to the hdf5 file\n    Returns:\n        ?: Keras model\n    '''\n# Fix tensorflow GPU if not already done (useful if we reload a model)\ntry:\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\nexcept Exception:\npass\n# We check if we already have the custom objects\nif hasattr(self, 'custom_objects') and self.custom_objects is not None:\ncustom_objects = self.custom_objects\nelse:\nself.logger.warning(\"Can't find the attribute 'custom_objects' in the model to be reloaded\")\nself.logger.warning(\"Backup on the default custom_objects of utils_deep_keras\")\ncustom_objects = utils_deep_keras.custom_objects\n# Loading of the model\nkeras_model = load_model(hdf5_path, custom_objects=custom_objects)\n# Set trained to true if not already true\nif not self.trained:\nself.trained = True\nself.nb_fit = 1\n# Return\nreturn keras_model\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_keras/#template_nlp.models_training.models_tensorflow.model_keras.ModelKeras.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_keras.py</code> <pre><code>@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'keras'\njson_data['batch_size'] = self.batch_size\njson_data['epochs'] = self.epochs\njson_data['validation_split'] = self.validation_split\njson_data['patience'] = self.patience\njson_data['embedding_name'] = self.embedding_name\njson_data['keras_params'] = self.keras_params\nif self.model is not None:\njson_data['keras_model'] = json.loads(self.model.to_json())\nelse:\njson_data['keras_model'] = None\n# Add _get_model code if not in json_data\nif '_get_model' not in json_data.keys():\njson_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n# Add _get_learning_rate_scheduler code if not in json_data\nif '_get_learning_rate_scheduler' not in json_data.keys():\njson_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n# Add custom_objects code if not in json_data\nif 'custom_objects' not in json_data.keys():\ncustom_objects_str = self.custom_objects.copy()\nfor key in custom_objects_str.keys():\nif callable(custom_objects_str[key]):\n# Nominal case\nif not type(custom_objects_str[key]) == functools.partial:\ncustom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n# Manage partials\nelse:\ncustom_objects_str[key] = {\n'type': 'partial',\n'args': custom_objects_str[key].args,\n'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n}\njson_data['custom_objects'] = custom_objects_str\n# Save strategy :\n# - best.hdf5 already saved in fit()\n# - We don't want it in the .pkl as it is heavy &amp; already saved\nkeras_model = self.model\nself.model = None\nsuper().save(json_data=json_data)\nself.model = keras_model\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_tfidf_dense/","title":"Model tfidf dense","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/model_tfidf_dense/#template_nlp.models_training.models_tensorflow.model_tfidf_dense.ModelTfidfDense","title":"<code>ModelTfidfDense</code>","text":"<p>         Bases: <code>ModelKeras</code></p> <p>Model for predictions via TF-IDF + Dense</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_tfidf_dense.py</code> <pre><code>class ModelTfidfDense(ModelKeras):\n'''Model for predictions via TF-IDF + Dense'''\n_default_name = 'model_tfidf_dense'\ndef __init__(self, tfidf_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments).\n        Kwargs:\n            tfidf_params (dict) : Parameters for the tfidf\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nif tfidf_params is None:\ntfidf_params = {}\nself.tfidf = TfidfVectorizer(**tfidf_params)\ndef _prepare_x_train(self, x_train) -&gt; np.ndarray:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n# Fit tfidf &amp; return x transformed\nself.tfidf.fit(x_train)\n# TODO: Use of todense because tensorflow 2.3 does not support sparse data anymore\nreturn self.tfidf.transform(x_train).todense()\ndef _prepare_x_test(self, x_test) -&gt; np.ndarray:\n'''Prepares the input data for the model. Called when fitting the model\n        Args:\n            x_test (?): Array-like, shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): Prepared data\n        '''\n# Get tf-idf &amp; fit on train\n# TODO: Use of todense because tensorflow 2.3 does not support sparse data anymore\nreturn self.tfidf.transform(x_test).todense()\ndef _get_model(self) -&gt; Model:\n'''Gets a model structure - returns the instance model instead if already defined\n        Returns:\n            (Model): a Keras model\n        '''\n# Return model if already set\nif self.model is not None:\nreturn self.model\n# Get input/output dimensions\ninput_dim = len(self.tfidf.get_feature_names())\nnum_classes = len(self.list_classes)\n# Process\nmodel = Sequential()\nmodel.add(Dense(128, activation=None, kernel_initializer='he_uniform', input_shape=(input_dim,)))\nmodel.add(BatchNormalization(momentum=0.9))\nmodel.add(ELU(alpha=1.0))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation=None, kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization(momentum=0.9))\nmodel.add(ELU(alpha=1.0))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation=None, kernel_initializer='he_uniform'))\nmodel.add(BatchNormalization(momentum=0.9))\nmodel.add(ELU(alpha=1.0))\nmodel.add(Dropout(0.5))\n# Last layer\nactivation = 'sigmoid' if self.multi_label else 'softmax'\nmodel.add(Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform'))\n# Compile model\nlr = self.keras_params.get('learning_rate', 0.002)\ndecay = self.keras_params.get('decay', 0.0)\nself.logger.info(f\"Learning rate: {lr}\")\nself.logger.info(f\"Decay: {decay}\")\noptimizer = Adam(lr=lr, decay=decay)\n# loss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\nloss = 'binary_crossentropy' if self.multi_label else 'categorical_crossentropy'  # utils_deep_keras.f1_loss also possible if multi-labels\nmetrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall]\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nmodel.summary()\n# Try to save model as png if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._save_model_png(model)\n# Return\nreturn model\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add tfidf params\nconfs = self.tfidf.get_params()\n# Get rid of some non serializable conf\nfor special_conf in ['dtype', 'base_estimator']:\nif special_conf in confs.keys():\nconfs[special_conf] = str(confs[special_conf])\njson_data['tfidf_confs'] = confs\n# Save tfidf if not None &amp; level_save &gt; LOW\nif (self.tfidf is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntfidf_path = os.path.join(self.model_dir, \"tfidf_standalone.pkl\")\n# Save as pickle\nwith open(tfidf_path, 'wb') as f:\npickle.dump(self.tfidf, f)\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            tf_idf_path (str): Path to tfidf file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If tf_idf_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object tf_idf_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntfidf_path = kwargs.get('tfidf_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tfidf_path is None:\nraise ValueError(\"The argument tfidf_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tfidf_path):\nraise FileNotFoundError(f\"The file {tfidf_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'keras_params', 'embedding_name']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tfidf\nwith open(tfidf_path, 'rb') as f:\nself.tfidf = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_tfidf_dense/#template_nlp.models_training.models_tensorflow.model_tfidf_dense.ModelTfidfDense.__init__","title":"<code>__init__(tfidf_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelKeras for more arguments).</p> Kwargs <p>tfidf_params (dict) : Parameters for the tfidf</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_tfidf_dense.py</code> <pre><code>def __init__(self, tfidf_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelKeras for more arguments).\n    Kwargs:\n        tfidf_params (dict) : Parameters for the tfidf\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\nif tfidf_params is None:\ntfidf_params = {}\nself.tfidf = TfidfVectorizer(**tfidf_params)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_tfidf_dense/#template_nlp.models_training.models_tensorflow.model_tfidf_dense.ModelTfidfDense.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file tf_idf_path (str): Path to tfidf file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hdf5_path is None</p> <code>ValueError</code> <p>If tf_idf_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object tf_idf_path is not an existing file</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_tfidf_dense.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        tf_idf_path (str): Path to tfidf file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If tf_idf_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object tf_idf_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\ntfidf_path = kwargs.get('tfidf_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif tfidf_path is None:\nraise ValueError(\"The argument tfidf_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(tfidf_path):\nraise FileNotFoundError(f\"The file {tfidf_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'keras_params', 'embedding_name']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload tfidf\nwith open(tfidf_path, 'rb') as f:\nself.tfidf = pickle.load(f)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/model_tfidf_dense/#template_nlp.models_training.models_tensorflow.model_tfidf_dense.ModelTfidfDense.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_nlp/models_training/models_tensorflow/model_tfidf_dense.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Add tfidf params\nconfs = self.tfidf.get_params()\n# Get rid of some non serializable conf\nfor special_conf in ['dtype', 'base_estimator']:\nif special_conf in confs.keys():\nconfs[special_conf] = str(confs[special_conf])\njson_data['tfidf_confs'] = confs\n# Save tfidf if not None &amp; level_save &gt; LOW\nif (self.tfidf is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\ntfidf_path = os.path.join(self.model_dir, \"tfidf_standalone.pkl\")\n# Save as pickle\nwith open(tfidf_path, 'wb') as f:\npickle.dump(self.tfidf, f)\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/","title":"Utils deep keras","text":""},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.AttentionAverage","title":"<code>AttentionAverage</code>","text":"<p>         Bases: <code>Layer</code></p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>class AttentionAverage(Layer):\ndef __init__(self, attention_hops, **kwargs) -&gt; None:\nself.attention_hops = attention_hops\nself.applied_axis = 1\nsuper(AttentionAverage, self).__init__()\ndef get_config(self) -&gt; Any:\n'''Gets the config'''\nconfig = super().get_config().copy()\nconfig.update({\n'attention_hops': self.attention_hops\n})\nreturn config\ndef call(self, input) -&gt; Any:\nreturn tf.divide(tf.reduce_sum(input, self.applied_axis), self.attention_hops)\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.AttentionAverage.get_config","title":"<code>get_config()</code>","text":"<p>Gets the config</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def get_config(self) -&gt; Any:\n'''Gets the config'''\nconfig = super().get_config().copy()\nconfig.update({\n'attention_hops': self.attention_hops\n})\nreturn config\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.AttentionWithContext","title":"<code>AttentionWithContext</code>","text":"<p>         Bases: <code>Layer</code></p> <p>Attention operation, with a context/query vector, for temporal data. Supports Masking. Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf] \"Hierarchical Attention Networks for Document Classification\" by using a context vector to assist the attention</p>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.AttentionWithContext--input-shape","title":"Input shape","text":"<pre><code>3D tensor with shape: `(samples, steps, features)`.\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.AttentionWithContext--output-shape","title":"Output shape","text":"<pre><code>2D tensor with shape: `(samples, features)`.\n</code></pre> <p>:param kwargs: Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True. The dimensions are inferred based on the output shape of the RNN.</p> Example <p>model.add(LSTM(64, return_sequences=True)) model.add(AttentionWithContext())</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>class AttentionWithContext(Layer):\n'''Attention operation, with a context/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    :param kwargs:\n    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n    '''\ndef __init__(self, W_regularizer=None, u_regularizer=None, b_regularizer=None,\nW_constraint=None, u_constraint=None, b_constraint=None, bias=True,\nreturn_attention=False, **kwargs):\nself.return_attention = return_attention\nself.bias = bias\nsuper(AttentionWithContext, self).__init__(**kwargs)\ndef get_config(self) -&gt; Any:\nconfig = super().get_config().copy()\nconfig.update({\n'return_attention': self.return_attention,\n'bias': self.bias\n})\nreturn config\ndef build(self, input_shape) -&gt; None:\nassert len(input_shape) == 3\ninput_shape_list = input_shape.as_list()\nself.W = self.add_weight(shape=((input_shape_list[-1], input_shape_list[-1])),\nname='{}_W'.format(self.name))\nif self.bias:\nself.b = self.add_weight(shape=(input_shape_list[-1],),\nname='{}_b'.format(self.name))\nself.u = self.add_weight(shape=(input_shape_list[-1],),\nname='{}_u'.format(self.name))\nsuper(AttentionWithContext, self).build(input_shape.as_list())\ndef compute_mask(self, input, input_mask=None) -&gt; Any:\n# do not pass the mask to the next layers\nreturn None\ndef call(self, x, mask=None) -&gt; Any:\nuit = tf.tensordot(x, self.W, axes=1)\nif self.bias:\nuit += self.b\nuit = activations.tanh(uit)\nait = tf.tensordot(uit, self.u, axes=1)\na = activations.exponential(ait)\n# Apply mask after the exp. will be re-normalized next\nif mask is not None:\n# Cast the mask to floatX to avoid float64 upcasting in theano\na *= tf.cast(mask, K.floatx())\n# In some cases especially in the early stages of training the sum may be almost zero\n# and this results in NaN's. A workaround is to add a very small positive number \u03b5 to the sum.\n# a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\na /= tf.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\na = K.expand_dims(a)\nweighted_input = x * a\nresult = K.sum(weighted_input, axis=1)\nif self.return_attention:\nreturn [result, a]\nreturn result\ndef compute_output_shape(self, input_shape) -&gt; Any:\nif self.return_attention:\nreturn [(input_shape[0], input_shape[-1]), (input_shape[0], input_shape[1])]\nelse:\nreturn tf.TensorShape([input_shape[0].value, input_shape[-1].value])\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.f1","title":"<code>f1(y_true, y_pred)</code>","text":"<p>f1 score, to use as custom metrics</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def f1(y_true, y_pred) -&gt; float:\n'''f1 score, to use as custom metrics\n    - /!\\\\ To use with a big batch size /!\\\\ -\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n# Round pred to 0 &amp; 1\ny_pred = K.round(y_pred)\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ny_pred = K.round(y_pred)\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n# tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\np = tp / (tp + fp + K.epsilon())\nr = tp / (tp + fn + K.epsilon())\nf1 = 2 * p * r / (p + r + K.epsilon())\nf1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\nweighted_f1 = f1 * ground_positives / K.sum(ground_positives)\nweighted_f1 = K.sum(weighted_f1)\nreturn weighted_f1\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.f1_loss","title":"<code>f1_loss(y_true, y_pred)</code>","text":"<p>f1 loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def f1_loss(y_true, y_pred) -&gt; float:\n'''f1 loss, to use as custom loss\n    - /!\\\\ To use with a big batch size /!\\\\ -\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n# TODO : Find a mean of rounding y_pred\n# TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n# We can't round here :(\n# Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n# Common ops without gradient: K.argmax, K.round, K.eval.\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n# tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\np = tp / (tp + fp + K.epsilon())\nr = tp / (tp + fn + K.epsilon())\nf1 = 2 * p * r / (p + r + K.epsilon())\nf1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\nweighted_f1 = f1 * ground_positives / K.sum(ground_positives)\nweighted_f1 = K.sum(weighted_f1)\nreturn 1 - weighted_f1\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.fb_loss","title":"<code>fb_loss(b, y_true, y_pred)</code>","text":"<p>fB loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> required <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def fb_loss(b: float, y_true, y_pred) -&gt; float:\n'''fB loss, to use as custom loss\n    - /!\\\\ To use with a big batch size /!\\\\ -\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n    Args:\n        b (float): importance recall in the calculation of the fB score\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n# TODO : Find a mean of rounding y_pred\n# TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n# We can't round here :(\n# Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n# Common ops without gradient: K.argmax, K.round, K.eval.\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n# tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\np = tp / (tp + fp + K.epsilon())\nr = tp / (tp + fn + K.epsilon())\nfb = (1 + b**2) * p * r / ((p * b**2) + r + K.epsilon())\nfb = tf.where(tf.math.is_nan(fb), tf.zeros_like(fb), fb)\nweighted_fb = fb * ground_positives / K.sum(ground_positives)\nweighted_fb = K.sum(weighted_fb)\nreturn 1 - weighted_fb\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.get_fb_loss","title":"<code>get_fb_loss(b=2.0)</code>","text":"<p>Gets a fB-score loss</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>fb_loss</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def get_fb_loss(b: float = 2.0) -&gt; Callable:\n''' Gets a fB-score loss\n    Args:\n        b (float): importance recall in the calculation of the fB score\n    Returns:\n        Callable: fb_loss\n    '''\n# - /!\\ Utilisation partial obligatoire pour pouvoir pickle des fonctions dynamiques ! /!\\ -\nfn = partial(fb_loss, b)\n# FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\nfn.__name__ = 'fb_loss'  # type: ignore\nreturn fn\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.get_weighted_binary_crossentropy","title":"<code>get_weighted_binary_crossentropy(pos_weight=10.0)</code>","text":"<p>Gets a \"weighted binary crossentropy\" loss From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>Weight of the positive class, to be tuned</p> <code>10.0</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Weighted binary crossentropy loss</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def get_weighted_binary_crossentropy(pos_weight: float = 10.0) -&gt; Callable:\n''' Gets a \"weighted binary crossentropy\" loss\n    From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu\n    TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)\n    Args:\n        pos_weight (float): Weight of the positive class, to be tuned\n    Returns:\n        Callable: Weighted binary crossentropy loss\n    '''\n# - /!\\ Use of partial mandatory in order to be able to pickle dynamical functions ! /!\\ -\nfn = partial(weighted_binary_crossentropy, pos_weight)\n# FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\nfn.__name__ = 'weighted_binary_crossentropy'  # type: ignore\nreturn fn\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.precision","title":"<code>precision(y_true, y_pred)</code>","text":"<p>Precision, to use as custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def precision(y_true, y_pred) -&gt; float:\n'''Precision, to use as custom metrics\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\ny_pred = K.round(y_pred)\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nprecision = tp / (tp + fp + K.epsilon())\nprecision = tf.where(tf.math.is_nan(precision), tf.zeros_like(precision), precision)\nweighted_precision = precision * ground_positives / K.sum(ground_positives)\nweighted_precision = K.sum(weighted_precision)\nreturn weighted_precision\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.recall","title":"<code>recall(y_true, y_pred)</code>","text":"<p>Recall to use as a custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def recall(y_true, y_pred) -&gt; float:\n'''Recall to use as a custom metrics\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\ny_pred = K.round(y_pred)\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\nrecall = tp / (tp + fn + K.epsilon())\nrecall = tf.where(tf.math.is_nan(recall), tf.zeros_like(recall), recall)\nweighted_recall = recall * ground_positives / K.sum(ground_positives)\nweighted_recall = K.sum(weighted_recall)\nreturn weighted_recall\n</code></pre>"},{"location":"reference/template_nlp/models_training/models_tensorflow/utils_deep_keras/#template_nlp.models_training.models_tensorflow.utils_deep_keras.weighted_binary_crossentropy","title":"<code>weighted_binary_crossentropy(pos_weight, target, output)</code>","text":"<p>Weighted binary crossentropy between an output tensor and a target tensor. pos_weight is used as a multiplier for the positive targets.</p> <p>Combination of the following functions: * keras.losses.binary_crossentropy * keras.backend.tensorflow_backend.binary_crossentropy * tf.nn.weighted_cross_entropy_with_logits</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>poid classe positive, to be tuned</p> required <code>target</code> <p>Target tensor</p> required <code>output</code> <p>Output tensor</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_nlp/models_training/models_tensorflow/utils_deep_keras.py</code> <pre><code>def weighted_binary_crossentropy(pos_weight: float, target, output) -&gt; float:\n'''Weighted binary crossentropy between an output tensor\n    and a target tensor. pos_weight is used as a multiplier\n    for the positive targets.\n    Combination of the following functions:\n    * keras.losses.binary_crossentropy\n    * keras.backend.tensorflow_backend.binary_crossentropy\n    * tf.nn.weighted_cross_entropy_with_logits\n    Args:\n        pos_weight (float): poid classe positive, to be tuned\n        target: Target tensor\n        output: Output tensor\n    Returns:\n        float: metric\n    '''\ntarget = K.cast(target, 'float32')\noutput = K.cast(output, 'float32')\n# transform back to logits\n_epsilon = tf.convert_to_tensor(K.epsilon(), output.dtype.base_dtype)\noutput = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\noutput = tf.math.log(output / (1 - output))\n# compute weighted loss\nloss = tf.nn.weighted_cross_entropy_with_logits(target, output, pos_weight=pos_weight)\nreturn tf.reduce_mean(loss, axis=-1)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/","title":"Monitoring","text":""},{"location":"reference/template_nlp/monitoring/mlflow_logger/","title":"Mlflow logger","text":""},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger","title":"<code>MLflowLogger</code>","text":"<p>Abstracts how MlFlow works</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>class MLflowLogger:\n'''Abstracts how MlFlow works'''\ndef __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n'''Class initialization\n        Args:\n            experiment_name (str):  Name of the experiment to activate\n        Kwargs:\n            tracking_uri (str): URI of the tracking server\n            artifact_uri (str): URI where to store artifacts\n        '''\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Backup to local save if no uri (i.e. empty string)\nif not tracking_uri:\ntracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n# Add \"file\" scheme if no scheme in the tracking_uri\nelif not urlparse(tracking_uri).scheme:\ntracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n# If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n# Otherwise we suppose artifact_uri is configured by the system\nif not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\nartifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n# Set tracking URI &amp; experiment name\nself.tracking_uri = tracking_uri\n# Get the experiment if it exists and check if there is a connection error by doing it\ntry:\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nexcept Exception as e:\nself.logger.error(repr(e))\nraise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n# If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\nif experiment:\nexperiment_id = experiment.experiment_id\nartifact_uri = experiment.artifact_location\n# Otherwise we create a new experiment with the provided artifact_uri\nelse:\nexperiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nartifact_uri = experiment.artifact_location\nmlflow.set_experiment(experiment_id=experiment_id)\nself.__experiment_id = experiment_id\nself.__experiment_name = experiment_name\nself.__artifact_uri = artifact_uri\nself.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n@property\ndef tracking_uri(self) -&gt; str:\n'''Current tracking uri'''\nreturn mlflow.get_tracking_uri()\n@tracking_uri.setter\ndef tracking_uri(self, uri:str) -&gt; None:\n'''Set tracking uri'''\nmlflow.set_tracking_uri(uri)\n@property\ndef experiment_id(self) -&gt; str:\n'''Experiment id. It can not be changed.'''\nreturn self.__experiment_id\n@property\ndef experiment_name(self) -&gt; str:\n'''Experiment name. It can not be changed.'''\nreturn self.__experiment_name\n@property\ndef artifact_uri(self) -&gt; str:\n'''Experiment artifact URI. It can not be changed.'''\nreturn self.__artifact_uri\ndef end_run(self) -&gt; None:\n'''Stops an MLflow run'''\ntry:\nmlflow.end_run()\nexcept Exception:\nself.logger.error(\"Can't stop mlflow run\")\ndef log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n'''Logs a metric on mlflow\n        Args:\n            key (str): Name of the metric\n            value (float, ?): Value of the metric\n        Kwargs:\n            step (int): Step of the metric\n        '''\n# Check for None\nif value is None:\nvalue = math.nan\n# Log metric\nmlflow.log_metric(key, value, step)\ndef log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n'''Logs a set of metrics in mlflow\n        Args:\n            metrics (dict): Metrics to log\n        Kwargs:\n            step (int): Step of the metric\n        '''\n# Check for Nones\nfor k, v in metrics.items():\nif v is None:\nmetrics[k] = math.nan\n# Log metrics\nmlflow.log_metrics(metrics, step)\ndef log_param(self, key: str, value) -&gt; None:\n'''Logs a parameter in mlflow\n        Args:\n            key (str): Name of the parameter\n            value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n        '''\nif value is None:\nvalue = 'None'\n# Log parameter\nmlflow.log_param(key, value)\ndef log_params(self, params: dict) -&gt; None:\n'''Logs a set of parameters in mlflow\n        Args:\n            params (dict): Name and value of each parameter\n        '''\n# Check for Nones\nfor k, v in params.items():\nif v is None:\nparams[k] = 'None'\n# Log parameters\nmlflow.log_params(params)\ndef set_tag(self, key: str, value) -&gt; None:\n'''Logs a tag in mlflow\n        Args:\n            key (str): Name of the tag\n            value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n        Raises:\n            ValueError: If the object value is None\n        '''\nif value is None:\nraise ValueError('value must not be None')\n# Log tag\nmlflow.set_tag(key, value)\ndef set_tags(self, tags: dict) -&gt; None:\n'''Logs a set of tags in mlflow\n        Args:\n            tags (dict): Name and value of each tag\n        '''\n# Log tags\nmlflow.set_tags(tags)\ndef valid_name(self, key: str) -&gt; bool:\n'''Validates key names\n        Args:\n            key (str): Key to check\n        Returns:\n            bool: If key is a valid mlflow key\n        '''\nif mlflow.mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\nreturn True\nelse:\nreturn False\ndef log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n'''Log a dataframe containing metrics from a training\n        Args:\n            df_stats (pd.Dataframe): Dataframe containing metrics from a training\n        Kwargs:\n            label_col (str): default labelc column name\n        '''\nif label_col not in df_stats.columns:\nraise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n# Get metrics columns\nmetrics_columns = [col for col in df_stats.columns if col != label_col]\n# Log labels\nlabels = df_stats[label_col].values\nfor i, label in enumerate(labels):  # type: ignore\nself.log_param(f'Label {i}', label)\n# Log metrics\nml_flow_metrics = {}\nfor i, row in df_stats.iterrows():\nfor j, col in enumerate(metrics_columns):\nmetric_key = f\"{row[label_col]} --- {col}\"\n# Check that mlflow accepts the key, otherwise, replace it\n# TODO: could be improved ...\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- {col}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"{row[label_col]} --- Col {j}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- Col {j}\"\nml_flow_metrics[metric_key] = row[col]\n# Log metrics\nself.log_metrics(ml_flow_metrics)\ndef log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n'''Logs a dictionary as an artifact in MLflow\n        Args:\n            dictionary (dict): A dictionary\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\nmlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\ndef log_text(self, text: str, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n        Args:\n            text (str): A text\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\nmlflow.log_text(text=text, artifact_file=artifact_file)\ndef log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n        Args:\n            figure (matplotlib.figure.Figure): A matplotlib figure\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n        '''\nmlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.artifact_uri","title":"<code>artifact_uri: str</code>  <code>property</code>","text":"<p>Experiment artifact URI. It can not be changed.</p>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.experiment_id","title":"<code>experiment_id: str</code>  <code>property</code>","text":"<p>Experiment id. It can not be changed.</p>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.experiment_name","title":"<code>experiment_name: str</code>  <code>property</code>","text":"<p>Experiment name. It can not be changed.</p>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.tracking_uri","title":"<code>tracking_uri: str</code>  <code>writable</code> <code>property</code>","text":"<p>Current tracking uri</p>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.__init__","title":"<code>__init__(experiment_name, tracking_uri='', artifact_uri='')</code>","text":"<p>Class initialization</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment to activate</p> required Kwargs <p>tracking_uri (str): URI of the tracking server artifact_uri (str): URI where to store artifacts</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n'''Class initialization\n    Args:\n        experiment_name (str):  Name of the experiment to activate\n    Kwargs:\n        tracking_uri (str): URI of the tracking server\n        artifact_uri (str): URI where to store artifacts\n    '''\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Backup to local save if no uri (i.e. empty string)\nif not tracking_uri:\ntracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n# Add \"file\" scheme if no scheme in the tracking_uri\nelif not urlparse(tracking_uri).scheme:\ntracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n# If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n# Otherwise we suppose artifact_uri is configured by the system\nif not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\nartifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n# Set tracking URI &amp; experiment name\nself.tracking_uri = tracking_uri\n# Get the experiment if it exists and check if there is a connection error by doing it\ntry:\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nexcept Exception as e:\nself.logger.error(repr(e))\nraise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n# If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\nif experiment:\nexperiment_id = experiment.experiment_id\nartifact_uri = experiment.artifact_location\n# Otherwise we create a new experiment with the provided artifact_uri\nelse:\nexperiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nartifact_uri = experiment.artifact_location\nmlflow.set_experiment(experiment_id=experiment_id)\nself.__experiment_id = experiment_id\nself.__experiment_name = experiment_name\nself.__artifact_uri = artifact_uri\nself.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.end_run","title":"<code>end_run()</code>","text":"<p>Stops an MLflow run</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def end_run(self) -&gt; None:\n'''Stops an MLflow run'''\ntry:\nmlflow.end_run()\nexcept Exception:\nself.logger.error(\"Can't stop mlflow run\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_df_stats","title":"<code>log_df_stats(df_stats, label_col='Label')</code>","text":"<p>Log a dataframe containing metrics from a training</p> <p>Parameters:</p> Name Type Description Default <code>df_stats</code> <code>pd.Dataframe</code> <p>Dataframe containing metrics from a training</p> required Kwargs <p>label_col (str): default labelc column name</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n'''Log a dataframe containing metrics from a training\n    Args:\n        df_stats (pd.Dataframe): Dataframe containing metrics from a training\n    Kwargs:\n        label_col (str): default labelc column name\n    '''\nif label_col not in df_stats.columns:\nraise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n# Get metrics columns\nmetrics_columns = [col for col in df_stats.columns if col != label_col]\n# Log labels\nlabels = df_stats[label_col].values\nfor i, label in enumerate(labels):  # type: ignore\nself.log_param(f'Label {i}', label)\n# Log metrics\nml_flow_metrics = {}\nfor i, row in df_stats.iterrows():\nfor j, col in enumerate(metrics_columns):\nmetric_key = f\"{row[label_col]} --- {col}\"\n# Check that mlflow accepts the key, otherwise, replace it\n# TODO: could be improved ...\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- {col}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"{row[label_col]} --- Col {j}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- Col {j}\"\nml_flow_metrics[metric_key] = row[col]\n# Log metrics\nself.log_metrics(ml_flow_metrics)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_dict","title":"<code>log_dict(dictionary, artifact_file)</code>","text":"<p>Logs a dictionary as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>A dictionary</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n'''Logs a dictionary as an artifact in MLflow\n    Args:\n        dictionary (dict): A dictionary\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\nmlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_figure","title":"<code>log_figure(figure, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>matplotlib.figure.Figure</code> <p>A matplotlib figure</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the figure is saved</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n    Args:\n        figure (matplotlib.figure.Figure): A matplotlib figure\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n    '''\nmlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_metric","title":"<code>log_metric(key, value, step=None)</code>","text":"<p>Logs a metric on mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the metric</p> required <code>value</code> <code>float, ?</code> <p>Value of the metric</p> required Kwargs <p>step (int): Step of the metric</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n'''Logs a metric on mlflow\n    Args:\n        key (str): Name of the metric\n        value (float, ?): Value of the metric\n    Kwargs:\n        step (int): Step of the metric\n    '''\n# Check for None\nif value is None:\nvalue = math.nan\n# Log metric\nmlflow.log_metric(key, value, step)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_metrics","title":"<code>log_metrics(metrics, step=None)</code>","text":"<p>Logs a set of metrics in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict</code> <p>Metrics to log</p> required Kwargs <p>step (int): Step of the metric</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n'''Logs a set of metrics in mlflow\n    Args:\n        metrics (dict): Metrics to log\n    Kwargs:\n        step (int): Step of the metric\n    '''\n# Check for Nones\nfor k, v in metrics.items():\nif v is None:\nmetrics[k] = math.nan\n# Log metrics\nmlflow.log_metrics(metrics, step)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_param","title":"<code>log_param(key, value)</code>","text":"<p>Logs a parameter in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the parameter</p> required <code>value</code> <code>str, ?</code> <p>Value of the parameter (which will be cast to str if not already of type str)</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_param(self, key: str, value) -&gt; None:\n'''Logs a parameter in mlflow\n    Args:\n        key (str): Name of the parameter\n        value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n    '''\nif value is None:\nvalue = 'None'\n# Log parameter\nmlflow.log_param(key, value)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_params","title":"<code>log_params(params)</code>","text":"<p>Logs a set of parameters in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Name and value of each parameter</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_params(self, params: dict) -&gt; None:\n'''Logs a set of parameters in mlflow\n    Args:\n        params (dict): Name and value of each parameter\n    '''\n# Check for Nones\nfor k, v in params.items():\nif v is None:\nparams[k] = 'None'\n# Log parameters\nmlflow.log_params(params)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.log_text","title":"<code>log_text(text, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>A text</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def log_text(self, text: str, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n    Args:\n        text (str): A text\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\nmlflow.log_text(text=text, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.set_tag","title":"<code>set_tag(key, value)</code>","text":"<p>Logs a tag in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the tag</p> required <code>value</code> <code>str, ?</code> <p>Value of the tag (which will be cast to str if not already of type str)</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object value is None</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def set_tag(self, key: str, value) -&gt; None:\n'''Logs a tag in mlflow\n    Args:\n        key (str): Name of the tag\n        value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n    Raises:\n        ValueError: If the object value is None\n    '''\nif value is None:\nraise ValueError('value must not be None')\n# Log tag\nmlflow.set_tag(key, value)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.set_tags","title":"<code>set_tags(tags)</code>","text":"<p>Logs a set of tags in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>dict</code> <p>Name and value of each tag</p> required Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def set_tags(self, tags: dict) -&gt; None:\n'''Logs a set of tags in mlflow\n    Args:\n        tags (dict): Name and value of each tag\n    '''\n# Log tags\nmlflow.set_tags(tags)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/mlflow_logger/#template_nlp.monitoring.mlflow_logger.MLflowLogger.valid_name","title":"<code>valid_name(key)</code>","text":"<p>Validates key names</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>If key is a valid mlflow key</p> Source code in <code>template_nlp/monitoring/mlflow_logger.py</code> <pre><code>def valid_name(self, key: str) -&gt; bool:\n'''Validates key names\n    Args:\n        key (str): Key to check\n    Returns:\n        bool: If key is a valid mlflow key\n    '''\nif mlflow.mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\nreturn True\nelse:\nreturn False\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/","title":"Model explainer","text":""},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer","title":"<code>Explainer</code>","text":"<p>Parent class for the explainers</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>class Explainer:\n'''Parent class for the explainers'''\ndef __init__(self, *args, **kwargs) -&gt; None:\n'''Initialization of the parent class'''\nself.logger = logging.getLogger(__name__)\ndef explain_instance(self, content: str, **kwargs) -&gt; Any:\n'''Explains a prediction\n        Args:\n            content (str): Text to be explained\n        Returns:\n            (?): An explanation object\n        '''\nraise NotImplementedError(\"'explain_instance' needs to be overridden\")\ndef explain_instance_as_html(self, content: str, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n        Args:\n            content (str): Text to be explained\n        Returns:\n            str: An HTML code with the explanation\n        '''\nraise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\ndef explain_instance_as_json(self, content: str, **kwargs) -&gt; Union[dict, list]:\n'''Explains a prediction - returns an JSON serializable object\n        Args:\n            content (str): Text to be explained\n        Returns:\n            str: A JSON serializable object with the explanation\n        '''\nraise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\ndef explain_instance_as_list(self, content: str, **kwargs) -&gt; list:\n'''Explains a prediction - returns a list object\n        Args:\n            content (str): Text to be explained\n        Returns:\n            list: List of tuples with words and corresponding weights\n        '''\nraise NotImplementedError(\"'explain_instance_as_list' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialization of the parent class</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n'''Initialization of the parent class'''\nself.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer.explain_instance","title":"<code>explain_instance(content, **kwargs)</code>","text":"<p>Explains a prediction</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:</p> Type Description <code>?</code> <p>An explanation object</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: str, **kwargs) -&gt; Any:\n'''Explains a prediction\n    Args:\n        content (str): Text to be explained\n    Returns:\n        (?): An explanation object\n    '''\nraise NotImplementedError(\"'explain_instance' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>An HTML code with the explanation</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: str, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n    Args:\n        content (str): Text to be explained\n    Returns:\n        str: An HTML code with the explanation\n    '''\nraise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an JSON serializable object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Union[dict, list]</code> <p>A JSON serializable object with the explanation</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: str, **kwargs) -&gt; Union[dict, list]:\n'''Explains a prediction - returns an JSON serializable object\n    Args:\n        content (str): Text to be explained\n    Returns:\n        str: A JSON serializable object with the explanation\n    '''\nraise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.Explainer.explain_instance_as_list","title":"<code>explain_instance_as_list(content, **kwargs)</code>","text":"<p>Explains a prediction - returns a list object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of tuples with words and corresponding weights</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_list(self, content: str, **kwargs) -&gt; list:\n'''Explains a prediction - returns a list object\n    Args:\n        content (str): Text to be explained\n    Returns:\n        list: List of tuples with words and corresponding weights\n    '''\nraise NotImplementedError(\"'explain_instance_as_list' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer","title":"<code>LimeExplainer</code>","text":"<p>         Bases: <code>Explainer</code></p> <p>Lime Explainer wrapper class</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>class LimeExplainer(Explainer):\n'''Lime Explainer wrapper class'''\ndef __init__(self, model: Type[ModelClass], model_conf: dict) -&gt; None:\n''' Initialization\n        Args:\n            model: A model instance with predict &amp; predict_proba functions, and list_classes attribute\n            model_conf (dict): The model's configuration\n        Raises:\n            TypeError: If the provided model does not implement a `predict_proba` function\n            TypeError: If the provided model does not have a `list_classes` attribute\n        '''\nsuper().__init__()\npred_proba_op = getattr(model, \"predict_proba\", None)\nif pred_proba_op is None or not callable(pred_proba_op):\nraise TypeError(\"The supplied model must implement a predict_proba() function\")\nif getattr(model, \"list_classes\", None) is None:\nraise TypeError(\"The supplied model must have a list_classes attribute\")\nself.model = model\nself.model_conf = model_conf\nself.class_names = self.model.list_classes\n# Create the explainer\nself.explainer = LimeTextExplainer(class_names=self.class_names)\ndef classifier_fn(self, content_list: list) -&gt; np.ndarray:\n'''Function to get probabilities from a list of (not preprocessed) texts\n        Args:\n            content_list (list): texts to be considered\n        Returns:\n            np.array: probabilities\n        '''\n# Get preprocessor\nif 'preprocess_str' in self.model_conf.keys():\npreprocess_str = self.model_conf['preprocess_str']\nelse:\npreprocess_str = 'no_preprocess'\npreprocessor = preprocess.get_preprocessor(preprocess_str)\n# Preprocess\ncontent_prep = preprocessor(content_list)\n# Get probabilities\nreturn self.model.predict_proba(content_prep)\ndef explain_instance(self, content: str, class_or_label_index: Union[None, int, Iterable] = None,\nmax_features: int = 15, **kwargs) -&gt; Explanation:\n'''Explains a prediction\n        This function calls the Lime module. It creates a linear model around the input text to evaluate\n        the weight of each word in the final prediction.\n        Args:\n            content (str): Text to be explained\n        Kwargs:\n            class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered.\n            max_features (int): Maximum number of features (cf. Lime documentation)\n        Returns:\n            (?): An explanation object\n        '''\n# If a numpy object is passed as class_or_label_index we convert it to an object of a builtin type\nif is_ndarray_convertable(class_or_label_index):\nclass_or_label_index = ndarray_to_builtin_object(class_or_label_index)\nif class_or_label_index is None:\nprobas = self.classifier_fn([content])[0]\nclass_or_label_index = (i for i, p in enumerate(probas) if p &gt;= 0.5)\nelif isinstance(class_or_label_index, int):\nclass_or_label_index = (class_or_label_index, )\n# Get explanations\nreturn self.explainer.explain_instance(content, self.classifier_fn, labels=class_or_label_index, num_features=max_features)\ndef explain_instance_as_html(self, content: str, class_or_label_index: Union[int, None] = None,\nmax_features: int = 15, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n        Args:\n            content (str): Text to be explained\n        Kwargs:\n            class_or_label_index (int): for classification only. Class or label index to be considered.\n            max_features (int): Maximum number of features (cf. Lime documentation)\n        Returns:\n            str: An HTML code with the explanation\n        '''\nreturn self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs).as_html()\ndef explain_instance_as_list(self, content: str, class_or_label_index: Union[int, None] = None,\nmax_features: int = 15, **kwargs) -&gt; list:\n'''Explains a prediction - returns a list object\n        Args:\n            content (str): Text to be explained\n        Kwargs:\n            class_or_label_index (int): for classification only. Class or label index to be considered.\n            max_features (int): Maximum number of features (cf. Lime documentation)\n        Returns:\n            list: List of tuples with words and corresponding weights\n        '''\nexplanation = self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs)\n# as_list can only return lime weight for a single label so in case no class_or_label_index\n# was specified we return only the weight for the class with the max probabilty\nif class_or_label_index is None:\nclass_or_label_index = explanation.predict_proba.argmax()\n# Return as list for selected class or label\nreturn explanation.as_list(label=class_or_label_index)\ndef explain_instance_as_json(self, content: str, class_or_label_index: Union[None, int, Iterable] = None,\nmax_features: int = 15, **kwargs) -&gt; Union[dict, list]:\n'''Explains a prediction - returns a JSON serializable object\n        Args:\n            content (str): Text to be explained\n        Kwargs:\n            class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered.\n            max_features (int): Maximum number of features (cf. Lime documentation)\n        Returns:\n            Union[list, dict]: JSON serializable object containing a list of tuples with words and corresponding weights\n        '''\nexplanation = self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs)\nreturn explanation.as_map()\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.__init__","title":"<code>__init__(model, model_conf)</code>","text":"<p>Initialization</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[ModelClass]</code> <p>A model instance with predict &amp; predict_proba functions, and list_classes attribute</p> required <code>model_conf</code> <code>dict</code> <p>The model's configuration</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the provided model does not implement a <code>predict_proba</code> function</p> <code>TypeError</code> <p>If the provided model does not have a <code>list_classes</code> attribute</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def __init__(self, model: Type[ModelClass], model_conf: dict) -&gt; None:\n''' Initialization\n    Args:\n        model: A model instance with predict &amp; predict_proba functions, and list_classes attribute\n        model_conf (dict): The model's configuration\n    Raises:\n        TypeError: If the provided model does not implement a `predict_proba` function\n        TypeError: If the provided model does not have a `list_classes` attribute\n    '''\nsuper().__init__()\npred_proba_op = getattr(model, \"predict_proba\", None)\nif pred_proba_op is None or not callable(pred_proba_op):\nraise TypeError(\"The supplied model must implement a predict_proba() function\")\nif getattr(model, \"list_classes\", None) is None:\nraise TypeError(\"The supplied model must have a list_classes attribute\")\nself.model = model\nself.model_conf = model_conf\nself.class_names = self.model.list_classes\n# Create the explainer\nself.explainer = LimeTextExplainer(class_names=self.class_names)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.classifier_fn","title":"<code>classifier_fn(content_list)</code>","text":"<p>Function to get probabilities from a list of (not preprocessed) texts</p> <p>Parameters:</p> Name Type Description Default <code>content_list</code> <code>list</code> <p>texts to be considered</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.array: probabilities</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def classifier_fn(self, content_list: list) -&gt; np.ndarray:\n'''Function to get probabilities from a list of (not preprocessed) texts\n    Args:\n        content_list (list): texts to be considered\n    Returns:\n        np.array: probabilities\n    '''\n# Get preprocessor\nif 'preprocess_str' in self.model_conf.keys():\npreprocess_str = self.model_conf['preprocess_str']\nelse:\npreprocess_str = 'no_preprocess'\npreprocessor = preprocess.get_preprocessor(preprocess_str)\n# Preprocess\ncontent_prep = preprocessor(content_list)\n# Get probabilities\nreturn self.model.predict_proba(content_prep)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.explain_instance","title":"<code>explain_instance(content, class_or_label_index=None, max_features=15, **kwargs)</code>","text":"<p>Explains a prediction</p> <p>This function calls the Lime module. It creates a linear model around the input text to evaluate the weight of each word in the final prediction.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required Kwargs <p>class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered. max_features (int): Maximum number of features (cf. Lime documentation)</p> <p>Returns:</p> Type Description <code>?</code> <p>An explanation object</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: str, class_or_label_index: Union[None, int, Iterable] = None,\nmax_features: int = 15, **kwargs) -&gt; Explanation:\n'''Explains a prediction\n    This function calls the Lime module. It creates a linear model around the input text to evaluate\n    the weight of each word in the final prediction.\n    Args:\n        content (str): Text to be explained\n    Kwargs:\n        class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered.\n        max_features (int): Maximum number of features (cf. Lime documentation)\n    Returns:\n        (?): An explanation object\n    '''\n# If a numpy object is passed as class_or_label_index we convert it to an object of a builtin type\nif is_ndarray_convertable(class_or_label_index):\nclass_or_label_index = ndarray_to_builtin_object(class_or_label_index)\nif class_or_label_index is None:\nprobas = self.classifier_fn([content])[0]\nclass_or_label_index = (i for i, p in enumerate(probas) if p &gt;= 0.5)\nelif isinstance(class_or_label_index, int):\nclass_or_label_index = (class_or_label_index, )\n# Get explanations\nreturn self.explainer.explain_instance(content, self.classifier_fn, labels=class_or_label_index, num_features=max_features)\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, class_or_label_index=None, max_features=15, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required Kwargs <p>class_or_label_index (int): for classification only. Class or label index to be considered. max_features (int): Maximum number of features (cf. Lime documentation)</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>An HTML code with the explanation</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: str, class_or_label_index: Union[int, None] = None,\nmax_features: int = 15, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n    Args:\n        content (str): Text to be explained\n    Kwargs:\n        class_or_label_index (int): for classification only. Class or label index to be considered.\n        max_features (int): Maximum number of features (cf. Lime documentation)\n    Returns:\n        str: An HTML code with the explanation\n    '''\nreturn self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs).as_html()\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, class_or_label_index=None, max_features=15, **kwargs)</code>","text":"<p>Explains a prediction - returns a JSON serializable object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required Kwargs <p>class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered. max_features (int): Maximum number of features (cf. Lime documentation)</p> <p>Returns:</p> Type Description <code>Union[dict, list]</code> <p>Union[list, dict]: JSON serializable object containing a list of tuples with words and corresponding weights</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: str, class_or_label_index: Union[None, int, Iterable] = None,\nmax_features: int = 15, **kwargs) -&gt; Union[dict, list]:\n'''Explains a prediction - returns a JSON serializable object\n    Args:\n        content (str): Text to be explained\n    Kwargs:\n        class_or_label_index (Union[None, int, Iterable]): for classification only. Class or label index to be considered.\n        max_features (int): Maximum number of features (cf. Lime documentation)\n    Returns:\n        Union[list, dict]: JSON serializable object containing a list of tuples with words and corresponding weights\n    '''\nexplanation = self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs)\nreturn explanation.as_map()\n</code></pre>"},{"location":"reference/template_nlp/monitoring/model_explainer/#template_nlp.monitoring.model_explainer.LimeExplainer.explain_instance_as_list","title":"<code>explain_instance_as_list(content, class_or_label_index=None, max_features=15, **kwargs)</code>","text":"<p>Explains a prediction - returns a list object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required Kwargs <p>class_or_label_index (int): for classification only. Class or label index to be considered. max_features (int): Maximum number of features (cf. Lime documentation)</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of tuples with words and corresponding weights</p> Source code in <code>template_nlp/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_list(self, content: str, class_or_label_index: Union[int, None] = None,\nmax_features: int = 15, **kwargs) -&gt; list:\n'''Explains a prediction - returns a list object\n    Args:\n        content (str): Text to be explained\n    Kwargs:\n        class_or_label_index (int): for classification only. Class or label index to be considered.\n        max_features (int): Maximum number of features (cf. Lime documentation)\n    Returns:\n        list: List of tuples with words and corresponding weights\n    '''\nexplanation = self.explain_instance(content, class_or_label_index=class_or_label_index, max_features=max_features, **kwargs)\n# as_list can only return lime weight for a single label so in case no class_or_label_index\n# was specified we return only the weight for the class with the max probabilty\nif class_or_label_index is None:\nclass_or_label_index = explanation.predict_proba.argmax()\n# Return as list for selected class or label\nreturn explanation.as_list(label=class_or_label_index)\n</code></pre>"},{"location":"reference/template_nlp/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/template_nlp/preprocessing/preprocess/","title":"Preprocess","text":""},{"location":"reference/template_nlp/preprocessing/preprocess/#template_nlp.preprocessing.preprocess.get_preprocessor","title":"<code>get_preprocessor(preprocess_str)</code>","text":"<p>Gets a preprocessing (function) from its name</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_str</code> <code>str</code> <p>Name of the preprocess</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the name of the preprocess is not known</p> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Function to be used for the preprocessing</p> Source code in <code>template_nlp/preprocessing/preprocess.py</code> <pre><code>def get_preprocessor(preprocess_str: str) -&gt; Callable:\n'''Gets a preprocessing (function) from its name\n    Args:\n        preprocess_str (str): Name of the preprocess\n    Raises:\n        ValueError: If the name of the preprocess is not known\n    Returns:\n        Callable: Function to be used for the preprocessing\n    '''\n# Process\npreprocessors_dict = get_preprocessors_dict()\nif preprocess_str not in preprocessors_dict.keys():\nraise ValueError(f\"The preprocess {preprocess_str} is not known.\")\n# Get preprocessor\npreprocessor = preprocessors_dict[preprocess_str]\n# Return\nreturn preprocessor\n</code></pre>"},{"location":"reference/template_nlp/preprocessing/preprocess/#template_nlp.preprocessing.preprocess.get_preprocessors_dict","title":"<code>get_preprocessors_dict()</code>","text":"<p>Gets a dictionary of available preprocessing</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of preprocessing</p> Source code in <code>template_nlp/preprocessing/preprocess.py</code> <pre><code>def get_preprocessors_dict() -&gt; dict:\n'''Gets a dictionary of available preprocessing\n    Returns:\n        dict: Dictionary of preprocessing\n    '''\npreprocessors_dict = {\n'no_preprocess': lambda x: x,  # - /!\\ DO NOT DELETE -&gt; necessary for compatibility /!\\ -\n'preprocess_P1': preprocess_sentence_P1,  # Example of a preprocessing\n#  'preprocess_P2': preprocess_sentence_P2 , ETC ...\n}\nreturn preprocessors_dict\n</code></pre>"},{"location":"reference/template_nlp/preprocessing/preprocess/#template_nlp.preprocessing.preprocess.preprocess_sentence_P1","title":"<code>preprocess_sentence_P1(docs)</code>","text":"<p>Applies \"default\" preprocess to a list of documents (text)</p> <p>Parameters:</p> Name Type Description Default <code>docs</code> <code>pd.Series</code> <p>Documents to be preprocessed</p> required <p>Returns:</p> Type Description <code>pd.Series</code> <p>pd.Series: Preprocessed documents</p> Source code in <code>template_nlp/preprocessing/preprocess.py</code> <pre><code>@wnf_utils.data_agnostic\n@wnf_utils.regroup_data_series\ndef preprocess_sentence_P1(docs: pd.Series) -&gt; pd.Series:\n'''Applies \"default\" preprocess to a list of documents (text)\n    Args:\n        docs (pd.Series): Documents to be preprocessed\n    Returns:\n        pd.Series: Preprocessed documents\n    '''\npipeline = ['remove_non_string', 'get_true_spaces', 'remove_punct', 'to_lower', 'trim_string',\n'remove_leading_and_ending_spaces']\nreturn api.preprocess_pipeline(docs, pipeline=pipeline, chunksize=100000)\n</code></pre>"},{"location":"reference/template_num/","title":"Template num","text":""},{"location":"reference/template_num/utils/","title":"Utils","text":""},{"location":"reference/template_num/utils/#template_num.utils.NpEncoder","title":"<code>NpEncoder</code>","text":"<p>         Bases: <code>json.JSONEncoder</code></p> <p>JSON encoder to manage numpy objects</p> Source code in <code>template_num/utils.py</code> <pre><code>class NpEncoder(json.JSONEncoder):\n'''JSON encoder to manage numpy objects'''\ndef default(self, obj) -&gt; Any:\nif is_ndarray_convertable(obj):\nreturn ndarray_to_builtin_object(obj)\nelif isinstance(obj, set):\nreturn list(obj)\nelse:\nreturn super(NpEncoder, self).default(obj)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.display_shape","title":"<code>display_shape(df)</code>","text":"<p>Displays the number of line and of column of a table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Table to parse</p> required Source code in <code>template_num/utils.py</code> <pre><code>def display_shape(df: pd.DataFrame) -&gt; None:\n'''Displays the number of line and of column of a table.\n    Args:\n        df (pd.DataFrame): Table to parse\n    '''\n# Display\nlogger.info(f\"Number of lines : {df.shape[0]}. Number of columns : {df.shape[1]}.\")\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.find_folder_path","title":"<code>find_folder_path(folder_name, base_folder=None)</code>","text":"<p>Find a folder in a base folder and its subfolders. If base_folder is None, considers folder_name as a path and check it exists</p> <p>i.e., with the following structure : - C:/     - base_folder/         - folderA/             - folderB/         - folderC/ find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC find_folder_path(folderB, None) raises an error</p> <p>Parameters:</p> Name Type Description Default <code>folder_name</code> <code>str</code> <p>name of the folder to find. If base_folder is None, consider a path instead.</p> required Kwargs <p>base_folder (str): path of the base folder. If None, consider folder_name as a path.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>if we can't find folder_name in base_folder</p> <code>FileNotFoundError</code> <p>if folder_name is not a valid path (case where base_folder is None)</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>path to the wanted folder</p> Source code in <code>template_num/utils.py</code> <pre><code>def find_folder_path(folder_name: str, base_folder: Union[str, None] = None) -&gt; str:\n'''Find a folder in a base folder and its subfolders.\n    If base_folder is None, considers folder_name as a path and check it exists\n    i.e., with the following structure :\n    - C:/\n        - base_folder/\n            - folderA/\n                - folderB/\n            - folderC/\n    find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA\n    find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB\n    find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC\n    find_folder_path(folderB, None) raises an error\n    Args:\n        folder_name (str): name of the folder to find. If base_folder is None, consider a path instead.\n    Kwargs:\n        base_folder (str): path of the base folder. If None, consider folder_name as a path.\n    Raises:\n        FileNotFoundError: if we can't find folder_name in base_folder\n        FileNotFoundError: if folder_name is not a valid path (case where base_folder is None)\n    Returns:\n        str: path to the wanted folder\n    '''\nif base_folder is not None:\nfolder_path = None\nfor path, subdirs, files in os.walk(base_folder):\nfor name in subdirs:\nif name == folder_name:\nfolder_path = os.path.join(path, name)\nif folder_path is None:\nraise FileNotFoundError(f\"Can't find folder {folder_name} inside {base_folder} and its subfolders\")\nelse:\nfolder_path = folder_name\nif not os.path.exists(folder_path):\nraise FileNotFoundError(f\"Can't find folder {folder_path} (considered as a path)\")\nreturn folder_path\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.flatten","title":"<code>flatten(my_list)</code>","text":"<p>Flattens a list of mixed elements (ie. some iterable, some not) e.g. [[1, 2], 3, [4]] -&gt; [1, 2, 3, 4] From : https://stackoverflow.com/questions/2158395/flatten-an-irregular-list-of-lists</p> <p>Parameters:</p> Name Type Description Default <code>my_list</code> <code>Iterable</code> <p>List to consider</p> required Results <p>generator: Flattened list (generator format)</p> Source code in <code>template_num/utils.py</code> <pre><code>def flatten(my_list: Iterable) -&gt; Generator:\n'''Flattens a list of mixed elements (ie. some iterable, some not)\n    e.g. [[1, 2], 3, [4]] -&gt; [1, 2, 3, 4]\n    From : https://stackoverflow.com/questions/2158395/flatten-an-irregular-list-of-lists\n    Args:\n        my_list (Iterable): List to consider\n    Results:\n        generator: Flattened list (generator format)\n    '''\nfor el in my_list:\nif isinstance(el, Iterable) and not isinstance(el, (str, bytes)):\nyield from flatten(el)\nelse:\nyield el\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_chunk_limits","title":"<code>get_chunk_limits(x, chunksize=10000)</code>","text":"<p>Gets chunk limits from a pandas series or dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>pd.Series or pd.DataFrame</code> <p>Documents to consider</p> required Kwargs <p>chunksize (int): The chunk size</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the chunk size is negative</p> <p>Returns:</p> Type Description <code>List[Tuple[int]]</code> <p>list: the chunk limits Source code in <code>template_num/utils.py</code> <pre><code>def get_chunk_limits(x: Union[pd.DataFrame, pd.Series], chunksize: int = 10000) -&gt; List[Tuple[int]]:\n'''Gets chunk limits from a pandas series or dataframe.\n    Args:\n        x (pd.Series or pd.DataFrame): Documents to consider\n    Kwargs:\n        chunksize (int): The chunk size\n    Raises:\n        ValueError: If the chunk size is negative\n    Returns:\n        list&lt;tuple&gt;: the chunk limits\n    '''\nif chunksize &lt; 0:\nraise ValueError('The object chunksize must not be negative.')\n# Processs\nif chunksize == 0 or chunksize &gt;= x.shape[0]:\nchunks_limits = [(0, x.shape[0])]\nelse:\nchunks_limits = [(i * chunksize, min((i + 1) * chunksize, x.shape[0]))\nfor i in range(1 + ((x.shape[0] - 1) // chunksize))]\nreturn chunks_limits  # type: ignore\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_data_path","title":"<code>get_data_path()</code>","text":"<p>Returns the path to the data folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the data folder</p> Source code in <code>template_num/utils.py</code> <pre><code>def get_data_path() -&gt; str:\n'''Returns the path to the data folder\n    Returns:\n        str: Path of the data folder\n    '''\nif DIR_PATH is None:\ndir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_num-data')\nelse:\ndir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_num-data')\nif not os.path.isdir(dir_path):\nos.mkdir(dir_path)\nreturn os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_models_path","title":"<code>get_models_path()</code>","text":"<p>Returns the path to the models folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the models folder</p> Source code in <code>template_num/utils.py</code> <pre><code>def get_models_path() -&gt; str:\n'''Returns the path to the models folder\n    Returns:\n        str: Path of the models folder\n    '''\nif DIR_PATH is None:\ndir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_num-models')\nelse:\ndir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_num-models')\nif not os.path.isdir(dir_path):\nos.mkdir(dir_path)\nreturn os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_new_column_name","title":"<code>get_new_column_name(column_list, wanted_name)</code>","text":"<p>Gets a new column name from a list of existing ones &amp; a wanted name</p> <p>If the wanted name does not exists, return it. Otherwise get a new column prefixed by the wanted name.</p> <p>Parameters:</p> Name Type Description Default <code>column_list</code> <code>list</code> <p>List of existing columns</p> required <code>wanted_name</code> <code>str</code> <p>Wanted name</p> required Source code in <code>template_num/utils.py</code> <pre><code>def get_new_column_name(column_list: list, wanted_name: str) -&gt; str:\n'''Gets a new column name from a list of existing ones &amp; a wanted name\n    If the wanted name does not exists, return it.\n    Otherwise get a new column prefixed by the wanted name.\n    Args:\n        column_list (list): List of existing columns\n        wanted_name (str): Wanted name\n    '''\nif wanted_name not in column_list:\nreturn wanted_name\nelse:\nnew_name = f'{wanted_name}_{str(uuid.uuid4())[:8]}'\n# It should not happen, but we still check if new_name is available (bad luck ?)\nreturn get_new_column_name(column_list, new_name)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_package_version","title":"<code>get_package_version()</code>","text":"<p>Returns the current version of the package</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>version of the package</p> Source code in <code>template_num/utils.py</code> <pre><code>def get_package_version() -&gt; str:\n'''Returns the current version of the package\n    Returns:\n        str: version of the package\n    '''\nversion = pkg_resources.get_distribution('template_num').version\nreturn version\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_pipelines_path","title":"<code>get_pipelines_path()</code>","text":"<p>Returns the path to the pipelines folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the pipelines folder</p> Source code in <code>template_num/utils.py</code> <pre><code>def get_pipelines_path() -&gt; str:\n'''Returns the path to the pipelines folder\n    Returns:\n        str: Path of the pipelines folder\n    '''\nif DIR_PATH is None:\ndir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_num-pipelines')\nelse:\ndir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_num-pipelines')\nif not os.path.isdir(dir_path):\nos.mkdir(dir_path)\nreturn os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.get_ressources_path","title":"<code>get_ressources_path()</code>","text":"<p>Returns the path to the ressources folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the ressources folder</p> Source code in <code>template_num/utils.py</code> <pre><code>def get_ressources_path() -&gt; str:\n'''Returns the path to the ressources folder\n    Returns:\n        str: Path of the ressources folder\n    '''\ndir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_num-ressources')\nif not os.path.isdir(dir_path):\nos.mkdir(dir_path)\nreturn os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.is_ndarray_convertable","title":"<code>is_ndarray_convertable(obj)</code>","text":"<p>Returns True if the object is covertable to a builtin type in the same way a np.ndarray is</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>an object to test</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the object is covertable to a list as a np.ndarray is</p> Source code in <code>template_num/utils.py</code> <pre><code>def is_ndarray_convertable(obj: Any) -&gt; bool:\n'''Returns True if the object is covertable to a builtin type in the same way a np.ndarray is\n    Args:\n        obj (Any): an object to test\n    Returns:\n        bool: True if the object is covertable to a list as a np.ndarray is\n    '''\nreturn hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\")\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.ndarray_to_builtin_object","title":"<code>ndarray_to_builtin_object(obj)</code>","text":"<p>Transform a numpy.ndarray like object to a builtin type like int, float or list</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>An object</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raise a ValueError when obj is not ndarray convertable</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The object converted to a builtin type like int, float or list</p> Source code in <code>template_num/utils.py</code> <pre><code>def ndarray_to_builtin_object(obj: Any) -&gt; Any:\n'''Transform a numpy.ndarray like object to a builtin type like int, float or list\n    Args:\n        obj (Any): An object\n    Raises:\n        ValueError: Raise a ValueError when obj is not ndarray convertable\n    Returns:\n        Any: The object converted to a builtin type like int, float or list\n    '''\nif is_ndarray_convertable(obj):\nif np.issubdtype(obj.dtype, np.integer):\nreturn obj.astype(int).tolist()\nelif np.issubdtype(obj.dtype, np.number):\nreturn obj.astype(float).tolist()\nelse:\nreturn obj.tolist()\nelse:\nraise ValueError(f\"{obj} is not ndarray convertable\")\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.read_csv","title":"<code>read_csv(file_path, sep=';', encoding='utf-8', **kwargs)</code>","text":"<p>Reads a .csv file and parses the first line.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>Path to the .csv file containing the data</p> required Kwargs <p>sep (str): Separator of the data file encoding (str): Encoding of the data file kwargs: Pandas' kwargs</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the file_path object does not point to an existing file</p> <p>Returns:</p> Name Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: Data</p> <code>str</code> <code>Union[str, None]</code> <p>First line of the .csv (None if not beginning with #) and with no line break</p> Source code in <code>template_num/utils.py</code> <pre><code>def read_csv(file_path: str, sep: str = ';', encoding: str = 'utf-8', **kwargs) -&gt; Tuple[pd.DataFrame, Union[str, None]]:\n'''Reads a .csv file and parses the first line.\n    Args:\n        file_path (str): Path to the .csv file containing the data\n    Kwargs:\n        sep (str): Separator of the data file\n        encoding (str): Encoding of the data file\n        kwargs: Pandas' kwargs\n    Raises:\n        FileNotFoundError: If the file_path object does not point to an existing file\n    Returns:\n        pd.DataFrame: Data\n        str: First line of the .csv (None if not beginning with #) and with no line break\n    '''\nif not os.path.isfile(file_path):\nraise FileNotFoundError(f\"The file {file_path} does not exist\")\n# We get the first line\nwith open(file_path, 'r', encoding=encoding) as f:\nfirst_line = f.readline()\n# We check if the first line contains metadata\nhas_metada = True if first_line.startswith('#') else False\n# We load the dataset\nif has_metada:\ndf = pd.read_csv(file_path, sep=sep, encoding=encoding, skiprows=1, **kwargs)\nelse:\ndf = pd.read_csv(file_path, sep=sep, encoding=encoding, **kwargs)\n# If no metadata, return only the dataframe\nif not has_metada:\nreturn df, None\n# Else process the first_line\nelse:\n# Deletion of the line break\nif first_line is not None and first_line.endswith('\\n'):\nfirst_line = first_line[:-1]\n# Deletion of the return carriage\nif first_line is not None and first_line.endswith('\\r'):\nfirst_line = first_line[:-1]\n# Return\nreturn df, first_line\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.to_csv","title":"<code>to_csv(df, file_path, first_line=None, sep=';', encoding='utf-8', **kwargs)</code>","text":"<p>Writes a .csv and manages the first line.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Data to write</p> required <code>file_path</code> <code>str</code> <p>Path to the file to create</p> required Kwargs <p>first_line (str): First line to write (without line break which is done in this function) sep (str): Separator for the data file encoding (str): Encoding of the data file kwargs: pandas' kwargs</p> Source code in <code>template_num/utils.py</code> <pre><code>def to_csv(df: pd.DataFrame, file_path: str, first_line: Union[str, None] = None, sep: str = ';',\nencoding: str = 'utf-8', **kwargs) -&gt; None:\n'''Writes a .csv and manages the first line.\n    Args:\n        df (pd.DataFrame): Data to write\n        file_path (str): Path to the file to create\n    Kwargs:\n        first_line (str): First line to write (without line break which is done in this function)\n        sep (str): Separator for the data file\n        encoding (str): Encoding of the data file\n        kwargs: pandas' kwargs\n    '''\n# We get the first line\nwith open(file_path, 'w', encoding=encoding) as f:\nif first_line is not None:\nf.write(first_line + '\\n')  # We add the first line if metadata are present\ndf.to_csv(f, sep=sep, encoding=encoding, index=None, **kwargs)\n</code></pre>"},{"location":"reference/template_num/utils/#template_num.utils.trained_needed","title":"<code>trained_needed(function)</code>","text":"<p>Decorator to ensure that a model has been trained.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>func</code> <p>Function to decorate</p> required <p>Returns:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The decorated function</p> Source code in <code>template_num/utils.py</code> <pre><code>def trained_needed(function: Callable) -&gt; Callable:\n'''Decorator to ensure that a model has been trained.\n    Args:\n        function (func): Function to decorate\n    Returns:\n        function: The decorated function\n    '''\n# Get wrapper\ndef wrapper(self, *args, **kwargs):\n'''Wrapper'''\nif not self.trained:\nraise AttributeError(f\"The function {function.__name__} can't be called as long as the model hasn't been fitted\")\nelse:\nreturn function(self, *args, **kwargs)\nreturn wrapper\n</code></pre>"},{"location":"reference/template_num/models_training/","title":"Models training","text":""},{"location":"reference/template_num/models_training/model_class/","title":"Model class","text":""},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass","title":"<code>ModelClass</code>","text":"<p>Parent class for the models</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>class ModelClass:\n'''Parent class for the models'''\n_default_name = 'none'\n# Variable annotation : https://www.python.org/dev/peps/pep-0526/\n# Solves lots of typing errors, cf mypy\nmulti_label: Union[bool, None]\nlist_classes: Union[list, None]\ndict_classes: Union[dict, None]\n# Not implemented :\n# -&gt; fit\n# -&gt; predict\n# -&gt; predict_proba\n# -&gt; inverse_transform\n# -&gt; get_and_save_metrics\ndef __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None,\nx_col: Union[list, None] = None, y_col: Union[str, int, list, None] = None,\npreprocess_pipeline: Union[ColumnTransformer, None] = None, level_save: str = 'HIGH', **kwargs) -&gt; None:\n'''Initialization of the parent class.\n        Kwargs:\n            model_dir (str): Folder where to save the model\n                If None, creates a directory based on the model's name and the date (most common usage)\n            model_name (str): The name of the model\n            x_col (list): Names of the columns used for the training - x\n            y_col (str or int or list if multi-labels): Name of the model's target column(s) - y\n            preprocess_pipeline (ColumnTransformer): The pipeline used for preprocessing. If None -&gt; no preprocessing !\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOW + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n            NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n        '''\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type -&gt; 'classifier' or 'regressor' depending on the model\nself.model_type = None\n# Model name\nself.model_name = self._default_name if model_name is None else model_name\n# Names of the columns used\nself.x_col = x_col\nself.y_col = y_col\n# Can be None if reloading a model\nif x_col is None:\nself.logger.warning(\"Warning, the attribute x_col is not given! The model might not work as intended.\")\nif y_col is None:\nself.logger.warning(\"Warning, the attribute y_col is not given! The model might not work as intended.\")\n# Model folder\nif model_dir is None:\nself.model_dir = self._get_model_dir()\nelse:\nif not os.path.exists(model_dir):\nos.makedirs(model_dir)\nif not os.path.isdir(model_dir):\nraise NotADirectoryError(f\"{model_dir} is not a valid directory\")\nself.model_dir = os.path.abspath(model_dir)\n# Preprocessing pipeline\nself.preprocess_pipeline = preprocess_pipeline\nif self.preprocess_pipeline is not None:\ntry:\ncheck_is_fitted(self.preprocess_pipeline)\nexcept NotFittedError as e:\nself.logger.error(\"The preprocessing pipeline hasn't been fitted !\")\nself.logger.error(repr(e))\nraise NotFittedError()\n# We get the associated columns (and a check if there has been a fit is done)\nself.columns_in, self.mandatory_columns = utils_models.get_columns_pipeline(self.preprocess_pipeline)\nelse:\n# We can't define a \"no_preprocess\" pipeline since we should fit it\n# So we take care of that at the first fit\nself.logger.warning(\"Warning, no preprocessing pipeline given !\")\nself.columns_in, self.mandatory_columns = None, None\n# Other options\nself.level_save = level_save\n# is trained ?\nself.trained = False\nself.nb_fit = 0\n# Configuration dict. to be logged. Set on save.\nself.json_dict: Dict[Any, Any] = {}\ndef fit(self, x_train, y_train, **kwargs) -&gt; None:\n'''Trains the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        '''\nraise NotImplementedError(\"'fit' needs to be overridden\")\ndef predict(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predictions on the test set\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nraise NotImplementedError(\"'predict' needs to be overridden\")\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nraise NotImplementedError(\"'predict_proba' needs to be overridden\")\ndef inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Gets the final format of prediction\n            - Classification : classes from predictions\n            - Regression : values (identity function)\n        Args:\n            y (list | np.ndarray): Array-like, shape = [n_samples,]\n                   OR 1D array shape = [n_classes] (only one prediction)\n        Returns:\n            (?): Array, shape = [n_samples, ?]\n        '''\nraise NotImplementedError(\"'inverse_transform' needs to be overridden\")\ndef get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\nseries_to_add: Union[List[pd.Series], None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_targets]\n            y_pred (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n            series_to_add (list): List of pd.Series to add to the dataframe\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing the statistics\n        '''\nraise NotImplementedError(\"'get_and_save_metrics' needs to be overridden\")\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Manage paths\npkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\npreprocess_pipeline_path = os.path.join(self.model_dir, \"preprocess_pipeline.pkl\")\nconf_path = os.path.join(self.model_dir, \"configurations.json\")\n# Save the model &amp; preprocessing pipeline if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nwith open(pkl_path, 'wb') as f:\npickle.dump(self, f)\n# Useful for reload_from_standalone, otherwise, saved as a class attribute\nwith open(preprocess_pipeline_path, 'wb') as f:\npickle.dump(self.preprocess_pipeline, f)\n# Saving JSON configuration\njson_dict = {\n'maintainers': 'Agence DataServices',\n'gabarit_version': '1.2.5-dev-local',\n'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n'package_version': utils.get_package_version(),\n'model_name': self.model_name,\n'model_dir': self.model_dir,\n'model_type': self.model_type,\n'trained': self.trained,\n'nb_fit': self.nb_fit,\n'x_col': self.x_col,\n'y_col': self.y_col,\n'columns_in': self.columns_in,\n'mandatory_columns': self.mandatory_columns,\n'level_save': self.level_save,\n'librairie': None,\n}\n# Merge json_data if not None\nif json_data is not None:\n# Priority given to json_data !\njson_dict = {**json_dict, **json_data}\n# Add conf to attributes\nself.json_dict = json_dict\n# Save conf\nwith open(conf_path, 'w', encoding='utf-8') as json_file:\njson.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n# Now, save a proprietes file for the model upload\nself._save_upload_properties(json_dict)\ndef _save_upload_properties(self, json_dict: Union[dict, None] = None) -&gt; None:\n'''Prepares a configuration file for a future export (e.g on an artifactory)\n        Kwargs:\n            json_dict: Configurations to save\n        '''\nif json_dict is None:\njson_dict = {}\n# Manage paths\nproprietes_path = os.path.join(self.model_dir, \"proprietes.json\")\nvanilla_model_upload_instructions = os.path.join(utils.get_ressources_path(), 'model_upload_instructions.md')\nspecific_model_upload_instructions = os.path.join(self.model_dir, \"model_upload_instructions.md\")\n# First, we define a list of \"allowed\" properties\nallowed_properties = [\"maintainers\", \"gabarit_version\", \"date\", \"package_version\", \"model_name\", \"list_classes\",\n\"librairie\", \"fit_time\"]\n# Now we filter these properties\nfinal_dict = {k: v for k, v in json_dict.items() if k in allowed_properties}\n# Save\nwith open(proprietes_path, 'w', encoding='utf-8') as f:\njson.dump(final_dict, f, indent=4, cls=utils.NpEncoder)\n# Add instructions to upload a model to a storage solution (e.g. Artifactory)\nwith open(vanilla_model_upload_instructions, 'r', encoding='utf-8') as f:\ncontent = f.read()\n# TODO: to be improved\nnew_content = content.replace('model_dir_path_identifier', os.path.abspath(self.model_dir))\nwith open(specific_model_upload_instructions, 'w', encoding='utf-8') as f:\nf.write(new_content)\ndef _get_model_dir(self) -&gt; str:\n'''Gets a folder where to save the model\n        Returns:\n            str: Path to the folder\n        '''\nmodels_dir = utils.get_models_path()\nsubfolder = os.path.join(models_dir, self.model_name)\nfolder_name = datetime.now().strftime(f\"{self.model_name}_%Y_%m_%d-%H_%M_%S\")\nmodel_dir = os.path.join(subfolder, folder_name)\nif os.path.isdir(model_dir):\ntime.sleep(1)  # Wait 1 second so that the 'date' changes...\nreturn self._get_model_dir()  # Get new directory name\nelse:\nos.makedirs(model_dir)\nreturn model_dir\ndef _check_input_format(self, x_input: Union[pd.DataFrame, np.ndarray], y_input: Union[pd.DataFrame, pd.Series, np.ndarray, None] = None,\nfit_function: bool = False) -&gt; Tuple[Union[pd.DataFrame, np.ndarray], Union[pd.DataFrame, pd.Series, np.ndarray, None]]:\n'''Checks the inputs of a function. We check the number of columns and the ordering.\n        Strategy :\n            - If fit function, set preprocessing pipeline, columns_in, mandatory_columns, x_col (if not set), y_col (if not set) with input data\n            - Then, for both x &amp; y\n                - If input data has a column attribute\n                    - If we can find all needed columns, reorder the dataset using only the needed columns (so it works if we have more columns)\n                    - Else, raise an error if length do not match (otherwise log a warning)\n                - Else, raise an error if length do not match (otherwise log a warning)\n        We also set the pipeline to a passthrough pipeline if None\n        Args:\n            x_input (pd.DataFrame, np.ndarray): Array-like, shape = [n_samples, n_features]\n        Kwargs:\n            y_input (pd.DataFrame, pd.Series, np.ndarray): Array-like, shape = [n_samples, n_target]\n                Mandatory if fit_function\n            fit_function (bool): If it is a fit function\n        Raises:\n            AttributeError: If fit_function == True, but y_input is None\n            ValueError: If one of the inputs hasn't the right number of columns\n        Returns:\n            (pd.DataFrame, np.ndarray): x_input, may be reordered if needed\n            (pd.DataFrame, pd.Series, np.ndarray): y_input, may be reordered if needed\n        '''\n# Getting some info first\nx_input_shape = x_input.shape[-1] if len(x_input.shape) &gt; 1 else 1\nif y_input is not None:\ny_input_shape = y_input.shape[-1] if len(y_input.shape) &gt; 1 else 1\nelse:\ny_input_shape = 0  # not used\n# Manage fit_function = True\nif fit_function:\nif y_input is None:\nraise AttributeError(\"The argument y_input is mandatory if fit_function == True\")\n# Set x_col if not set yet\nif self.x_col is None:\nself.logger.warning(\"Warning, the attribute x_col was not given when creating the model\")\nself.logger.warning(\"We set it now with the input data of the fit function\")\nif hasattr(x_input, 'columns'):\n# TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\nself.x_col = list(x_input.columns)  # type: ignore\nelse:\nself.x_col = [_ for _ in range(x_input_shape)]\n# Same thing for y_col\nif self.y_col is None:\nself.logger.warning(\"Warning, the attribute y_col was not given when creating the model\")\nself.logger.warning(\"We set it now with the input data of the fit function\")\nif hasattr(y_input, 'columns'):\n# TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\nself.y_col = list(y_input.columns)  # type: ignore\nelse:\nself.y_col = [_ for _ in range(y_input_shape)]\n# If there is only one element, we get rid of the list\nif y_input_shape == 1:\nself.y_col = self.y_col[0]\n# If pipeline, columns_in or mandatory_columns is None, sets it\nif self.preprocess_pipeline is None:  # ie no pipeline given when initializing the class\npreprocess_str = \"no_preprocess\"\npreprocess_pipeline = preprocess.get_pipeline(preprocess_str)  # Warning, the pipeline must be fitted\npreprocess_pipeline.fit(x_input)  # We fit the pipeline to set the necessary columns for the pipeline\n# Set attributes\nself.preprocess_pipeline = preprocess_pipeline\nself.columns_in, self.mandatory_columns = utils_models.get_columns_pipeline(self.preprocess_pipeline)\n# Checking x_input\nif self.x_col is None:\nself.logger.warning(\"Can't check the input format (x) because x_col is not set...\")\nelse:\nx_col_len = len(self.x_col)\n# We check the presence of the columns\nif hasattr(x_input, 'columns'):\ncan_reorder = True\nfor col in self.x_col:\n# TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\nif col not in x_input.columns:  # type: ignore\ncan_reorder = False\nself.logger.warning(f\"The column {col} is missing from the input (x)\")\n# If we can't reorder :\n# 1. Exact number of columns : we write a warning message and continue with columns renamed\n# 2. Not the correct number of column : raise an error\nif not can_reorder:\nif x_input_shape != x_col_len:\nraise ValueError(f\"Input data (x) is not in the right format ({x_input_shape} != {x_col_len})\")\nself.logger.warning(\"The names of the columns (x) do not match. The process continues since there is the right number of columns\")\nx_input = x_input.copy()  # needs a copy as we wil change columns names\nx_input.columns = self.x_col  # type: ignore\n# If we can reorder :\n# 1. Same number of inputs but not the same order -&gt; we just reorder\n# 2. More columns ? -&gt; we just take the needed subset + log a warning message\nelse:\n# TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\nif list(x_input.columns) != self.x_col:  # type: ignore\nif x_input_shape == x_col_len:\nself.logger.warning(\"The input columns (x) are not in the right order -&gt; automatic reordering !\")\nelse:\nself.logger.warning(\"More columns in input (x) than needed, but we can find the needed columns -&gt; only considering the needed columns\")\nx_input = x_input[self.x_col]\nelse:\nif x_input_shape != len(self.x_col):\nraise ValueError(f\"Input data (x) is not in the right format ({x_input_shape} != {x_col_len})\")\nself.logger.warning(\"The input (x) does not have the 'columns' attribute -&gt; can't check the ordering of the columns\")\n# Checking y_input\nif y_input is not None:\nif self.y_col is None:\nself.logger.warning(\"Can't check the input format (y) because y_col is not set...\")\nelse:\n# Checking y_input format\ny_col_len = len(self.y_col) if type(self.y_col) == list else 1\n# We check the presence of the columns\nif hasattr(y_input, 'columns'):\ncan_reorder = True\ntarget_cols = self.y_col if type(self.y_col) == list else [self.y_col]\nfor col in target_cols:\n# TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\nif col not in y_input.columns:  # type: ignore\ncan_reorder = False\nself.logger.warning(f\"The column {col} is missing from the input (y)\")\n# If we can't reorder :\n# 1. Exact number of columns : we write a warning message and continue with columns renamed\n# 2. Not the correct number of column : raise an error\nif not can_reorder:\nif y_input_shape != y_col_len:\nraise ValueError(f\"Input data (y) is not in the right format ({y_input_shape} != {y_col_len})\")\nself.logger.warning(\"The names of the columns (y) do not match. The process continues since there is the right number of columns\")\ny_input = y_input.copy()  # needs a copy as we wil change columns names\ny_input.columns = self.y_col  # type: ignore\n# If we can reorder :\n# 1. Same number of inputs but not the same order -&gt; we just reorder\n# 2. More columns ? -&gt; we just take the needed subset + log a warning message\nelse:\n# TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\nif list(y_input.columns) != target_cols:  # type: ignore\nif y_input_shape == y_col_len:\nself.logger.warning(\"The input columns (y) are not in the right order -&gt; automatic reordering !\")\nelse:\nself.logger.warning(\"More columns in input (y) than needed, but we can find the needed columns -&gt; only considering the needed columns\")\ny_input = y_input[target_cols]\nelse:\nif y_input_shape != y_col_len:\nraise ValueError(f\"Input data (y) is not in the right format ({y_input_shape} != {y_col_len})\")\nself.logger.warning(\"The input (y) does not have the 'columns' attribute -&gt; can't check the ordering of the columns\")\n# Return\nreturn x_input, y_input\ndef display_if_gpu_activated(self) -&gt; None:\n'''Displays if a GPU is being used'''\nif self._is_gpu_activated():\nself.logger.info(\"GPU activated\")\ndef _is_gpu_activated(self) -&gt; bool:\n'''Checks if we use a GPU\n        Returns:\n            bool: whether GPU is available or not\n        '''\n# By default, no GPU\nreturn False\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.__init__","title":"<code>__init__(model_dir=None, model_name=None, x_col=None, y_col=None, preprocess_pipeline=None, level_save='HIGH', **kwargs)</code>","text":"<p>Initialization of the parent class.</p> Kwargs <p>model_dir (str): Folder where to save the model     If None, creates a directory based on the model's name and the date (most common usage) model_name (str): The name of the model x_col (list): Names of the columns used for the training - x y_col (str or int or list if multi-labels): Name of the model's target column(s) - y preprocess_pipeline (ColumnTransformer): The pipeline used for preprocessing. If None -&gt; no preprocessing ! level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOW + hdf5 + pkl + plots     HIGH: MEDIUM + predictions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])</p> <code>NotADirectoryError</code> <p>If a provided model directory is not a directory (i.e. it's a file)</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None,\nx_col: Union[list, None] = None, y_col: Union[str, int, list, None] = None,\npreprocess_pipeline: Union[ColumnTransformer, None] = None, level_save: str = 'HIGH', **kwargs) -&gt; None:\n'''Initialization of the parent class.\n    Kwargs:\n        model_dir (str): Folder where to save the model\n            If None, creates a directory based on the model's name and the date (most common usage)\n        model_name (str): The name of the model\n        x_col (list): Names of the columns used for the training - x\n        y_col (str or int or list if multi-labels): Name of the model's target column(s) - y\n        preprocess_pipeline (ColumnTransformer): The pipeline used for preprocessing. If None -&gt; no preprocessing !\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOW + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n    '''\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type -&gt; 'classifier' or 'regressor' depending on the model\nself.model_type = None\n# Model name\nself.model_name = self._default_name if model_name is None else model_name\n# Names of the columns used\nself.x_col = x_col\nself.y_col = y_col\n# Can be None if reloading a model\nif x_col is None:\nself.logger.warning(\"Warning, the attribute x_col is not given! The model might not work as intended.\")\nif y_col is None:\nself.logger.warning(\"Warning, the attribute y_col is not given! The model might not work as intended.\")\n# Model folder\nif model_dir is None:\nself.model_dir = self._get_model_dir()\nelse:\nif not os.path.exists(model_dir):\nos.makedirs(model_dir)\nif not os.path.isdir(model_dir):\nraise NotADirectoryError(f\"{model_dir} is not a valid directory\")\nself.model_dir = os.path.abspath(model_dir)\n# Preprocessing pipeline\nself.preprocess_pipeline = preprocess_pipeline\nif self.preprocess_pipeline is not None:\ntry:\ncheck_is_fitted(self.preprocess_pipeline)\nexcept NotFittedError as e:\nself.logger.error(\"The preprocessing pipeline hasn't been fitted !\")\nself.logger.error(repr(e))\nraise NotFittedError()\n# We get the associated columns (and a check if there has been a fit is done)\nself.columns_in, self.mandatory_columns = utils_models.get_columns_pipeline(self.preprocess_pipeline)\nelse:\n# We can't define a \"no_preprocess\" pipeline since we should fit it\n# So we take care of that at the first fit\nself.logger.warning(\"Warning, no preprocessing pipeline given !\")\nself.columns_in, self.mandatory_columns = None, None\n# Other options\nself.level_save = level_save\n# is trained ?\nself.trained = False\nself.nb_fit = 0\n# Configuration dict. to be logged. Set on save.\nself.json_dict: Dict[Any, Any] = {}\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.display_if_gpu_activated","title":"<code>display_if_gpu_activated()</code>","text":"<p>Displays if a GPU is being used</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def display_if_gpu_activated(self) -&gt; None:\n'''Displays if a GPU is being used'''\nif self._is_gpu_activated():\nself.logger.info(\"GPU activated\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.fit","title":"<code>fit(x_train, y_train, **kwargs)</code>","text":"<p>Trains the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def fit(self, x_train, y_train, **kwargs) -&gt; None:\n'''Trains the model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    '''\nraise NotImplementedError(\"'fit' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, df_x=None, series_to_add=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required Kwargs <p>df_x (pd.DataFrame or None): Input dataFrame used for the prediction series_to_add (list): List of pd.Series to add to the dataframe type_data (str): Type of dataset (validation, test, ...)</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing the statistics</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\nseries_to_add: Union[List[pd.Series], None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_targets]\n        y_pred (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n        series_to_add (list): List of pd.Series to add to the dataframe\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing the statistics\n    '''\nraise NotImplementedError(\"'get_and_save_metrics' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets the final format of prediction     - Classification : classes from predictions     - Regression : values (identity function)</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>list | np.ndarray</code> <p>Array-like, shape = [n_samples,]    OR 1D array shape = [n_classes] (only one prediction)</p> required <p>Returns:</p> Type Description <code>?</code> <p>Array, shape = [n_samples, ?]</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Gets the final format of prediction\n        - Classification : classes from predictions\n        - Regression : values (identity function)\n    Args:\n        y (list | np.ndarray): Array-like, shape = [n_samples,]\n               OR 1D array shape = [n_classes] (only one prediction)\n    Returns:\n        (?): Array, shape = [n_samples, ?]\n    '''\nraise NotImplementedError(\"'inverse_transform' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.predict","title":"<code>predict(x_test, **kwargs)</code>","text":"<p>Predictions on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def predict(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predictions on the test set\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\nraise NotImplementedError(\"'predict' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\nraise NotImplementedError(\"'predict_proba' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_class/#template_num.models_training.model_class.ModelClass.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/model_class.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Manage paths\npkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\npreprocess_pipeline_path = os.path.join(self.model_dir, \"preprocess_pipeline.pkl\")\nconf_path = os.path.join(self.model_dir, \"configurations.json\")\n# Save the model &amp; preprocessing pipeline if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nwith open(pkl_path, 'wb') as f:\npickle.dump(self, f)\n# Useful for reload_from_standalone, otherwise, saved as a class attribute\nwith open(preprocess_pipeline_path, 'wb') as f:\npickle.dump(self.preprocess_pipeline, f)\n# Saving JSON configuration\njson_dict = {\n'maintainers': 'Agence DataServices',\n'gabarit_version': '1.2.5-dev-local',\n'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n'package_version': utils.get_package_version(),\n'model_name': self.model_name,\n'model_dir': self.model_dir,\n'model_type': self.model_type,\n'trained': self.trained,\n'nb_fit': self.nb_fit,\n'x_col': self.x_col,\n'y_col': self.y_col,\n'columns_in': self.columns_in,\n'mandatory_columns': self.mandatory_columns,\n'level_save': self.level_save,\n'librairie': None,\n}\n# Merge json_data if not None\nif json_data is not None:\n# Priority given to json_data !\njson_dict = {**json_dict, **json_data}\n# Add conf to attributes\nself.json_dict = json_dict\n# Save conf\nwith open(conf_path, 'w', encoding='utf-8') as json_file:\njson.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n# Now, save a proprietes file for the model upload\nself._save_upload_properties(json_dict)\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/","title":"Model keras","text":""},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras","title":"<code>ModelKeras</code>","text":"<p>         Bases: <code>ModelClass</code></p> <p>Generic model for Keras NN</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>class ModelKeras(ModelClass):\n'''Generic model for Keras NN'''\n_default_name = 'model_keras'\n# Not implemented :\n# -&gt; _get_model\n# -&gt; reload_from_standalone\ndef __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2,\npatience: int = 5, keras_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n        Kwargs:\n            batch_size (int): Batch size\n            epochs (int): Number of epochs\n            validation_split (float): Percentage for the validation set split\n                Only used if no input validation set when fitting\n            patience (int): Early stopping patience\n            keras_params (dict): Parameters used by keras models.\n                e.g. learning_rate, nb_lstm_units, etc...\n                The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n                This parameter was initially added in order to do an hyperparameters search\n        '''\n# TODO: learning rate should be an attribute !\n# Init.\nsuper().__init__(**kwargs)\n# Fix tensorflow GPU\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Param. model\nself.batch_size = batch_size\nself.epochs = epochs\nself.validation_split = validation_split\nself.patience = patience\n# Model set on fit\nself.model: Any = None\n# Keras params\nif keras_params is None:\nkeras_params = {}\nself.keras_params = keras_params.copy()\n# Keras custom objects : we get the ones specified in utils_deep_keras\nself.custom_objects = utils_deep_keras.custom_objects\ndef fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Fits the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n            y_valid (?): Array-like, shape = [n_samples, n_targets]\n            with_shuffle (bool): If x, y must be shuffled before fitting\n                This should be used if y is not shuffled as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            AssertionError: If different classes when comparing an already fitted model and a new dataset\n        '''\n##############################################\n# Manage retrain\n##############################################\n# If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n# And we save the old conf\nif self.trained:\n# Get src files to save\nsrc_files = [os.path.join(self.model_dir, \"configurations.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\nsrc_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Change model dir\nself.model_dir = self._get_model_dir()\n# Get dst files\ndst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\ndst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Copies\nfor src, dst in zip(src_files, dst_files):\ntry:\nshutil.copyfile(src, dst)\nexcept Exception as e:\nself.logger.error(f\"Unable to copy {src} to {dst}\")\nself.logger.error(\"We still go on\")\nself.logger.error(repr(e))\n##############################################\n# Prepare x_train, x_valid, y_train &amp; y_valid\n# Also extract list of classes if classification\n##############################################\n# Checking input formats\nx_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n# If the validation set is present, we check its format (but with fit_function=False)\nif y_valid is not None:\nx_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n# If classification, we need to transform y\nif self.model_type == 'classifier':\n# if not multilabel, transform y_train as dummies (should already be the case for multi-labels)\nif not self.multi_label:\n# If len(array.shape)==2, we flatten the array if the second dimension is useless\nif isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\ny_train = np.ravel(y_train)\nif isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\ny_valid = np.ravel(y_valid)\n# Dummies transformation\ny_train = pd.get_dummies(y_train)\ny_valid = pd.get_dummies(y_valid) if y_valid is not None else None\n# Important : get_dummies reorder the columns in alphabetical order\n# Thus, there is no problem if we fit again on a new dataframe with shuffled data\nlist_classes = list(y_train.columns)\n# FIX: valid test might miss some classes, hence we need to add them back to y_valid\nif y_valid is not None and y_train.shape[1] != y_valid.shape[1]:\nfor cl in list_classes:\n# Add missing columns\nif cl not in y_valid.columns:\ny_valid[cl] = 0\ny_valid = y_valid[list_classes]  # Reorder\n# Else keep it as it is\nelse:\ny_train = y_train\ny_valid = y_valid\nif hasattr(y_train, 'columns'):\n# TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\nlist_classes = list(y_train.columns)  # type: ignore\nelse:\nself.logger.warning(\n\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n)\n# We still create a list of classes in order to be compatible with other functions\nlist_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n# Set dict_classes based on list classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# Validate classes if already trained, else set them\nif self.trained:\nassert self.list_classes == list_classes, \\\n                    \"Error: the new dataset does not match with the already fitted model\"\nassert self.dict_classes == dict_classes, \\\n                    \"Error: the new dataset does not match with the already fitted model\"\nelse:\nself.list_classes = list_classes\nself.dict_classes = dict_classes\n# Shuffle x, y if wanted\n# It is advised as validation_split from keras does not shufle the data\n# Hence, for classificationt task, we might have classes in the validation data that we never met in the training data\nif with_shuffle:\np = np.random.permutation(len(x_train))\nx_train = np.array(x_train)[p]\ny_train = np.array(y_train)[p]\n# Else still transform to numpy array\nelse:\nx_train = np.array(x_train)\ny_train = np.array(y_train)\n# Also get y_valid as numpy &amp; get validation_data (tuple) if available\nvalidation_data: Optional[tuple] = None  # Def. None if y_valid is None\nif y_valid is not None:\ny_valid = np.array(y_valid)\nvalidation_data = (x_valid, y_valid)\nif validation_data is None:\nself.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n##############################################\n# Fit\n##############################################\n# Get model (if already fitted, _get_model returns instance model)\nself.model = self._get_model()\n# Get callbacks (early stopping &amp; checkpoint)\ncallbacks = self._get_callbacks()\n# Fit\n# We use a try...except in order to save the model if an error arises\n# after more than a minute into training\nstart_time = time.time()\ntry:\nfit_history = self.model.fit(  # type: ignore\nx_train,\ny_train,\nbatch_size=self.batch_size,\nepochs=self.epochs,\nvalidation_split=self.validation_split if validation_data is None else None,\nvalidation_data=validation_data,\ncallbacks=callbacks,\nverbose=1,\n)\nexcept (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, tf.errors.ResourceExhaustedError, tf.errors.InternalError,\ntf.errors.UnavailableError, tf.errors.UnimplementedError, tf.errors.UnknownError, Exception) as e:\n# Steps:\n# 1. Display tensor flow error\n# 2. Check if more than one minute elapsed &amp; existence best.hdf5\n# 3. Reload best model\n# 4. We consider that a fit occured (trained = True, nb_fit += 1)\n# 5. Save &amp; create a warning file\n# 6. Display error messages\n# 7. Raise an error\n# 1.\nself.logger.error(repr(e))\n# 2.\nbest_path = os.path.join(self.model_dir, 'best.hdf5')\ntime_spent = time.time() - start_time\nif time_spent &gt;= 60 and os.path.exists(best_path):\n# 3.\nself.model = load_model(best_path, custom_objects=self.custom_objects)\n# 4.\nself.trained = True\nself.nb_fit += 1\n# 5.\nself.save()\nwith open(os.path.join(self.model_dir, \"0_MODEL_INCOMPLETE\"), 'w'):\npass\nwith open(os.path.join(self.model_dir, \"1_TRAINING_NEEDS_TO_BE_RESUMED\"), 'w'):\npass\n# 6.\nself.logger.error(\"[EXPERIMENTAL] Error during model training\")\nself.logger.error(f\"[EXPERIMENTAL] The error happened after {round(time_spent, 2)}s of training\")\nself.logger.error(\"[EXPERIMENTAL] A saving of the model is done but this model won't be usable as is.\")\nself.logger.error(f\"[EXPERIMENTAL] In order to resume the training, we have to specify this model ({ntpath.basename(self.model_dir)}) in the file 2_training.py\")\nself.logger.error(\"[EXPERIMENTAL] Warning, the preprocessing is not saved in the configuration file\")\nself.logger.error(\"[EXPERIMENTAL] Warning, the best model might be corrupted in some cases\")\n# 7.\nraise RuntimeError(\"Error during model training\")\n# Print accuracy &amp; loss if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._plot_metrics_and_loss(fit_history)\n# Reload best model\nself.model = load_model(\nos.path.join(self.model_dir, 'best.hdf5'),\ncustom_objects=self.custom_objects\n)\n# Set trained\nself.trained = True\nself.nb_fit += 1\n@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, experimental_version: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes\n            experimental_version (bool): If an experimental (but faster) version must be used\n        Raises:\n            ValueError: If the model is not classifier and return_proba=True\n            ValueError: If the model is neither a classifier nor a regressor\n        Returns:\n            (np.ndarray): Array\n                # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n                # Else, shape = [n_samples, n_classes]\n        '''\n# Manage errors\nif return_proba and self.model_type != 'classifier':\nraise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Predict depends on model type\nif self.model_type == 'classifier':\nreturn self._predict_classifier(x_test, return_proba=return_proba,\nexperimental_version=experimental_version)\nelif self.model_type == 'regressor':\nreturn self._predict_regressor(x_test, experimental_version=experimental_version)\nelse:\nraise ValueError(f\"The model type ({self.model_type}) must be 'classifier' or 'regressor'\")\n@utils.trained_needed\ndef _predict_classifier(self, x_test: pd.DataFrame, return_proba: bool = False,\nexperimental_version: bool = False) -&gt; np.ndarray:\n'''Predictions on test\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            return_proba (boolean): If the function should return the probabilities instead of the classes\n            experimental_version (bool): If an experimental (but faster) version must be used\n        Raises:\n            ValueError: If the model is not of classifier type\n        Returns:\n            (np.ndarray): Array\n                # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n                # Else, shape = [n_samples, n_classes]\n        '''\nif self.model_type != 'classifier':\nraise ValueError(f\"Models of type {self.model_type} do not implement the method predict_classifier\")\n# Getting the predictions\nif experimental_version:\npredicted_proba = self.experimental_predict_proba(x_test)\nelse:\npredicted_proba = self.model.predict(x_test, batch_size=128, verbose=1)  # type: ignore\n# We return the probabilities if wanted\nif return_proba:\nreturn predicted_proba\n# Finally, we get the classes predictions\nreturn self.get_classes_from_proba(predicted_proba)  # type: ignore\n@utils.trained_needed\ndef _predict_regressor(self, x_test, experimental_version: bool = False) -&gt; np.ndarray:\n'''Predictions on test\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            experimental_version (bool): If an experimental (but faster) version must be used\n        Raises:\n            ValueError: If the model is not of regressor type\n        Returns:\n            (np.ndarray): Array, shape = [n_samples]\n        '''\nif self.model_type != 'regressor':\nraise ValueError(f\"Models of type {self.model_type} do not implement the method predict_regressor\")\n# Getting the predictions\nif experimental_version:\npredictions = self.experimental_predict_proba(x_test)\nelse:\npredictions = self.model.predict(x_test, batch_size=128, verbose=1)  # type: ignore\n# Finally, we get the final format\n# TODO : should certainly be changed for multi-output\n# TODO : create an equivalent of get_classes_from_proba for regression ?\nreturn np.array([pred[0] for pred in predictions])\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        Args:\n            x_test (pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        Raises:\n            ValueError: If model not classifier\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nif self.model_type != 'classifier':\nraise ValueError(f\"Models of type {self.model_type} do not implement the method predict_proba\")\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# We use predict again\nreturn self.predict(x_test, return_proba=True)\n@utils.trained_needed\ndef experimental_predict_proba(self, x_test: pd.DataFrame) -&gt; np.ndarray:\n'''Predictions on test set - simple pass forward - experimental\n        Args:\n            x_test (pd.DataFrame): Array-like, shape = [n_samples]\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n@tf.function\ndef serve(x):\nreturn self.model(x, training=False)\nreturn serve(x_test).numpy()\ndef _get_model(self) -&gt; Model:\n'''Gets a model structure - returns the instance model instead if already defined\n        Returns:\n            (Model): a Keras model\n        '''\nraise NotImplementedError(\"'_get_model' needs to be overridden\")\ndef _get_callbacks(self) -&gt; list:\n'''Gets model callbacks\n        Returns:\n            list: List of callbacks\n        '''\n# Get classic callbacks\ncallbacks = [EarlyStopping(monitor='val_loss', patience=self.patience, restore_best_weights=True)]\nif self.level_save in ['MEDIUM', 'HIGH']:\ncallbacks.append(\nModelCheckpoint(\nfilepath=os.path.join(self.model_dir, f'best.hdf5'), monitor='val_loss', save_best_only=True, mode='auto'\n)\n)\ncallbacks.append(CSVLogger(filename=os.path.join(self.model_dir, f'logger.csv'), separator=';', append=False))\ncallbacks.append(TerminateOnNaN())\n# Get LearningRateScheduler\nscheduler = self._get_learning_rate_scheduler()\nif scheduler is not None:\ncallbacks.append(LearningRateScheduler(scheduler))\n# Manage tensorboard\nif self.level_save in ['HIGH']:\n# Get log directory\nmodels_path = utils.get_models_path()\ntensorboard_dir = os.path.join(models_path, 'tensorboard_logs')\n# We add a prefix so that the function load_model works correctly (it looks for a sub-folder with model name)\nlog_dir = os.path.join(tensorboard_dir, f\"tensorboard_{ntpath.basename(self.model_dir)}\")\nif not os.path.exists(log_dir):\nos.makedirs(log_dir)\n# TODO: check if this class does not slow proccesses\n# -&gt; For now: comment\n# Create custom class to monitore LR changes\n# https://stackoverflow.com/questions/49127214/keras-how-to-output-learning-rate-onto-tensorboard\n# class LRTensorBoard(TensorBoard):\n#     def __init__(self, log_dir, **kwargs) -&gt; None:  # add other arguments to __init__ if you need\n#         super().__init__(log_dir=log_dir, **kwargs)\n#\n#     def on_epoch_end(self, epoch, logs=None):\n#         logs.update({'lr': K.eval(self.model.optimizer.lr)})\n#         super().on_epoch_end(epoch, logs)\n# Append tensorboard callback\n# TODO: check compatibility tensorflow 2.3\n# WARNING : https://stackoverflow.com/questions/63619763/model-training-hangs-forever-when-using-a-tensorboard-callback-with-an-lstm-laye\n# A compatibility problem TensorBoard / TensorFlow 2.3 (cuDNN implementation of LSTM/GRU) can arise\n# In this case, the training of the model can be \"blocked\" and does not respond anymore\n# This problem has arisen two times on P\u00f4le Emploi computers (windows 7 &amp; VM Ubuntu on windows 7 host)\n# No problem on Valeuriad computers (windows 10)\n# Thus, TensorBoard is deactivated by default for now\n# While awaiting a possible fix, you are responsible for checking if TensorBoard works on your computer\nself.logger.warning(\" ###################### \")\nself.logger.warning(\"TensorBoard deactivated : compatibility problem TensorBoard / TensorFlow 2.3 (cuDNN implementation of LSTM/GRU) can arise\")\nself.logger.warning(\"https://stackoverflow.com/questions/63619763/model-training-hangs-forever-when-using-a-tensorboard-callback-with-an-lstm-laye\")\nself.logger.warning(\" In order to activate if, one has to modify the method _get_callbacks of model_keras.py\")\nself.logger.warning(\" ###################### \")\n# callbacks.append(TensorBoard(log_dir=log_dir, write_grads=False, write_images=False))\n# self.logger.info(f\"To start tensorboard: python -m tensorboard.main --logdir {tensorboard_dir}\")\nreturn callbacks\ndef _get_learning_rate_scheduler(self) -&gt; Union[Callable, None]:\n'''Defines a Learning Rate Scheduler\n           -&gt; if it returns None, no scheduler will be used. (def.)\n           -&gt; This function will be save directly in the model configuration file\n           -&gt; This can be overridden at runing time\n        Returns:\n            (Callable | None): A learning rate Scheduler\n        '''\n# e.g.\n# def scheduler(epoch):\n#     lim_epoch = 75\n#     if epoch &lt; lim_epoch:\n#         return 0.01\n#     else:\n#         return max(0.001, 0.01 * math.exp(0.01 * (lim_epoch - epoch)))\nscheduler = None\nreturn scheduler\ndef _plot_metrics_and_loss(self, fit_history) -&gt; None:\n'''Plots some metrics &amp; loss\n        Arguments:\n            fit_history (?) : fit history\n        '''\n# Manage dir\nplots_path = os.path.join(self.model_dir, 'plots')\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\n# Get a dictionnary of possible metrics/loss plots\nmetrics_dir = {\n'acc': ['Accuracy', 'accuracy'],\n'loss': ['Loss', 'loss'],\n'categorical_accuracy': ['Categorical accuracy', 'categorical_accuracy'],\n'f1': ['F1-score', 'f1_score'],\n'precision': ['Precision', 'precision'],\n'recall': ['Recall', 'recall'],\n'mean_absolute_error': ['MAE', 'mae'],\n'mae': ['MAE', 'mae'],\n'mean_squared_error': ['MSE', 'mse'],\n'mse': ['MSE', 'mse'],\n'root_mean_squared_error': ['RMSE', 'rmse'],\n'rmse': ['RMSE', 'rmse'],\n}\n# Plot each available metric\nfor metric in fit_history.history.keys():\nif metric in metrics_dir.keys():\ntitle = metrics_dir[metric][0]\nfilename = metrics_dir[metric][1]\nplt.figure(figsize=(10, 8))\nplt.plot(fit_history.history[metric])\nplt.plot(fit_history.history[f'val_{metric}'])\nplt.title(f\"Model {title}\")\nplt.ylabel(title)\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n# Save\nfilename == f\"{filename}.jpeg\"\nplt.savefig(os.path.join(plots_path, filename))\n# Close figures\nplt.close('all')\ndef _save_model_png(self, model) -&gt; None:\n'''Tries to save the structure of the model in png format\n        Graphviz necessary\n        Args:\n            model (?): model to plot\n        '''\n# Check if graphiz is intalled\n# TODO : to be improved !\ngraphiz_path = 'C:/Program Files (x86)/Graphviz2.38/bin/'\nif os.path.isdir(graphiz_path):\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\nimg_path = os.path.join(self.model_dir, 'model.png')\nplot_model(model, to_file=img_path)\n@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'keras'\njson_data['batch_size'] = self.batch_size\njson_data['epochs'] = self.epochs\njson_data['validation_split'] = self.validation_split\njson_data['patience'] = self.patience\njson_data['keras_params'] = self.keras_params\nif self.model is not None:\njson_data['keras_model'] = json.loads(self.model.to_json())\nelse:\njson_data['keras_model'] = None\n# Add _get_model code if not in json_data\nif '_get_model' not in json_data.keys():\njson_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n# Add _get_learning_rate_scheduler code if not in json_data\nif '_get_learning_rate_scheduler' not in json_data.keys():\njson_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n# Add custom_objects code if not in json_data\nif 'custom_objects' not in json_data.keys():\ncustom_objects_str = self.custom_objects.copy()\nfor key in custom_objects_str.keys():\nif callable(custom_objects_str[key]):\n# Nominal case\nif not isinstance(custom_objects_str[key], functools.partial):\ncustom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n# Manage partials\nelse:\ncustom_objects_str[key] = {\n'type': 'partial',\n'args': custom_objects_str[key].args,\n'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n}\njson_data['custom_objects'] = custom_objects_str\n# Save strategy :\n# - best.hdf5 already saved in fit()\n# - can't pickle keras model, so we drop it, save, and reload it\nkeras_model = self.model\nself.model = None\nsuper().save(json_data=json_data)\nself.model = keras_model\ndef reload_model(self, hdf5_path: str) -&gt; Any:\n'''Loads a Keras model from a HDF5 file\n        Args:\n            hdf5_path (str): Path to the hdf5 file\n        Returns:\n            ?: Keras model\n        '''\n# Fix tensorflow GPU if not already done (useful if we reload a model)\ntry:\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\nexcept Exception:\npass\n# We check if we already have the custom objects\nif hasattr(self, 'custom_objects') and self.custom_objects is not None:\ncustom_objects = self.custom_objects\nelse:\nself.logger.warning(\"Can't find the attribute 'custom_objects' in the model to be reloaded\")\nself.logger.warning(\"Backup on the default custom_objects of utils_deep_keras\")\ncustom_objects = utils_deep_keras.custom_objects\n# Loading of the model\nkeras_model = load_model(hdf5_path, custom_objects=custom_objects)\n# Set trained to true if not already true\nif not self.trained:\nself.trained = True\nself.nb_fit = 1\n# Return\nreturn keras_model\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Needs to be overridden /!\\\\ -\n        '''\nraise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\ndef _is_gpu_activated(self) -&gt; bool:\n'''Checks if a GPU is used\n        Returns:\n            bool: whether GPU is available or not\n        '''\n# Checks for available GPU devices\nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) &gt; 0:\nreturn True\nelse:\nreturn False\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.__init__","title":"<code>__init__(batch_size=64, epochs=99, validation_split=0.2, patience=5, keras_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>batch_size (int): Batch size epochs (int): Number of epochs validation_split (float): Percentage for the validation set split     Only used if no input validation set when fitting patience (int): Early stopping patience keras_params (dict): Parameters used by keras models.     e.g. learning_rate, nb_lstm_units, etc...     The purpose of this dictionary is for the user to use it as they wants in the _get_model function     This parameter was initially added in order to do an hyperparameters search</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>def __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2,\npatience: int = 5, keras_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n    Kwargs:\n        batch_size (int): Batch size\n        epochs (int): Number of epochs\n        validation_split (float): Percentage for the validation set split\n            Only used if no input validation set when fitting\n        patience (int): Early stopping patience\n        keras_params (dict): Parameters used by keras models.\n            e.g. learning_rate, nb_lstm_units, etc...\n            The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n            This parameter was initially added in order to do an hyperparameters search\n    '''\n# TODO: learning rate should be an attribute !\n# Init.\nsuper().__init__(**kwargs)\n# Fix tensorflow GPU\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Param. model\nself.batch_size = batch_size\nself.epochs = epochs\nself.validation_split = validation_split\nself.patience = patience\n# Model set on fit\nself.model: Any = None\n# Keras params\nif keras_params is None:\nkeras_params = {}\nself.keras_params = keras_params.copy()\n# Keras custom objects : we get the ones specified in utils_deep_keras\nself.custom_objects = utils_deep_keras.custom_objects\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.experimental_predict_proba","title":"<code>experimental_predict_proba(x_test)</code>","text":"<p>Predictions on test set - simple pass forward - experimental</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>Array-like, shape = [n_samples]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>@utils.trained_needed\ndef experimental_predict_proba(self, x_test: pd.DataFrame) -&gt; np.ndarray:\n'''Predictions on test set - simple pass forward - experimental\n    Args:\n        x_test (pd.DataFrame): Array-like, shape = [n_samples]\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n@tf.function\ndef serve(x):\nreturn self.model(x, training=False)\nreturn serve(x_test).numpy()\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required Kwargs <p>x_valid (?): Array-like, shape = [n_samples, n_features] y_valid (?): Array-like, shape = [n_samples, n_targets] with_shuffle (bool): If x, y must be shuffled before fitting     This should be used if y is not shuffled as the split_validation takes the lines in order.     Thus, the validation set might get classes which are not in the train set ...</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If different classes when comparing an already fitted model and a new dataset</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Fits the model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features]\n        y_valid (?): Array-like, shape = [n_samples, n_targets]\n        with_shuffle (bool): If x, y must be shuffled before fitting\n            This should be used if y is not shuffled as the split_validation takes the lines in order.\n            Thus, the validation set might get classes which are not in the train set ...\n    Raises:\n        AssertionError: If different classes when comparing an already fitted model and a new dataset\n    '''\n##############################################\n# Manage retrain\n##############################################\n# If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n# And we save the old conf\nif self.trained:\n# Get src files to save\nsrc_files = [os.path.join(self.model_dir, \"configurations.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\nsrc_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Change model dir\nself.model_dir = self._get_model_dir()\n# Get dst files\ndst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\ndst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Copies\nfor src, dst in zip(src_files, dst_files):\ntry:\nshutil.copyfile(src, dst)\nexcept Exception as e:\nself.logger.error(f\"Unable to copy {src} to {dst}\")\nself.logger.error(\"We still go on\")\nself.logger.error(repr(e))\n##############################################\n# Prepare x_train, x_valid, y_train &amp; y_valid\n# Also extract list of classes if classification\n##############################################\n# Checking input formats\nx_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n# If the validation set is present, we check its format (but with fit_function=False)\nif y_valid is not None:\nx_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n# If classification, we need to transform y\nif self.model_type == 'classifier':\n# if not multilabel, transform y_train as dummies (should already be the case for multi-labels)\nif not self.multi_label:\n# If len(array.shape)==2, we flatten the array if the second dimension is useless\nif isinstance(y_train, np.ndarray) and len(y_train.shape) == 2 and y_train.shape[1] == 1:\ny_train = np.ravel(y_train)\nif isinstance(y_valid, np.ndarray) and len(y_valid.shape) == 2 and y_valid.shape[1] == 1:\ny_valid = np.ravel(y_valid)\n# Dummies transformation\ny_train = pd.get_dummies(y_train)\ny_valid = pd.get_dummies(y_valid) if y_valid is not None else None\n# Important : get_dummies reorder the columns in alphabetical order\n# Thus, there is no problem if we fit again on a new dataframe with shuffled data\nlist_classes = list(y_train.columns)\n# FIX: valid test might miss some classes, hence we need to add them back to y_valid\nif y_valid is not None and y_train.shape[1] != y_valid.shape[1]:\nfor cl in list_classes:\n# Add missing columns\nif cl not in y_valid.columns:\ny_valid[cl] = 0\ny_valid = y_valid[list_classes]  # Reorder\n# Else keep it as it is\nelse:\ny_train = y_train\ny_valid = y_valid\nif hasattr(y_train, 'columns'):\n# TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\nlist_classes = list(y_train.columns)  # type: ignore\nelse:\nself.logger.warning(\n\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n)\n# We still create a list of classes in order to be compatible with other functions\nlist_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n# Set dict_classes based on list classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# Validate classes if already trained, else set them\nif self.trained:\nassert self.list_classes == list_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nassert self.dict_classes == dict_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nelse:\nself.list_classes = list_classes\nself.dict_classes = dict_classes\n# Shuffle x, y if wanted\n# It is advised as validation_split from keras does not shufle the data\n# Hence, for classificationt task, we might have classes in the validation data that we never met in the training data\nif with_shuffle:\np = np.random.permutation(len(x_train))\nx_train = np.array(x_train)[p]\ny_train = np.array(y_train)[p]\n# Else still transform to numpy array\nelse:\nx_train = np.array(x_train)\ny_train = np.array(y_train)\n# Also get y_valid as numpy &amp; get validation_data (tuple) if available\nvalidation_data: Optional[tuple] = None  # Def. None if y_valid is None\nif y_valid is not None:\ny_valid = np.array(y_valid)\nvalidation_data = (x_valid, y_valid)\nif validation_data is None:\nself.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n##############################################\n# Fit\n##############################################\n# Get model (if already fitted, _get_model returns instance model)\nself.model = self._get_model()\n# Get callbacks (early stopping &amp; checkpoint)\ncallbacks = self._get_callbacks()\n# Fit\n# We use a try...except in order to save the model if an error arises\n# after more than a minute into training\nstart_time = time.time()\ntry:\nfit_history = self.model.fit(  # type: ignore\nx_train,\ny_train,\nbatch_size=self.batch_size,\nepochs=self.epochs,\nvalidation_split=self.validation_split if validation_data is None else None,\nvalidation_data=validation_data,\ncallbacks=callbacks,\nverbose=1,\n)\nexcept (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, tf.errors.ResourceExhaustedError, tf.errors.InternalError,\ntf.errors.UnavailableError, tf.errors.UnimplementedError, tf.errors.UnknownError, Exception) as e:\n# Steps:\n# 1. Display tensor flow error\n# 2. Check if more than one minute elapsed &amp; existence best.hdf5\n# 3. Reload best model\n# 4. We consider that a fit occured (trained = True, nb_fit += 1)\n# 5. Save &amp; create a warning file\n# 6. Display error messages\n# 7. Raise an error\n# 1.\nself.logger.error(repr(e))\n# 2.\nbest_path = os.path.join(self.model_dir, 'best.hdf5')\ntime_spent = time.time() - start_time\nif time_spent &gt;= 60 and os.path.exists(best_path):\n# 3.\nself.model = load_model(best_path, custom_objects=self.custom_objects)\n# 4.\nself.trained = True\nself.nb_fit += 1\n# 5.\nself.save()\nwith open(os.path.join(self.model_dir, \"0_MODEL_INCOMPLETE\"), 'w'):\npass\nwith open(os.path.join(self.model_dir, \"1_TRAINING_NEEDS_TO_BE_RESUMED\"), 'w'):\npass\n# 6.\nself.logger.error(\"[EXPERIMENTAL] Error during model training\")\nself.logger.error(f\"[EXPERIMENTAL] The error happened after {round(time_spent, 2)}s of training\")\nself.logger.error(\"[EXPERIMENTAL] A saving of the model is done but this model won't be usable as is.\")\nself.logger.error(f\"[EXPERIMENTAL] In order to resume the training, we have to specify this model ({ntpath.basename(self.model_dir)}) in the file 2_training.py\")\nself.logger.error(\"[EXPERIMENTAL] Warning, the preprocessing is not saved in the configuration file\")\nself.logger.error(\"[EXPERIMENTAL] Warning, the best model might be corrupted in some cases\")\n# 7.\nraise RuntimeError(\"Error during model training\")\n# Print accuracy &amp; loss if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._plot_metrics_and_loss(fit_history)\n# Reload best model\nself.model = load_model(\nos.path.join(self.model_dir, 'best.hdf5'),\ncustom_objects=self.custom_objects\n)\n# Set trained\nself.trained = True\nself.nb_fit += 1\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.predict","title":"<code>predict(x_test, return_proba=False, experimental_version=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required Kwargs <p>return_proba (bool): If the function should return the probabilities instead of the classes experimental_version (bool): If an experimental (but faster) version must be used</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not classifier and return_proba=True</p> <code>ValueError</code> <p>If the model is neither a classifier nor a regressor</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, experimental_version: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes\n        experimental_version (bool): If an experimental (but faster) version must be used\n    Raises:\n        ValueError: If the model is not classifier and return_proba=True\n        ValueError: If the model is neither a classifier nor a regressor\n    Returns:\n        (np.ndarray): Array\n            # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n            # Else, shape = [n_samples, n_classes]\n    '''\n# Manage errors\nif return_proba and self.model_type != 'classifier':\nraise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Predict depends on model type\nif self.model_type == 'classifier':\nreturn self._predict_classifier(x_test, return_proba=return_proba,\nexperimental_version=experimental_version)\nelif self.model_type == 'regressor':\nreturn self._predict_regressor(x_test, experimental_version=experimental_version)\nelse:\nraise ValueError(f\"The model type ({self.model_type}) must be 'classifier' or 'regressor'\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.predict--if-not-return_proba-shape-n_samples-or-n_samples-n_classes","title":"If not return_proba, shape = [n_samples,] or [n_samples, n_classes]","text":""},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.predict--else-shape-n_samples-n_classes","title":"Else, shape = [n_samples, n_classes]","text":""},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If model not classifier</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n    Args:\n        x_test (pd.DataFrame): Array-like, shape = [n_samples, n_features]\n    Raises:\n        ValueError: If model not classifier\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\nif self.model_type != 'classifier':\nraise ValueError(f\"Models of type {self.model_type} do not implement the method predict_proba\")\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# We use predict again\nreturn self.predict(x_test, return_proba=True)\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Needs to be overridden /! -</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Needs to be overridden /!\\\\ -\n    '''\nraise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.reload_model","title":"<code>reload_model(hdf5_path)</code>","text":"<p>Loads a Keras model from a HDF5 file</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to the hdf5 file</p> required <p>Returns:</p> Type Description <code>Any</code> <p>?: Keras model</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>def reload_model(self, hdf5_path: str) -&gt; Any:\n'''Loads a Keras model from a HDF5 file\n    Args:\n        hdf5_path (str): Path to the hdf5 file\n    Returns:\n        ?: Keras model\n    '''\n# Fix tensorflow GPU if not already done (useful if we reload a model)\ntry:\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\nexcept Exception:\npass\n# We check if we already have the custom objects\nif hasattr(self, 'custom_objects') and self.custom_objects is not None:\ncustom_objects = self.custom_objects\nelse:\nself.logger.warning(\"Can't find the attribute 'custom_objects' in the model to be reloaded\")\nself.logger.warning(\"Backup on the default custom_objects of utils_deep_keras\")\ncustom_objects = utils_deep_keras.custom_objects\n# Loading of the model\nkeras_model = load_model(hdf5_path, custom_objects=custom_objects)\n# Set trained to true if not already true\nif not self.trained:\nself.trained = True\nself.nb_fit = 1\n# Return\nreturn keras_model\n</code></pre>"},{"location":"reference/template_num/models_training/model_keras/#template_num.models_training.model_keras.ModelKeras.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/model_keras.py</code> <pre><code>@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'keras'\njson_data['batch_size'] = self.batch_size\njson_data['epochs'] = self.epochs\njson_data['validation_split'] = self.validation_split\njson_data['patience'] = self.patience\njson_data['keras_params'] = self.keras_params\nif self.model is not None:\njson_data['keras_model'] = json.loads(self.model.to_json())\nelse:\njson_data['keras_model'] = None\n# Add _get_model code if not in json_data\nif '_get_model' not in json_data.keys():\njson_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n# Add _get_learning_rate_scheduler code if not in json_data\nif '_get_learning_rate_scheduler' not in json_data.keys():\njson_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n# Add custom_objects code if not in json_data\nif 'custom_objects' not in json_data.keys():\ncustom_objects_str = self.custom_objects.copy()\nfor key in custom_objects_str.keys():\nif callable(custom_objects_str[key]):\n# Nominal case\nif not isinstance(custom_objects_str[key], functools.partial):\ncustom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n# Manage partials\nelse:\ncustom_objects_str[key] = {\n'type': 'partial',\n'args': custom_objects_str[key].args,\n'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n}\njson_data['custom_objects'] = custom_objects_str\n# Save strategy :\n# - best.hdf5 already saved in fit()\n# - can't pickle keras model, so we drop it, save, and reload it\nkeras_model = self.model\nself.model = None\nsuper().save(json_data=json_data)\nself.model = keras_model\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/","title":"Model pipeline","text":""},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline","title":"<code>ModelPipeline</code>","text":"<p>         Bases: <code>ModelClass</code></p> <p>Generic model for sklearn pipeline</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>class ModelPipeline(ModelClass):\n'''Generic model for sklearn pipeline'''\n_default_name = 'model_pipeline'\n# Not implemented :\n# -&gt; reload_from_standalone\ndef __init__(self, pipeline: Union[Pipeline, None] = None, **kwargs) -&gt; None:\n'''Class initialization (see ModelClass for more arguments)\n        Kwargs:\n            pipeline (Pipeline): Pipeline to use\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model (to implement for children class)\nself.pipeline = pipeline\ndef fit(self, x_train, y_train, **kwargs) -&gt; None:\n'''Trains the model\n           **kwargs permits compatibility with Keras model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Raises:\n            RuntimeError: If the model is already fitted\n            ValueError: If the type of model is not classifier or regressor\n        '''\nif self.trained:\nself.logger.error(\"We can't train again a pipeline sklearn model\")\nself.logger.error(\"Please train a new model\")\nraise RuntimeError(\"We can't train again a pipeline sklearn model\")\n# We check input format\nx_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\nif self.model_type == 'classifier':\nself._fit_classifier(x_train, y_train, **kwargs)\nelif self.model_type == 'regressor':\nself._fit_regressor(x_train, y_train, **kwargs)\nelse:\nraise ValueError(f\"The model type ({self.model_type}) must be 'classifier' or 'regressor'\")\ndef _fit_classifier(self, x_train, y_train, **kwargs) -&gt; None:\n'''Training of a model of model type classifier\n           **kwargs permits compatibility with Keras model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        '''\n# We \"only\" check if no multi-classes multi-labels (which can't be managed by most SKLEARN pipelines)\nif self.multi_label:\ndf_tmp = pd.DataFrame(y_train)\nfor col in df_tmp:\nuniques = df_tmp[col].unique()\nif len(uniques) &gt; 2:\nself.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\nself.logger.warning(\"Most sklearn pipelines can't manage multi-classes/multi-labels\")\nself.logger.warning(' - /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ - ')\n# We \"let\" the process crash by itself\nbreak\n# Fit pipeline\nself.pipeline.fit(x_train, y_train)\n# Set list classes\nif not self.multi_label:\nself.list_classes = list(self.pipeline.classes_)\nelse:\nif hasattr(y_train, 'columns'):\nself.list_classes = list(y_train.columns)\nelse:\nself.logger.warning(\n\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\"\n)\n# We still create a list of classes in order to be compatible with other functions\nself.list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n# Set dict_classes based on list classes\nself.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n# Set trained\nself.trained = True\nself.nb_fit += 1\ndef _fit_regressor(self, x_train, y_train, **kwargs) -&gt; None:\n'''Training of a regressor model\n           **kwargs permits compatibility with Keras model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        '''\n# Fit pipeline\nself.pipeline.fit(x_train, y_train)\n# Set trained\nself.trained = True\nself.nb_fit += 1\n@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n        Raises:\n            ValueError: If the model is not classifier and return_proba=True\n        Returns:\n            (np.ndarray): Array\n                # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n                # Else, shape = [n_samples, n_classes]\n        '''\n# Manage errors\nif return_proba is True and self.model_type != 'classifier':\nraise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n#\nif not return_proba:\nreturn np.array(self.pipeline.predict(x_test))\nelse:\nreturn self.predict_proba(x_test)\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set - Classifier only\n        Args:\n            x_test (pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        Raises:\n            ValueError: If model not classifier\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nif self.model_type != 'classifier':\nraise ValueError(f\"Models of type {self.model_type} do not implement the method predict_proba\")\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n#\nprobas = np.array(self.pipeline.predict_proba(x_test))\n# Very specific fix: in some cases, with OvR, strategy, all estimators return 0, which generates a division per 0 when normalizing\n# Hence, we replace NaNs with 1 / nb_classes\nif not np.isnan(probas).any():\nprobas = np.nan_to_num(probas, nan=1/len(self.list_classes))\n# If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n# Same thing for some base models\n# Correction in case where we detect a shape of length &gt; 2 (ie. equals to 3)\n# Reminder : we do not manage multi-labels/multi-classes\nif len(probas.shape) &gt; 2:\nprobas = np.swapaxes(probas[:, :, 1], 0, 1)\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'scikit-learn'\n# Add each pipeline steps' conf\nif self.pipeline is not None:\nfor step in self.pipeline.steps:\nname = step[0]\nconfs = step[1].get_params()\n# Get rid of some non serializable conf\nfor special_conf in ['dtype', 'base_estimator', 'estimator', 'estimator__base_estimator',\n'estimator__estimator', 'estimator__estimator__base_estimator']:\nif special_conf in confs.keys():\nconfs[special_conf] = str(confs[special_conf])\njson_data[f'{name}_confs'] = confs\n# Save\nsuper().save(json_data=json_data)\n# Save model standalone if wanted &amp; pipeline is not None &amp; level_save &gt; 'LOW'\nif self.pipeline is not None and self.level_save in ['MEDIUM', 'HIGH']:\npkl_path = os.path.join(self.model_dir, \"sklearn_pipeline_standalone.pkl\")\n# Save model\nwith open(pkl_path, 'wb') as f:\npickle.dump(self.pipeline, f)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Needs to be overridden /!\\\\ -\n        '''\nraise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.__init__","title":"<code>__init__(pipeline=None, **kwargs)</code>","text":"<p>Class initialization (see ModelClass for more arguments)</p> Kwargs <p>pipeline (Pipeline): Pipeline to use</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>def __init__(self, pipeline: Union[Pipeline, None] = None, **kwargs) -&gt; None:\n'''Class initialization (see ModelClass for more arguments)\n    Kwargs:\n        pipeline (Pipeline): Pipeline to use\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model (to implement for children class)\nself.pipeline = pipeline\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.fit","title":"<code>fit(x_train, y_train, **kwargs)</code>","text":"<p>Trains the model    **kwargs permits compatibility with Keras model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model is already fitted</p> <code>ValueError</code> <p>If the type of model is not classifier or regressor</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>def fit(self, x_train, y_train, **kwargs) -&gt; None:\n'''Trains the model\n       **kwargs permits compatibility with Keras model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Raises:\n        RuntimeError: If the model is already fitted\n        ValueError: If the type of model is not classifier or regressor\n    '''\nif self.trained:\nself.logger.error(\"We can't train again a pipeline sklearn model\")\nself.logger.error(\"Please train a new model\")\nraise RuntimeError(\"We can't train again a pipeline sklearn model\")\n# We check input format\nx_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\nif self.model_type == 'classifier':\nself._fit_classifier(x_train, y_train, **kwargs)\nelif self.model_type == 'regressor':\nself._fit_regressor(x_train, y_train, **kwargs)\nelse:\nraise ValueError(f\"The model type ({self.model_type}) must be 'classifier' or 'regressor'\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required Kwargs <p>return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not classifier and return_proba=True</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n    Raises:\n        ValueError: If the model is not classifier and return_proba=True\n    Returns:\n        (np.ndarray): Array\n            # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n            # Else, shape = [n_samples, n_classes]\n    '''\n# Manage errors\nif return_proba is True and self.model_type != 'classifier':\nraise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n#\nif not return_proba:\nreturn np.array(self.pipeline.predict(x_test))\nelse:\nreturn self.predict_proba(x_test)\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.predict--if-not-return_proba-shape-n_samples-or-n_samples-n_classes","title":"If not return_proba, shape = [n_samples,] or [n_samples, n_classes]","text":""},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.predict--else-shape-n_samples-n_classes","title":"Else, shape = [n_samples, n_classes]","text":""},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - Classifier only</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If model not classifier</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set - Classifier only\n    Args:\n        x_test (pd.DataFrame): Array-like, shape = [n_samples, n_features]\n    Raises:\n        ValueError: If model not classifier\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\nif self.model_type != 'classifier':\nraise ValueError(f\"Models of type {self.model_type} do not implement the method predict_proba\")\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n#\nprobas = np.array(self.pipeline.predict_proba(x_test))\n# Very specific fix: in some cases, with OvR, strategy, all estimators return 0, which generates a division per 0 when normalizing\n# Hence, we replace NaNs with 1 / nb_classes\nif not np.isnan(probas).any():\nprobas = np.nan_to_num(probas, nan=1/len(self.list_classes))\n# If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n# Same thing for some base models\n# Correction in case where we detect a shape of length &gt; 2 (ie. equals to 3)\n# Reminder : we do not manage multi-labels/multi-classes\nif len(probas.shape) &gt; 2:\nprobas = np.swapaxes(probas[:, :, 1], 0, 1)\nreturn probas\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Needs to be overridden /! -</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Needs to be overridden /!\\\\ -\n    '''\nraise NotImplementedError(\"'reload_from_standalone' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/models_training/model_pipeline/#template_num.models_training.model_pipeline.ModelPipeline.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/model_pipeline.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'scikit-learn'\n# Add each pipeline steps' conf\nif self.pipeline is not None:\nfor step in self.pipeline.steps:\nname = step[0]\nconfs = step[1].get_params()\n# Get rid of some non serializable conf\nfor special_conf in ['dtype', 'base_estimator', 'estimator', 'estimator__base_estimator',\n'estimator__estimator', 'estimator__estimator__base_estimator']:\nif special_conf in confs.keys():\nconfs[special_conf] = str(confs[special_conf])\njson_data[f'{name}_confs'] = confs\n# Save\nsuper().save(json_data=json_data)\n# Save model standalone if wanted &amp; pipeline is not None &amp; level_save &gt; 'LOW'\nif self.pipeline is not None and self.level_save in ['MEDIUM', 'HIGH']:\npkl_path = os.path.join(self.model_dir, \"sklearn_pipeline_standalone.pkl\")\n# Save model\nwith open(pkl_path, 'wb') as f:\npickle.dump(self.pipeline, f)\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/","title":"Utils deep keras","text":""},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.f1","title":"<code>f1(y_true, y_pred)</code>","text":"<p>f1 score, to use as custom metrics</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def f1(y_true, y_pred) -&gt; float:\n'''f1 score, to use as custom metrics\n    - /!\\\\ To use with a big batch size /!\\\\ -\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n# Round pred to 0 &amp; 1\ny_pred = K.round(y_pred)\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ny_pred = K.round(y_pred)\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n# tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\np = tp / (tp + fp + K.epsilon())\nr = tp / (tp + fn + K.epsilon())\nf1 = 2 * p * r / (p + r + K.epsilon())\nf1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\nweighted_f1 = f1 * ground_positives / K.sum(ground_positives)\nweighted_f1 = K.sum(weighted_f1)\nreturn weighted_f1\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.f1_loss","title":"<code>f1_loss(y_true, y_pred)</code>","text":"<p>f1 loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def f1_loss(y_true, y_pred) -&gt; float:\n'''f1 loss, to use as custom loss\n    - /!\\\\ To use with a big batch size /!\\\\ -\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n# TODO : Find a mean of rounding y_pred\n# TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n# We can't round here :(\n# Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n# Common ops without gradient: K.argmax, K.round, K.eval.\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n# tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\np = tp / (tp + fp + K.epsilon())\nr = tp / (tp + fn + K.epsilon())\nf1 = 2 * p * r / (p + r + K.epsilon())\nf1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\nweighted_f1 = f1 * ground_positives / K.sum(ground_positives)\nweighted_f1 = K.sum(weighted_f1)\nreturn 1 - weighted_f1\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.fb_loss","title":"<code>fb_loss(b, y_true, y_pred)</code>","text":"<p>fB loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> required <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def fb_loss(b: float, y_true, y_pred) -&gt; float:\n'''fB loss, to use as custom loss\n    - /!\\\\ To use with a big batch size /!\\\\ -\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n    Args:\n        b (float): importance recall in the calculation of the fB score\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n# TODO : Find a mean of rounding y_pred\n# TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n# We can't round here :(\n# Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n# Common ops without gradient: K.argmax, K.round, K.eval.\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n# tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\np = tp / (tp + fp + K.epsilon())\nr = tp / (tp + fn + K.epsilon())\nfb = (1 + b**2) * p * r / ((p * b**2) + r + K.epsilon())\nfb = tf.where(tf.math.is_nan(fb), tf.zeros_like(fb), fb)\nweighted_fb = fb * ground_positives / K.sum(ground_positives)\nweighted_fb = K.sum(weighted_fb)\nreturn 1 - weighted_fb\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.get_fb_loss","title":"<code>get_fb_loss(b=2.0)</code>","text":"<p>Gets a fB-score loss</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>fb_loss</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def get_fb_loss(b: float = 2.0) -&gt; Callable:\n''' Gets a fB-score loss\n    Args:\n        b (float): importance recall in the calculation of the fB score\n    Returns:\n        Callable: fb_loss\n    '''\n# - /!\\ Use of partial mandatory in order to be able to pickle dynamical functions ! /!\\ -\nfn = partial(fb_loss, b)\n# FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\nfn.__name__ = 'fb_loss'  # type: ignore\nreturn fn\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.get_weighted_binary_crossentropy","title":"<code>get_weighted_binary_crossentropy(pos_weight=10.0)</code>","text":"<p>Gets a \"weighted binary crossentropy\" loss From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>Weight of the positive class, to be tuned</p> <code>10.0</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Weighted binary crossentropy loss</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def get_weighted_binary_crossentropy(pos_weight: float = 10.0) -&gt; Callable:\n''' Gets a \"weighted binary crossentropy\" loss\n    From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu\n    TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)\n    Args:\n        pos_weight (float): Weight of the positive class, to be tuned\n    Returns:\n        Callable: Weighted binary crossentropy loss\n    '''\n# - /!\\ Use of partial mandatory in order to be able to pickle dynamical functions ! /!\\ -\nfn = partial(weighted_binary_crossentropy, pos_weight)\n# FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\nfn.__name__ = 'weighted_binary_crossentropy'  # type: ignore\nreturn fn\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.precision","title":"<code>precision(y_true, y_pred)</code>","text":"<p>Precision, to use as custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def precision(y_true, y_pred) -&gt; float:\n'''Precision, to use as custom metrics\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\ny_pred = K.round(y_pred)\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nprecision = tp / (tp + fp + K.epsilon())\nprecision = tf.where(tf.math.is_nan(precision), tf.zeros_like(precision), precision)\nweighted_precision = precision * ground_positives / K.sum(ground_positives)\nweighted_precision = K.sum(weighted_precision)\nreturn weighted_precision\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.recall","title":"<code>recall(y_true, y_pred)</code>","text":"<p>Recall to use as a custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def recall(y_true, y_pred) -&gt; float:\n'''Recall to use as a custom metrics\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\ny_pred = K.round(y_pred)\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\nrecall = tp / (tp + fn + K.epsilon())\nrecall = tf.where(tf.math.is_nan(recall), tf.zeros_like(recall), recall)\nweighted_recall = recall * ground_positives / K.sum(ground_positives)\nweighted_recall = K.sum(weighted_recall)\nreturn weighted_recall\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.root_mean_squared_error","title":"<code>root_mean_squared_error(y_true, y_pred)</code>","text":"<p>RMSE, to use as custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def root_mean_squared_error(y_true, y_pred) -&gt; float:\n'''RMSE, to use as custom metrics\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\nreturn K.sqrt(K.mean(K.square(y_pred - y_true)))\n</code></pre>"},{"location":"reference/template_num/models_training/utils_deep_keras/#template_num.models_training.utils_deep_keras.weighted_binary_crossentropy","title":"<code>weighted_binary_crossentropy(pos_weight, target, output)</code>","text":"<p>Weighted binary crossentropy between an output tensor and a target tensor. pos_weight is used as a multiplier for the positive targets.</p> <p>Combination of the following functions: * keras.losses.binary_crossentropy * keras.backend.tensorflow_backend.binary_crossentropy * tf.nn.weighted_cross_entropy_with_logits</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>poid classe positive, to be tuned</p> required <code>target</code> <p>Target tensor</p> required <code>output</code> <p>Output tensor</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_num/models_training/utils_deep_keras.py</code> <pre><code>def weighted_binary_crossentropy(pos_weight: float, target, output) -&gt; float:\n'''\n    Weighted binary crossentropy between an output tensor\n    and a target tensor. pos_weight is used as a multiplier\n    for the positive targets.\n    Combination of the following functions:\n    * keras.losses.binary_crossentropy\n    * keras.backend.tensorflow_backend.binary_crossentropy\n    * tf.nn.weighted_cross_entropy_with_logits\n    Args:\n        pos_weight (float): poid classe positive, to be tuned\n        target: Target tensor\n        output: Output tensor\n    Returns:\n        float: metric\n    '''\ntarget = K.cast(target, 'float32')\noutput = K.cast(output, 'float32')\n# transform back to logits\n_epsilon = tf.convert_to_tensor(K.epsilon(), output.dtype.base_dtype)\noutput = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\noutput = tf.math.log(output / (1 - output))\n# compute weighted loss\nloss = tf.nn.weighted_cross_entropy_with_logits(target, output, pos_weight=pos_weight)\nreturn tf.reduce_mean(loss, axis=-1)\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/","title":"Utils models","text":""},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.apply_pipeline","title":"<code>apply_pipeline(df, preprocess_pipeline)</code>","text":"<p>Applies a fitted pipeline to a dataframe</p> Problem <p>The pipeline expects as input the same columns and in the same order even if some columns are then dropped (and so useless)</p> <p>Solution (experimental 14/04/2021):     We add the \"useless\" columns as columns filled with NaNs</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe to preprocess</p> required <code>preprocess_pipeline</code> <code>ColumnTransformer</code> <p>Pipeline to use</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If some mandatory columns are missing</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: Preprocessed dataFrame</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def apply_pipeline(df: pd.DataFrame, preprocess_pipeline: ColumnTransformer) -&gt; pd.DataFrame:\n'''Applies a fitted pipeline to a dataframe\n    Problem :\n        The pipeline expects as input the same columns and in the same order\n        even if some columns are then dropped (and so useless)\n    Solution (experimental 14/04/2021):\n        We add the \"useless\" columns as columns filled with NaNs\n    Args:\n        df (pd.DataFrame): Dataframe to preprocess\n        preprocess_pipeline (ColumnTransformer): Pipeline to use\n    Raises:\n        ValueError: If some mandatory columns are missing\n    Returns:\n        pd.DataFrame: Preprocessed dataFrame\n    '''\ncolumns_in, mandatory_columns = get_columns_pipeline(preprocess_pipeline)\n# Removes the useless columns\ndf = df[[col for col in df.columns if col in columns_in]]\noptionals_columns = [col for col in columns_in if col not in mandatory_columns]\n# Checks if all the mandatory columns are present\nmissing_mandatory_columns = [col for col in mandatory_columns if col not in df.columns]\nif len(missing_mandatory_columns) &gt; 0:\nfor missing_col in missing_mandatory_columns:\nlogger.error(f\"Missing column in your dataset : {missing_col}\")\nraise ValueError(f\"There are some missing mandatory columns in order to preprocess the dataset : {missing_mandatory_columns}\")\n# We add the non mandatory columns if they are not already in df\n# Note : only relevant in the case remainder = \"drop\" (nominal case)\nmissing_optionals_columns = [col for col in optionals_columns if col not in df.columns]\nfor col in missing_optionals_columns:\nlogger.warning(f'The column {col} is missing in order to apply the preprocessing.')\nlogger.warning('Experimental : it should be useless -&gt; creation of an empty column')\ndf[col] = np.nan\n# Apply transform on reordered columns\npreprocessed_x = preprocess_pipeline.transform(df[columns_in])\n# Reconstruct dataframe &amp; return\npreprocessed_df = pd.DataFrame(preprocessed_x)\npreprocessed_df = preprocess.retrieve_columns_from_pipeline(preprocessed_df, preprocess_pipeline)\nreturn preprocessed_df\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.display_train_test_shape","title":"<code>display_train_test_shape(df_train, df_test, df_shape=None)</code>","text":"<p>Displays the size of a train/test split</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>pd.DataFrame</code> <p>Train dataset</p> required <code>df_test</code> <code>pd.DataFrame</code> <p>Test dataset</p> required Kwargs <p>df_shape (int): Size of the initial dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object df_shape is not positive</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def display_train_test_shape(df_train: pd.DataFrame, df_test: pd.DataFrame, df_shape: Union[int, None] = None) -&gt; None:\n'''Displays the size of a train/test split\n    Args:\n        df_train (pd.DataFrame): Train dataset\n        df_test (pd.DataFrame): Test dataset\n    Kwargs:\n        df_shape (int): Size of the initial dataset\n    Raises:\n        ValueError: If the object df_shape is not positive\n    '''\nif df_shape is not None and df_shape &lt; 1:\nraise ValueError(\"The object df_shape must be positive\")\n# Process\nif df_shape is None:\ndf_shape = df_train.shape[0] + df_test.shape[0]\nlogger.info(f\"There are {df_train.shape[0]} lines in the train dataset and {df_test.shape[0]} in the test dataset.\")\nlogger.info(f\"{round(100 * df_train.shape[0] / df_shape, 2)}% of data are in the train set\")\nlogger.info(f\"{round(100 * df_test.shape[0] / df_shape, 2)}% of data are in the test set\")\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.get_columns_pipeline","title":"<code>get_columns_pipeline(preprocess_pipeline)</code>","text":"<p>Retrieves a pipeline wanted columns, and mandatory ones</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_pipeline</code> <code>ColumnTransformer</code> <p>Preprocessing pipeline</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of columns in</p> <code>list</code> <code>list</code> <p>List of mandatory ones</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def get_columns_pipeline(preprocess_pipeline: ColumnTransformer) -&gt; Tuple[list, list]:\n'''Retrieves a pipeline wanted columns, and mandatory ones\n    Args:\n        preprocess_pipeline (ColumnTransformer): Preprocessing pipeline\n    Returns:\n        list: List of columns in\n        list: List of mandatory ones\n    '''\n# Checks if the pipeline is fitted\ncheck_is_fitted(preprocess_pipeline)\n# Gets the names of input columns\ncolumns_in = preprocess_pipeline.feature_names_in_.tolist()\n# Gets the names of the \"mandatory\" columns\nif preprocess_pipeline._remainder[1] == 'drop':\n# If drop, we get from _columns\nmandatory_columns = list(utils.flatten(preprocess_pipeline._columns))\nelse:\n# Otherwise, we need all the columns\nmandatory_columns = columns_in\n# Returns\nreturn columns_in, mandatory_columns\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.load_model","title":"<code>load_model(model_dir, is_path=False)</code>","text":"<p>Loads a model from a path</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19)</p> required Kwargs <p>is_path (bool): If folder path instead of name (permits to load model from elsewhere)</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>?: Model</p> <code>dict</code> <code>dict</code> <p>Model configurations</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def load_model(model_dir: str, is_path: bool = False) -&gt; Tuple[Any, dict]:\n'''Loads a model from a path\n    Args:\n        model_dir (str): Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19)\n    Kwargs:\n        is_path (bool): If folder path instead of name (permits to load model from elsewhere)\n    Returns:\n        ?: Model\n        dict: Model configurations\n    '''\n# Find model path\nbase_folder = None if is_path else utils.get_models_path()\nmodel_path = utils.find_folder_path(model_dir, base_folder)\n# Get configs\nconfiguration_path = os.path.join(model_path, 'configurations.json')\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys() and configs['dict_classes'] is not None:\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n# Load model\npkl_path = os.path.join(model_path, f\"{configs['model_name']}.pkl\")\nwith open(pkl_path, 'rb') as f:\nmodel = pickle.load(f)\n# Change model_dir if diff\nif model_path != model.model_dir:\nmodel.model_dir = model_path\nconfigs['model_dir'] = model_path\n# Load specifics\nhdf5_path = os.path.join(model_path, 'best.hdf5')\n# TODO : we should probably have a single function `load_self` and let the model manage it's reload\n# Check for keras model\nif os.path.exists(hdf5_path):\nmodel.model = model.reload_model(hdf5_path)\n# Display if GPU is being used\nmodel.display_if_gpu_activated()\n# Return model &amp; configs\nreturn model, configs\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.load_pipeline","title":"<code>load_pipeline(pipeline_dir, is_path=False)</code>","text":"<p>Loads a pipeline from the pipelines folder</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_dir</code> <code>str</code> <p>Name of the folder containing the pipeline to get. If None, backups on \"no_preprocess\"</p> required Kwargs <p>is_path (bool): If path to the folder instead of the name (permits the loading from elsewhere)</p> <p>Returns:</p> Name Type Description <code>Pipeline</code> <code>Pipeline</code> <p>Reloaded pipeline</p> <code>str</code> <code>str</code> <p>Name of the preprocessing used</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def load_pipeline(pipeline_dir: Union[str, None], is_path: bool = False) -&gt; Tuple[Pipeline, str]:\n'''Loads a pipeline from the pipelines folder\n    Args:\n        pipeline_dir (str): Name of the folder containing the pipeline to get. If None,\n            backups on \"no_preprocess\"\n    Kwargs:\n        is_path (bool): If path to the folder instead of the name (permits the loading from elsewhere)\n    Returns:\n        Pipeline: Reloaded pipeline\n        str: Name of the preprocessing used\n    '''\n# If pipeline_dir is None, backups on \"no_preprocess\"\nif pipeline_dir is None:\nlogger.warning(\"The folder of the pipeline is None. Backups on 'no_preprocess'\")\npreprocess_str = \"no_preprocess\"\npreprocess_pipeline = preprocess.get_pipeline(preprocess_str)  # Warning, must be fitted\nreturn preprocess_pipeline, preprocess_str\n# Otherwise, nominal case\n# Find pipeline path\nbase_folder = None if is_path else utils.get_pipelines_path()\npipeline_path = utils.find_folder_path(pipeline_dir, base_folder)\n# Get pipeline\npipeline_path = os.path.join(pipeline_path, 'pipeline.pkl')\nwith open(pipeline_path, 'rb') as f:\npipeline_dict = pickle.load(f)\n# Return\nreturn pipeline_dict['preprocess_pipeline'], pipeline_dict['preprocess_str']\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.normal_split","title":"<code>normal_split(df, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe containing the data</p> required Kwargs <p>test_size (float): Proportion representing the size of the expected test set seed (int): random seed</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object test_size is not between 0 and 1</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>Train dataframe</p> <code>DataFrame</code> <code>pd.DataFrame</code> <p>Test dataframe</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def normal_split(df: pd.DataFrame, test_size: float = 0.25, seed: Union[int, None] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n'''Splits a DataFrame into train and test sets\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\nif not 0 &lt;= test_size &lt;= 1:\nraise ValueError('The object test_size must be between 0 and 1')\n# Normal split\nlogger.info(\"Normal split\")\ndf_train, df_test = train_test_split(df, test_size=test_size, random_state=seed)\n# Display\ndisplay_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n# Return\nreturn df_train, df_test\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.predict","title":"<code>predict(content, model, **kwargs)</code>","text":"<p>Gets predictions of a model on a dataset</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>pd.DataFrame</code> <p>New dataset to be predicted</p> required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <p>Returns:</p> Name Type Description <code>REGRESSION</code> <code>list</code> <p>float: prediction</p> <code>list</code> <p>MONO-LABEL CLASSIFICATION: str: prediction</p> <code>list</code> <p>MULTI-LABELS CLASSIFICATION: tuple: predictions</p> <code>list</code> <p>If several elements -&gt; list</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def predict(content: pd.DataFrame, model, **kwargs) -&gt; list:\n'''Gets predictions of a model on a dataset\n    Args:\n        content (pd.DataFrame): New dataset to be predicted\n        model (ModelClass): Model to use\n    Returns:\n        REGRESSION :\n            float: prediction\n        MONO-LABEL CLASSIFICATION:\n            str: prediction\n        MULTI-LABELS CLASSIFICATION:\n            tuple: predictions\n        If several elements -&gt; list\n    '''\n# Apply preprocessing\nif model.preprocess_pipeline is not None:\ndf_prep = apply_pipeline(content, model.preprocess_pipeline)\nelse:\ndf_prep = content.copy()\nlogger.warning(\"No preprocessing pipeline found - we consider no preprocessing, but it should not be so !\")\n# Get predictions\npredictions = model.predict(df_prep)\n# Inverse transform (needed for classification)\npredictions = model.inverse_transform(predictions)\n# Return\nreturn predictions\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.predict_with_proba","title":"<code>predict_with_proba(content, model)</code>","text":"<p>Gets probabilities predictions of a model on a dataset</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>pd.DataFrame</code> <p>New dataset to be predicted</p> required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model type is not classifier</p> <p>Returns:</p> Type Description <code>Union[Tuple[List[str], List[float]], Tuple[List[tuple], List[tuple]]]</code> <p>MONO-LABEL CLASSIFICATION: List[str]: predictions List[float]: probabilities</p> <code>Union[Tuple[List[str], List[float]], Tuple[List[tuple], List[tuple]]]</code> <p>MULTI-LABELS CLASSIFICATION: List[tuple]: predictions List[tuple]: probabilities</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def predict_with_proba(content: pd.DataFrame, model) -&gt; Union[Tuple[List[str], List[float]], Tuple[List[tuple], List[tuple]]]:\n'''Gets probabilities predictions of a model on a dataset\n    Args:\n        content (pd.DataFrame): New dataset to be predicted\n        model (ModelClass): Model to use\n    Raises:\n        ValueError: If the model type is not classifier\n    Returns:\n        MONO-LABEL CLASSIFICATION:\n            List[str]: predictions\n            List[float]: probabilities\n        MULTI-LABELS CLASSIFICATION:\n            List[tuple]: predictions\n            List[tuple]: probabilities\n    '''\n# Regressions\nif not model.model_type == 'classifier':\nraise ValueError(f\"The model type ({model.model_type}) is not supported by the method predict_with_proba\")\n# Apply preprocessing\nif model.preprocess_pipeline is not None:\ndf_prep = apply_pipeline(content, model.preprocess_pipeline)\nelse:\ndf_prep = content.copy()\nlogger.warning(\"No preprocessing pipeline found - we consider no preprocessing, but it should not be so !\")\n# Get predictions\npredictions, probas = model.predict_with_proba(df_prep)\n# Rework format\nif not model.multi_label:\nprediction = model.inverse_transform(predictions)\nproba = list(probas.max(axis=1))\nelse:\nprediction = [tuple(np.array(model.list_classes).compress(indicators)) for indicators in predictions]\nproba = [tuple(np.array(probas[i]).compress(indicators)) for i, indicators in enumerate(predictions)]\n# Return prediction &amp; proba\nreturn prediction, proba\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.preprocess_model_multilabel","title":"<code>preprocess_model_multilabel(df, y_col, classes=None)</code>","text":"<p>Prepares a dataframe for a multi-labels classification</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Training dataset This dataset must be preprocessed. Example:     # Group by &amp; apply tuple to y_col     x_cols = [col for col in list(df.columns) if col != y_col]     df = pd.DataFrame(df.groupby(x_cols)[y_col].apply(tuple))</p> required <code>y_col</code> <code>str or int</code> <p>Name of the column to be used for training - y</p> required Kwargs <p>classes (list): List of classes to consider</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>Dataframe for training</p> <code>list</code> <code>list</code> <p>List of 'y' columns</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def preprocess_model_multilabel(df: pd.DataFrame, y_col: Union[str, int], classes: Union[list, None] = None) -&gt; Tuple[pd.DataFrame, list]:\n'''Prepares a dataframe for a multi-labels classification\n    Args:\n        df (pd.DataFrame): Training dataset\n            This dataset must be preprocessed.\n            Example:\n                # Group by &amp; apply tuple to y_col\n                x_cols = [col for col in list(df.columns) if col != y_col]\n                df = pd.DataFrame(df.groupby(x_cols)[y_col].apply(tuple))\n        y_col (str or int): Name of the column to be used for training - y\n    Kwargs:\n        classes (list): List of classes to consider\n    Returns:\n        DataFrame: Dataframe for training\n        list: List of 'y' columns\n    '''\n# TODO: add possibility to have sparse output\nlogger.info(\"Preprocess dataframe for multi-labels model\")\n# Process\nlogger.info(\"Preparing dataset for multi-labels format. Might take several minutes.\")\n# /!\\ The reset_index is compulsory in order to have the same indexes between df, and MLB transformed values\ndf = df.reset_index(drop=True)\n# Apply MLB\nmlb = MultiLabelBinarizer(classes=classes)\ndf = df.assign(**pd.DataFrame(mlb.fit_transform(df[y_col]), columns=mlb.classes_))\n# Return dataframe &amp; y_cols (i.e. classes)\nreturn df, list(mlb.classes_)\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.remove_small_classes","title":"<code>remove_small_classes(df, col, min_rows=2)</code>","text":"<p>Deletes the classes with small numbers of elements</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str | int</code> <p>Columns containing the classes</p> required Kwargs <p>min_rows (int): Minimal number of lines in the training set (default: 2)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object min_rows is not positive</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: New dataset</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def remove_small_classes(df: pd.DataFrame, col: Union[str, int], min_rows: int = 2) -&gt; pd.DataFrame:\n'''Deletes the classes with small numbers of elements\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str | int): Columns containing the classes\n    Kwargs:\n        min_rows (int): Minimal number of lines in the training set (default: 2)\n    Raises:\n        ValueError: If the object min_rows is not positive\n    Returns:\n        pd.DataFrame: New dataset\n    '''\nif min_rows &lt; 1:\nraise ValueError(\"The object min_rows must be positive\")\n# Looking for classes with less than min_rows lines\nv_count = df[col].value_counts()\nclasses_to_remove = list(v_count[v_count &lt; min_rows].index.values)\nfor cl in classes_to_remove:\nlogger.warning(f\"/!\\\\ /!\\\\ /!\\\\ Class {cl} has less than {min_rows} lines in the training set.\")\nlogger.warning(\"/!\\\\ /!\\\\ /!\\\\ This class is automatically removed from the dataset.\")\nreturn df[~df[col].isin(classes_to_remove)]\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.search_hp_cv","title":"<code>search_hp_cv(model_cls, model_params, hp_params, scoring_fn, kwargs_fit, n_splits=5)</code>","text":"<p>Searches for hyperparameters - works only with classifiers !</p> <p>Parameters:</p> Name Type Description Default <code>model_cls</code> <code>?</code> <p>Class of models on which to do a hyperparameters search</p> required <code>model_params</code> <code>dict</code> <p>Set of \"fixed\" parameters of the model (e.g. x_col, y_col). Must contain 'multi_label'.</p> required <code>hp_params</code> <code>dict</code> <p>Set of \"variable\" parameters on which to do a hyperparameters search</p> required <code>scoring_fn</code> <code>str or func</code> <p>Scoring function to maximize This function must take as input a dictionary containing metrics e.g. {'F1-Score': 0.85, 'Accuracy': 0.57, 'Precision': 0.64, 'Recall': 0.90}</p> required <code>kwargs_fit</code> <code>dict</code> <p>Set of kwargs to input in the fit function Must contain 'x_train' and 'y_train'</p> required Kwargs <p>n_splits (int): Number of folds to use</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If scoring_fn is not a known string</p> <code>ValueError</code> <p>If multi_label is not a key in model_params</p> <code>ValueError</code> <p>If x_train is not a key in kwargs_fit</p> <code>ValueError</code> <p>If y_train is not a key in kwargs_fit</p> <code>ValueError</code> <p>If model_params and hp_params share some keys</p> <code>ValueError</code> <p>If hp_params values are not the same length</p> <code>ValueError</code> <p>If the number of crossvalidation split is less or equal to 1</p> <p>Returns:</p> Name Type Description <code>ModelClass</code> <p>best model to be \"fitted\" on the dataset</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def search_hp_cv(model_cls, model_params: dict, hp_params: dict, scoring_fn: Union[str, Callable],\nkwargs_fit: dict, n_splits: int = 5):\n'''Searches for hyperparameters - works only with classifiers !\n    Args:\n        model_cls (?): Class of models on which to do a hyperparameters search\n        model_params (dict): Set of \"fixed\" parameters of the model (e.g. x_col, y_col).\n            Must contain 'multi_label'.\n        hp_params (dict): Set of \"variable\" parameters on which to do a hyperparameters search\n        scoring_fn (str or func): Scoring function to maximize\n            This function must take as input a dictionary containing metrics\n            e.g. {'F1-Score': 0.85, 'Accuracy': 0.57, 'Precision': 0.64, 'Recall': 0.90}\n        kwargs_fit (dict): Set of kwargs to input in the fit function\n            Must contain 'x_train' and 'y_train'\n    Kwargs:\n        n_splits (int): Number of folds to use\n    Raises:\n        ValueError: If scoring_fn is not a known string\n        ValueError: If multi_label is not a key in model_params\n        ValueError: If x_train is not a key in kwargs_fit\n        ValueError: If y_train is not a key in kwargs_fit\n        ValueError: If model_params and hp_params share some keys\n        ValueError: If hp_params values are not the same length\n        ValueError: If the number of crossvalidation split is less or equal to 1\n    Returns:\n        ModelClass: best model to be \"fitted\" on the dataset\n    '''\nlist_known_scoring = ['accuracy', 'f1', 'precision', 'recall']\n# We can't really check if classifier ...\n#################\n# Manage errors\n#################\nif isinstance(scoring_fn, str) and scoring_fn not in list_known_scoring:\nraise ValueError(f\"The input {scoring_fn} is not a known value for scoring_fn (known values : {list_known_scoring})\")\nif 'multi_label' not in model_params.keys():\nraise ValueError(\"The key 'multi_label' must be present in the dictionary model_params\")\nif 'x_train' not in kwargs_fit.keys():\nraise ValueError(\"The key 'x_train' must be present in the dictionary kwargs_fit\")\nif 'y_train' not in kwargs_fit.keys():\nraise ValueError(\"The key 'y_train' must be present in the dictionary kwargs_fit\")\nif any([k in hp_params.keys() for k in model_params.keys()]):\n# A key can't be \"fixed\" and \"variable\"\nraise ValueError(\"The dictionaries model_params and hp_params share at least one key\")\nif len(set([len(_) for _ in hp_params.values()])) != 1:\nraise ValueError(\"The values of hp_params must have the same length\")\nif n_splits &lt;= 1:\nraise ValueError(f\"The number of crossvalidation splits ({n_splits}) must be more than 1\")\n#################\n# Manage scoring\n#################\n# Get scoring functions\nif scoring_fn == 'accuracy':\nscoring_fn = lambda x: x['Accuracy']\nelif scoring_fn == 'f1':\nscoring_fn = lambda x: x['F1-Score']\nelif scoring_fn == 'precision':\nscoring_fn = lambda x: x['Precision']\nelif scoring_fn == 'recall':\nscoring_fn = lambda x: x['Recall']\n#################\n# Manage x_train &amp; y_train format\n#################\nif not isinstance(kwargs_fit['x_train'], (pd.DataFrame, pd.Series)):\nkwargs_fit['x_train'] = pd.Series(kwargs_fit['x_train'].copy())\nif not isinstance(kwargs_fit['y_train'], (pd.DataFrame, pd.Series)):\nkwargs_fit['y_train'] = pd.Series(kwargs_fit['y_train'].copy())\n#################\n# Process\n#################\n# Loop on hyperparameters\nnb_search = len(list(hp_params.values())[0])\nlogger.info(\"Beginning of hyperparameters search\")\nlogger.info(f\"Number of model fits : {nb_search} (search number) x {n_splits} (CV splits number) = {nb_search * n_splits}\")\n# DataFrame for stocking metrics :\nmetrics_df = pd.DataFrame(columns=['index_params', 'index_fold', 'Score', 'Accuracy', 'F1-Score', 'Precision', 'Recall'])\nfor i in range(nb_search):\n# Display informations\nlogger.info(f\"Search n\u00b0{i + 1}\")\ntmp_hp_params = {k: v[i] for k, v in hp_params.items()}\nlogger.info(\"Tested hyperparameters : \")\nlogger.info(pprint.pformat(tmp_hp_params))\n# Get folds (shuffle recommended since the classes could be ordered)\nif model_params['multi_label']:\nk_fold = KFold(n_splits=n_splits, shuffle=True)  # Can't stratify on multi-labels\nelse:\nk_fold = StratifiedKFold(n_splits=n_splits, shuffle=True)\n# Process each fold\nfor j, (train_index, valid_index) in enumerate(k_fold.split(kwargs_fit['x_train'], kwargs_fit['y_train'])):\nlogger.info(f\"Search n\u00b0{i + 1}/{nb_search} - fit n\u00b0{j + 1}/{n_splits}\")\n# get tmp x, y\nx_train, x_valid = kwargs_fit['x_train'].iloc[train_index], kwargs_fit['x_train'].iloc[valid_index]\ny_train, y_valid = kwargs_fit['y_train'].iloc[train_index], kwargs_fit['y_train'].iloc[valid_index]\n# Get tmp model\n# Manage model_dir\ntmp_model_dir = os.path.join(utils.get_models_path(), datetime.now().strftime(\"tmp_%Y_%m_%d-%H_%M_%S\"))\n# The next line prioritize the last dictionary\n# We force a temporary folder and a low save level (we only want the metrics)\nmodel_tmp = model_cls(**{**model_params, **tmp_hp_params, **{'model_dir': tmp_model_dir, 'level_save': 'LOW'}})\n# Setting log level to ERROR\nmodel_tmp.logger.setLevel(logging.ERROR)\n# Let's fit ! (priority to the last dictionary)\nmodel_tmp.fit(**{**kwargs_fit, **{'x_train': x_train, 'y_train': y_train, 'x_valid': x_valid, 'y_valid': y_valid}})\n# Let's predict !\ny_pred = model_tmp.predict(x_valid)\n# Get metrics !\nmetrics_func = model_tmp.get_metrics_simple_multilabel if model_tmp.multi_label else model_tmp.get_metrics_simple_monolabel\nmetrics_tmp = metrics_func(y_valid, y_pred)\nmetrics_tmp = metrics_tmp[metrics_tmp.Label == \"All\"].copy()  # Add .copy() to avoid pandas settingwithcopy\nmetrics_tmp[\"Score\"] = scoring_fn(metrics_tmp.iloc[0].to_dict())  # type: ignore\nmetrics_tmp[\"index_params\"] = i\nmetrics_tmp[\"index_fold\"] = j\nmetrics_tmp = metrics_tmp[metrics_df.columns]  # Keeping only the necessary columns\nmetrics_df = pd.concat([metrics_df, metrics_tmp], ignore_index=True)\n# Delete the temporary model : the final model must be refitted on the whole dataset\ndel model_tmp\ngc.collect()\nshutil.rmtree(tmp_model_dir)\n# Display score\nlogger.info(f\"Score for search n\u00b0{i + 1}: {metrics_df[metrics_df['index_params'] == i]['Score'].mean()}\")\n# Metric agregation for all the folds\nmetrics_df = metrics_df.join(metrics_df[['index_params', 'Score']].groupby('index_params').mean().rename({'Score': 'mean_score'}, axis=1), on='index_params', how='left')\n# Select the set of parameters with the best mean score (on the folds)\nbest_index = metrics_df[metrics_df.mean_score == metrics_df.mean_score.max()][\"index_params\"].values[0]\nbest_params = {k: v[best_index] for k, v in hp_params.items()}\nlogger.info(f\"Best results for the set of parameter n\u00b0{best_index + 1}: {pprint.pformat(best_params)}\")\n# Instanciation of a model with the best parameters\nbest_model = model_cls(**{**model_params, **best_params})\n# Save the metrics report of the hyperparameters search and the tested parameters\ncsv_path = os.path.join(best_model.model_dir, \"hyper_params_results.csv\")\nmetrics_df.to_csv(csv_path, sep=';', index=False, encoding='utf-8')\njson_data = {\n'model_params': model_params,\n'scoring_fn': pickle.source.getsourcelines(scoring_fn)[0],\n'n_splits': n_splits,\n'hp_params_set': {i: {k: v[i] for k, v in hp_params.items()} for i in range(nb_search)},\n}\njson_path = os.path.join(best_model.model_dir, \"hyper_params_tested.json\")\nwith open(json_path, 'w', encoding='utf-8') as f:\njson.dump(json_data, f, indent=4, cls=utils.NpEncoder)\n# TODO: We are forced to reset the logging level which is linked to the class\nbest_model.logger.setLevel(logging.getLogger('template_num').getEffectiveLevel())\n# Return model to be fitted\nreturn best_model\n</code></pre>"},{"location":"reference/template_num/models_training/utils_models/#template_num.models_training.utils_models.stratified_split","title":"<code>stratified_split(df, col, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets - Stratified strategy</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str or int</code> <p>column on which to do the stratified split</p> required Kwargs <p>test_size (float): Proportion representing the size of the expected test set seed (int): Random seed</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object test_size is not between 0 and 1</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>Train dataframe</p> <code>DataFrame</code> <code>pd.DataFrame</code> <p>Test dataframe</p> Source code in <code>template_num/models_training/utils_models.py</code> <pre><code>def stratified_split(df: pd.DataFrame, col: Union[str, int], test_size: float = 0.25, seed: int = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n'''Splits a DataFrame into train and test sets - Stratified strategy\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str or int): column on which to do the stratified split\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): Random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\nif not 0 &lt;= test_size &lt;= 1:\nraise ValueError('The object test_size must be between 0 and 1')\n# Stratified split\nlogger.info(\"Stratified split\")\ndf = remove_small_classes(df, col, min_rows=math.ceil(1 / test_size))  # minimum lines number per category to split\ndf_train, df_test = train_test_split(df, stratify=df[col], test_size=test_size, random_state=seed)\n# Display\ndisplay_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n# Return\nreturn df_train, df_test\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/","title":"Classifiers","text":""},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/","title":"Model aggregation classifier","text":""},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier","title":"<code>ModelAggregationClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelClass</code></p> <p>Model for aggregating several classifier models</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>class ModelAggregationClassifier(ModelClassifierMixin, ModelClass):\n'''Model for aggregating several classifier models'''\n_default_name = 'model_aggregation_classifier'\n_dict_aggregation_function = {'majority_vote': {'aggregation_function': majority_vote, 'using_proba': False, 'multi_label': False},\n'proba_argmax': {'aggregation_function': proba_argmax, 'using_proba': True, 'multi_label': False},\n'all_predictions': {'aggregation_function': all_predictions, 'using_proba': False, 'multi_label': True},\n'vote_labels': {'aggregation_function': vote_labels, 'using_proba': False, 'multi_label': True}}\ndef __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'majority_vote',\nusing_proba: bool = False, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n        This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg)\n        from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes.\n        However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.\n        Kwargs:\n            list_models (list) : The list of models to be aggregated (can be None if reloading from standalones)\n            aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg)\n            using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).\n        Raises:\n            ValueError: All the aggregated sub_models have not the same multi_label attributes\n            ValueError: The multi_label attributes of the aggregated models are inconsistent with multi_label\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Set attributes\nself.using_proba = using_proba\nself.aggregation_function = aggregation_function\n# Manage submodels\nself.sub_models = list_models  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n# Check if only classifiers are present\nif False in [isinstance(sub_model['model'], ModelClassifierMixin) for sub_model in self.sub_models]:\nraise ValueError(f\"model_aggregation_classifier only accepts classifier models\")\n# Check for multi-labels inconsistencies\nset_multi_label = {sub_model['model'].multi_label for sub_model in self.sub_models}\nif len(set_multi_label) &gt; 1:\nraise ValueError(f\"All the aggregated sub_models have not the same multi_label attribute\")\nif len(set_multi_label.union({self.multi_label})) &gt; 1:\nraise ValueError(f\"The multi_label attributes of the aggregated models are inconsistent with self.multi_label = {self.multi_label}.\")\n# Set trained &amp; classes info from submodels\nself.trained, self.list_classes, self.dict_classes = self._check_trained()\n# Set nb_fit to 1 if already trained\nif self.trained:\nself.nb_fit = 1\n@property\ndef aggregation_function(self):\n'''Getter for aggregation_function'''\nreturn self._aggregation_function\n@aggregation_function.setter\ndef aggregation_function(self, agg_function: Union[Callable, str]):\n'''Setter for aggregation_function\n        If a string, try to match a predefined function\n        Raises:\n            ValueError: If the object aggregation_function is a str but not found in the dictionary of predefined aggregation functions\n            ValueError: If the object aggregation_function is incompatible with multi_label\n        '''\n# Retrieve aggregation function from dict if a string\nif isinstance(agg_function, str):\n# Get infos\nif agg_function not in self._dict_aggregation_function.keys():\nraise ValueError(f\"The aggregation_function ({agg_function}) is not a valid option (must be chosen in {self._dict_aggregation_function.keys()})\")\nusing_proba = self._dict_aggregation_function[agg_function]['using_proba']\nmulti_label = self._dict_aggregation_function[agg_function]['multi_label']\nagg_function = self._dict_aggregation_function[agg_function]['aggregation_function']  # type: ignore\n# Apply checks\nif self.using_proba != using_proba:\nself.logger.warning(f\"using_proba {self.using_proba} is incompatible with the selected aggregation function '{agg_function}'. We force using_proba to {using_proba}.\")\nself.using_proba = using_proba  # type: ignore\nif self.multi_label != multi_label:\nraise ValueError(f\"multi_label {self.multi_label} is incompatible with the selected aggregation function '{agg_function}'.\")\nself._aggregation_function = agg_function\n@aggregation_function.deleter\ndef aggregation_function(self):\n'''Deleter for aggregation_function'''\nself._aggregation_function = None\n@property\ndef sub_models(self):\n'''Getter for sub_models'''\nreturn self._sub_models\n@sub_models.setter\ndef sub_models(self, list_models: Union[list, None] = None):\n'''Setter for sub_models\n        Kwargs:\n            list_models (list) : The list of models to be aggregated\n        '''\nlist_models = [] if list_models is None else list_models\nsub_models = []  # Init list of models\nfor model in list_models:\n# If a string (a model name), reload it\nif isinstance(model, str):\nreal_model, _ = utils_models.load_model(model)\ndict_model = {'name': model, 'model': real_model}\nelse:\ndict_model = {'name': os.path.split(model.model_dir)[-1], 'model': model}\nsub_models.append(dict_model.copy())\nself._sub_models = sub_models.copy()\n@sub_models.deleter\ndef sub_models(self):\n'''Deleter for sub_models'''\nself._sub_models = None\ndef _check_trained(self) -&gt; Tuple[bool, list, dict]:\n'''Checks and sets various attributes related to the fitting of underlying models\n        Returns:\n            bool: is the aggregation model is considered fitted\n            list: list of classes\n            dict: dict of classes\n        '''\n# Check fitted\nmodels_trained = {sub_model['model'].trained for sub_model in self.sub_models}\nif len(models_trained) &gt; 0 and all(models_trained):\n# All models trained\ntrained = True\n# Set list_classes\nlist_classes = list({label for sub_model in self.sub_models for label in sub_model['model'].list_classes})\nlist_classes.sort()\n# Set dict_classes based on self.list_classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# No model or not fitted\nelse:\ntrained, list_classes, dict_classes = False, [], {}\nreturn trained, list_classes, dict_classes\ndef fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Fits the model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models\n            y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models\n            with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models\n        '''\n# Fit each model\nfor sub_model in self.sub_models:\nmodel = sub_model['model']\nif not model.trained:\nmodel.fit(x_train, y_train, x_valid=x_valid, y_valid=y_valid, with_shuffle=True, **kwargs)\n# Set nb_fit to 1 if not already trained\nif not self.trained:\nself.nb_fit = 1\n# Update attributes\nself.trained, self.list_classes, self.dict_classes = self._check_trained()\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Prediction\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n            return_proba (bool): If the function should return the probabilities instead of the classes\n        Returns:\n            np.ndarray: array of shape = [n_samples]\n        '''\n# We decide whether to rely on each model's probas or their predictions\nif return_proba:\nreturn self.predict_proba(x_test)\nelse:\n# Get what we want (probas or preds) and use the aggregation function\nif self.using_proba:\npreds_or_probas = self._predict_probas_sub_models(x_test, **kwargs)\nelse:\npreds_or_probas = self._predict_sub_models(x_test, **kwargs)\nreturn np.array([self.aggregation_function(array, list_classes=self.list_classes) for array in preds_or_probas])  # type: ignore\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Returns:\n            np.ndarray: array of shape = [n_samples, n_classes]\n        '''\nprobas_sub_models = self._predict_probas_sub_models(x_test, **kwargs)\n# The probas of all models are averaged\nreturn np.sum(probas_sub_models, axis=1) / probas_sub_models.shape[1]\n@utils.trained_needed\ndef _predict_sub_models(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Recover the predictions of each model being aggregated\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Returns:\n            np.ndarray: not multi_label : array of shape = [n_samples, nb_model]\n                        multi_label : array of shape = [n_samples, nb_model, n_classes]\n        '''\nif self.multi_label:\narray_predict = np.array([self._predict_full_list_classes(sub_model['model'], x_test, return_proba=False) for sub_model in self.sub_models])\narray_predict = np.transpose(array_predict, (1, 0, 2))\nelse:\narray_predict = np.array([sub_model['model'].predict(x_test) for sub_model in self.sub_models])\narray_predict = np.transpose(array_predict, (1, 0))\nreturn array_predict\n@utils.trained_needed\ndef _predict_probas_sub_models(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Recover the probabilities of each model being aggregated\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Returns:\n            np.ndarray: array of shape = [n_samples, nb_model, nb_classes]\n        '''\narray_probas = np.array([self._predict_full_list_classes(sub_model['model'], x_test, return_proba=True) for sub_model in self.sub_models])\narray_probas = np.transpose(array_probas, (1, 0, 2))\nreturn array_probas\ndef _predict_full_list_classes(self, model: Type[ModelClass], x_test, return_proba: bool = False) -&gt; np.ndarray:\n'''For multi_label: adds missing columns in the prediction of model (class missing in their list_classes)\n        Or, if return_proba, adds a proba of zero to the missing classes in their list_classes\n        Args:\n            model (ModelClass): Model to use\n            x_test (?): Array-like or sparse matrix of shape = [n_samples, n_features]\n            return_proba (bool): If the function should return the probabilities instead of the classes\n        Returns:\n            np.ndarray: The array with the missing columns added\n        '''\n# Get predictions or probas\npreds_or_probas = model.predict(x_test, return_proba=return_proba) # type: ignore\n# Manage each cases. Reorder predictions or probas according to aggregation model list_classes\n# Multi label, proba = True\n# Multi label, proba = False\n# Mono label, proba = True\nif model.multi_label or return_proba:\ndf_all = pd.DataFrame(np.zeros((len(preds_or_probas), len(self.list_classes))), columns=self.list_classes)  # type: ignore\ndf_model = pd.DataFrame(preds_or_probas, columns=model.list_classes)\nfor col in model.list_classes:\ndf_all[col] = df_model[col]\nreturn df_all.to_numpy()\n# Mono label, proba = False\nelse:\nreturn preds_or_probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\nif json_data is None:\njson_data = {}\n# Specific aggregation - save some wanted entries\ntrain_keys = ['filename', 'filename_valid', 'preprocess_str']\ndefault_json_data = {key: json_data.get(key, None) for key in train_keys}\ndefault_json_data['aggregator_dir'] = self.model_dir\n# Save each trained and unsaved model\nfor sub_model in self.sub_models:\npath_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\nif os.path.exists(path_config):\nwith open(path_config, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\ntrained = configs.get('trained', False)\nif not trained:\nsub_model['model'].save(default_json_data)\nelse:\nsub_model['model'].save(default_json_data)\n# Add some specific information\njson_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\njson_data['using_proba'] = self.using_proba\n# Save aggregation_function if not None &amp; level_save &gt; LOW\nif (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\naggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n# Save as pickle\nwith open(aggregation_function_path, 'wb') as f:\npickle.dump(self.aggregation_function, f)\n# Save\nmodels_list = [sub_model['name'] for sub_model in self.sub_models]\naggregation_function = self.aggregation_function\ndelattr(self, \"sub_models\")\ndelattr(self, \"aggregation_function\")\nsuper().save(json_data=json_data)\nsetattr(self, \"aggregation_function\", aggregation_function)\nsetattr(self, \"sub_models\", models_list)  # Setter needs list of models, not sub_models itself\n# Add message in model_upload_instructions.md\nmd_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\nline = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it .  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\nself.prepend_line(md_path, line)\ndef prepend_line(self, file_name: str, line: str) -&gt; None:\n''' Insert given string as a new line at the beginning of a file\n        Kwargs:\n            file_name (str): Path to file\n            line (str): line to insert\n        '''\nwith open(file_name, 'r+') as f:\nlines = f.readlines()\nlines.insert(0, line)\nf.seek(0)\nf.writelines(lines)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model aggregation from its configuration and \"standalones\" files\n           Reloads the sub_models from their files\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n            aggregation_function_path (str): Path to aggregation_function_path\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If preprocess_pipeline_path is None\n            ValueError: If aggregation_function_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n            FileNotFoundError: If the object aggregation_function_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\naggregation_function_path = kwargs.get('aggregation_function_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif aggregation_function_path is None:\nraise ValueError(\"The argument aggregation_function_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\nif not os.path.exists(aggregation_function_path):\nraise FileNotFoundError(f\"The file {aggregation_function_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n# Reload aggregation_function_path\nwith open(aggregation_function_path, 'rb') as f:\nself.aggregation_function = pickle.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nself.sub_models = configs.get('list_models_name', [])  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\nfor attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'using_proba']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.aggregation_function","title":"<code>aggregation_function</code>  <code>deletable</code> <code>writable</code> <code>property</code>","text":"<p>Getter for aggregation_function</p>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.sub_models","title":"<code>sub_models</code>  <code>deletable</code> <code>writable</code> <code>property</code>","text":"<p>Getter for sub_models</p>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.__init__","title":"<code>__init__(list_models=None, aggregation_function='majority_vote', using_proba=False, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments) This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg) from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes. However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.</p> Kwargs <p>list_models (list) : The list of models to be aggregated (can be None if reloading from standalones) aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg) using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>All the aggregated sub_models have not the same multi_label attributes</p> <code>ValueError</code> <p>The multi_label attributes of the aggregated models are inconsistent with multi_label</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'majority_vote',\nusing_proba: bool = False, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n    This model will aggregate the predictions of several model. The user can choose an aggregation function (with **kwargs if not using a list_classes arg)\n    from existing ones, or create its own. All models must be either mono label or multi label, we do not accept mixes.\n    However, we accept models that do not have the same class / labels. We will consider a meta model with joined classes / labels.\n    Kwargs:\n        list_models (list) : The list of models to be aggregated (can be None if reloading from standalones)\n        aggregation_function (Callable or str) : The aggregation function used (custom function must use **kwargs if not using a list_classes arg)\n        using_proba (bool) : Which object is being aggregated (the probabilities or the predictions).\n    Raises:\n        ValueError: All the aggregated sub_models have not the same multi_label attributes\n        ValueError: The multi_label attributes of the aggregated models are inconsistent with multi_label\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Set attributes\nself.using_proba = using_proba\nself.aggregation_function = aggregation_function\n# Manage submodels\nself.sub_models = list_models  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n# Check if only classifiers are present\nif False in [isinstance(sub_model['model'], ModelClassifierMixin) for sub_model in self.sub_models]:\nraise ValueError(f\"model_aggregation_classifier only accepts classifier models\")\n# Check for multi-labels inconsistencies\nset_multi_label = {sub_model['model'].multi_label for sub_model in self.sub_models}\nif len(set_multi_label) &gt; 1:\nraise ValueError(f\"All the aggregated sub_models have not the same multi_label attribute\")\nif len(set_multi_label.union({self.multi_label})) &gt; 1:\nraise ValueError(f\"The multi_label attributes of the aggregated models are inconsistent with self.multi_label = {self.multi_label}.\")\n# Set trained &amp; classes info from submodels\nself.trained, self.list_classes, self.dict_classes = self._check_trained()\n# Set nb_fit to 1 if already trained\nif self.trained:\nself.nb_fit = 1\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required Kwargs <p>x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Fits the model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features] - not used by sklearn models\n        y_valid (?): Array-like, shape = [n_samples, n_targets] - not used by sklearn models\n        with_shuffle (bool): If x, y must be shuffled before fitting - not used by sklearn models\n    '''\n# Fit each model\nfor sub_model in self.sub_models:\nmodel = sub_model['model']\nif not model.trained:\nmodel.fit(x_train, y_train, x_valid=x_valid, y_valid=y_valid, with_shuffle=True, **kwargs)\n# Set nb_fit to 1 if not already trained\nif not self.trained:\nself.nb_fit = 1\n# Update attributes\nself.trained, self.list_classes, self.dict_classes = self._check_trained()\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Prediction</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <code>return_proba</code> <code>bool</code> <p>If the function should return the probabilities instead of the classes</p> <code>False</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: array of shape = [n_samples]</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Prediction\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        return_proba (bool): If the function should return the probabilities instead of the classes\n    Returns:\n        np.ndarray: array of shape = [n_samples]\n    '''\n# We decide whether to rely on each model's probas or their predictions\nif return_proba:\nreturn self.predict_proba(x_test)\nelse:\n# Get what we want (probas or preds) and use the aggregation function\nif self.using_proba:\npreds_or_probas = self._predict_probas_sub_models(x_test, **kwargs)\nelse:\npreds_or_probas = self._predict_sub_models(x_test, **kwargs)\nreturn np.array([self.aggregation_function(array, list_classes=self.list_classes) for array in preds_or_probas])  # type: ignore\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: array of shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n    Returns:\n        np.ndarray: array of shape = [n_samples, n_classes]\n    '''\nprobas_sub_models = self._predict_probas_sub_models(x_test, **kwargs)\n# The probas of all models are averaged\nreturn np.sum(probas_sub_models, axis=1) / probas_sub_models.shape[1]\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.prepend_line","title":"<code>prepend_line(file_name, line)</code>","text":"<p>Insert given string as a new line at the beginning of a file</p> Kwargs <p>file_name (str): Path to file line (str): line to insert</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def prepend_line(self, file_name: str, line: str) -&gt; None:\n''' Insert given string as a new line at the beginning of a file\n    Kwargs:\n        file_name (str): Path to file\n        line (str): line to insert\n    '''\nwith open(file_name, 'r+') as f:\nlines = f.readlines()\nlines.insert(0, line)\nf.seek(0)\nf.writelines(lines)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model aggregation from its configuration and \"standalones\" files    Reloads the sub_models from their files</p> Kwargs <p>configuration_path (str): Path to configuration file preprocess_pipeline_path (str): Path to preprocess pipeline aggregation_function_path (str): Path to aggregation_function_path</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>ValueError</code> <p>If aggregation_function_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object aggregation_function_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model aggregation from its configuration and \"standalones\" files\n       Reloads the sub_models from their files\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n        aggregation_function_path (str): Path to aggregation_function_path\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If preprocess_pipeline_path is None\n        ValueError: If aggregation_function_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        FileNotFoundError: If the object aggregation_function_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\naggregation_function_path = kwargs.get('aggregation_function_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif aggregation_function_path is None:\nraise ValueError(\"The argument aggregation_function_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\nif not os.path.exists(aggregation_function_path):\nraise FileNotFoundError(f\"The file {aggregation_function_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n# Reload aggregation_function_path\nwith open(aggregation_function_path, 'rb') as f:\nself.aggregation_function = pickle.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nself.sub_models = configs.get('list_models_name', [])  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\nfor attribute in ['x_col', 'y_col', 'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'using_proba']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.ModelAggregationClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\nif json_data is None:\njson_data = {}\n# Specific aggregation - save some wanted entries\ntrain_keys = ['filename', 'filename_valid', 'preprocess_str']\ndefault_json_data = {key: json_data.get(key, None) for key in train_keys}\ndefault_json_data['aggregator_dir'] = self.model_dir\n# Save each trained and unsaved model\nfor sub_model in self.sub_models:\npath_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\nif os.path.exists(path_config):\nwith open(path_config, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\ntrained = configs.get('trained', False)\nif not trained:\nsub_model['model'].save(default_json_data)\nelse:\nsub_model['model'].save(default_json_data)\n# Add some specific information\njson_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\njson_data['using_proba'] = self.using_proba\n# Save aggregation_function if not None &amp; level_save &gt; LOW\nif (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\naggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n# Save as pickle\nwith open(aggregation_function_path, 'wb') as f:\npickle.dump(self.aggregation_function, f)\n# Save\nmodels_list = [sub_model['name'] for sub_model in self.sub_models]\naggregation_function = self.aggregation_function\ndelattr(self, \"sub_models\")\ndelattr(self, \"aggregation_function\")\nsuper().save(json_data=json_data)\nsetattr(self, \"aggregation_function\", aggregation_function)\nsetattr(self, \"sub_models\", models_list)  # Setter needs list of models, not sub_models itself\n# Add message in model_upload_instructions.md\nmd_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\nline = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it .  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\nself.prepend_line(md_path, line)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.all_predictions","title":"<code>all_predictions(predictions, **kwargs)</code>","text":"<p>Calculates the sum of the arrays along axis 0 casts it to bool and then to int. Expects a numpy array containing only zeroes and ones. When used as an aggregation function, keeps all the prediction of each model (multi-labels)</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray) </code> <p>Array of shape : (n_models, n_classes)</p> required Return <p>np.ndarray: The prediction</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def all_predictions(predictions: np.ndarray, **kwargs) -&gt; np.ndarray:\n'''Calculates the sum of the arrays along axis 0 casts it to bool and then to int.\n    Expects a numpy array containing only zeroes and ones.\n    When used as an aggregation function, keeps all the prediction of each model (multi-labels)\n    Args:\n        predictions (np.ndarray) : Array of shape : (n_models, n_classes)\n    Return:\n        np.ndarray: The prediction\n    '''\nreturn np.sum(predictions, axis=0, dtype=bool).astype(int)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.majority_vote","title":"<code>majority_vote(predictions, **kwargs)</code>","text":"<p>Gives the class corresponding to the most present prediction in the given predictions. In case of a tie, gives the prediction of the first model involved in the tie</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray</code> <p>The array containing the predictions of each model (shape (n_models))</p> required <p>Returns:</p> Type Description <p>The prediction</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def majority_vote(predictions: np.ndarray, **kwargs):\n'''Gives the class corresponding to the most present prediction in the given predictions.\n    In case of a tie, gives the prediction of the first model involved in the tie\n    Args:\n        predictions (np.ndarray): The array containing the predictions of each model (shape (n_models))\n    Returns:\n        The prediction\n    '''\nlabels, counts = np.unique(predictions, return_counts=True)\nvotes = [(label, count) for label, count in zip(labels, counts)]\nvotes = sorted(votes, key=lambda x: x[1], reverse=True)\npossible_classes = {vote[0] for vote in votes if vote[1]==votes[0][1]}\nreturn [prediction for prediction in predictions if prediction in possible_classes][0]\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.proba_argmax","title":"<code>proba_argmax(proba, list_classes, **kwargs)</code>","text":"<p>Gives the class corresponding to the argmax of the average of the given probabilities</p> <p>Parameters:</p> Name Type Description Default <code>proba</code> <code>np.ndarray</code> <p>The probabilities of each model for each class, array of shape (nb_models, nb_classes)</p> required <code>list_classes</code> <code>list</code> <p>List of classes</p> required <p>Returns:</p> Type Description <p>The prediction</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def proba_argmax(proba: np.ndarray, list_classes: list, **kwargs):\n'''Gives the class corresponding to the argmax of the average of the given probabilities\n    Args:\n        proba (np.ndarray): The probabilities of each model for each class, array of shape (nb_models, nb_classes)\n        list_classes (list): List of classes\n    Returns:\n        The prediction\n    '''\nproba_average = np.sum(proba, axis=0) / proba.shape[0]\nindex_class = np.argmax(proba_average)\nreturn list_classes[index_class]\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_aggregation_classifier/#template_num.models_training.classifiers.model_aggregation_classifier.vote_labels","title":"<code>vote_labels(predictions, **kwargs)</code>","text":"<p>Gives the result of majority_vote applied on the second axis. When used as an aggregation_function, for each class, performs a majority vote for the aggregated models. It gives a multi-labels result</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray</code> <p>array of shape : (n_models, n_classes)</p> required Return <p>np.ndarray: prediction</p> Source code in <code>template_num/models_training/classifiers/model_aggregation_classifier.py</code> <pre><code>def vote_labels(predictions: np.ndarray, **kwargs) -&gt; np.ndarray:\n'''Gives the result of majority_vote applied on the second axis.\n    When used as an aggregation_function, for each class, performs a majority vote for the aggregated models.\n    It gives a multi-labels result\n    Args:\n        predictions (np.ndarray): array of shape : (n_models, n_classes)\n    Return:\n        np.ndarray: prediction\n    '''\nreturn np.apply_along_axis(majority_vote, 0, predictions)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/","title":"Model classifier","text":""},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin","title":"<code>ModelClassifierMixin</code>","text":"<p>Parent class (Mixin) for classifier models</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>class ModelClassifierMixin:\n'''Parent class (Mixin) for classifier models'''\n# Not implemented :\n# -&gt; predict : To be implementd by the parent class when using this mixin\ndef __init__(self, level_save: str = 'HIGH', multi_label: bool = False, **kwargs) -&gt; None:\n'''Initialization of the class\n        Kwargs:\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOWlevel_save + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n            multi_label (bool): If the classification must be multi-labels\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        '''\nsuper().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type\nself.model_type = 'classifier'\n# Multi-labels ?\nself.multi_label = multi_label\n# Classes list to use (set on fit)\nself.list_classes = None\nself.dict_classes = None\n# Other options\nself.level_save = level_save\n@utils.trained_needed\ndef predict_with_proba(self, x_test: pd.DataFrame) -&gt; Tuple[np.ndarray, np.ndarray]:\n'''Predictions on test with probabilities\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            predicted_class (np.ndarray): The predicted classes, shape = [n_samples, n_classes] if multi-labels, shape = [n_samples, 1] otherwise\n            predicted_proba (np.ndarray): The predicted probabilities for each class, shape = [n_samples, n_classes]\n        '''\n# Process\npredicted_proba = self.predict(x_test, return_proba=True)\npredicted_class = self.get_classes_from_proba(predicted_proba)\nreturn predicted_class, predicted_proba\n@utils.trained_needed\ndef get_predict_position(self, x_test: pd.DataFrame, y_true) -&gt; np.ndarray:\n'''Gets the order of predictions of y_true.\n        Positions start at 1 (not 0)\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n            y_true (?): Array-like, shape = [n_samples, n_targets]\n        Raises:\n            ValueError: Can't use this method with multi-labels tasks\n        Returns:\n            predict_positions (np.ndarray): The order of prediction of y_true shape = [n_samples, ]\n        '''\nif self.multi_label:\nraise ValueError(\"The method 'get_predict_position' is unavailable with multi-labels tasks\")\n# Process\n# Cast as pd.Series\ny_true = pd.Series(y_true)\n# Get predicted probabilities\npredicted_proba = self.predict(x_test, return_proba=True)\n# Get position\norder = predicted_proba.argsort()\nranks = len(self.list_classes) - order.argsort()\ndf_probas = pd.DataFrame(ranks, columns=self.list_classes)\npredict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\nreturn predict_positions\ndef get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n'''Gets the classes from probabilities\n        Args:\n            predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n        Returns:\n            predicted_class (np.ndarray): Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise\n        '''\nif not self.multi_label:\npredicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\nelse:\n# If multi-labels, returns a list of 0 and 1\npredicted_class = np.vectorize(lambda x: 1 if x &gt;= 0.5 else 0)(predicted_proba)\nreturn predicted_class\ndef get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n'''Gets the Top n predictions from probabilities\n        Args:\n            predicted_proba (np.ndarray): Predicted probabilities = [n_samples, n_classes]\n        kwargs:\n            n (int): Number of classes to return\n        Raises:\n            ValueError: If the number of classes to return is greater than the number of classes of the model\n        Returns:\n            top_n (list): Top n predicted classes\n            top_n_proba (list): Top n probabilities (corresponding to the top_n list of classes)\n        '''\n# TODO: Make this method available with multi-labels tasks\nif self.multi_label:\nraise ValueError(\"The method 'get_top_n_from_proba' is unavailable with multi-labels tasks\")\nif self.list_classes is not None and n &gt; len(self.list_classes):\nraise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n# Process\nidx = predicted_proba.argsort()[:, -n:][:, ::-1]\ntop_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\ntop_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))\nreturn top_n, top_n_proba\ndef inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Gets a list of classes from the predictions (mainly useful for multi-labels)\n        Args:\n            y (list | np.ndarray): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n                   OR 1D array shape = [n_classes] (only one prediction)\n        Raises:\n            ValueError: If the size of y does not correspond to the number of classes of the model\n        Returns:\n            List of tuple if multi-labels and several predictions\n            Tuple if multi-labels and one prediction\n            List of classes if mono-label\n        '''\n# If multi-labels, get classes in tuple\nif self.multi_label:\nif y.shape[-1] != len(self.list_classes):  # We consider \"-1\" in order to take care of the case where y is 1D\nraise ValueError(f\"The size of y ({y.shape[-1]}) does not correspond\"\nf\" to the number of classes of the model : ({len(self.list_classes)})\")\n# Manage 1D array (only one prediction)\nif len(y.shape) == 1:\nreturn tuple(np.array(self.list_classes).compress(y))\n# Several predictions\nelse:\nreturn [tuple(np.array(self.list_classes).compress(indicators)) for indicators in y]\n# If mono-label, just cast in list if y is np array\nelse:\nreturn list(y) if isinstance(y, np.ndarray) else y\ndef get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\nseries_to_add: Union[List[pd.Series], None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_targets]\n            y_pred (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n            series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing the statistics\n        '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif not self.multi_label:\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Save a predictionn file if wanted\nif self.level_save == 'HIGH':\n# Inverse transform\ny_true_df = list(self.inverse_transform(y_true))\ny_pred_df = list(self.inverse_transform(y_pred))\n# Concat in a dataframe\nif df_x is not None:\ndf = df_x.copy()\ndf['y_true'] = y_true_df\ndf['y_pred'] = y_pred_df\nelse:\ndf = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n# Add a matched column\ndf['matched'] = (df['y_true'] == df['y_pred']).astype(int)\n# Add some more columns\nif series_to_add is not None:\nfor ser in series_to_add:\ndf[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex\n# Save predictions\nfile_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\ndf.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nif self.multi_label:\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\nfalses = len(y_true) - trues\nacc_tot = trues / len(y_true)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nsupport = list(pd.DataFrame(y_true).sum().values)\nsupport = [_ / sum(support) for _ in support] + [1.0]\nelse:\n# We use 'weighted' even in the mono-label case since there can be several classes !\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.] * len(self.list_classes) + [1.0]\nfor i, cl in enumerate(self.list_classes):\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# Global Statistics\nself.logger.info('-- * * * * * * * * * * * * * * --')\nself.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\nself.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\nself.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\nself.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\nself.logger.info('--------------------------------')\n# Metrics file\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Add metrics depending on mono/multi labels &amp; manage confusion matrices\nlabels = self.list_classes\nlog_stats = len(labels) &lt; 50\nif self.multi_label:\n# Details per category\nmcm = multilabel_confusion_matrix(y_true, y_pred)\nfor i, label in enumerate(labels):\nc_mat = mcm[i]\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat, label, log_info=log_stats), ignore_index=True)\n# Plot individual confusion matrix if level_save &gt; LOW\nif self.level_save in ['MEDIUM', 'HIGH']:\nnone_class = 'not_' + label\ntmp_label = re.sub(r',|:|\\s', '_', label)\nself._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\nnormalized=False, subdir=type_data)\nself._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\nnormalized=True, subdir=type_data)\nelse:\n# Plot confusion matrices if level_save &gt; LOW\nif self.level_save in ['MEDIUM', 'HIGH']:\nif len(labels) &gt; 50:\nself.logger.warning(\nf\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n\"Heavy chances of slowness/display bugs/crashes...\\n\"\n\"SKIP the plots\"\n)\nelse:\n# Global statistics\nc_mat = confusion_matrix(y_true, y_pred, labels=labels)\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)\n# Get statistics per class\nfor label in labels:\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Save .csv\nfile_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\ndf_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n# Save accuracy\nacc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\nwith open(acc_path, 'w'):\npass\nreturn df_stats\ndef get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on mono-label predictions\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n        Args:\n            y_true (?): Array-like, shape = [n_samples,]\n            y_pred (?): Array-like, shape = [n_samples,]\n        Raises:\n            ValueError: If not in mono-label mode\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\nif self.multi_label:\nraise ValueError(\"The method get_metrics_simple_monolabel only works for the mono-label case\")\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.] * len(self.list_classes) + [1.0]\nfor i, cl in enumerate(self.list_classes):\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# DataFrame metrics\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Get statistics per class\nlabels = self.list_classes\nfor label in labels:\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Return dataframe\nreturn df_stats\ndef get_metrics_simple_multilabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on multi-label predictions\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n        Args:\n            y_true (?): Array-like, shape = [n_samples, n_targets]\n            y_pred (?): Array-like, shape = [n_samples, n_targets]\n        Raises:\n            ValueError: If not with multi-labels tasks\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\nif not self.multi_label:\nraise ValueError(\"The method get_metrics_simple_multilabel only works for multi-labels cases\")\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\nfalses = len(y_true) - trues\nacc_tot = trues / len(y_true)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nsupport = list(pd.DataFrame(y_true).sum().values)\nsupport = [_ / sum(support) for _ in support] + [1.0]\n# DataFrame metrics\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Add metrics\nlabels = self.list_classes\n# Details per category\nmcm = multilabel_confusion_matrix(y_true, y_pred)\nfor i, label in enumerate(labels):\nc_mat = mcm[i]\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Return dataframe\nreturn df_stats\ndef _update_info_from_c_mat(self, c_mat: np.ndarray, label: str, log_info: bool = True) -&gt; dict:\n'''Updates a dataframe for the method get_and_save_metrics, given a confusion matrix\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            label (str): Label to use\n        Kwargs:\n            log_info (bool): If the statistics must be logged\n        Returns:\n            dict: Dictionary with the information for the update of the dataframe\n        '''\n# Extract all needed info from c_mat\ntrue_negative = c_mat[0][0]\ntrue_positive = c_mat[1][1]\nfalse_negative = c_mat[1][0]\nfalse_positive = c_mat[0][1]\ncondition_positive = false_negative + true_positive\ncondition_negative = false_positive + true_negative\npredicted_positive = false_positive + true_positive\npredicted_negative = false_negative + true_negative\ntrues_cat = true_negative + true_positive\nfalses_cat = false_negative + false_positive\naccuracy = (true_negative + true_positive) / (true_negative + true_positive + false_negative + false_positive)\nprecision = 0 if predicted_positive == 0 else true_positive / predicted_positive\nrecall = 0 if condition_positive == 0 else true_positive / condition_positive\nf1 = 0 if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n# Display some info\nif log_info:\nself.logger.info(\nf\"F1-score: {round(f1, 5)}  \\t Precision: {round(100 * precision, 2)}% \\t\"\nf\"Recall: {round(100 * recall, 2)}% \\t Trues: {trues_cat} \\t Falses: {falses_cat} \\t\\t --- {label} \"\n)\n# Return result\nreturn {\n'Label': f'{label}',\n'F1-Score': f1,\n'Accuracy': accuracy,\n'Precision': precision,\n'Recall': recall,\n'Trues': trues_cat,\n'Falses': falses_cat,\n'True positive': true_positive,\n'True negative': true_negative,\n'False positive': false_positive,\n'False negative': false_negative,\n'Condition positive': condition_positive,\n'Condition negative': condition_negative,\n'Predicted positive': predicted_positive,\n'Predicted negative': predicted_negative,\n}\ndef _plot_confusion_matrix(self, c_mat: np.ndarray, labels: list, type_data: str = '',\nnormalized: bool = False, subdir: Union[str, None] = None) -&gt; None:\n'''Plots a confusion matrix\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            labels (list): Labels to plot\n        Kwargs:\n            type_data (str): Type of dataset (validation, test, ...)\n            normalized (bool): If the confusion matrix should be normalized\n            subdir (str): Sub-directory for writing the plot\n        '''\n# Get title\nif normalized:\ntitle = f\"Normalized confusion matrix{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\nelse:\ntitle = f\"Confusion matrix, without normalization{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\n# Init. plot\nwidth = round(10 + 0.5 * len(c_mat))\nheight = round(4 / 5 * width)\nfig, ax = plt.subplots(figsize=(width, height))\n# Plot\nif normalized:\nc_mat = c_mat.astype('float') / c_mat.sum(axis=1)[:, np.newaxis]\nsns.heatmap(c_mat, annot=True, fmt=\".2f\", cmap=plt.cm.Blues, ax=ax)\nelse:\nsns.heatmap(c_mat, annot=True, fmt=\"d\", cmap=plt.cm.Blues, ax=ax)\n# labels, title and ticks\nax.set_xlabel('Predicted classes', fontsize=height * 2)\nax.set_ylabel('Real classes', fontsize=height * 2)\nax.set_title(title, fontsize=width * 2)\nax.xaxis.set_ticklabels(labels)\nax.yaxis.set_ticklabels(labels)\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\nplt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\nplt.tight_layout()\n# Save\nplots_path = os.path.join(self.model_dir, 'plots')\nif subdir is not None:  # Add subdir\nplots_path = os.path.join(plots_path, subdir)\nfile_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}confusion_matrix{'_normalized' if normalized else ''}.png\"\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\nplt.savefig(os.path.join(plots_path, file_name))\n# Close figures\nplt.close('all')\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['list_classes'] = self.list_classes\njson_data['dict_classes'] = self.dict_classes\njson_data['multi_label'] = self.multi_label\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.__init__","title":"<code>__init__(level_save='HIGH', multi_label=False, **kwargs)</code>","text":"<p>Initialization of the class</p> Kwargs <p>level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOWlevel_save + hdf5 + pkl + plots     HIGH: MEDIUM + predictions multi_label (bool): If the classification must be multi-labels</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def __init__(self, level_save: str = 'HIGH', multi_label: bool = False, **kwargs) -&gt; None:\n'''Initialization of the class\n    Kwargs:\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOWlevel_save + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n        multi_label (bool): If the classification must be multi-labels\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n    '''\nsuper().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type\nself.model_type = 'classifier'\n# Multi-labels ?\nself.multi_label = multi_label\n# Classes list to use (set on fit)\nself.list_classes = None\nself.dict_classes = None\n# Other options\nself.level_save = level_save\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, df_x=None, series_to_add=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required Kwargs <p>df_x (pd.DataFrame or None): Input dataFrame used for the prediction series_to_add (list): List of pd.Series to add to the dataframe type_data (str): Type of dataset (validation, test, ...) <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing the statistics</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\nseries_to_add: Union[List[pd.Series], None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_targets]\n        y_pred (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n        series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing the statistics\n    '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif not self.multi_label:\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Save a predictionn file if wanted\nif self.level_save == 'HIGH':\n# Inverse transform\ny_true_df = list(self.inverse_transform(y_true))\ny_pred_df = list(self.inverse_transform(y_pred))\n# Concat in a dataframe\nif df_x is not None:\ndf = df_x.copy()\ndf['y_true'] = y_true_df\ndf['y_pred'] = y_pred_df\nelse:\ndf = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n# Add a matched column\ndf['matched'] = (df['y_true'] == df['y_pred']).astype(int)\n# Add some more columns\nif series_to_add is not None:\nfor ser in series_to_add:\ndf[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex\n# Save predictions\nfile_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\ndf.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nif self.multi_label:\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\nfalses = len(y_true) - trues\nacc_tot = trues / len(y_true)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nsupport = list(pd.DataFrame(y_true).sum().values)\nsupport = [_ / sum(support) for _ in support] + [1.0]\nelse:\n# We use 'weighted' even in the mono-label case since there can be several classes !\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.] * len(self.list_classes) + [1.0]\nfor i, cl in enumerate(self.list_classes):\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# Global Statistics\nself.logger.info('-- * * * * * * * * * * * * * * --')\nself.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\nself.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\nself.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\nself.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\nself.logger.info('--------------------------------')\n# Metrics file\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Add metrics depending on mono/multi labels &amp; manage confusion matrices\nlabels = self.list_classes\nlog_stats = len(labels) &lt; 50\nif self.multi_label:\n# Details per category\nmcm = multilabel_confusion_matrix(y_true, y_pred)\nfor i, label in enumerate(labels):\nc_mat = mcm[i]\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat, label, log_info=log_stats), ignore_index=True)\n# Plot individual confusion matrix if level_save &gt; LOW\nif self.level_save in ['MEDIUM', 'HIGH']:\nnone_class = 'not_' + label\ntmp_label = re.sub(r',|:|\\s', '_', label)\nself._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\nnormalized=False, subdir=type_data)\nself._plot_confusion_matrix(c_mat, [none_class, label], type_data=f\"{tmp_label}_{type_data}\",\nnormalized=True, subdir=type_data)\nelse:\n# Plot confusion matrices if level_save &gt; LOW\nif self.level_save in ['MEDIUM', 'HIGH']:\nif len(labels) &gt; 50:\nself.logger.warning(\nf\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n\"Heavy chances of slowness/display bugs/crashes...\\n\"\n\"SKIP the plots\"\n)\nelse:\n# Global statistics\nc_mat = confusion_matrix(y_true, y_pred, labels=labels)\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)\n# Get statistics per class\nfor label in labels:\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Save .csv\nfile_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\ndf_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n# Save accuracy\nacc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\nwith open(acc_path, 'w'):\npass\nreturn df_stats\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_classes_from_proba","title":"<code>get_classes_from_proba(predicted_proba)</code>","text":"<p>Gets the classes from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>np.ndarray</code> <p>The probabilities predicted by the model, shape = [n_samples, n_classes]</p> required <p>Returns:</p> Name Type Description <code>predicted_class</code> <code>np.ndarray</code> <p>Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n'''Gets the classes from probabilities\n    Args:\n        predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n    Returns:\n        predicted_class (np.ndarray): Shape = [n_samples, n_classes] if multi-labels, shape = [n_samples] otherwise\n    '''\nif not self.multi_label:\npredicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\nelse:\n# If multi-labels, returns a list of 0 and 1\npredicted_class = np.vectorize(lambda x: 1 if x &gt;= 0.5 else 0)(predicted_proba)\nreturn predicted_class\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_metrics_simple_monolabel","title":"<code>get_metrics_simple_monolabel(y_true, y_pred)</code>","text":"<p>Gets metrics on mono-label predictions Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If not in mono-label mode</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on mono-label predictions\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n    Args:\n        y_true (?): Array-like, shape = [n_samples,]\n        y_pred (?): Array-like, shape = [n_samples,]\n    Raises:\n        ValueError: If not in mono-label mode\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\nif self.multi_label:\nraise ValueError(\"The method get_metrics_simple_monolabel only works for the mono-label case\")\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.] * len(self.list_classes) + [1.0]\nfor i, cl in enumerate(self.list_classes):\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# DataFrame metrics\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Get statistics per class\nlabels = self.list_classes\nfor label in labels:\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Return dataframe\nreturn df_stats\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_metrics_simple_multilabel","title":"<code>get_metrics_simple_multilabel(y_true, y_pred)</code>","text":"<p>Gets metrics on multi-label predictions Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If not with multi-labels tasks</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def get_metrics_simple_multilabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on multi-label predictions\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n    Args:\n        y_true (?): Array-like, shape = [n_samples, n_targets]\n        y_pred (?): Array-like, shape = [n_samples, n_targets]\n    Raises:\n        ValueError: If not with multi-labels tasks\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\nif not self.multi_label:\nraise ValueError(\"The method get_metrics_simple_multilabel only works for multi-labels cases\")\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(np.all(np.equal(y_true, y_pred), axis=1))\nfalses = len(y_true) - trues\nacc_tot = trues / len(y_true)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nsupport = list(pd.DataFrame(y_true).sum().values)\nsupport = [_ / sum(support) for _ in support] + [1.0]\n# DataFrame metrics\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Add metrics\nlabels = self.list_classes\n# Details per category\nmcm = multilabel_confusion_matrix(y_true, y_pred)\nfor i, label in enumerate(labels):\nc_mat = mcm[i]\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Return dataframe\nreturn df_stats\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_predict_position","title":"<code>get_predict_position(x_test, y_true)</code>","text":"<p>Gets the order of predictions of y_true. Positions start at 1 (not 0)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Can't use this method with multi-labels tasks</p> <p>Returns:</p> Name Type Description <code>predict_positions</code> <code>np.ndarray</code> <p>The order of prediction of y_true shape = [n_samples, ]</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>@utils.trained_needed\ndef get_predict_position(self, x_test: pd.DataFrame, y_true) -&gt; np.ndarray:\n'''Gets the order of predictions of y_true.\n    Positions start at 1 (not 0)\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        y_true (?): Array-like, shape = [n_samples, n_targets]\n    Raises:\n        ValueError: Can't use this method with multi-labels tasks\n    Returns:\n        predict_positions (np.ndarray): The order of prediction of y_true shape = [n_samples, ]\n    '''\nif self.multi_label:\nraise ValueError(\"The method 'get_predict_position' is unavailable with multi-labels tasks\")\n# Process\n# Cast as pd.Series\ny_true = pd.Series(y_true)\n# Get predicted probabilities\npredicted_proba = self.predict(x_test, return_proba=True)\n# Get position\norder = predicted_proba.argsort()\nranks = len(self.list_classes) - order.argsort()\ndf_probas = pd.DataFrame(ranks, columns=self.list_classes)\npredict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\nreturn predict_positions\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.get_top_n_from_proba","title":"<code>get_top_n_from_proba(predicted_proba, n=5)</code>","text":"<p>Gets the Top n predictions from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>np.ndarray</code> <p>Predicted probabilities = [n_samples, n_classes]</p> required kwargs <p>n (int): Number of classes to return</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of classes to return is greater than the number of classes of the model</p> <p>Returns:</p> Name Type Description <code>top_n</code> <code>list</code> <p>Top n predicted classes</p> <code>top_n_proba</code> <code>list</code> <p>Top n probabilities (corresponding to the top_n list of classes)</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n'''Gets the Top n predictions from probabilities\n    Args:\n        predicted_proba (np.ndarray): Predicted probabilities = [n_samples, n_classes]\n    kwargs:\n        n (int): Number of classes to return\n    Raises:\n        ValueError: If the number of classes to return is greater than the number of classes of the model\n    Returns:\n        top_n (list): Top n predicted classes\n        top_n_proba (list): Top n probabilities (corresponding to the top_n list of classes)\n    '''\n# TODO: Make this method available with multi-labels tasks\nif self.multi_label:\nraise ValueError(\"The method 'get_top_n_from_proba' is unavailable with multi-labels tasks\")\nif self.list_classes is not None and n &gt; len(self.list_classes):\nraise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n# Process\nidx = predicted_proba.argsort()[:, -n:][:, ::-1]\ntop_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\ntop_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))\nreturn top_n, top_n_proba\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets a list of classes from the predictions (mainly useful for multi-labels)</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>list | np.ndarray</code> <p>Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s    OR 1D array shape = [n_classes] (only one prediction)</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the size of y does not correspond to the number of classes of the model</p> <p>Returns:</p> Type Description <code>Union[list, tuple]</code> <p>List of tuple if multi-labels and several predictions</p> <code>Union[list, tuple]</code> <p>Tuple if multi-labels and one prediction</p> <code>Union[list, tuple]</code> <p>List of classes if mono-label</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Gets a list of classes from the predictions (mainly useful for multi-labels)\n    Args:\n        y (list | np.ndarray): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n               OR 1D array shape = [n_classes] (only one prediction)\n    Raises:\n        ValueError: If the size of y does not correspond to the number of classes of the model\n    Returns:\n        List of tuple if multi-labels and several predictions\n        Tuple if multi-labels and one prediction\n        List of classes if mono-label\n    '''\n# If multi-labels, get classes in tuple\nif self.multi_label:\nif y.shape[-1] != len(self.list_classes):  # We consider \"-1\" in order to take care of the case where y is 1D\nraise ValueError(f\"The size of y ({y.shape[-1]}) does not correspond\"\nf\" to the number of classes of the model : ({len(self.list_classes)})\")\n# Manage 1D array (only one prediction)\nif len(y.shape) == 1:\nreturn tuple(np.array(self.list_classes).compress(y))\n# Several predictions\nelse:\nreturn [tuple(np.array(self.list_classes).compress(indicators)) for indicators in y]\n# If mono-label, just cast in list if y is np array\nelse:\nreturn list(y) if isinstance(y, np.ndarray) else y\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.predict_with_proba","title":"<code>predict_with_proba(x_test)</code>","text":"<p>Predictions on test with probabilities</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:</p> Name Type Description <code>predicted_class</code> <code>np.ndarray</code> <p>The predicted classes, shape = [n_samples, n_classes] if multi-labels, shape = [n_samples, 1] otherwise</p> <code>predicted_proba</code> <code>np.ndarray</code> <p>The predicted probabilities for each class, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_with_proba(self, x_test: pd.DataFrame) -&gt; Tuple[np.ndarray, np.ndarray]:\n'''Predictions on test with probabilities\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        predicted_class (np.ndarray): The predicted classes, shape = [n_samples, n_classes] if multi-labels, shape = [n_samples, 1] otherwise\n        predicted_proba (np.ndarray): The predicted probabilities for each class, shape = [n_samples, n_classes]\n    '''\n# Process\npredicted_proba = self.predict(x_test, return_proba=True)\npredicted_class = self.get_classes_from_proba(predicted_proba)\nreturn predicted_class, predicted_proba\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_classifier/#template_num.models_training.classifiers.model_classifier.ModelClassifierMixin.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/model_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['list_classes'] = self.list_classes\njson_data['dict_classes'] = self.dict_classes\njson_data['multi_label'] = self.multi_label\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/","title":"Model xgboost classifier","text":""},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier","title":"<code>ModelXgboostClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelClass</code></p> <p>Xgboost model for classification</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>class ModelXgboostClassifier(ModelClassifierMixin, ModelClass):\n'''Xgboost model for classification'''\n_default_name = 'model_xgboost_classifier'\ndef __init__(self, xgboost_params: Union[dict, None] = None, early_stopping_rounds: int = 5, validation_split: float = 0.2, **kwargs) -&gt; None:\n'''Initialization of the class  (see ModelClass &amp; ModelClassifierMixin for more arguments)\n        Kwargs:\n            xgboost_params (dict): Parameters for the Xgboost\n                -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n            early_stopping_rounds (int): Number of rounds for early stopping\n            validation_split (float): Validation split fraction.\n                Only used if not validation dataset in the fit input\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Set parameters\nif xgboost_params is None:\nxgboost_params = {}\nself.xgboost_params = xgboost_params\nself.early_stopping_rounds = early_stopping_rounds\nself.validation_split = validation_split\n# Set objective (if not in params) &amp; init. model\nif 'objective' not in self.xgboost_params.keys():\nself.xgboost_params['objective'] = 'binary:logistic'\n#  List of objectives https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n# WARNING, if multi-classes, AUTOMATIC backup on multi:softprob (by xgboost)\n# https://stackoverflow.com/questions/57986259/multiclass-classification-with-xgboost-classifier\nself.model = XGBClassifier(**self.xgboost_params)\n# If multi-labels, we use MultiOutputClassifier\nif self.multi_label:\nself.model = MyMultiOutputClassifier(self.model)\ndef fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Trains the model\n           **kwargs permits compatibility with Keras model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples, n_targets]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n            y_valid (?): Array-like, shape = [n_samples, n_targets]\n            with_shuffle (bool): If x, y must be shuffled before fitting\n        Raises:\n            RuntimeError: If the model is already fitted\n        '''\n# TODO: Check if the training can be continued\nif self.trained:\nself.logger.error(\"We can't train again a xgboost model\")\nself.logger.error(\"Please train a new model\")\nraise RuntimeError(\"We can't train again a xgboost model\")\n# We check input format\nx_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n# If there is a validation set, we also check the format (but fit_function to False)\nif y_valid is not None and x_valid is not None:\nx_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n# Otherwise, we do a random split\nelse:\nself.logger.warning(f\"Warning, no validation dataset. We split the training set (fraction valid = {self.validation_split})\")\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=self.validation_split)\n# Gets the input columns\noriginal_list_classes: Optional[List[Any]] = None  # None if no 'columns' attribute or mono-label\nif self.multi_label and hasattr(y_train, 'columns'):\n# TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\noriginal_list_classes = list(y_train.columns)  # type: ignore\n# Shuffle x, y if wanted\nif with_shuffle:\np = np.random.permutation(len(x_train))\nx_train = np.array(x_train)[p]\ny_train = np.array(y_train)[p]\n# Else still transform to numpy array\nelse:\nx_train = np.array(x_train)\ny_train = np.array(y_train)\n# Also get x_valid &amp; y_valid as numpy\nx_valid = np.array(x_valid)\ny_valid = np.array(y_valid)\n# NEW from XGBOOST 1.3.2 : class columns should start from 0\n# https://stackoverflow.com/questions/71996617/invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2-3-4-5-got\n# We set list_classes and dict_classes now + replace target for mono-label cases (multi-label should already be OHE with 0s and 1s)\nif not self.multi_label:\nself.list_classes = list(np.unique(y_train))\n# Set dict_classes based on list classes\nself.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\ninv_dict_classes = {v: k for k, v in self.dict_classes.items()}\n# Update y_train and y_valid\nmap_func = lambda x: inv_dict_classes[x]\ny_train = np.vectorize(map_func)(y_train)\ny_valid = np.vectorize(map_func)(y_valid)\nelse:\nif original_list_classes is not None:\nself.list_classes = original_list_classes\nelse:\nself.logger.warning(\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\")\n# We still create a list of classes in order to be compatible with other functions\nself.list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n# Set dict_classes based on list classes\nself.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n# Nothing to update as targets are already OHE\n# Set eval set and train\neval_set = [(x_train, y_train), (x_valid, y_valid)]  # If there\u2019s more than one item in eval_set, the last entry will be used for early stopping.\nprior_objective = self.model.objective if not self.multi_label else self.model.estimator.objective\nself.model.fit(x_train, y_train, eval_set=eval_set, early_stopping_rounds=self.early_stopping_rounds, verbose=True)\npost_objective = self.model.objective if not self.multi_label else self.model.estimator.objective\nif prior_objective != post_objective:\nself.logger.warning(\"Warning: the objective function was automatically changed by XGBOOST\")\nself.logger.warning(f\"Before: {prior_objective}\")\nself.logger.warning(f\"After: {post_objective}\")\n# Set trained\nself.trained = True\nself.nb_fit += 1\n@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n        Returns:\n            (np.ndarray): Array\n                # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n                # Else, shape = [n_samples, n_classes]\n        '''\n# If we want probabilities, we use predict_proba\nif return_proba:\nreturn self.predict_proba(x_test, **kwargs)\n# Otherwise, returns the prediction :\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Warning, \"The method returns the model from the last iteration\"\n# But : \"Predict with X. If the model is trained with early stopping, then best_iteration is used automatically.\"\ny_proba = self.predict_proba(x_test)\ny_pred = self.get_classes_from_proba(y_proba)\nreturn y_pred\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        Args:\n            x_test (pd.DataFrame): DataFrame to be predicted -&gt; retrieve the probabilities\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n#\nprobas = np.array(self.model.predict_proba(x_test))\n# If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n# Correction in cas where we detect a shape of length &gt; 2 (ie. equals to 3)\n# Reminder : we do not manage multi-labels multi-classes\nif len(probas.shape) &gt; 2:\nprobas = np.swapaxes(probas[:, :, 1], 0, 1)\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'xgboost'\njson_data['xgboost_params'] = self.xgboost_params\njson_data['early_stopping_rounds'] = self.early_stopping_rounds\njson_data['validation_split'] = self.validation_split\n# Save xgboost standalone\nif self.level_save in ['MEDIUM', 'HIGH']:\nif not self.multi_label:\nif self.trained:\nsave_path = os.path.join(self.model_dir, 'xbgoost_standalone.model')\nself.model.save_model(save_path)\nelse:\nself.logger.warning(\"Can't save the XGboost in standalone because it hasn't been already fitted\")\nelse:\n# If multi-labels, we use a multi-output and fits several xgboost (cf. strategy sklearn)\n# We can't save only one xgboost, so we use pickle to save\n# Problem : the pickle won't be compatible with updates :'(\nsave_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\nwith open(save_path, 'wb') as f:\npickle.dump(self.model, f)\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            xgboost_path (str): Path to standalone xgboost\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If xgboost_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object xgboost_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nxgboost_path = kwargs.get('xgboost_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif xgboost_path is None:\nraise ValueError(\"The argument xgboost_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(xgboost_path):\nraise FileNotFoundError(f\"The file {xgboost_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'xgboost_params', 'early_stopping_rounds', 'validation_split']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload xgboost model\nif not self.multi_label:\nself.model.load_model(xgboost_path)\nelse:\nwith open(xgboost_path, 'rb') as f:  # type: ignore\nself.model = pickle.load(f)\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.__init__","title":"<code>__init__(xgboost_params=None, early_stopping_rounds=5, validation_split=0.2, **kwargs)</code>","text":"<p>Initialization of the class  (see ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>xgboost_params (dict): Parameters for the Xgboost     -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier early_stopping_rounds (int): Number of rounds for early stopping validation_split (float): Validation split fraction.     Only used if not validation dataset in the fit input</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>def __init__(self, xgboost_params: Union[dict, None] = None, early_stopping_rounds: int = 5, validation_split: float = 0.2, **kwargs) -&gt; None:\n'''Initialization of the class  (see ModelClass &amp; ModelClassifierMixin for more arguments)\n    Kwargs:\n        xgboost_params (dict): Parameters for the Xgboost\n            -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBClassifier\n        early_stopping_rounds (int): Number of rounds for early stopping\n        validation_split (float): Validation split fraction.\n            Only used if not validation dataset in the fit input\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Set parameters\nif xgboost_params is None:\nxgboost_params = {}\nself.xgboost_params = xgboost_params\nself.early_stopping_rounds = early_stopping_rounds\nself.validation_split = validation_split\n# Set objective (if not in params) &amp; init. model\nif 'objective' not in self.xgboost_params.keys():\nself.xgboost_params['objective'] = 'binary:logistic'\n#  List of objectives https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\n# WARNING, if multi-classes, AUTOMATIC backup on multi:softprob (by xgboost)\n# https://stackoverflow.com/questions/57986259/multiclass-classification-with-xgboost-classifier\nself.model = XGBClassifier(**self.xgboost_params)\n# If multi-labels, we use MultiOutputClassifier\nif self.multi_label:\nself.model = MyMultiOutputClassifier(self.model)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Trains the model    **kwargs permits compatibility with Keras model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_targets]</p> required Kwargs <p>x_valid (?): Array-like, shape = [n_samples, n_features] y_valid (?): Array-like, shape = [n_samples, n_targets] with_shuffle (bool): If x, y must be shuffled before fitting</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model is already fitted</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Trains the model\n       **kwargs permits compatibility with Keras model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples, n_targets]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features]\n        y_valid (?): Array-like, shape = [n_samples, n_targets]\n        with_shuffle (bool): If x, y must be shuffled before fitting\n    Raises:\n        RuntimeError: If the model is already fitted\n    '''\n# TODO: Check if the training can be continued\nif self.trained:\nself.logger.error(\"We can't train again a xgboost model\")\nself.logger.error(\"Please train a new model\")\nraise RuntimeError(\"We can't train again a xgboost model\")\n# We check input format\nx_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n# If there is a validation set, we also check the format (but fit_function to False)\nif y_valid is not None and x_valid is not None:\nx_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n# Otherwise, we do a random split\nelse:\nself.logger.warning(f\"Warning, no validation dataset. We split the training set (fraction valid = {self.validation_split})\")\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=self.validation_split)\n# Gets the input columns\noriginal_list_classes: Optional[List[Any]] = None  # None if no 'columns' attribute or mono-label\nif self.multi_label and hasattr(y_train, 'columns'):\n# TODO : tmp mypy fix https://github.com/python/mypy/pull/13544\noriginal_list_classes = list(y_train.columns)  # type: ignore\n# Shuffle x, y if wanted\nif with_shuffle:\np = np.random.permutation(len(x_train))\nx_train = np.array(x_train)[p]\ny_train = np.array(y_train)[p]\n# Else still transform to numpy array\nelse:\nx_train = np.array(x_train)\ny_train = np.array(y_train)\n# Also get x_valid &amp; y_valid as numpy\nx_valid = np.array(x_valid)\ny_valid = np.array(y_valid)\n# NEW from XGBOOST 1.3.2 : class columns should start from 0\n# https://stackoverflow.com/questions/71996617/invalid-classes-inferred-from-unique-values-of-y-expected-0-1-2-3-4-5-got\n# We set list_classes and dict_classes now + replace target for mono-label cases (multi-label should already be OHE with 0s and 1s)\nif not self.multi_label:\nself.list_classes = list(np.unique(y_train))\n# Set dict_classes based on list classes\nself.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\ninv_dict_classes = {v: k for k, v in self.dict_classes.items()}\n# Update y_train and y_valid\nmap_func = lambda x: inv_dict_classes[x]\ny_train = np.vectorize(map_func)(y_train)\ny_valid = np.vectorize(map_func)(y_valid)\nelse:\nif original_list_classes is not None:\nself.list_classes = original_list_classes\nelse:\nself.logger.warning(\"Can't read the name of the columns of y_train -&gt; inverse transformation won't be possible\")\n# We still create a list of classes in order to be compatible with other functions\nself.list_classes = [str(_) for _ in range(pd.DataFrame(y_train).shape[1])]\n# Set dict_classes based on list classes\nself.dict_classes = {i: col for i, col in enumerate(self.list_classes)}\n# Nothing to update as targets are already OHE\n# Set eval set and train\neval_set = [(x_train, y_train), (x_valid, y_valid)]  # If there\u2019s more than one item in eval_set, the last entry will be used for early stopping.\nprior_objective = self.model.objective if not self.multi_label else self.model.estimator.objective\nself.model.fit(x_train, y_train, eval_set=eval_set, early_stopping_rounds=self.early_stopping_rounds, verbose=True)\npost_objective = self.model.objective if not self.multi_label else self.model.estimator.objective\nif prior_objective != post_objective:\nself.logger.warning(\"Warning: the objective function was automatically changed by XGBOOST\")\nself.logger.warning(f\"Before: {prior_objective}\")\nself.logger.warning(f\"After: {post_objective}\")\n# Set trained\nself.trained = True\nself.nb_fit += 1\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required Kwargs <p>return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n    Returns:\n        (np.ndarray): Array\n            # If not return_proba, shape = [n_samples,] or [n_samples, n_classes]\n            # Else, shape = [n_samples, n_classes]\n    '''\n# If we want probabilities, we use predict_proba\nif return_proba:\nreturn self.predict_proba(x_test, **kwargs)\n# Otherwise, returns the prediction :\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Warning, \"The method returns the model from the last iteration\"\n# But : \"Predict with X. If the model is trained with early stopping, then best_iteration is used automatically.\"\ny_proba = self.predict_proba(x_test)\ny_pred = self.get_classes_from_proba(y_proba)\nreturn y_pred\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.predict--if-not-return_proba-shape-n_samples-or-n_samples-n_classes","title":"If not return_proba, shape = [n_samples,] or [n_samples, n_classes]","text":""},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.predict--else-shape-n_samples-n_classes","title":"Else, shape = [n_samples, n_classes]","text":""},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame to be predicted -&gt; retrieve the probabilities</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n    Args:\n        x_test (pd.DataFrame): DataFrame to be predicted -&gt; retrieve the probabilities\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n#\nprobas = np.array(self.model.predict_proba(x_test))\n# If use of MultiOutputClassifier -&gt;  returns probabilities of 0 and 1 for all elements and all classes\n# Correction in cas where we detect a shape of length &gt; 2 (ie. equals to 3)\n# Reminder : we do not manage multi-labels multi-classes\nif len(probas.shape) &gt; 2:\nprobas = np.swapaxes(probas[:, :, 1], 0, 1)\nreturn probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file xgboost_path (str): Path to standalone xgboost preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If xgboost_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object xgboost_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        xgboost_path (str): Path to standalone xgboost\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If xgboost_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object xgboost_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nxgboost_path = kwargs.get('xgboost_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif xgboost_path is None:\nraise ValueError(\"The argument xgboost_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(xgboost_path):\nraise FileNotFoundError(f\"The file {xgboost_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'xgboost_params', 'early_stopping_rounds', 'validation_split']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload xgboost model\nif not self.multi_label:\nself.model.load_model(xgboost_path)\nelse:\nwith open(xgboost_path, 'rb') as f:  # type: ignore\nself.model = pickle.load(f)\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.ModelXgboostClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'xgboost'\njson_data['xgboost_params'] = self.xgboost_params\njson_data['early_stopping_rounds'] = self.early_stopping_rounds\njson_data['validation_split'] = self.validation_split\n# Save xgboost standalone\nif self.level_save in ['MEDIUM', 'HIGH']:\nif not self.multi_label:\nif self.trained:\nsave_path = os.path.join(self.model_dir, 'xbgoost_standalone.model')\nself.model.save_model(save_path)\nelse:\nself.logger.warning(\"Can't save the XGboost in standalone because it hasn't been already fitted\")\nelse:\n# If multi-labels, we use a multi-output and fits several xgboost (cf. strategy sklearn)\n# We can't save only one xgboost, so we use pickle to save\n# Problem : the pickle won't be compatible with updates :'(\nsave_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\nwith open(save_path, 'wb') as f:\npickle.dump(self.model, f)\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.MyMultiOutputClassifier","title":"<code>MyMultiOutputClassifier</code>","text":"<p>         Bases: <code>MultiOutputClassifier</code></p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>class MyMultiOutputClassifier(MultiOutputClassifier):\n@_deprecate_positional_args\ndef __init__(self, estimator, *, n_jobs=None) -&gt; None:\nsuper().__init__(estimator, n_jobs=n_jobs)\ndef fit(self, X, y, sample_weight=None, **fit_params) -&gt; Any:\n''' Fit the model to data.\n        Fit a separate model for each output variable.\n        Parameters\n        ----------\n        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n            Data.\n        y : {array-like, sparse matrix} of shape (n_samples, n_outputs)\n            Multi-output targets. An indicator matrix turns on multilabel\n            estimation.\n        sample_weight : array-like of shape (n_samples,), default=None\n            Sample weights. If None, then samples are equally weighted.\n            Only supported if the underlying regressor supports sample\n            weights.\n        **fit_params : dict of string -&gt; object\n            Parameters passed to the ``estimator.fit`` method of each step.\n            .. versionadded:: 0.23\n        Returns\n        -------\n        self : object\n        '''\nif not hasattr(self.estimator, \"fit\"):\nraise ValueError(\"The base estimator should implement\"\n\" a fit method\")\nX, y = self._validate_data(X, y,\nforce_all_finite=False,\nmulti_output=True, accept_sparse=True)\nif is_classifier(self):\ncheck_classification_targets(y)\nif y.ndim == 1:\nraise ValueError(\"y must have at least two dimensions for \"\n\"multi-output regression but has only one.\")\nif (sample_weight is not None and not has_fit_parameter(self.estimator, 'sample_weight')):\nraise ValueError(\"Underlying estimator does not support\"\n\" sample weights.\")\nfit_params_validated = _check_fit_params(X, fit_params)\n# New : extract eval_set\nif 'eval_set' in fit_params_validated.keys():\neval_set = fit_params_validated.pop('eval_set')\nself.estimators_ = Parallel(n_jobs=self.n_jobs)(\ndelayed(_fit_estimator)(\nself.estimator, X, y[:, i], sample_weight,\n**fit_params_validated,\neval_set=[(X_test, Y_test[:, i]) for X_test, Y_test in eval_set])\nfor i in range(y.shape[1]))\n# Pas d'eval_set\nelse:\nself.estimators_ = Parallel(n_jobs=self.n_jobs)(\ndelayed(_fit_estimator)(\nself.estimator, X, y[:, i], sample_weight,\n**fit_params_validated)\nfor i in range(y.shape[1]))\nreturn self\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/model_xgboost_classifier/#template_num.models_training.classifiers.model_xgboost_classifier.MyMultiOutputClassifier.fit","title":"<code>fit(X, y, sample_weight=None, **fit_params)</code>","text":"<p>Fit the model to data. Fit a separate model for each output variable. Parameters</p> {array-like, sparse matrix} of shape (n_samples, n_features) <p>Data.</p> {array-like, sparse matrix} of shape (n_samples, n_outputs) <p>Multi-output targets. An indicator matrix turns on multilabel estimation.</p> array-like of shape (n_samples,), default=None <p>Sample weights. If None, then samples are equally weighted. Only supported if the underlying regressor supports sample weights.</p> <p>**fit_params : dict of string -&gt; object     Parameters passed to the <code>estimator.fit</code> method of each step.     .. versionadded:: 0.23 Returns</p> <p>self : object</p> Source code in <code>template_num/models_training/classifiers/model_xgboost_classifier.py</code> <pre><code>def fit(self, X, y, sample_weight=None, **fit_params) -&gt; Any:\n''' Fit the model to data.\n    Fit a separate model for each output variable.\n    Parameters\n    ----------\n    X : {array-like, sparse matrix} of shape (n_samples, n_features)\n        Data.\n    y : {array-like, sparse matrix} of shape (n_samples, n_outputs)\n        Multi-output targets. An indicator matrix turns on multilabel\n        estimation.\n    sample_weight : array-like of shape (n_samples,), default=None\n        Sample weights. If None, then samples are equally weighted.\n        Only supported if the underlying regressor supports sample\n        weights.\n    **fit_params : dict of string -&gt; object\n        Parameters passed to the ``estimator.fit`` method of each step.\n        .. versionadded:: 0.23\n    Returns\n    -------\n    self : object\n    '''\nif not hasattr(self.estimator, \"fit\"):\nraise ValueError(\"The base estimator should implement\"\n\" a fit method\")\nX, y = self._validate_data(X, y,\nforce_all_finite=False,\nmulti_output=True, accept_sparse=True)\nif is_classifier(self):\ncheck_classification_targets(y)\nif y.ndim == 1:\nraise ValueError(\"y must have at least two dimensions for \"\n\"multi-output regression but has only one.\")\nif (sample_weight is not None and not has_fit_parameter(self.estimator, 'sample_weight')):\nraise ValueError(\"Underlying estimator does not support\"\n\" sample weights.\")\nfit_params_validated = _check_fit_params(X, fit_params)\n# New : extract eval_set\nif 'eval_set' in fit_params_validated.keys():\neval_set = fit_params_validated.pop('eval_set')\nself.estimators_ = Parallel(n_jobs=self.n_jobs)(\ndelayed(_fit_estimator)(\nself.estimator, X, y[:, i], sample_weight,\n**fit_params_validated,\neval_set=[(X_test, Y_test[:, i]) for X_test, Y_test in eval_set])\nfor i in range(y.shape[1]))\n# Pas d'eval_set\nelse:\nself.estimators_ = Parallel(n_jobs=self.n_jobs)(\ndelayed(_fit_estimator)(\nself.estimator, X, y[:, i], sample_weight,\n**fit_params_validated)\nfor i in range(y.shape[1]))\nreturn self\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/","title":"Models sklearn","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/","title":"Model gbt classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/#template_num.models_training.classifiers.models_sklearn.model_gbt_classifier.ModelGBTClassifier","title":"<code>ModelGBTClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Gradient Boosted Tree model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_gbt_classifier.py</code> <pre><code>class ModelGBTClassifier(ModelClassifierMixin, ModelPipeline):\n'''Gradient Boosted Tree model for classification'''\n_default_name = 'model_gbt_classifier'\ndef __init__(self, gbt_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n        Kwargs:\n            gbt_params (dict) : Parameters for the Gradient Boosted Tree\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n                Warning, 'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif gbt_params is None:\ngbt_params = {}\nself.gbt = GradientBoostingClassifier(**gbt_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('gbt', OneVsRestClassifier(self.gbt))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('gbt', OneVsOneClassifier(self.gbt))])\nelse:\nself.pipeline = Pipeline([('gbt', self.gbt)])\n# GradientBoostingClassifier does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('gbt', MultiOutputClassifier(self.gbt))])\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test=x_test, **kwargs)\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.gbt = self.pipeline['gbt']\nelse:\nself.gbt = self.pipeline['gbt'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/#template_num.models_training.classifiers.models_sklearn.model_gbt_classifier.ModelGBTClassifier.__init__","title":"<code>__init__(gbt_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>gbt_params (dict) : Parameters for the Gradient Boosted Tree multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.     Warning, 'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_gbt_classifier.py</code> <pre><code>def __init__(self, gbt_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n    Kwargs:\n        gbt_params (dict) : Parameters for the Gradient Boosted Tree\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n            Warning, 'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif gbt_params is None:\ngbt_params = {}\nself.gbt = GradientBoostingClassifier(**gbt_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('gbt', OneVsRestClassifier(self.gbt))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('gbt', OneVsOneClassifier(self.gbt))])\nelse:\nself.pipeline = Pipeline([('gbt', self.gbt)])\n# GradientBoostingClassifier does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('gbt', MultiOutputClassifier(self.gbt))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/#template_num.models_training.classifiers.models_sklearn.model_gbt_classifier.ModelGBTClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set 'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_gbt_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n    'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test=x_test, **kwargs)\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/#template_num.models_training.classifiers.models_sklearn.model_gbt_classifier.ModelGBTClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_gbt_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.gbt = self.pipeline['gbt']\nelse:\nself.gbt = self.pipeline['gbt'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_gbt_classifier/#template_num.models_training.classifiers.models_sklearn.model_gbt_classifier.ModelGBTClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_gbt_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/","title":"Model knn classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/#template_num.models_training.classifiers.models_sklearn.model_knn_classifier.ModelKNNClassifier","title":"<code>ModelKNNClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>K-nearest Neighbors model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_knn_classifier.py</code> <pre><code>class ModelKNNClassifier(ModelClassifierMixin, ModelPipeline):\n'''K-nearest Neighbors model for classification'''\n_default_name = 'model_knn_classifier'\ndef __init__(self, knn_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n        Kwargs:\n            knn_params (dict) : Parameters for the K-nearest Neighbors\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif knn_params is None:\nknn_params = {}\nself.knn = KNeighborsClassifier(**knn_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('knn', OneVsRestClassifier(self.knn))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('knn', OneVsOneClassifier(self.knn))])\nelse:\nself.pipeline = Pipeline([('knn', self.knn)])\n# LKNeighborsClassifier natively supports multi_labels\nif self.multi_label:\nself.pipeline = Pipeline([('knn', self.knn)])\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n            'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test=x_test, **kwargs)\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is not None:\nself.knn = self.pipeline['knn'].estimator\nelse:\nself.knn = self.pipeline['knn']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/#template_num.models_training.classifiers.models_sklearn.model_knn_classifier.ModelKNNClassifier.__init__","title":"<code>__init__(knn_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>knn_params (dict) : Parameters for the K-nearest Neighbors multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If multiclass_strategy is not 'ovo', 'ovr' or None</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_knn_classifier.py</code> <pre><code>def __init__(self, knn_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n    Kwargs:\n        knn_params (dict) : Parameters for the K-nearest Neighbors\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        ValueError: If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif knn_params is None:\nknn_params = {}\nself.knn = KNeighborsClassifier(**knn_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('knn', OneVsRestClassifier(self.knn))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('knn', OneVsOneClassifier(self.knn))])\nelse:\nself.pipeline = Pipeline([('knn', self.knn)])\n# LKNeighborsClassifier natively supports multi_labels\nif self.multi_label:\nself.pipeline = Pipeline([('knn', self.knn)])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/#template_num.models_training.classifiers.models_sklearn.model_knn_classifier.ModelKNNClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set     'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_knn_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test=x_test, **kwargs)\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/#template_num.models_training.classifiers.models_sklearn.model_knn_classifier.ModelKNNClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_knn_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is not None:\nself.knn = self.pipeline['knn'].estimator\nelse:\nself.knn = self.pipeline['knn']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_knn_classifier/#template_num.models_training.classifiers.models_sklearn.model_knn_classifier.ModelKNNClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_knn_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/","title":"Model lgbm classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/#template_num.models_training.classifiers.models_sklearn.model_lgbm_classifier.ModelLGBMClassifier","title":"<code>ModelLGBMClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Light GBM model for Classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier.py</code> <pre><code>class ModelLGBMClassifier(ModelClassifierMixin, ModelPipeline):\n'''Light GBM model for Classification'''\n_default_name = 'model_lgbm_classifier'\ndef __init__(self, lgbm_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n        Kwargs:\n            lgbm_params (dict) : Parameters for the Light GBM\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif lgbm_params is None:\nlgbm_params = {}\nself.lgbm = LGBMClassifier(**lgbm_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('lgbm', OneVsRestClassifier(self.lgbm))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('lgbm', OneVsOneClassifier(self.lgbm))])\nelse:\nself.pipeline = Pipeline([('lgbm', self.lgbm)])\n# LGBMClassifier does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('lgbm', MultiOutputClassifier(self.lgbm))])\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n            'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test=x_test, **kwargs)\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.lgbm = self.pipeline['lgbm']\nelse:\nself.lgbm = self.pipeline['lgbm'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/#template_num.models_training.classifiers.models_sklearn.model_lgbm_classifier.ModelLGBMClassifier.__init__","title":"<code>__init__(lgbm_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>lgbm_params (dict) : Parameters for the Light GBM multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier.py</code> <pre><code>def __init__(self, lgbm_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n    Kwargs:\n        lgbm_params (dict) : Parameters for the Light GBM\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif lgbm_params is None:\nlgbm_params = {}\nself.lgbm = LGBMClassifier(**lgbm_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('lgbm', OneVsRestClassifier(self.lgbm))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('lgbm', OneVsOneClassifier(self.lgbm))])\nelse:\nself.pipeline = Pipeline([('lgbm', self.lgbm)])\n# LGBMClassifier does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('lgbm', MultiOutputClassifier(self.lgbm))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/#template_num.models_training.classifiers.models_sklearn.model_lgbm_classifier.ModelLGBMClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set     'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test=x_test, **kwargs)\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/#template_num.models_training.classifiers.models_sklearn.model_lgbm_classifier.ModelLGBMClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.lgbm = self.pipeline['lgbm']\nelse:\nself.lgbm = self.pipeline['lgbm'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier/#template_num.models_training.classifiers.models_sklearn.model_lgbm_classifier.ModelLGBMClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_lgbm_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/","title":"Model logistic regression classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/#template_num.models_training.classifiers.models_sklearn.model_logistic_regression_classifier.ModelLogisticRegressionClassifier","title":"<code>ModelLogisticRegressionClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Logistic Regression mode for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier.py</code> <pre><code>class ModelLogisticRegressionClassifier(ModelClassifierMixin, ModelPipeline):\n'''Logistic Regression mode for classification'''\n_default_name = 'model_lr_classifier'\ndef __init__(self, lr_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n        Kwargs:\n            lr_params (dict) : Parameters for the Logistic Regression\n            multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif lr_params is None:\nlr_params = {}\nself.lr = LogisticRegression(**lr_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('lr', OneVsRestClassifier(self.lr))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('lr', OneVsOneClassifier(self.lr))])\nelse:\nself.pipeline = Pipeline([('lr', self.lr)])\n# LogisticRegression does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('lr', MultiOutputClassifier(self.lr))])\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n            'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test=x_test, **kwargs)\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.lr = self.pipeline['lr']\nelse:\nself.lr = self.pipeline['lr'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/#template_num.models_training.classifiers.models_sklearn.model_logistic_regression_classifier.ModelLogisticRegressionClassifier.__init__","title":"<code>__init__(lr_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>lr_params (dict) : Parameters for the Logistic Regression multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> <p>Raises:</p> Type Description <code>multiclass_strategy(str)</code> <p>Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier.py</code> <pre><code>def __init__(self, lr_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n    Kwargs:\n        lr_params (dict) : Parameters for the Logistic Regression\n        multiclass_strategy (str): Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif lr_params is None:\nlr_params = {}\nself.lr = LogisticRegression(**lr_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('lr', OneVsRestClassifier(self.lr))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('lr', OneVsOneClassifier(self.lr))])\nelse:\nself.pipeline = Pipeline([('lr', self.lr)])\n# LogisticRegression does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('lr', MultiOutputClassifier(self.lr))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/#template_num.models_training.classifiers.models_sklearn.model_logistic_regression_classifier.ModelLogisticRegressionClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set     'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test=x_test, **kwargs)\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/#template_num.models_training.classifiers.models_sklearn.model_logistic_regression_classifier.ModelLogisticRegressionClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.lr = self.pipeline['lr']\nelse:\nself.lr = self.pipeline['lr'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier/#template_num.models_training.classifiers.models_sklearn.model_logistic_regression_classifier.ModelLogisticRegressionClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_logistic_regression_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/","title":"Model rf classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/#template_num.models_training.classifiers.models_sklearn.model_rf_classifier.ModelRFClassifier","title":"<code>ModelRFClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Random Forest model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_rf_classifier.py</code> <pre><code>class ModelRFClassifier(ModelClassifierMixin, ModelPipeline):\n'''Random Forest model for classification'''\n_default_name = 'model_rf_classifier'\ndef __init__(self, rf_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n        Kwargs:\n            rf_params (dict) : Parameters for the Random Forest\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif rf_params is None:\nrf_params = {}\nself.rf = RandomForestClassifier(**rf_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('rf', OneVsRestClassifier(self.rf))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('rf', OneVsOneClassifier(self.rf))])\nelse:\nself.pipeline = Pipeline([('rf', self.rf)])\n# RandomForest natively supports multi_labels\nif self.multi_label:\nself.pipeline = Pipeline([('rf', self.rf)])\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n            'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test=x_test, **kwargs)\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is not None:\nself.rf = self.pipeline['rf'].estimator\nelse:\nself.rf = self.pipeline['rf']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/#template_num.models_training.classifiers.models_sklearn.model_rf_classifier.ModelRFClassifier.__init__","title":"<code>__init__(rf_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>rf_params (dict) : Parameters for the Random Forest multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_rf_classifier.py</code> <pre><code>def __init__(self, rf_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n    Kwargs:\n        rf_params (dict) : Parameters for the Random Forest\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif rf_params is None:\nrf_params = {}\nself.rf = RandomForestClassifier(**rf_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('rf', OneVsRestClassifier(self.rf))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('rf', OneVsOneClassifier(self.rf))])\nelse:\nself.pipeline = Pipeline([('rf', self.rf)])\n# RandomForest natively supports multi_labels\nif self.multi_label:\nself.pipeline = Pipeline([('rf', self.rf)])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/#template_num.models_training.classifiers.models_sklearn.model_rf_classifier.ModelRFClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set     'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_rf_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Uses super() of the ModelPipeline class if != 'ovo' or multi-labels\nif self.multi_label or self.multiclass_strategy != 'ovo':\nreturn super().predict_proba(x_test=x_test, **kwargs)\nelse:\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nreturn probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/#template_num.models_training.classifiers.models_sklearn.model_rf_classifier.ModelRFClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_rf_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is not None:\nself.rf = self.pipeline['rf'].estimator\nelse:\nself.rf = self.pipeline['rf']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_rf_classifier/#template_num.models_training.classifiers.models_sklearn.model_rf_classifier.ModelRFClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_rf_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/","title":"Model ridge classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/#template_num.models_training.classifiers.models_sklearn.model_ridge_classifier.ModelRidgeClassifier","title":"<code>ModelRidgeClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Ridge Classifier model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_ridge_classifier.py</code> <pre><code>class ModelRidgeClassifier(ModelClassifierMixin, ModelPipeline):\n'''Ridge Classifier model for classification'''\n_default_name = 'model_ridge_classifier'\ndef __init__(self, ridge_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n        Kwargs:\n            ridge_params (dict) : Parameters for the Ridge Classifier\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif ridge_params is None:\nridge_params = {}\nself.ridge = RidgeClassifier(**ridge_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('ridge', OneVsRestClassifier(self.ridge))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('ridge', OneVsOneClassifier(self.ridge))])\nelse:\nself.pipeline = Pipeline([('ridge', self.ridge)])\n# RidgeClassifier does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('ridge', MultiOutputClassifier(self.ridge))])\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        - /!\\\\ RIDGE CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /!\\\\ -\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\nif not self.multi_label:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nelse:\npreds = self.pipeline.predict(x_test)\n# Already right format, but in int !\nprobas = np.array([[float(_) for _ in x] for x in preds])\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.ridge = self.pipeline['ridge']\nelse:\nself.ridge = self.pipeline['ridge'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/#template_num.models_training.classifiers.models_sklearn.model_ridge_classifier.ModelRidgeClassifier.__init__","title":"<code>__init__(ridge_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>ridge_params (dict) : Parameters for the Ridge Classifier multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_ridge_classifier.py</code> <pre><code>def __init__(self, ridge_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n    Kwargs:\n        ridge_params (dict) : Parameters for the Ridge Classifier\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif ridge_params is None:\nridge_params = {}\nself.ridge = RidgeClassifier(**ridge_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('ridge', OneVsRestClassifier(self.ridge))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('ridge', OneVsOneClassifier(self.ridge))])\nelse:\nself.pipeline = Pipeline([('ridge', self.ridge)])\n# RidgeClassifier does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('ridge', MultiOutputClassifier(self.ridge))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/#template_num.models_training.classifiers.models_sklearn.model_ridge_classifier.ModelRidgeClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - /! RIDGE CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /! -</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_ridge_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n    - /!\\\\ RIDGE CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /!\\\\ -\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\nif not self.multi_label:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nelse:\npreds = self.pipeline.predict(x_test)\n# Already right format, but in int !\nprobas = np.array([[float(_) for _ in x] for x in preds])\nreturn probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/#template_num.models_training.classifiers.models_sklearn.model_ridge_classifier.ModelRidgeClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_ridge_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.ridge = self.pipeline['ridge']\nelse:\nself.ridge = self.pipeline['ridge'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_ridge_classifier/#template_num.models_training.classifiers.models_sklearn.model_ridge_classifier.ModelRidgeClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_ridge_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/","title":"Model sgd classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/#template_num.models_training.classifiers.models_sklearn.model_sgd_classifier.ModelSGDClassifier","title":"<code>ModelSGDClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Stochastic Gradient Descent model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_sgd_classifier.py</code> <pre><code>class ModelSGDClassifier(ModelClassifierMixin, ModelPipeline):\n'''Stochastic Gradient Descent model for classification'''\n_default_name = 'model_sgd_classifier'\ndef __init__(self, sgd_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n        Kwargs:\n            sgd_params (dict) : Parameters for the Stochastic Gradient Descent\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif sgd_params is None:\nsgd_params = {}\nself.sgd = SGDClassifier(**sgd_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('sgd', OneVsRestClassifier(self.sgd))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('sgd', OneVsOneClassifier(self.sgd))])\nelse:\nself.pipeline = Pipeline([('sgd', self.sgd)])\n# SGDClassifier does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('sgd', MultiOutputClassifier(self.sgd))])\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n            'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Can't use probabilities if loss not in ['log', 'modified_huber'] or 'ovo' and not multi-labels\nif self.sgd.loss not in ['log', 'modified_huber'] or (self.multiclass_strategy == 'ovo' and not self.multi_label):\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\nif not self.multi_label:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nelse:\npreds = self.pipeline.predict(x_test)\n# Already right format, but in int !\nprobas = np.array([[float(_) for _ in x] for x in preds])\n# Otherwise, use super() of the pipeline class if != 'ovo' or multi-labels\nelse:\nreturn super().predict_proba(x_test=x_test, **kwargs)\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.sgd = self.pipeline['sgd']\nelse:\nself.sgd = self.pipeline['sgd'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/#template_num.models_training.classifiers.models_sklearn.model_sgd_classifier.ModelSGDClassifier.__init__","title":"<code>__init__(sgd_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>sgd_params (dict) : Parameters for the Stochastic Gradient Descent multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_sgd_classifier.py</code> <pre><code>def __init__(self, sgd_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n    Kwargs:\n        sgd_params (dict) : Parameters for the Stochastic Gradient Descent\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif sgd_params is None:\nsgd_params = {}\nself.sgd = SGDClassifier(**sgd_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('sgd', OneVsRestClassifier(self.sgd))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('sgd', OneVsOneClassifier(self.sgd))])\nelse:\nself.pipeline = Pipeline([('sgd', self.sgd)])\n# SGDClassifier does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('sgd', MultiOutputClassifier(self.sgd))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/#template_num.models_training.classifiers.models_sklearn.model_sgd_classifier.ModelSGDClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set     'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_sgd_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        'ovo' can't predict probabilities : by default, return 1 for the predicted class, 0 otherwise.\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Can't use probabilities if loss not in ['log', 'modified_huber'] or 'ovo' and not multi-labels\nif self.sgd.loss not in ['log', 'modified_huber'] or (self.multiclass_strategy == 'ovo' and not self.multi_label):\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Get preds\nif not self.multi_label:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nelse:\npreds = self.pipeline.predict(x_test)\n# Already right format, but in int !\nprobas = np.array([[float(_) for _ in x] for x in preds])\n# Otherwise, use super() of the pipeline class if != 'ovo' or multi-labels\nelse:\nreturn super().predict_proba(x_test=x_test, **kwargs)\nreturn probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/#template_num.models_training.classifiers.models_sklearn.model_sgd_classifier.ModelSGDClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_sgd_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.sgd = self.pipeline['sgd']\nelse:\nself.sgd = self.pipeline['sgd'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_sgd_classifier/#template_num.models_training.classifiers.models_sklearn.model_sgd_classifier.ModelSGDClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_sgd_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/","title":"Model svm classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/#template_num.models_training.classifiers.models_sklearn.model_svm_classifier.ModelSVMClassifier","title":"<code>ModelSVMClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelPipeline</code></p> <p>Support Vector Machine model for classification</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_svm_classifier.py</code> <pre><code>class ModelSVMClassifier(ModelClassifierMixin, ModelPipeline):\n'''Support Vector Machine model for classification'''\n_default_name = 'model_svm_classifier'\ndef __init__(self, svm_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n        Kwargs:\n            svm_params (dict) : Parameters for the Support Vector Machine\n            multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n        Raises:\n            If multiclass_strategy is not 'ovo', 'ovr' or None\n        '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif svm_params is None:\nsvm_params = {}\nself.svm = SVC(**svm_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('svm', OneVsRestClassifier(self.svm))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('svm', OneVsOneClassifier(self.svm))])\nelse:\nself.pipeline = Pipeline([('svm', self.svm)])\n# SVC does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('svm', MultiOutputClassifier(self.svm))])\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n        - /!\\\\ SVC CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /!\\\\ -\n            (in truth, you could use probability = True in the definition of the SVC but the probabilities are 'inconsistent' with the predictions)\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# We check input format\nx_test, _ = self._check_input_format(x_test)\nif not self.multi_label:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nelse:\npreds = self.pipeline.predict(x_test)\n# Already right format, but in int !\nprobas = np.array([[float(_) for _ in x] for x in preds])\nreturn probas\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.svm = self.pipeline['svm']\nelse:\nself.svm = self.pipeline['svm'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/#template_num.models_training.classifiers.models_sklearn.model_svm_classifier.ModelSVMClassifier.__init__","title":"<code>__init__(svm_params=None, multiclass_strategy=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>svm_params (dict) : Parameters for the Support Vector Machine multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_svm_classifier.py</code> <pre><code>def __init__(self, svm_params: Union[dict, None] = None, multiclass_strategy: Union[str, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelClassifierMixin for more arguments)\n    Kwargs:\n        svm_params (dict) : Parameters for the Support Vector Machine\n        multiclass_strategy (str):  Multi-classes strategy, 'ovr' (OneVsRest), or 'ovo' (OneVsOne). If None, use the default of the algorithm.\n    Raises:\n        If multiclass_strategy is not 'ovo', 'ovr' or None\n    '''\nif multiclass_strategy is not None and multiclass_strategy not in ['ovo', 'ovr']:\nraise ValueError(f\"The value of 'multiclass_strategy' must be 'ovo' or 'ovr' (not {multiclass_strategy})\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif svm_params is None:\nsvm_params = {}\nself.svm = SVC(**svm_params)\nself.multiclass_strategy = multiclass_strategy\n# Can't do multi-labels / multi-classes\nif not self.multi_label:\n# If not multi-classes : no impact\nif multiclass_strategy == 'ovr':\nself.pipeline = Pipeline([('svm', OneVsRestClassifier(self.svm))])\nelif multiclass_strategy == 'ovo':\nself.pipeline = Pipeline([('svm', OneVsOneClassifier(self.svm))])\nelse:\nself.pipeline = Pipeline([('svm', self.svm)])\n# SVC does not natively support multi-labels\nif self.multi_label:\nself.pipeline = Pipeline([('svm', MultiOutputClassifier(self.svm))])\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/#template_num.models_training.classifiers.models_sklearn.model_svm_classifier.ModelSVMClassifier.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - /! SVC CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /! -     (in truth, you could use probability = True in the definition of the SVC but the probabilities are 'inconsistent' with the predictions)</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_svm_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set\n    - /!\\\\ SVC CLASSIFIER DOESN'T RETURN PROBABILITIES, THE MODEL WILL GIVES 0 AND 1 AS PROBABILITIES /!\\\\ -\n        (in truth, you could use probability = True in the definition of the SVC but the probabilities are 'inconsistent' with the predictions)\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# We check input format\nx_test, _ = self._check_input_format(x_test)\nif not self.multi_label:\npreds = self.pipeline.predict(x_test)\n# Format ['a', 'b', 'c', 'a', ..., 'b']\n# Transform to \"proba\"\ntransform_dict = {col: [0. if _ != i else 1. for _ in range(len(self.list_classes))] for i, col in enumerate(self.list_classes)}\nprobas = np.array([transform_dict[x] for x in preds])\nelse:\npreds = self.pipeline.predict(x_test)\n# Already right format, but in int !\nprobas = np.array([[float(_) for _ in x] for x in preds])\nreturn probas\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/#template_num.models_training.classifiers.models_sklearn.model_svm_classifier.ModelSVMClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_svm_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'multiclass_strategy']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Manage multi-labels or multi-classes\nif not self.multi_label and self.multiclass_strategy is None:\nself.svm = self.pipeline['svm']\nelse:\nself.svm = self.pipeline['svm'].estimator\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_sklearn/model_svm_classifier/#template_num.models_training.classifiers.models_sklearn.model_svm_classifier.ModelSVMClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_sklearn/model_svm_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['multiclass_strategy'] = self.multiclass_strategy\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/","title":"Models tensorflow","text":""},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/model_dense_classifier/","title":"Model dense classifier","text":""},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/model_dense_classifier/#template_num.models_training.classifiers.models_tensorflow.model_dense_classifier.ModelDenseClassifier","title":"<code>ModelDenseClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelKeras</code></p> <p>Dense model for classification</p> Source code in <code>template_num/models_training/classifiers/models_tensorflow/model_dense_classifier.py</code> <pre><code>class ModelDenseClassifier(ModelClassifierMixin, ModelKeras):\n'''Dense model for classification'''\n_default_name = 'model_dense_classifier'\ndef __init__(self, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)'''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\ndef _get_model(self) -&gt; Model:\n'''Gets a model structure - returns the instance model instead if already defined\n        Returns:\n            (Model): a Keras model\n        '''\n# Return model if already set\nif self.model is not None:\nreturn self.model\n# Get input/output dimensions\ninput_dim = len(self.x_col)\nnum_classes = len(self.list_classes)\n# Process\ninput_layer = Input(shape=(input_dim,))\nx = Dense(64, activation=None, kernel_initializer=\"he_uniform\")(input_layer)\nx = BatchNormalization(momentum=0.9)(x)\nx = ELU(alpha=1.0)(x)\nx = Dropout(0.2)(x)\nx = Dense(64, activation=None, kernel_initializer=\"he_uniform\")(x)\nx = BatchNormalization(momentum=0.9)(x)\nx = ELU(alpha=1.0)(x)\nx = Dropout(0.2)(x)\n# Last layer\nactivation = 'sigmoid' if self.multi_label else 'softmax'\nout = Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform')(x)\n# Set model\nmodel = Model(inputs=input_layer, outputs=[out])\n# Set optimizer\nlr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.001\ndecay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\nself.logger.info(f\"Learning rate: {lr}\")\nself.logger.info(f\"Decay: {decay}\")\noptimizer = Adam(lr=lr, decay=decay)\n# Set loss &amp; metrics\nloss = utils_deep_keras.f1_loss if self.multi_label else 'categorical_crossentropy'\nmetrics: List[Union[str, Callable]] = ['accuracy'] if not self.multi_label else ['categorical_accuracy', 'categorical_crossentropy', utils_deep_keras.f1, utils_deep_keras.precision, utils_deep_keras.recall, utils_deep_keras.f1_loss]  # type: ignore\n# Compile model\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nmodel.summary()\n# Try to save model as png if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._save_model_png(model)\n# Return\nreturn model\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience', 'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/model_dense_classifier/#template_num.models_training.classifiers.models_tensorflow.model_dense_classifier.ModelDenseClassifier.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)</p> Source code in <code>template_num/models_training/classifiers/models_tensorflow/model_dense_classifier.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)'''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/model_dense_classifier/#template_num.models_training.classifiers.models_tensorflow.model_dense_classifier.ModelDenseClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hdf5_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/classifiers/models_tensorflow/model_dense_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'list_classes', 'dict_classes', 'multi_label', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience', 'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/classifiers/models_tensorflow/model_dense_classifier/#template_num.models_training.classifiers.models_tensorflow.model_dense_classifier.ModelDenseClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/classifiers/models_tensorflow/model_dense_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/","title":"Regressors","text":""},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/","title":"Model aggregation regressor","text":""},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor","title":"<code>ModelAggregationRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelClass</code></p> <p>Model for aggregating several regressor models</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>class ModelAggregationRegressor(ModelRegressorMixin, ModelClass):\n'''Model for aggregating several regressor models'''\n_default_name = 'model_aggregation_regressor'\n_dict_aggregation_function = {'median_predict': median_predict,\n'mean_predict': mean_predict}\ndef __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'median_predict', **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n        Kwargs:\n            list_models (list) : The list of model to be aggregated\n            aggregation_function (Callable or str) : The aggregation function used\n        Raises:\n            ValueError : If the object list_model has other model than model regressor (model_aggregation_regressor is only compatible with model regressor)\n            ValueError : If the object aggregation_function is a str but not found in the dictionary dict_aggregation_function\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Get the aggregation function\nif isinstance(aggregation_function, str):\nif aggregation_function not in self._dict_aggregation_function.keys():\nraise ValueError(f\"The aggregation_function ({aggregation_function}) is not a valid option ({self._dict_aggregation_function.keys()})\")\naggregation_function = self._dict_aggregation_function[aggregation_function] # type: ignore\n# Manage aggregated models\nself.aggregation_function = aggregation_function\nself.sub_models = list_models # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n# Error: The classifier and regressor models cannot be combined in list_models\nif False in [isinstance(sub_model['model'], ModelRegressorMixin) for sub_model in self.sub_models]:\nraise ValueError(f\"model_aggregation_classifier only accepts classifier models\")\nself.trained = self._check_trained()\n# Set nb_fit to 1 if already trained\nif self.trained:\nself.nb_fit = 1\n@property\ndef aggregation_function(self):\n'''Getter for aggregation_function'''\nreturn self._aggregation_function\n@aggregation_function.setter\ndef aggregation_function(self, agg_function: Union[Callable, str]):\n'''Setter for aggregation_function\n        If a string, try to match a predefined function\n        Raises:\n            ValueError: If the object aggregation_function is a str but not found in the dictionary of predefined aggregation functions\n        '''\n# Retrieve aggregation function from dict if a string\nif isinstance(agg_function, str):\n# Get infos\nif agg_function not in self._dict_aggregation_function.keys():\nraise ValueError(f\"The aggregation_function ({agg_function}) is not a valid option (must be chosen in {self._dict_aggregation_function.keys()})\")\nagg_function = self._dict_aggregation_function[agg_function]\n# Apply checks\nself._aggregation_function = agg_function\n@aggregation_function.deleter\ndef aggregation_function(self):\n'''Deleter for aggregation_function'''\nself._aggregation_function = None\n@property\ndef sub_models(self):\n'''Getter for sub_models'''\nreturn self._sub_models\n@sub_models.setter\ndef sub_models(self, list_models: Union[list, None] = None):\n'''Setter for sub_models\n        Kwargs:\n            list_models (list) : The list of models to be aggregated\n        '''\nlist_models = [] if list_models is None else list_models\nsub_models = []  # Init list of models\nfor model in list_models:\n# If a string (a model name), reload it\nif isinstance(model, str):\nreal_model, _ = utils_models.load_model(model)\ndict_model = {'name': model, 'model': real_model}\nelse:\ndict_model = {'name': os.path.split(model.model_dir)[-1], 'model': model}\nsub_models.append(dict_model.copy())\nself._sub_models = sub_models.copy()\n@sub_models.deleter\ndef sub_models(self):\n'''Deleter for sub_models'''\nself._sub_models = None\ndef _check_trained(self) -&gt; bool:\n'''Checks various attributes related to the fitting of underlying models\n        Returns:\n            bool: is the aggregation model is considered fitted\n        '''\n# Check fitted\nmodels_trained = {sub_model['model'].trained for sub_model in self.sub_models}\nif len(models_trained) &gt; 0 and all(models_trained):\n# All models trained\ntrained = True\n# No model or not fitted\nelse:\ntrained = False\nreturn trained\ndef fit(self, x_train, y_train, **kwargs) -&gt; None:\n'''Trains the model\n           **kwargs enables Keras model compatibility.\n        Args:\n            x_train (?): Array-like, shape = [n_samples]\n            y_train (?): Array-like, shape = [n_samples]\n        '''\n# We check input format\nx_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n# Fit each model\nfor sub_model in self.sub_models:\nif not sub_model['model'].trained:\nsub_model['model'].fit(x_train, y_train, **kwargs)\n# Set nb_fit to 1 if not already trained\nif not self.trained:\nself.nb_fit = 1\nself.trained = self._check_trained()\n@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Prediction\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n            return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n        Returns:\n            (np.ndarray): Array of shape = [n_samples]\n        Raises:\n            ValueError: If return_proba=True\n        '''\nif return_proba:\nraise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\npreds = self._predict_sub_models(x_test, **kwargs)\nreturn np.array([self.aggregation_function(array) for array in preds]) # type: ignore\n@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; None:\n'''Predicts the probabilities on the test set - raise ValueError\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Raises:\n            ValueError: Models of type regressor do not implement the method predict_proba\n        '''\nraise ValueError(f\"Models of type regressor do not implement the method predict_proba\")\n@utils.trained_needed\ndef _predict_sub_models(self, x_test, **kwargs) -&gt; np.ndarray:\n'''Recover the predictions of each model being aggregated\n        Args:\n            x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        Returns:\n            (np.ndarray): array of shape = [n_samples, nb_model]\n        '''\narray_predict = np.array([sub_model['model'].predict(x_test) for sub_model in self.sub_models])\narray_predict = np.transpose(array_predict, (1, 0))\nreturn array_predict\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\nif json_data is None:\njson_data = {}\n# Specific aggregation - save some wanted entries\ntrain_keys = ['filename', 'filename_valid', 'preprocess_str']\ndefault_json_data = {key: json_data.get(key, None) for key in train_keys}\ndefault_json_data['aggregator_dir'] = self.model_dir\n# Save each trained and unsaved model\nfor sub_model in self.sub_models:\npath_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\nif os.path.exists(path_config):\nwith open(path_config, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\ntrained = configs.get('trained', False)\nif not trained:\nsub_model['model'].save(default_json_data)\nelse:\nsub_model['model'].save(default_json_data)\njson_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\naggregation_function = self.aggregation_function\n# Save aggregation_function if not None &amp; level_save &gt; LOW\nif (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\naggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n# Save as pickle\nwith open(aggregation_function_path, 'wb') as f:\npickle.dump(self.aggregation_function, f)\n# Save\nmodels_list = [sub_model['name'] for sub_model in self.sub_models]\ndelattr(self, \"sub_models\")\ndelattr(self, \"aggregation_function\")\nsuper().save(json_data=json_data)\nsetattr(self, \"aggregation_function\", aggregation_function)\nsetattr(self, \"sub_models\", models_list)\n# Add message in model_upload_instructions.md\nmd_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\nline = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\nself.prepend_line(md_path, line)\ndef prepend_line(self, file_name: str, line: str) -&gt; None:\n''' Insert given string as a new line at the beginning of a file\n        Kwargs:\n            file_name (str): Path to file\n            line (str): line to insert\n        '''\nwith open(file_name, 'r+') as f:\nlines = f.readlines()\nlines.insert(0, line)\nf.seek(0)\nf.writelines(lines)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model aggregation from its configuration and \"standalones\" files\n            Reloads list model from \"list_models\" files\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n            aggregation_function_path (str): Path to aggregation_function_path\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If preprocess_pipeline_path is None\n            ValueError: If aggregation_function_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n            FileNotFoundError: If the object aggregation_function_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\naggregation_function_path = kwargs.get('aggregation_function_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif aggregation_function_path is None:\nraise ValueError(\"The argument aggregation_function_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\nif not os.path.exists(aggregation_function_path):\nraise FileNotFoundError(f\"The file {aggregation_function_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n# Reload aggregation_function_path\nwith open(aggregation_function_path, 'rb') as f:\nself.aggregation_function = pickle.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\nself.sub_models = configs.get('list_models_name', [])  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col', 'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.aggregation_function","title":"<code>aggregation_function</code>  <code>deletable</code> <code>writable</code> <code>property</code>","text":"<p>Getter for aggregation_function</p>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.sub_models","title":"<code>sub_models</code>  <code>deletable</code> <code>writable</code> <code>property</code>","text":"<p>Getter for sub_models</p>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.__init__","title":"<code>__init__(list_models=None, aggregation_function='median_predict', **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>list_models (list) : The list of model to be aggregated aggregation_function (Callable or str) : The aggregation function used</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object list_model has other model than model regressor (model_aggregation_regressor is only compatible with model regressor)</p> <code>ValueError</code> <p>If the object aggregation_function is a str but not found in the dictionary dict_aggregation_function</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def __init__(self, list_models: Union[list, None] = None, aggregation_function: Union[Callable, str] = 'median_predict', **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n    Kwargs:\n        list_models (list) : The list of model to be aggregated\n        aggregation_function (Callable or str) : The aggregation function used\n    Raises:\n        ValueError : If the object list_model has other model than model regressor (model_aggregation_regressor is only compatible with model regressor)\n        ValueError : If the object aggregation_function is a str but not found in the dictionary dict_aggregation_function\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Get the aggregation function\nif isinstance(aggregation_function, str):\nif aggregation_function not in self._dict_aggregation_function.keys():\nraise ValueError(f\"The aggregation_function ({aggregation_function}) is not a valid option ({self._dict_aggregation_function.keys()})\")\naggregation_function = self._dict_aggregation_function[aggregation_function] # type: ignore\n# Manage aggregated models\nself.aggregation_function = aggregation_function\nself.sub_models = list_models # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n# Error: The classifier and regressor models cannot be combined in list_models\nif False in [isinstance(sub_model['model'], ModelRegressorMixin) for sub_model in self.sub_models]:\nraise ValueError(f\"model_aggregation_classifier only accepts classifier models\")\nself.trained = self._check_trained()\n# Set nb_fit to 1 if already trained\nif self.trained:\nself.nb_fit = 1\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.fit","title":"<code>fit(x_train, y_train, **kwargs)</code>","text":"<p>Trains the model    **kwargs enables Keras model compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples]</p> required Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def fit(self, x_train, y_train, **kwargs) -&gt; None:\n'''Trains the model\n       **kwargs enables Keras model compatibility.\n    Args:\n        x_train (?): Array-like, shape = [n_samples]\n        y_train (?): Array-like, shape = [n_samples]\n    '''\n# We check input format\nx_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n# Fit each model\nfor sub_model in self.sub_models:\nif not sub_model['model'].trained:\nsub_model['model'].fit(x_train, y_train, **kwargs)\n# Set nb_fit to 1 if not already trained\nif not self.trained:\nself.nb_fit = 1\nself.trained = self._check_trained()\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Prediction</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <code>return_proba</code> <code>bool</code> <p>If the function should return the probabilities instead of the classes (Keras compatibility)</p> <code>False</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array of shape = [n_samples]</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If return_proba=True</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Prediction\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n        return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n    Returns:\n        (np.ndarray): Array of shape = [n_samples]\n    Raises:\n        ValueError: If return_proba=True\n    '''\nif return_proba:\nraise ValueError(f\"Models of the type {self.model_type} can't handle probabilities\")\npreds = self._predict_sub_models(x_test, **kwargs)\nreturn np.array([self.aggregation_function(array) for array in preds]) # type: ignore\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set - raise ValueError</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>?</code> <p>array-like or sparse matrix of shape = [n_samples, n_features]</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Models of type regressor do not implement the method predict_proba</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test, **kwargs) -&gt; None:\n'''Predicts the probabilities on the test set - raise ValueError\n    Args:\n        x_test (?): array-like or sparse matrix of shape = [n_samples, n_features]\n    Raises:\n        ValueError: Models of type regressor do not implement the method predict_proba\n    '''\nraise ValueError(f\"Models of type regressor do not implement the method predict_proba\")\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.prepend_line","title":"<code>prepend_line(file_name, line)</code>","text":"<p>Insert given string as a new line at the beginning of a file</p> Kwargs <p>file_name (str): Path to file line (str): line to insert</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def prepend_line(self, file_name: str, line: str) -&gt; None:\n''' Insert given string as a new line at the beginning of a file\n    Kwargs:\n        file_name (str): Path to file\n        line (str): line to insert\n    '''\nwith open(file_name, 'r+') as f:\nlines = f.readlines()\nlines.insert(0, line)\nf.seek(0)\nf.writelines(lines)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model aggregation from its configuration and \"standalones\" files     Reloads list model from \"list_models\" files</p> Kwargs <p>configuration_path (str): Path to configuration file preprocess_pipeline_path (str): Path to preprocess pipeline aggregation_function_path (str): Path to aggregation_function_path</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>ValueError</code> <p>If aggregation_function_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object aggregation_function_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model aggregation from its configuration and \"standalones\" files\n        Reloads list model from \"list_models\" files\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n        aggregation_function_path (str): Path to aggregation_function_path\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If preprocess_pipeline_path is None\n        ValueError: If aggregation_function_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        FileNotFoundError: If the object aggregation_function_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\naggregation_function_path = kwargs.get('aggregation_function_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif aggregation_function_path is None:\nraise ValueError(\"The argument aggregation_function_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\nif not os.path.exists(aggregation_function_path):\nraise FileNotFoundError(f\"The file {aggregation_function_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n# Reload aggregation_function_path\nwith open(aggregation_function_path, 'rb') as f:\nself.aggregation_function = pickle.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\nself.sub_models = configs.get('list_models_name', [])  # Transform the list into a list of dictionnaries [{'name': xxx, 'model': xxx}, ...]\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['x_col', 'y_col', 'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.ModelAggregationRegressor.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\nif json_data is None:\njson_data = {}\n# Specific aggregation - save some wanted entries\ntrain_keys = ['filename', 'filename_valid', 'preprocess_str']\ndefault_json_data = {key: json_data.get(key, None) for key in train_keys}\ndefault_json_data['aggregator_dir'] = self.model_dir\n# Save each trained and unsaved model\nfor sub_model in self.sub_models:\npath_config = os.path.join(sub_model['model'].model_dir, 'configurations.json')\nif os.path.exists(path_config):\nwith open(path_config, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\ntrained = configs.get('trained', False)\nif not trained:\nsub_model['model'].save(default_json_data)\nelse:\nsub_model['model'].save(default_json_data)\njson_data['list_models_name'] = [sub_model['name'] for sub_model in self.sub_models]\naggregation_function = self.aggregation_function\n# Save aggregation_function if not None &amp; level_save &gt; LOW\nif (self.aggregation_function is not None) and (self.level_save in ['MEDIUM', 'HIGH']):\n# Manage paths\naggregation_function_path = os.path.join(self.model_dir, \"aggregation_function.pkl\")\n# Save as pickle\nwith open(aggregation_function_path, 'wb') as f:\npickle.dump(self.aggregation_function, f)\n# Save\nmodels_list = [sub_model['name'] for sub_model in self.sub_models]\ndelattr(self, \"sub_models\")\ndelattr(self, \"aggregation_function\")\nsuper().save(json_data=json_data)\nsetattr(self, \"aggregation_function\", aggregation_function)\nsetattr(self, \"sub_models\", models_list)\n# Add message in model_upload_instructions.md\nmd_path = os.path.join(self.model_dir, f\"model_upload_instructions.md\")\nline = \"/!\\\\/!\\\\/!\\\\/!\\\\/!\\\\   The aggregation model is a special model, please ensure that all sub-models and the aggregation model are manually saved together in order to be able to load it  /!\\\\/!\\\\/!\\\\/!\\\\/!\\\\ \\n\"\nself.prepend_line(md_path, line)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.mean_predict","title":"<code>mean_predict(predictions)</code>","text":"<p>Returns the mean of predictions of each model</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray) </code> <p>The array containing the predictions of each models (shape (n_models))</p> required Return <p>(np.float64) : The mean of the predictions</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def mean_predict(predictions: np.ndarray) -&gt; np.float64:\n'''Returns the mean of predictions of each model\n    Args:\n        predictions (np.ndarray) : The array containing the predictions of each models (shape (n_models))\n    Return:\n        (np.float64) : The mean of the predictions\n    '''\nreturn np.mean(predictions)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_aggregation_regressor/#template_num.models_training.regressors.model_aggregation_regressor.median_predict","title":"<code>median_predict(predictions)</code>","text":"<p>Returns the median of the predictions of each model</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>np.ndarray) </code> <p>The array containing the predictions of each models (shape (n_models))</p> required Return <p>(np.float64) : The median of the predictions</p> Source code in <code>template_num/models_training/regressors/model_aggregation_regressor.py</code> <pre><code>def median_predict(predictions: np.ndarray) -&gt; np.float64:\n'''Returns the median of the predictions of each model\n    Args:\n        predictions (np.ndarray) : The array containing the predictions of each models (shape (n_models))\n    Return:\n        (np.float64) : The median of the predictions\n    '''\nreturn np.median(predictions)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/","title":"Model regressor","text":""},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin","title":"<code>ModelRegressorMixin</code>","text":"<p>Parent class (Mixin) for regressor models</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>class ModelRegressorMixin:\n'''Parent class (Mixin) for regressor models'''\ndef __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n'''Initialization of the class\n        Kwargs:\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOW + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n        Raises:\n             ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        '''\nsuper().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type\nself.model_type = 'regressor'\n# TODO: add multi-outputs !\n# Other options\nself.level_save = level_save\ndef inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Identity function - Manages compatibility with classifiers\n        Args:\n            y (list | np.ndarray): Array-like, shape = [n_samples, 1]\n        Returns:\n            (np.ndarray): List, shape = [n_samples, 1]\n        '''\nreturn list(y) if isinstance(y, np.ndarray) else y\ndef get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\nseries_to_add: Union[List[pd.Series], None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n        Args:\n            y_true (?): Array-like, shape = [n_samples,]\n            y_pred (?): Array-like, shape = [n_samples,]\n        Kwargs:\n            df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n            series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing the statistics\n        '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Save a predictionn file if wanted\nif self.level_save == 'HIGH':\n# Inverse transform\ny_true_df = list(self.inverse_transform(y_true))\ny_pred_df = list(self.inverse_transform(y_pred))\n# Concat in a dataframe\nif df_x is not None:\ndf = df_x.copy()\ndf['y_true'] = y_true_df\ndf['y_pred'] = y_pred_df\nelse:\ndf = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n# Add column abs_err\ndf.loc[:, 'abs_err'] = df[['y_true', 'y_pred']].apply(lambda x: x.y_true - x.y_pred, axis=1)\n# Add column rel_err\ndf.loc[:, 'rel_err'] = df[['y_true', 'y_pred']].apply(lambda x: (x.y_true - x.y_pred) / abs(x.y_true), axis=1)\n# Add some more columns\nif series_to_add is not None:\nfor ser in series_to_add:\ndf[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex correctly\n# Save predictions\nfile_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\ndf.sort_values('abs_err', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n# Get global metrics\nmetric_mae = mean_absolute_error(y_true, y_pred)\nmetric_mse = mean_squared_error(y_true, y_pred)\nmetric_rmse = mean_squared_error(y_true, y_pred, squared=False)\nmetric_explained_variance_score = explained_variance_score(y_true, y_pred)\nmetric_r2 = r2_score(y_true, y_pred)\n# Global statistics\nself.logger.info('-- * * * * * * * * * * * * * * --')\nself.logger.info(f\"Statistics{' ' + type_data if len(type_data) &gt; 0 else ''}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"MAE : {round(metric_mae, 5)}\")\nself.logger.info(f\"MSE : {round(metric_mse, 5)}\")\nself.logger.info(f\"RMSE : {round(metric_rmse, 5)}\")\nself.logger.info(f\"Explained variance : {round(metric_explained_variance_score, 5)}\")\nself.logger.info(f\"R\u00b2 (coefficient of determination) : {round(metric_r2, 5)}\")\nself.logger.info('--------------------------------')\n# Metrics file\ndf_stats = pd.DataFrame(columns=['Label', 'MAE', 'MSE',\n'RMSE', 'Explained variance',\n'Coefficient of determination'])\n# TODO : add multi-outputs and stats for each output\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'MAE': metric_mae,\n'MSE': metric_mse,\n'RMSE': metric_rmse,\n'Explained variance': metric_explained_variance_score,\n'Coefficient of determination': metric_r2,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Save .csv\nfile_path = os.path.join(self.model_dir, f\"mae{'_' + type_data if len(type_data) &gt; 0 else ''}@{metric_mae}.csv\")\ndf_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n# Save some metrics\nmae_path = os.path.join(self.model_dir, f\"mae{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_mae, 5)}\")\nwith open(mae_path, 'w'):\npass\nmse_path = os.path.join(self.model_dir, f\"mse{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_mse, 5)}\")\nwith open(mse_path, 'w'):\npass\nrmse_path = os.path.join(self.model_dir, f\"rmse{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_rmse, 5)}\")\nwith open(rmse_path, 'w'):\npass\nexplained_variance_path = os.path.join(self.model_dir, f\"explained_variance{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_explained_variance_score, 5)}\")\nwith open(explained_variance_path, 'w'):\npass\nr2_path = os.path.join(self.model_dir, f\"r2{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_r2, 5)}\")\nwith open(r2_path, 'w'):\npass\n# Plots\nif self.level_save in ['MEDIUM', 'HIGH']:\n# TODO: put a condition on the maximum number of points ?\nis_train = True if type_data == 'train' else False\nif is_train:\nself.plot_prediction_errors(y_true_train=y_true, y_pred_train=y_pred,\ny_true_test=None, y_pred_test=None,\ntype_data=type_data)\nself.plot_residuals(y_true_train=y_true, y_pred_train=y_pred,\ny_true_test=None, y_pred_test=None,\ntype_data=type_data)\nelse:\nself.plot_prediction_errors(y_true_train=None, y_pred_train=None,\ny_true_test=y_true, y_pred_test=y_pred,\ntype_data=type_data)\nself.plot_residuals(y_true_train=None, y_pred_train=None,\ny_true_test=y_true, y_pred_test=y_pred,\ntype_data=type_data)\n# Return metrics\nreturn df_stats\ndef get_metrics_simple(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on predictions (single-output for now)\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n        Args:\n            y_true (?): Array-like, shape = [n_samples]\n            y_pred (?): Array-like, shape = [n_samples]\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Get global metrics:\nmetric_mae = mean_absolute_error(y_true, y_pred)\nmetric_mse = mean_squared_error(y_true, y_pred)\nmetric_rmse = mean_squared_error(y_true, y_pred, squared=False)\nmetric_explained_variance_score = explained_variance_score(y_true, y_pred)\nmetric_r2 = r2_score(y_true, y_pred)\n# Metrics file\ndf_stats = pd.DataFrame(columns=['Label', 'MAE', 'MSE',\n'RMSE', 'Explained variance',\n'Coefficient of determination'])\n# TODO : add multi-outputs and stats for each output\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'MAE': metric_mae,\n'MSE': metric_mse,\n'RMSE': metric_rmse,\n'Explained variance': metric_explained_variance_score,\n'Coefficient of determination': metric_r2,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Return dataframe\nreturn df_stats\ndef plot_prediction_errors(self, y_true_train: Union[np.ndarray, None] = None, y_pred_train: Union[np.ndarray, None] = None,\ny_true_test: Union[np.ndarray, None] = None, y_pred_test: Union[np.ndarray, None] = None,\ntype_data: str = '') -&gt; None:\n'''Plots prediction errors\n        We use yellowbrick for the plots + a trick to be model agnostic\n        Kwargs:\n            y_true_train (np.ndarray): Array-like, shape = [n_samples]\n            y_pred_train (np.ndarray): Array-like, shape = [n_samples]\n            y_true_test (np.ndarray): Array-like, shape = [n_samples]\n            y_pred_test (np.ndarray): Array-like, shape = [n_samples]\n            type_data (str): Type of the dataset (validation, test, ...)\n        Raises:\n            ValueError: If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)\n        '''\nif (y_true_train is not None and y_pred_train is None) or (y_true_train is None and y_pred_train is not None):\nraise ValueError('\"true\" and \"pred\" must both be given, or not at all - train')\nif (y_true_test is not None and y_pred_test is None) or (y_true_test is None and y_pred_test is not None):\nraise ValueError('\"true\" and \"pred\" must both be given, or not at all - test')\n# Get figure &amp; ax\nfig, ax = plt.subplots(figsize=(12, 10))\n# Set visualizer\nvisualizer = PredictionError(LinearRegression(), ax=ax, bestfit=False, is_fitted=True)  # Trick model not used\nvisualizer.name = self.model_name\n# PredictionError does not support train and test at the same time :'(\n# Train\nif y_true_train is not None:\nvisualizer.score_ = r2_score(y_true_train, y_pred_train)\nvisualizer.draw(y_true_train, y_pred_train)\n# Test\nif y_true_test is not None:\nvisualizer.score_ = r2_score(y_true_test, y_pred_test)\nvisualizer.draw(y_true_test, y_pred_test)\n# Save\nplots_path = os.path.join(self.model_dir, 'plots')\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\nfile_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}errors.png\"\nvisualizer.show(outpath=os.path.join(plots_path, file_name))\n# Close figures\nplt.close('all')\ndef plot_residuals(self, y_true_train: Union[np.ndarray, None] = None, y_pred_train: Union[np.ndarray, None] = None,\ny_true_test: Union[np.ndarray, None] = None, y_pred_test: Union[np.ndarray, None] = None,\ntype_data: str = '') -&gt; None:\n'''Plots the \"residuals\" from the predictions\n        Uses yellowbrick for the plots plus a trick in order to be model agnostic\n        Kwargs:\n            y_true_train (np.ndarray): Array-like, shape = [n_samples]\n            y_pred_train (np.ndarray): Array-like, shape = [n_samples]\n            y_true_test (np.ndarray): Array-like, shape = [n_samples]\n            y_pred_test (np.ndarray): Array-like, shape = [n_samples]\n            type_data (str): Type of the dataset (validation, test, ...)\n        Raises:\n            ValueError: If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)\n        '''\nif (y_true_train is not None and y_pred_train is None) or (y_true_train is None and y_pred_train is not None):\nraise ValueError('\"true\" and \"pred\" must both be given, or not at all - train')\nif (y_true_test is not None and y_pred_test is None) or (y_true_test is None and y_pred_test is not None):\nraise ValueError('\"true\" and \"pred\" must both be given, or not at all - test')\n# Get figure &amp; ax\nfig, ax = plt.subplots(figsize=(12, 10))\n# Set visualizer\nvisualizer = ResidualsPlot(LinearRegression(), ax=ax, is_fitted=True)  # Trick model not used\nvisualizer.name = self.model_name\n# Train\nif y_true_train is not None:\nvisualizer.train_score_ = r2_score(y_true_train, y_pred_train)\nresiduals = y_pred_train - y_true_train\nvisualizer.draw(y_pred_train, residuals, train=True)\n# Test\nif y_true_test is not None:\nvisualizer.test_score_ = r2_score(y_true_test, y_pred_test)\nresiduals = y_pred_test - y_true_test\nvisualizer.draw(y_pred_test, residuals, train=False)\n# Save\nplots_path = os.path.join(self.model_dir, 'plots')\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\nfile_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}residuals.png\"\nvisualizer.show(outpath=os.path.join(plots_path, file_name))\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.__init__","title":"<code>__init__(level_save='HIGH', **kwargs)</code>","text":"<p>Initialization of the class</p> Kwargs <p>level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOW + hdf5 + pkl + plots     HIGH: MEDIUM + predictions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n'''Initialization of the class\n    Kwargs:\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOW + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n    Raises:\n         ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n    '''\nsuper().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type\nself.model_type = 'regressor'\n# TODO: add multi-outputs !\n# Other options\nself.level_save = level_save\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, df_x=None, series_to_add=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required Kwargs <p>df_x (pd.DataFrame or None): Input dataFrame used for the prediction series_to_add (list): List of pd.Series to add to the dataframe type_data (str): Type of dataset (validation, test, ...) <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing the statistics</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, df_x: Union[pd.DataFrame, None] = None,\nseries_to_add: Union[List[pd.Series], None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n    Args:\n        y_true (?): Array-like, shape = [n_samples,]\n        y_pred (?): Array-like, shape = [n_samples,]\n    Kwargs:\n        df_x (pd.DataFrame or None): Input dataFrame used for the prediction\n        series_to_add (list&lt;pd.Series&gt;): List of pd.Series to add to the dataframe\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing the statistics\n    '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Save a predictionn file if wanted\nif self.level_save == 'HIGH':\n# Inverse transform\ny_true_df = list(self.inverse_transform(y_true))\ny_pred_df = list(self.inverse_transform(y_pred))\n# Concat in a dataframe\nif df_x is not None:\ndf = df_x.copy()\ndf['y_true'] = y_true_df\ndf['y_pred'] = y_pred_df\nelse:\ndf = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n# Add column abs_err\ndf.loc[:, 'abs_err'] = df[['y_true', 'y_pred']].apply(lambda x: x.y_true - x.y_pred, axis=1)\n# Add column rel_err\ndf.loc[:, 'rel_err'] = df[['y_true', 'y_pred']].apply(lambda x: (x.y_true - x.y_pred) / abs(x.y_true), axis=1)\n# Add some more columns\nif series_to_add is not None:\nfor ser in series_to_add:\ndf[ser.name] = ser.reset_index(drop=True).reindex(index=df.index)  # Reindex correctly\n# Save predictions\nfile_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\ndf.sort_values('abs_err', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n# Get global metrics\nmetric_mae = mean_absolute_error(y_true, y_pred)\nmetric_mse = mean_squared_error(y_true, y_pred)\nmetric_rmse = mean_squared_error(y_true, y_pred, squared=False)\nmetric_explained_variance_score = explained_variance_score(y_true, y_pred)\nmetric_r2 = r2_score(y_true, y_pred)\n# Global statistics\nself.logger.info('-- * * * * * * * * * * * * * * --')\nself.logger.info(f\"Statistics{' ' + type_data if len(type_data) &gt; 0 else ''}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"MAE : {round(metric_mae, 5)}\")\nself.logger.info(f\"MSE : {round(metric_mse, 5)}\")\nself.logger.info(f\"RMSE : {round(metric_rmse, 5)}\")\nself.logger.info(f\"Explained variance : {round(metric_explained_variance_score, 5)}\")\nself.logger.info(f\"R\u00b2 (coefficient of determination) : {round(metric_r2, 5)}\")\nself.logger.info('--------------------------------')\n# Metrics file\ndf_stats = pd.DataFrame(columns=['Label', 'MAE', 'MSE',\n'RMSE', 'Explained variance',\n'Coefficient of determination'])\n# TODO : add multi-outputs and stats for each output\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'MAE': metric_mae,\n'MSE': metric_mse,\n'RMSE': metric_rmse,\n'Explained variance': metric_explained_variance_score,\n'Coefficient of determination': metric_r2,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Save .csv\nfile_path = os.path.join(self.model_dir, f\"mae{'_' + type_data if len(type_data) &gt; 0 else ''}@{metric_mae}.csv\")\ndf_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n# Save some metrics\nmae_path = os.path.join(self.model_dir, f\"mae{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_mae, 5)}\")\nwith open(mae_path, 'w'):\npass\nmse_path = os.path.join(self.model_dir, f\"mse{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_mse, 5)}\")\nwith open(mse_path, 'w'):\npass\nrmse_path = os.path.join(self.model_dir, f\"rmse{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_rmse, 5)}\")\nwith open(rmse_path, 'w'):\npass\nexplained_variance_path = os.path.join(self.model_dir, f\"explained_variance{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_explained_variance_score, 5)}\")\nwith open(explained_variance_path, 'w'):\npass\nr2_path = os.path.join(self.model_dir, f\"r2{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(metric_r2, 5)}\")\nwith open(r2_path, 'w'):\npass\n# Plots\nif self.level_save in ['MEDIUM', 'HIGH']:\n# TODO: put a condition on the maximum number of points ?\nis_train = True if type_data == 'train' else False\nif is_train:\nself.plot_prediction_errors(y_true_train=y_true, y_pred_train=y_pred,\ny_true_test=None, y_pred_test=None,\ntype_data=type_data)\nself.plot_residuals(y_true_train=y_true, y_pred_train=y_pred,\ny_true_test=None, y_pred_test=None,\ntype_data=type_data)\nelse:\nself.plot_prediction_errors(y_true_train=None, y_pred_train=None,\ny_true_test=y_true, y_pred_test=y_pred,\ntype_data=type_data)\nself.plot_residuals(y_true_train=None, y_pred_train=None,\ny_true_test=y_true, y_pred_test=y_pred,\ntype_data=type_data)\n# Return metrics\nreturn df_stats\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.get_metrics_simple","title":"<code>get_metrics_simple(y_true, y_pred)</code>","text":"<p>Gets metrics on predictions (single-output for now) Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples]</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def get_metrics_simple(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on predictions (single-output for now)\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n    Args:\n        y_true (?): Array-like, shape = [n_samples]\n        y_pred (?): Array-like, shape = [n_samples]\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Get global metrics:\nmetric_mae = mean_absolute_error(y_true, y_pred)\nmetric_mse = mean_squared_error(y_true, y_pred)\nmetric_rmse = mean_squared_error(y_true, y_pred, squared=False)\nmetric_explained_variance_score = explained_variance_score(y_true, y_pred)\nmetric_r2 = r2_score(y_true, y_pred)\n# Metrics file\ndf_stats = pd.DataFrame(columns=['Label', 'MAE', 'MSE',\n'RMSE', 'Explained variance',\n'Coefficient of determination'])\n# TODO : add multi-outputs and stats for each output\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'MAE': metric_mae,\n'MSE': metric_mse,\n'RMSE': metric_rmse,\n'Explained variance': metric_explained_variance_score,\n'Coefficient of determination': metric_r2,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Return dataframe\nreturn df_stats\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Identity function - Manages compatibility with classifiers</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>list | np.ndarray</code> <p>Array-like, shape = [n_samples, 1]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>List, shape = [n_samples, 1]</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Identity function - Manages compatibility with classifiers\n    Args:\n        y (list | np.ndarray): Array-like, shape = [n_samples, 1]\n    Returns:\n        (np.ndarray): List, shape = [n_samples, 1]\n    '''\nreturn list(y) if isinstance(y, np.ndarray) else y\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.plot_prediction_errors","title":"<code>plot_prediction_errors(y_true_train=None, y_pred_train=None, y_true_test=None, y_pred_test=None, type_data='')</code>","text":"<p>Plots prediction errors</p> <p>We use yellowbrick for the plots + a trick to be model agnostic</p> Kwargs <p>y_true_train (np.ndarray): Array-like, shape = [n_samples] y_pred_train (np.ndarray): Array-like, shape = [n_samples] y_true_test (np.ndarray): Array-like, shape = [n_samples] y_pred_test (np.ndarray): Array-like, shape = [n_samples] type_data (str): Type of the dataset (validation, test, ...)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def plot_prediction_errors(self, y_true_train: Union[np.ndarray, None] = None, y_pred_train: Union[np.ndarray, None] = None,\ny_true_test: Union[np.ndarray, None] = None, y_pred_test: Union[np.ndarray, None] = None,\ntype_data: str = '') -&gt; None:\n'''Plots prediction errors\n    We use yellowbrick for the plots + a trick to be model agnostic\n    Kwargs:\n        y_true_train (np.ndarray): Array-like, shape = [n_samples]\n        y_pred_train (np.ndarray): Array-like, shape = [n_samples]\n        y_true_test (np.ndarray): Array-like, shape = [n_samples]\n        y_pred_test (np.ndarray): Array-like, shape = [n_samples]\n        type_data (str): Type of the dataset (validation, test, ...)\n    Raises:\n        ValueError: If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)\n    '''\nif (y_true_train is not None and y_pred_train is None) or (y_true_train is None and y_pred_train is not None):\nraise ValueError('\"true\" and \"pred\" must both be given, or not at all - train')\nif (y_true_test is not None and y_pred_test is None) or (y_true_test is None and y_pred_test is not None):\nraise ValueError('\"true\" and \"pred\" must both be given, or not at all - test')\n# Get figure &amp; ax\nfig, ax = plt.subplots(figsize=(12, 10))\n# Set visualizer\nvisualizer = PredictionError(LinearRegression(), ax=ax, bestfit=False, is_fitted=True)  # Trick model not used\nvisualizer.name = self.model_name\n# PredictionError does not support train and test at the same time :'(\n# Train\nif y_true_train is not None:\nvisualizer.score_ = r2_score(y_true_train, y_pred_train)\nvisualizer.draw(y_true_train, y_pred_train)\n# Test\nif y_true_test is not None:\nvisualizer.score_ = r2_score(y_true_test, y_pred_test)\nvisualizer.draw(y_true_test, y_pred_test)\n# Save\nplots_path = os.path.join(self.model_dir, 'plots')\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\nfile_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}errors.png\"\nvisualizer.show(outpath=os.path.join(plots_path, file_name))\n# Close figures\nplt.close('all')\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_regressor/#template_num.models_training.regressors.model_regressor.ModelRegressorMixin.plot_residuals","title":"<code>plot_residuals(y_true_train=None, y_pred_train=None, y_true_test=None, y_pred_test=None, type_data='')</code>","text":"<p>Plots the \"residuals\" from the predictions</p> <p>Uses yellowbrick for the plots plus a trick in order to be model agnostic</p> Kwargs <p>y_true_train (np.ndarray): Array-like, shape = [n_samples] y_pred_train (np.ndarray): Array-like, shape = [n_samples] y_true_test (np.ndarray): Array-like, shape = [n_samples] y_pred_test (np.ndarray): Array-like, shape = [n_samples] type_data (str): Type of the dataset (validation, test, ...)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)</p> Source code in <code>template_num/models_training/regressors/model_regressor.py</code> <pre><code>def plot_residuals(self, y_true_train: Union[np.ndarray, None] = None, y_pred_train: Union[np.ndarray, None] = None,\ny_true_test: Union[np.ndarray, None] = None, y_pred_test: Union[np.ndarray, None] = None,\ntype_data: str = '') -&gt; None:\n'''Plots the \"residuals\" from the predictions\n    Uses yellowbrick for the plots plus a trick in order to be model agnostic\n    Kwargs:\n        y_true_train (np.ndarray): Array-like, shape = [n_samples]\n        y_pred_train (np.ndarray): Array-like, shape = [n_samples]\n        y_true_test (np.ndarray): Array-like, shape = [n_samples]\n        y_pred_test (np.ndarray): Array-like, shape = [n_samples]\n        type_data (str): Type of the dataset (validation, test, ...)\n    Raises:\n        ValueError: If a \"true\" is given, but not the corresponding \"pred\" (or vice-versa)\n    '''\nif (y_true_train is not None and y_pred_train is None) or (y_true_train is None and y_pred_train is not None):\nraise ValueError('\"true\" and \"pred\" must both be given, or not at all - train')\nif (y_true_test is not None and y_pred_test is None) or (y_true_test is None and y_pred_test is not None):\nraise ValueError('\"true\" and \"pred\" must both be given, or not at all - test')\n# Get figure &amp; ax\nfig, ax = plt.subplots(figsize=(12, 10))\n# Set visualizer\nvisualizer = ResidualsPlot(LinearRegression(), ax=ax, is_fitted=True)  # Trick model not used\nvisualizer.name = self.model_name\n# Train\nif y_true_train is not None:\nvisualizer.train_score_ = r2_score(y_true_train, y_pred_train)\nresiduals = y_pred_train - y_true_train\nvisualizer.draw(y_pred_train, residuals, train=True)\n# Test\nif y_true_test is not None:\nvisualizer.test_score_ = r2_score(y_true_test, y_pred_test)\nresiduals = y_pred_test - y_true_test\nvisualizer.draw(y_pred_test, residuals, train=False)\n# Save\nplots_path = os.path.join(self.model_dir, 'plots')\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\nfile_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}residuals.png\"\nvisualizer.show(outpath=os.path.join(plots_path, file_name))\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/","title":"Model xgboost regressor","text":""},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor","title":"<code>ModelXgboostRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelClass</code></p> <p>Xgboost model for regression</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>class ModelXgboostRegressor(ModelRegressorMixin, ModelClass):\n'''Xgboost model for regression'''\n_default_name = 'model_xgboost_regressor'\ndef __init__(self, xgboost_params: Union[dict, None] = None, early_stopping_rounds: int = 5, validation_split: float = 0.2, **kwargs) -&gt; None:\n'''Initialization of the class  (see ModelClass &amp; ModelRegressorMixin for more arguments)\n        Kwargs:\n            xgboost_params (dict): Parameters for the Xgboost\n                -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n            early_stopping_rounds (int): Number of rounds for early stopping\n            validation_split (float): Validation split fraction.\n                Only used if not validation dataset in the fit input\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Set parameters\nif xgboost_params is None:\nxgboost_params = {}\nself.xgboost_params = xgboost_params\nself.early_stopping_rounds = early_stopping_rounds\nself.validation_split = validation_split\n# Set objective (if not in params) &amp; init. model\nif 'objective' not in self.xgboost_params.keys():\nself.xgboost_params['objective'] = 'reg:squarederror'\n#  List of objectives https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\nself.model = XGBRegressor(**self.xgboost_params)\ndef fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Trains the model\n           **kwargs permits compatibility with Keras model\n        Args:\n            x_train (?): Array-like, shape = [n_samples, n_features]\n            y_train (?): Array-like, shape = [n_samples,]\n        Kwargs:\n            x_valid (?): Array-like, shape = [n_samples, n_features]\n            y_valid (?): Array-like, shape = [n_samples,]\n            with_shuffle (bool): If x, y must be shuffled before fitting\n        Raises:\n            RuntimeError: If the model is already fitted\n        '''\n# TODO: Check if we can continue the training of a xgboost\nif self.trained:\nself.logger.error(\"We can't train again a xgboost model\")\nself.logger.error(\"Please train a new model\")\nraise RuntimeError(\"We can't train again a xgboost model\")\n# We check input format\nx_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n# If there is a validation set, we also check the format (but fit_function to False)\nif y_valid is not None and x_valid is not None:\nx_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n# Otherwise, we do a random split\nelse:\nself.logger.warning(f\"Warning, no validation dataset. We split the training set (fraction valid = {self.validation_split})\")\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=self.validation_split)\n# Shuffle x, y if wanted\nif with_shuffle:\np = np.random.permutation(len(x_train))\nx_train = np.array(x_train)[p]\ny_train = np.array(y_train)[p]\n# Else still transform to numpy array\nelse:\nx_train = np.array(x_train)\ny_train = np.array(y_train)\n# Also get x_valid &amp; y_valid as numpy\nx_valid = np.array(x_valid)\ny_valid = np.array(y_valid)\n# Set eval set and train\neval_set = [(x_train, y_train), (x_valid, y_valid)]  # If there\u2019s more than one item in eval_set, the last entry will be used for early stopping.\nprior_objective = self.model.objective\nself.model.fit(x_train, y_train, eval_set=eval_set, early_stopping_rounds=self.early_stopping_rounds, verbose=True)\npost_objective = self.model.objective\nif prior_objective != post_objective:\nself.logger.warning(\"Warning: the objective function was automatically changed by XGBOOST\")\nself.logger.warning(f\"Before: {prior_objective}\")\nself.logger.warning(f\"After: {post_objective}\")\n# Set trained\nself.trained = True\nself.nb_fit += 1\n@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            return_proba (bool): Present for compatibility with other models. Raises an error if True\n        Raises:\n            ValueError: If return_proba is True\n        Returns:\n            (np.ndarray): Array, shape = [n_samples,]\n        '''\n# Manage errors\nif return_proba:\nraise ValueError(\"Models of type model_xgboost_regressor can't handle probabilities\")\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Warning, \"The method returns the model from the last iteration\"\n# But : \"Predict with X. If the model is trained with early stopping, then best_iteration is used automatically.\"\ny_pred = self.model.predict(x_test)\nreturn y_pred\n@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set. Here for compatibility\n        Args:\n            x_test (pd.DataFrame): Array-like, shape = [n_samples]\n        Raises:\n            ValueError: Model_xgboost_regressor does not implement predict_proba\n        Returns:\n            (np.ndarray): Array, shape = [n_samples,]\n        '''\n# For compatibility\nraise ValueError(\"Models of type model_xgboost_regressor do not implement the method predict_proba\")\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'xgboost'\njson_data['xgboost_params'] = self.xgboost_params\njson_data['early_stopping_rounds'] = self.early_stopping_rounds\njson_data['validation_split'] = self.validation_split\n# Save xgboost standalone\nif self.level_save in ['MEDIUM', 'HIGH']:\nif self.trained:\nsave_path = os.path.join(self.model_dir, 'xbgoost_standalone.model')\nself.model.save_model(save_path)\nelse:\nself.logger.warning(\"Can't save the XGboost in standalone because it hasn't been already fitted\")\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            xgboost_path (str): Path to standalone xgboost\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If xgboost_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object xgboost_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nxgboost_path = kwargs.get('xgboost_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif xgboost_path is None:\nraise ValueError(\"The argument xgboost_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(xgboost_path):\nraise FileNotFoundError(f\"The file {xgboost_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save', 'xgboost_params', 'early_stopping_rounds', 'validation_split']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload xgboost model\nself.model.load_model(xgboost_path)  # load data\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.__init__","title":"<code>__init__(xgboost_params=None, early_stopping_rounds=5, validation_split=0.2, **kwargs)</code>","text":"<p>Initialization of the class  (see ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>xgboost_params (dict): Parameters for the Xgboost     -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor early_stopping_rounds (int): Number of rounds for early stopping validation_split (float): Validation split fraction.     Only used if not validation dataset in the fit input</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>def __init__(self, xgboost_params: Union[dict, None] = None, early_stopping_rounds: int = 5, validation_split: float = 0.2, **kwargs) -&gt; None:\n'''Initialization of the class  (see ModelClass &amp; ModelRegressorMixin for more arguments)\n    Kwargs:\n        xgboost_params (dict): Parameters for the Xgboost\n            -&gt; https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor\n        early_stopping_rounds (int): Number of rounds for early stopping\n        validation_split (float): Validation split fraction.\n            Only used if not validation dataset in the fit input\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Set parameters\nif xgboost_params is None:\nxgboost_params = {}\nself.xgboost_params = xgboost_params\nself.early_stopping_rounds = early_stopping_rounds\nself.validation_split = validation_split\n# Set objective (if not in params) &amp; init. model\nif 'objective' not in self.xgboost_params.keys():\nself.xgboost_params['objective'] = 'reg:squarederror'\n#  List of objectives https://xgboost.readthedocs.io/en/latest/parameter.html#learning-task-parameters\nself.model = XGBRegressor(**self.xgboost_params)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.fit","title":"<code>fit(x_train, y_train, x_valid=None, y_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Trains the model    **kwargs permits compatibility with Keras model</p> <p>Parameters:</p> Name Type Description Default <code>x_train</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y_train</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required Kwargs <p>x_valid (?): Array-like, shape = [n_samples, n_features] y_valid (?): Array-like, shape = [n_samples,] with_shuffle (bool): If x, y must be shuffled before fitting</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If the model is already fitted</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>def fit(self, x_train, y_train, x_valid=None, y_valid=None, with_shuffle: bool = True, **kwargs) -&gt; None:\n'''Trains the model\n       **kwargs permits compatibility with Keras model\n    Args:\n        x_train (?): Array-like, shape = [n_samples, n_features]\n        y_train (?): Array-like, shape = [n_samples,]\n    Kwargs:\n        x_valid (?): Array-like, shape = [n_samples, n_features]\n        y_valid (?): Array-like, shape = [n_samples,]\n        with_shuffle (bool): If x, y must be shuffled before fitting\n    Raises:\n        RuntimeError: If the model is already fitted\n    '''\n# TODO: Check if we can continue the training of a xgboost\nif self.trained:\nself.logger.error(\"We can't train again a xgboost model\")\nself.logger.error(\"Please train a new model\")\nraise RuntimeError(\"We can't train again a xgboost model\")\n# We check input format\nx_train, y_train = self._check_input_format(x_train, y_train, fit_function=True)\n# If there is a validation set, we also check the format (but fit_function to False)\nif y_valid is not None and x_valid is not None:\nx_valid, y_valid = self._check_input_format(x_valid, y_valid, fit_function=False)\n# Otherwise, we do a random split\nelse:\nself.logger.warning(f\"Warning, no validation dataset. We split the training set (fraction valid = {self.validation_split})\")\nx_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=self.validation_split)\n# Shuffle x, y if wanted\nif with_shuffle:\np = np.random.permutation(len(x_train))\nx_train = np.array(x_train)[p]\ny_train = np.array(y_train)[p]\n# Else still transform to numpy array\nelse:\nx_train = np.array(x_train)\ny_train = np.array(y_train)\n# Also get x_valid &amp; y_valid as numpy\nx_valid = np.array(x_valid)\ny_valid = np.array(y_valid)\n# Set eval set and train\neval_set = [(x_train, y_train), (x_valid, y_valid)]  # If there\u2019s more than one item in eval_set, the last entry will be used for early stopping.\nprior_objective = self.model.objective\nself.model.fit(x_train, y_train, eval_set=eval_set, early_stopping_rounds=self.early_stopping_rounds, verbose=True)\npost_objective = self.model.objective\nif prior_objective != post_objective:\nself.logger.warning(\"Warning: the objective function was automatically changed by XGBOOST\")\nself.logger.warning(f\"Before: {prior_objective}\")\nself.logger.warning(f\"After: {post_objective}\")\n# Set trained\nself.trained = True\nself.nb_fit += 1\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required Kwargs <p>return_proba (bool): Present for compatibility with other models. Raises an error if True</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If return_proba is True</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples,]</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Kwargs:\n        return_proba (bool): Present for compatibility with other models. Raises an error if True\n    Raises:\n        ValueError: If return_proba is True\n    Returns:\n        (np.ndarray): Array, shape = [n_samples,]\n    '''\n# Manage errors\nif return_proba:\nraise ValueError(\"Models of type model_xgboost_regressor can't handle probabilities\")\n# We check input format\nx_test, _ = self._check_input_format(x_test)\n# Warning, \"The method returns the model from the last iteration\"\n# But : \"Predict with X. If the model is trained with early stopping, then best_iteration is used automatically.\"\ny_pred = self.model.predict(x_test)\nreturn y_pred\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.predict_proba","title":"<code>predict_proba(x_test, **kwargs)</code>","text":"<p>Predicts the probabilities on the test set. Here for compatibility</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>Array-like, shape = [n_samples]</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Model_xgboost_regressor does not implement predict_proba</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples,]</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, x_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts the probabilities on the test set. Here for compatibility\n    Args:\n        x_test (pd.DataFrame): Array-like, shape = [n_samples]\n    Raises:\n        ValueError: Model_xgboost_regressor does not implement predict_proba\n    Returns:\n        (np.ndarray): Array, shape = [n_samples,]\n    '''\n# For compatibility\nraise ValueError(\"Models of type model_xgboost_regressor do not implement the method predict_proba\")\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file xgboost_path (str): Path to standalone xgboost preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If xgboost_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object xgboost_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        xgboost_path (str): Path to standalone xgboost\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If xgboost_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object xgboost_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nxgboost_path = kwargs.get('xgboost_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif xgboost_path is None:\nraise ValueError(\"The argument xgboost_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(xgboost_path):\nraise FileNotFoundError(f\"The file {xgboost_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save', 'xgboost_params', 'early_stopping_rounds', 'validation_split']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload xgboost model\nself.model.load_model(xgboost_path)  # load data\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/model_xgboost_regressor/#template_num.models_training.regressors.model_xgboost_regressor.ModelXgboostRegressor.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_num/models_training/regressors/model_xgboost_regressor.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'xgboost'\njson_data['xgboost_params'] = self.xgboost_params\njson_data['early_stopping_rounds'] = self.early_stopping_rounds\njson_data['validation_split'] = self.validation_split\n# Save xgboost standalone\nif self.level_save in ['MEDIUM', 'HIGH']:\nif self.trained:\nsave_path = os.path.join(self.model_dir, 'xbgoost_standalone.model')\nself.model.save_model(save_path)\nelse:\nself.logger.warning(\"Can't save the XGboost in standalone because it hasn't been already fitted\")\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/","title":"Models sklearn","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor/","title":"Model bayesian ridge regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_bayesian_ridge_regressor.ModelBayesianRidgeRegressor","title":"<code>ModelBayesianRidgeRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Bayesian ridge model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor.py</code> <pre><code>class ModelBayesianRidgeRegressor(ModelRegressorMixin, ModelPipeline):\n'''Bayesian ridge model for regression'''\n_default_name = 'model_bayesian_ridge_regressor'\ndef __init__(self, bayesian_ridge_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n        Kwargs:\n            bayesian_ridge_params (dict) : Parameters for Bayesian Ridge\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif bayesian_ridge_params is None:\nbayesian_ridge_params = {}\nself.bayesian_ridge = BayesianRidge(**bayesian_ridge_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('bayesian_ridge', self.bayesian_ridge)])\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.bayesian_ridge = self.pipeline['bayesian_ridge']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_bayesian_ridge_regressor.ModelBayesianRidgeRegressor.__init__","title":"<code>__init__(bayesian_ridge_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>bayesian_ridge_params (dict) : Parameters for Bayesian Ridge</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor.py</code> <pre><code>def __init__(self, bayesian_ridge_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n    Kwargs:\n        bayesian_ridge_params (dict) : Parameters for Bayesian Ridge\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif bayesian_ridge_params is None:\nbayesian_ridge_params = {}\nself.bayesian_ridge = BayesianRidge(**bayesian_ridge_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('bayesian_ridge', self.bayesian_ridge)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_bayesian_ridge_regressor.ModelBayesianRidgeRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_bayesian_ridge_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.bayesian_ridge = self.pipeline['bayesian_ridge']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor/","title":"Model elasticnet regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor/#template_num.models_training.regressors.models_sklearn.model_elasticnet_regressor.ModelElasticNetRegressor","title":"<code>ModelElasticNetRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Elastic Net model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor.py</code> <pre><code>class ModelElasticNetRegressor(ModelRegressorMixin, ModelPipeline):\n'''Elastic Net model for regression'''\n_default_name = 'model_elasticnet_regressor'\ndef __init__(self, elasticnet_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n        Kwargs:\n            elasticnet_params (dict) : Parameters for the Elastic Net\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif elasticnet_params is None:\nelasticnet_params = {}\nself.elasticnet = ElasticNet(**elasticnet_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('elasticnet', self.elasticnet)])\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.elasticnet = self.pipeline['elasticnet']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor/#template_num.models_training.regressors.models_sklearn.model_elasticnet_regressor.ModelElasticNetRegressor.__init__","title":"<code>__init__(elasticnet_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>elasticnet_params (dict) : Parameters for the Elastic Net</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor.py</code> <pre><code>def __init__(self, elasticnet_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n    Kwargs:\n        elasticnet_params (dict) : Parameters for the Elastic Net\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif elasticnet_params is None:\nelasticnet_params = {}\nself.elasticnet = ElasticNet(**elasticnet_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('elasticnet', self.elasticnet)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor/#template_num.models_training.regressors.models_sklearn.model_elasticnet_regressor.ModelElasticNetRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_elasticnet_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.elasticnet = self.pipeline['elasticnet']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_gbt_regressor/","title":"Model gbt regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_gbt_regressor/#template_num.models_training.regressors.models_sklearn.model_gbt_regressor.ModelGBTRegressor","title":"<code>ModelGBTRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Gradient Boosting Tree model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_gbt_regressor.py</code> <pre><code>class ModelGBTRegressor(ModelRegressorMixin, ModelPipeline):\n'''Gradient Boosting Tree model for regression'''\n_default_name = 'model_gbt_regressor'\ndef __init__(self, gbt_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n        Kwargs:\n            gbt_params (dict) : Parameters for the Gradient Boosting Tree\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif gbt_params is None:\ngbt_params = {}\nself.gbt = GradientBoostingRegressor(**gbt_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('gbt', self.gbt)])\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.gbt = self.pipeline['gbt']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_gbt_regressor/#template_num.models_training.regressors.models_sklearn.model_gbt_regressor.ModelGBTRegressor.__init__","title":"<code>__init__(gbt_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>gbt_params (dict) : Parameters for the Gradient Boosting Tree</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_gbt_regressor.py</code> <pre><code>def __init__(self, gbt_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n    Kwargs:\n        gbt_params (dict) : Parameters for the Gradient Boosting Tree\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif gbt_params is None:\ngbt_params = {}\nself.gbt = GradientBoostingRegressor(**gbt_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('gbt', self.gbt)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_gbt_regressor/#template_num.models_training.regressors.models_sklearn.model_gbt_regressor.ModelGBTRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_gbt_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.gbt = self.pipeline['gbt']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor/","title":"Model kernel ridge regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_kernel_ridge_regressor.ModelKernelRidgeRegressor","title":"<code>ModelKernelRidgeRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Kernel Ridge model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor.py</code> <pre><code>class ModelKernelRidgeRegressor(ModelRegressorMixin, ModelPipeline):\n'''Kernel Ridge model for regression'''\n_default_name = 'model_kernel_ridge_regressor'\ndef __init__(self, kernel_ridge_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n        Kwargs:\n            kernel_ridge_params (dict) : Parameters for the Kernel Ridge\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif kernel_ridge_params is None:\nkernel_ridge_params = {}\nself.kernel_ridge = KernelRidge(**kernel_ridge_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('kernel_ridge', self.kernel_ridge)])\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.kernel_ridge = self.pipeline['kernel_ridge']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_kernel_ridge_regressor.ModelKernelRidgeRegressor.__init__","title":"<code>__init__(kernel_ridge_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>kernel_ridge_params (dict) : Parameters for the Kernel Ridge</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor.py</code> <pre><code>def __init__(self, kernel_ridge_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n    Kwargs:\n        kernel_ridge_params (dict) : Parameters for the Kernel Ridge\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif kernel_ridge_params is None:\nkernel_ridge_params = {}\nself.kernel_ridge = KernelRidge(**kernel_ridge_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('kernel_ridge', self.kernel_ridge)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor/#template_num.models_training.regressors.models_sklearn.model_kernel_ridge_regressor.ModelKernelRidgeRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_kernel_ridge_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.kernel_ridge = self.pipeline['kernel_ridge']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_knn_regressor/","title":"Model knn regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_knn_regressor/#template_num.models_training.regressors.models_sklearn.model_knn_regressor.ModelKNNRegressor","title":"<code>ModelKNNRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>K-nearest Neighbors model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_knn_regressor.py</code> <pre><code>class ModelKNNRegressor(ModelRegressorMixin, ModelPipeline):\n'''K-nearest Neighbors model for regression'''\n_default_name = 'model_knn_regressor'\ndef __init__(self, knn_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n        Kwargs:\n            knn_params (dict) : Parameters for the K-nearest Neighbors\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif knn_params is None:\nknn_params = {}\nself.knn = KNeighborsRegressor(**knn_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('knn', self.knn)])\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.knn = self.pipeline['knn']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_knn_regressor/#template_num.models_training.regressors.models_sklearn.model_knn_regressor.ModelKNNRegressor.__init__","title":"<code>__init__(knn_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>knn_params (dict) : Parameters for the K-nearest Neighbors</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_knn_regressor.py</code> <pre><code>def __init__(self, knn_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n    Kwargs:\n        knn_params (dict) : Parameters for the K-nearest Neighbors\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif knn_params is None:\nknn_params = {}\nself.knn = KNeighborsRegressor(**knn_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('knn', self.knn)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_knn_regressor/#template_num.models_training.regressors.models_sklearn.model_knn_regressor.ModelKNNRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_knn_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.knn = self.pipeline['knn']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_lgbm_regressor/","title":"Model lgbm regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_lgbm_regressor/#template_num.models_training.regressors.models_sklearn.model_lgbm_regressor.ModelLGBMRegressor","title":"<code>ModelLGBMRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Light GBM model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_lgbm_regressor.py</code> <pre><code>class ModelLGBMRegressor(ModelRegressorMixin, ModelPipeline):\n'''Light GBM model for regression'''\n_default_name = 'model_lgbm_regressor'\ndef __init__(self, lgbm_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n        Kwargs:\n            lgbm_params (dict) : Parameters for the Light GBM\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif lgbm_params is None:\nlgbm_params = {}\nself.lgbm = LGBMRegressor(**lgbm_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('lgbm', self.lgbm)])\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.lgbm = self.pipeline['lgbm']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_lgbm_regressor/#template_num.models_training.regressors.models_sklearn.model_lgbm_regressor.ModelLGBMRegressor.__init__","title":"<code>__init__(lgbm_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>lgbm_params (dict) : Parameters for the Light GBM</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_lgbm_regressor.py</code> <pre><code>def __init__(self, lgbm_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n    Kwargs:\n        lgbm_params (dict) : Parameters for the Light GBM\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif lgbm_params is None:\nlgbm_params = {}\nself.lgbm = LGBMRegressor(**lgbm_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('lgbm', self.lgbm)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_lgbm_regressor/#template_num.models_training.regressors.models_sklearn.model_lgbm_regressor.ModelLGBMRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_lgbm_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.lgbm = self.pipeline['lgbm']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_pls_regressor/","title":"Model pls regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_pls_regressor/#template_num.models_training.regressors.models_sklearn.model_pls_regressor.ModelPLSRegressor","title":"<code>ModelPLSRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Partial Least Squares model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_pls_regressor.py</code> <pre><code>class ModelPLSRegressor(ModelRegressorMixin, ModelPipeline):\n'''Partial Least Squares model for regression'''\n_default_name = 'model_pls_regressor'\ndef __init__(self, pls_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n        Kwargs:\n            pls_params (dict) : Parameters for the Partial Least Squares\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif pls_params is None:\npls_params = {}\nself.pls = PLSRegression(**pls_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('pls', self.pls)])\n@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set. We override this function because the PLS does not return the same\n        format for the predictions.\n        Args:\n            x_test (pd.DataFrame): DataFrame with the test data to be predicted\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n        Returns:\n            (np.ndarray): Array, shape = [n_samples,]\n        '''\nreturn np.array([_[0] for _ in super().predict(x_test, return_proba, **kwargs)])\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.pls = self.pipeline['pls']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_pls_regressor/#template_num.models_training.regressors.models_sklearn.model_pls_regressor.ModelPLSRegressor.__init__","title":"<code>__init__(pls_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>pls_params (dict) : Parameters for the Partial Least Squares</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_pls_regressor.py</code> <pre><code>def __init__(self, pls_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n    Kwargs:\n        pls_params (dict) : Parameters for the Partial Least Squares\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif pls_params is None:\npls_params = {}\nself.pls = PLSRegression(**pls_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('pls', self.pls)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_pls_regressor/#template_num.models_training.regressors.models_sklearn.model_pls_regressor.ModelPLSRegressor.predict","title":"<code>predict(x_test, return_proba=False, **kwargs)</code>","text":"<p>Predictions on test set. We override this function because the PLS does not return the same format for the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>x_test</code> <code>pd.DataFrame</code> <p>DataFrame with the test data to be predicted</p> required Kwargs <p>return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples,]</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_pls_regressor.py</code> <pre><code>@utils.trained_needed\ndef predict(self, x_test: pd.DataFrame, return_proba: bool = False, **kwargs) -&gt; np.ndarray:\n'''Predictions on test set. We override this function because the PLS does not return the same\n    format for the predictions.\n    Args:\n        x_test (pd.DataFrame): DataFrame with the test data to be predicted\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes (Keras compatibility)\n    Returns:\n        (np.ndarray): Array, shape = [n_samples,]\n    '''\nreturn np.array([_[0] for _ in super().predict(x_test, return_proba, **kwargs)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_pls_regressor/#template_num.models_training.regressors.models_sklearn.model_pls_regressor.ModelPLSRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_pls_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.pls = self.pipeline['pls']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_rf_regressor/","title":"Model rf regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_rf_regressor/#template_num.models_training.regressors.models_sklearn.model_rf_regressor.ModelRFRegressor","title":"<code>ModelRFRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Random Forest model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_rf_regressor.py</code> <pre><code>class ModelRFRegressor(ModelRegressorMixin, ModelPipeline):\n'''Random Forest model for regression'''\n_default_name = 'model_rf_regressor'\ndef __init__(self, rf_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n        Kwargs:\n            rf_params (dict) : Parameters for the Random Forest\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif rf_params is None:\nrf_params = {}\nself.rf = RandomForestRegressor(**rf_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('rf', self.rf)])\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.rf = self.pipeline['rf']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_rf_regressor/#template_num.models_training.regressors.models_sklearn.model_rf_regressor.ModelRFRegressor.__init__","title":"<code>__init__(rf_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>rf_params (dict) : Parameters for the Random Forest</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_rf_regressor.py</code> <pre><code>def __init__(self, rf_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n    Kwargs:\n        rf_params (dict) : Parameters for the Random Forest\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif rf_params is None:\nrf_params = {}\nself.rf = RandomForestRegressor(**rf_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('rf', self.rf)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_rf_regressor/#template_num.models_training.regressors.models_sklearn.model_rf_regressor.ModelRFRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_rf_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.rf = self.pipeline['rf']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_sgd_regressor/","title":"Model sgd regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_sgd_regressor/#template_num.models_training.regressors.models_sklearn.model_sgd_regressor.ModelSGDRegressor","title":"<code>ModelSGDRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Stochastic Gradient Descent model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_sgd_regressor.py</code> <pre><code>class ModelSGDRegressor(ModelRegressorMixin, ModelPipeline):\n'''Stochastic Gradient Descent model for regression'''\n_default_name = 'model_sgd_regressor'\ndef __init__(self, sgd_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n        Kwargs:\n            sgd_params (dict) : Parameters for the Stochastic Gradient Descent\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif sgd_params is None:\nsgd_params = {}\nself.sgd = SGDRegressor(**sgd_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('sgd', self.sgd)])\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.sgd = self.pipeline['sgd']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_sgd_regressor/#template_num.models_training.regressors.models_sklearn.model_sgd_regressor.ModelSGDRegressor.__init__","title":"<code>__init__(sgd_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>sgd_params (dict) : Parameters for the Stochastic Gradient Descent</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_sgd_regressor.py</code> <pre><code>def __init__(self, sgd_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n    Kwargs:\n        sgd_params (dict) : Parameters for the Stochastic Gradient Descent\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif sgd_params is None:\nsgd_params = {}\nself.sgd = SGDRegressor(**sgd_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('sgd', self.sgd)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_sgd_regressor/#template_num.models_training.regressors.models_sklearn.model_sgd_regressor.ModelSGDRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_sgd_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.sgd = self.pipeline['sgd']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_svr_regressor/","title":"Model svr regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_svr_regressor/#template_num.models_training.regressors.models_sklearn.model_svr_regressor.ModelSVRRegressor","title":"<code>ModelSVRRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelPipeline</code></p> <p>Support Vector Regression model for regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_svr_regressor.py</code> <pre><code>class ModelSVRRegressor(ModelRegressorMixin, ModelPipeline):\n'''Support Vector Regression model for regression'''\n_default_name = 'model_svr_regressor'\ndef __init__(self, svr_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n        Kwargs:\n            svr_params (dict) : Parameters for the Support Vector Regression\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif svr_params is None:\nsvr_params = {}\nself.svr = SVR(**svr_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('svr', self.svr)])\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            sklearn_pipeline_path (str): Path to standalone pipeline\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If sklearn_pipeline_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.svr = self.pipeline['svr']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_svr_regressor/#template_num.models_training.regressors.models_sklearn.model_svr_regressor.ModelSVRRegressor.__init__","title":"<code>__init__(svr_params=None, **kwargs)</code>","text":"<p>Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)</p> Kwargs <p>svr_params (dict) : Parameters for the Support Vector Regression</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_svr_regressor.py</code> <pre><code>def __init__(self, svr_params: Union[dict, None] = None, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelPipeline, ModelClass &amp; ModelRegressorMixin for more arguments)\n    Kwargs:\n        svr_params (dict) : Parameters for the Support Vector Regression\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Manage model\nif svr_params is None:\nsvr_params = {}\nself.svr = SVR(**svr_params)\n# We define a pipeline in order to be compatible with other models\nself.pipeline = Pipeline([('svr', self.svr)])\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_sklearn/model_svr_regressor/#template_num.models_training.regressors.models_sklearn.model_svr_regressor.ModelSVRRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file sklearn_pipeline_path (str): Path to standalone pipeline preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If sklearn_pipeline_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object sklearn_pipeline_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_sklearn/model_svr_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        sklearn_pipeline_path (str): Path to standalone pipeline\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If sklearn_pipeline_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object sklearn_pipeline_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nsklearn_pipeline_path = kwargs.get('sklearn_pipeline_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif sklearn_pipeline_path is None:\nraise ValueError(\"The argument sklearn_pipeline_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(sklearn_pipeline_path):\nraise FileNotFoundError(f\"The file {sklearn_pipeline_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload pipeline model\nwith open(sklearn_pipeline_path, 'rb') as f:\nself.pipeline = pickle.load(f)\n# Reload pipeline elements\nself.svr = self.pipeline['svr']\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_tensorflow/","title":"Models tensorflow","text":""},{"location":"reference/template_num/models_training/regressors/models_tensorflow/model_dense_regressor/","title":"Model dense regressor","text":""},{"location":"reference/template_num/models_training/regressors/models_tensorflow/model_dense_regressor/#template_num.models_training.regressors.models_tensorflow.model_dense_regressor.ModelDenseRegressor","title":"<code>ModelDenseRegressor</code>","text":"<p>         Bases: <code>ModelRegressorMixin</code>, <code>ModelKeras</code></p> <p>Dense model for regression</p> Source code in <code>template_num/models_training/regressors/models_tensorflow/model_dense_regressor.py</code> <pre><code>class ModelDenseRegressor(ModelRegressorMixin, ModelKeras):\n'''Dense model for regression'''\n_default_name = 'model_dense_regressor'\ndef __init__(self, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass, ModelKeras &amp; ModelRegressor for more arguments)'''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\ndef _get_model(self) -&gt; Model:\n'''Gets a model structure - returns the instance model instead if already defined\n        Returns:\n            (Model): A model\n        '''\n# Return model if already set\nif self.model is not None:\nreturn self.model\n# Get input/output dimensions\ninput_dim = len(self.x_col)\n# Process\ninput_layer = Input(shape=(input_dim,))\nx = Dense(64, activation=None, kernel_initializer=\"he_uniform\")(input_layer)\nx = BatchNormalization(momentum=0.9)(x)\nx = ELU(alpha=1.0)(x)\nx = Dropout(0.2)(x)\nx = Dense(64, activation=None, kernel_initializer=\"he_uniform\")(x)\nx = BatchNormalization(momentum=0.9)(x)\nx = ELU(alpha=1.0)(x)\nx = Dropout(0.2)(x)\n# Last layer\nactivation = None  # 'relu' if result should be &gt; 0\nout = Dense(1, activation=activation, kernel_initializer='glorot_uniform')(x)\n# Set model\nmodel = Model(inputs=input_layer, outputs=[out])\n# Set optimizer\nlr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.001\ndecay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\nself.logger.info(f\"Learning rate: {lr}\")\nself.logger.info(f\"Decay: {decay}\")\noptimizer = Adam(lr=lr, decay=decay)\n# Set loss &amp; metrics\nloss = 'mean_squared_error'  # could be 'mean_absolute_error'\nmetrics: List[Union[str, Callable]] = ['mean_squared_error', 'mean_absolute_error', utils_deep_keras.root_mean_squared_error]\n# Compile model\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nmodel.summary()\n# Try to save model as png if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._save_model_png(model)\n# Return\nreturn model\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            preprocess_pipeline_path (str): Path to preprocess pipeline\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If preprocess_pipeline_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save', 'batch_size', 'epochs', 'validation_split', 'patience',\n'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_tensorflow/model_dense_regressor/#template_num.models_training.regressors.models_tensorflow.model_dense_regressor.ModelDenseRegressor.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialization of the class (see ModelClass, ModelKeras &amp; ModelRegressor for more arguments)</p> Source code in <code>template_num/models_training/regressors/models_tensorflow/model_dense_regressor.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass, ModelKeras &amp; ModelRegressor for more arguments)'''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_num/models_training/regressors/models_tensorflow/model_dense_regressor/#template_num.models_training.regressors.models_tensorflow.model_dense_regressor.ModelDenseRegressor.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file preprocess_pipeline_path (str): Path to preprocess pipeline</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hdf5_path is None</p> <code>ValueError</code> <p>If preprocess_pipeline_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_pipeline_path is not an existing file</p> Source code in <code>template_num/models_training/regressors/models_tensorflow/model_dense_regressor.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        preprocess_pipeline_path (str): Path to preprocess pipeline\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If preprocess_pipeline_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object preprocess_pipeline_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\npreprocess_pipeline_path = kwargs.get('preprocess_pipeline_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif preprocess_pipeline_path is None:\nraise ValueError(\"The argument preprocess_pipeline_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(preprocess_pipeline_path):\nraise FileNotFoundError(f\"The file {preprocess_pipeline_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'x_col', 'y_col', 'columns_in', 'mandatory_columns',\n'level_save', 'batch_size', 'epochs', 'validation_split', 'patience',\n'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n# Reload pipeline preprocessing\nwith open(preprocess_pipeline_path, 'rb') as f:\nself.preprocess_pipeline = pickle.load(f)\n</code></pre>"},{"location":"reference/template_num/monitoring/","title":"Monitoring","text":""},{"location":"reference/template_num/monitoring/mlflow_logger/","title":"Mlflow logger","text":""},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger","title":"<code>MLflowLogger</code>","text":"<p>Abstracts how MlFlow works</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>class MLflowLogger:\n'''Abstracts how MlFlow works'''\ndef __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n'''Class initialization\n        Args:\n            experiment_name (str):  Name of the experiment to activate\n        Kwargs:\n            tracking_uri (str): URI of the tracking server\n            artifact_uri (str): URI where to store artifacts\n        '''\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Backup to local save if no uri (i.e. empty string)\nif not tracking_uri:\ntracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n# Add \"file\" scheme if no scheme in the tracking_uri\nelif not urlparse(tracking_uri).scheme:\ntracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n# If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n# Otherwise we suppose artifact_uri is configured by the system\nif not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\nartifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n# Set tracking URI &amp; experiment name\nself.tracking_uri = tracking_uri\n# Get the experiment if it exists and check if there is a connection error by doing it\ntry:\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nexcept Exception as e:\nself.logger.error(repr(e))\nraise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n# If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\nif experiment:\nexperiment_id = experiment.experiment_id\nartifact_uri = experiment.artifact_location\n# Otherwise we create a new experiment with the provided artifact_uri\nelse:\nexperiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nartifact_uri = experiment.artifact_location\nmlflow.set_experiment(experiment_id=experiment_id)\nself.__experiment_id = experiment_id\nself.__experiment_name = experiment_name\nself.__artifact_uri = artifact_uri\nself.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n@property\ndef tracking_uri(self) -&gt; str:\n'''Current tracking uri'''\nreturn mlflow.get_tracking_uri()\n@tracking_uri.setter\ndef tracking_uri(self, uri:str) -&gt; None:\n'''Set tracking uri'''\nmlflow.set_tracking_uri(uri)\n@property\ndef experiment_id(self) -&gt; str:\n'''Experiment id. It can not be changed.'''\nreturn self.__experiment_id\n@property\ndef experiment_name(self) -&gt; str:\n'''Experiment name. It can not be changed.'''\nreturn self.__experiment_name\n@property\ndef artifact_uri(self) -&gt; str:\n'''Experiment artifact URI. It can not be changed.'''\nreturn self.__artifact_uri\ndef end_run(self) -&gt; None:\n'''Stops an MLflow run'''\ntry:\nmlflow.end_run()\nexcept Exception:\nself.logger.error(\"Can't stop mlflow run\")\ndef log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n'''Logs a metric on mlflow\n        Args:\n            key (str): Name of the metric\n            value (float, ?): Value of the metric\n        Kwargs:\n            step (int): Step of the metric\n        '''\n# Check for None\nif value is None:\nvalue = math.nan\n# Log metric\nmlflow.log_metric(key, value, step)\ndef log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n'''Logs a set of metrics in mlflow\n        Args:\n            metrics (dict): Metrics to log\n        Kwargs:\n            step (int): Step of the metric\n        '''\n# Check for Nones\nfor k, v in metrics.items():\nif v is None:\nmetrics[k] = math.nan\n# Log metrics\nmlflow.log_metrics(metrics, step)\ndef log_param(self, key: str, value) -&gt; None:\n'''Logs a parameter in mlflow\n        Args:\n            key (str): Name of the parameter\n            value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n        '''\nif value is None:\nvalue = 'None'\n# Log parameter\nmlflow.log_param(key, value)\ndef log_params(self, params: dict) -&gt; None:\n'''Logs a set of parameters in mlflow\n        Args:\n            params (dict): Name and value of each parameter\n        '''\n# Check for Nones\nfor k, v in params.items():\nif v is None:\nparams[k] = 'None'\n# Log parameters\nmlflow.log_params(params)\ndef set_tag(self, key: str, value) -&gt; None:\n'''Logs a tag in mlflow\n        Args:\n            key (str): Name of the tag\n            value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n        Raises:\n            ValueError: If the object value is None\n        '''\nif value is None:\nraise ValueError('value must not be None')\n# Log tag\nmlflow.set_tag(key, value)\ndef set_tags(self, tags: dict) -&gt; None:\n'''Logs a set of tags in mlflow\n        Args:\n            tags (dict): Name and value of each tag\n        '''\n# Log tags\nmlflow.set_tags(tags)\ndef valid_name(self, key: str) -&gt; bool:\n'''Validates key names\n        Args:\n            key (str): Key to check\n        Returns:\n            bool: If key is a valid mlflow key\n        '''\nif mlflow.mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\nreturn True\nelse:\nreturn False\ndef log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n'''Log a dataframe containing metrics from a training\n        Args:\n            df_stats (pd.Dataframe): Dataframe containing metrics from a training\n        Kwargs:\n            label_col (str): default labelc column name\n        '''\nif label_col not in df_stats.columns:\nraise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n# Get metrics columns\nmetrics_columns = [col for col in df_stats.columns if col != label_col]\n# Log labels\nlabels = df_stats[label_col].values\nfor i, label in enumerate(labels):  # type: ignore\nself.log_param(f'Label {i}', label)\n# Log metrics\nml_flow_metrics = {}\nfor i, row in df_stats.iterrows():\nfor j, col in enumerate(metrics_columns):\nmetric_key = f\"{row[label_col]} --- {col}\"\n# Check that mlflow accepts the key, otherwise, replace it\n# TODO: could be improved ...\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- {col}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"{row[label_col]} --- Col {j}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- Col {j}\"\nml_flow_metrics[metric_key] = row[col]\n# Log metrics\nself.log_metrics(ml_flow_metrics)\ndef log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n'''Logs a dictionary as an artifact in MLflow\n        Args:\n            dictionary (dict): A dictionary\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\nmlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\ndef log_text(self, text: str, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n        Args:\n            text (str): A text\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\nmlflow.log_text(text=text, artifact_file=artifact_file)\ndef log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n        Args:\n            figure (matplotlib.figure.Figure): A matplotlib figure\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n        '''\nmlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.artifact_uri","title":"<code>artifact_uri: str</code>  <code>property</code>","text":"<p>Experiment artifact URI. It can not be changed.</p>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.experiment_id","title":"<code>experiment_id: str</code>  <code>property</code>","text":"<p>Experiment id. It can not be changed.</p>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.experiment_name","title":"<code>experiment_name: str</code>  <code>property</code>","text":"<p>Experiment name. It can not be changed.</p>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.tracking_uri","title":"<code>tracking_uri: str</code>  <code>writable</code> <code>property</code>","text":"<p>Current tracking uri</p>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.__init__","title":"<code>__init__(experiment_name, tracking_uri='', artifact_uri='')</code>","text":"<p>Class initialization</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment to activate</p> required Kwargs <p>tracking_uri (str): URI of the tracking server artifact_uri (str): URI where to store artifacts</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n'''Class initialization\n    Args:\n        experiment_name (str):  Name of the experiment to activate\n    Kwargs:\n        tracking_uri (str): URI of the tracking server\n        artifact_uri (str): URI where to store artifacts\n    '''\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Backup to local save if no uri (i.e. empty string)\nif not tracking_uri:\ntracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n# Add \"file\" scheme if no scheme in the tracking_uri\nelif not urlparse(tracking_uri).scheme:\ntracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n# If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n# Otherwise we suppose artifact_uri is configured by the system\nif not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\nartifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n# Set tracking URI &amp; experiment name\nself.tracking_uri = tracking_uri\n# Get the experiment if it exists and check if there is a connection error by doing it\ntry:\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nexcept Exception as e:\nself.logger.error(repr(e))\nraise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n# If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\nif experiment:\nexperiment_id = experiment.experiment_id\nartifact_uri = experiment.artifact_location\n# Otherwise we create a new experiment with the provided artifact_uri\nelse:\nexperiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nartifact_uri = experiment.artifact_location\nmlflow.set_experiment(experiment_id=experiment_id)\nself.__experiment_id = experiment_id\nself.__experiment_name = experiment_name\nself.__artifact_uri = artifact_uri\nself.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.end_run","title":"<code>end_run()</code>","text":"<p>Stops an MLflow run</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def end_run(self) -&gt; None:\n'''Stops an MLflow run'''\ntry:\nmlflow.end_run()\nexcept Exception:\nself.logger.error(\"Can't stop mlflow run\")\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_df_stats","title":"<code>log_df_stats(df_stats, label_col='Label')</code>","text":"<p>Log a dataframe containing metrics from a training</p> <p>Parameters:</p> Name Type Description Default <code>df_stats</code> <code>pd.Dataframe</code> <p>Dataframe containing metrics from a training</p> required Kwargs <p>label_col (str): default labelc column name</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n'''Log a dataframe containing metrics from a training\n    Args:\n        df_stats (pd.Dataframe): Dataframe containing metrics from a training\n    Kwargs:\n        label_col (str): default labelc column name\n    '''\nif label_col not in df_stats.columns:\nraise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n# Get metrics columns\nmetrics_columns = [col for col in df_stats.columns if col != label_col]\n# Log labels\nlabels = df_stats[label_col].values\nfor i, label in enumerate(labels):  # type: ignore\nself.log_param(f'Label {i}', label)\n# Log metrics\nml_flow_metrics = {}\nfor i, row in df_stats.iterrows():\nfor j, col in enumerate(metrics_columns):\nmetric_key = f\"{row[label_col]} --- {col}\"\n# Check that mlflow accepts the key, otherwise, replace it\n# TODO: could be improved ...\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- {col}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"{row[label_col]} --- Col {j}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- Col {j}\"\nml_flow_metrics[metric_key] = row[col]\n# Log metrics\nself.log_metrics(ml_flow_metrics)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_dict","title":"<code>log_dict(dictionary, artifact_file)</code>","text":"<p>Logs a dictionary as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>A dictionary</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n'''Logs a dictionary as an artifact in MLflow\n    Args:\n        dictionary (dict): A dictionary\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\nmlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_figure","title":"<code>log_figure(figure, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>matplotlib.figure.Figure</code> <p>A matplotlib figure</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the figure is saved</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n    Args:\n        figure (matplotlib.figure.Figure): A matplotlib figure\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n    '''\nmlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_metric","title":"<code>log_metric(key, value, step=None)</code>","text":"<p>Logs a metric on mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the metric</p> required <code>value</code> <code>float, ?</code> <p>Value of the metric</p> required Kwargs <p>step (int): Step of the metric</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n'''Logs a metric on mlflow\n    Args:\n        key (str): Name of the metric\n        value (float, ?): Value of the metric\n    Kwargs:\n        step (int): Step of the metric\n    '''\n# Check for None\nif value is None:\nvalue = math.nan\n# Log metric\nmlflow.log_metric(key, value, step)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_metrics","title":"<code>log_metrics(metrics, step=None)</code>","text":"<p>Logs a set of metrics in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict</code> <p>Metrics to log</p> required Kwargs <p>step (int): Step of the metric</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n'''Logs a set of metrics in mlflow\n    Args:\n        metrics (dict): Metrics to log\n    Kwargs:\n        step (int): Step of the metric\n    '''\n# Check for Nones\nfor k, v in metrics.items():\nif v is None:\nmetrics[k] = math.nan\n# Log metrics\nmlflow.log_metrics(metrics, step)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_param","title":"<code>log_param(key, value)</code>","text":"<p>Logs a parameter in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the parameter</p> required <code>value</code> <code>str, ?</code> <p>Value of the parameter (which will be cast to str if not already of type str)</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_param(self, key: str, value) -&gt; None:\n'''Logs a parameter in mlflow\n    Args:\n        key (str): Name of the parameter\n        value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n    '''\nif value is None:\nvalue = 'None'\n# Log parameter\nmlflow.log_param(key, value)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_params","title":"<code>log_params(params)</code>","text":"<p>Logs a set of parameters in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Name and value of each parameter</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_params(self, params: dict) -&gt; None:\n'''Logs a set of parameters in mlflow\n    Args:\n        params (dict): Name and value of each parameter\n    '''\n# Check for Nones\nfor k, v in params.items():\nif v is None:\nparams[k] = 'None'\n# Log parameters\nmlflow.log_params(params)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.log_text","title":"<code>log_text(text, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>A text</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def log_text(self, text: str, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n    Args:\n        text (str): A text\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\nmlflow.log_text(text=text, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.set_tag","title":"<code>set_tag(key, value)</code>","text":"<p>Logs a tag in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the tag</p> required <code>value</code> <code>str, ?</code> <p>Value of the tag (which will be cast to str if not already of type str)</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object value is None</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def set_tag(self, key: str, value) -&gt; None:\n'''Logs a tag in mlflow\n    Args:\n        key (str): Name of the tag\n        value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n    Raises:\n        ValueError: If the object value is None\n    '''\nif value is None:\nraise ValueError('value must not be None')\n# Log tag\nmlflow.set_tag(key, value)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.set_tags","title":"<code>set_tags(tags)</code>","text":"<p>Logs a set of tags in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>dict</code> <p>Name and value of each tag</p> required Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def set_tags(self, tags: dict) -&gt; None:\n'''Logs a set of tags in mlflow\n    Args:\n        tags (dict): Name and value of each tag\n    '''\n# Log tags\nmlflow.set_tags(tags)\n</code></pre>"},{"location":"reference/template_num/monitoring/mlflow_logger/#template_num.monitoring.mlflow_logger.MLflowLogger.valid_name","title":"<code>valid_name(key)</code>","text":"<p>Validates key names</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>If key is a valid mlflow key</p> Source code in <code>template_num/monitoring/mlflow_logger.py</code> <pre><code>def valid_name(self, key: str) -&gt; bool:\n'''Validates key names\n    Args:\n        key (str): Key to check\n    Returns:\n        bool: If key is a valid mlflow key\n    '''\nif mlflow.mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\nreturn True\nelse:\nreturn False\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/","title":"Model explainer","text":""},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.Explainer","title":"<code>Explainer</code>","text":"<p>Parent class for the explainers</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>class Explainer:\n'''Parent class for the explainers'''\ndef __init__(self, *args, **kwargs) -&gt; None:\n'''Initialization of the parent class'''\nself.logger = logging.getLogger(__name__)\ndef explain_instance(self, content: pd.DataFrame, **kwargs) -&gt; Any:\n'''Explains a prediction\n        Args:\n            content (pd.DataFrame): Single entry to be explained\n        Returns:\n            (?): An explanation object\n        '''\nraise NotImplementedError(\"'explain_instance' needs to be overridden\")\ndef explain_instance_as_html(self, content: pd.DataFrame, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n        Args:\n            content (pd.DataFrame): Single entry to be explained\n        Returns:\n            str: An HTML code with the explanation\n        '''\nraise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\ndef explain_instance_as_json(self, content: pd.DataFrame, **kwargs) -&gt; Union[dict, list]:\n'''Explains a prediction - returns an JSON serializable object\n        Args:\n            content (str): Text to be explained\n        Returns:\n            Union[dict, list]: A JSON serializable object containing the explanation\n        '''\nraise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.Explainer.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialization of the parent class</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n'''Initialization of the parent class'''\nself.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.Explainer.explain_instance","title":"<code>explain_instance(content, **kwargs)</code>","text":"<p>Explains a prediction</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>pd.DataFrame</code> <p>Single entry to be explained</p> required <p>Returns:</p> Type Description <code>?</code> <p>An explanation object</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: pd.DataFrame, **kwargs) -&gt; Any:\n'''Explains a prediction\n    Args:\n        content (pd.DataFrame): Single entry to be explained\n    Returns:\n        (?): An explanation object\n    '''\nraise NotImplementedError(\"'explain_instance' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.Explainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>pd.DataFrame</code> <p>Single entry to be explained</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>An HTML code with the explanation</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: pd.DataFrame, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n    Args:\n        content (pd.DataFrame): Single entry to be explained\n    Returns:\n        str: An HTML code with the explanation\n    '''\nraise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.Explainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an JSON serializable object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:</p> Type Description <code>Union[dict, list]</code> <p>Union[dict, list]: A JSON serializable object containing the explanation</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: pd.DataFrame, **kwargs) -&gt; Union[dict, list]:\n'''Explains a prediction - returns an JSON serializable object\n    Args:\n        content (str): Text to be explained\n    Returns:\n        Union[dict, list]: A JSON serializable object containing the explanation\n    '''\nraise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer","title":"<code>ShapExplainer</code>","text":"<p>         Bases: <code>Explainer</code></p> <p>Shap Explainer wrapper class</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>class ShapExplainer(Explainer):\n'''Shap Explainer wrapper class'''\ndef __init__(self, model: Type[ModelClass], anchor_data: pd.DataFrame, anchor_preprocessed: bool = False) -&gt; None:\n''' Initialization\n        Args:\n            model: A model instance with predict (regressors) or predict_proba (classifiers) functions\n            anchor_data (pd.DataFrame): data anchor needed by shap (usually 100 data points)\n        Kwargs:\n            anchor_preprocessed (bool): If the anchor data has already been preprocessed\n        Raises:\n            TypeError: If the provided model is a regressor and does not implement a `predict` function\n            TypeError: If the provided model is a classifier and does not implement a `predict_proba` function\n        '''\nsuper().__init__()\npred_op = getattr(model, \"predict\", None)\npred_proba_op = getattr(model, \"predict_proba\", None)\nif model.model_type == 'regressor':\nif pred_op is None or not callable(pred_op):\nraise TypeError(\"The supplied model must implement a predict() function\")\n# Check classifier\nif model.model_type == 'classifier':\nif pred_proba_op is None or not callable(pred_proba_op):\nraise TypeError(\"The supplied model must implement a predict_proba() function\")\n# Set attributes\nself.model = model\nself.model_type = model.model_type\n# Our explainers will explain a prediction for a given class / label\n# These atributes are set on the fly and will change the proba function used by the explainer\nself.current_class_or_label_index = 0\nfn_output = self.classifier_fn if self.model_type == 'classifier' else self.regressor_fn\n# Preprocess the anchor data\nif not anchor_preprocessed:\nif self.model.preprocess_pipeline is not None:\nanchor_prep = utils_models.apply_pipeline(anchor_data, self.model.preprocess_pipeline)\nelse:\nanchor_prep = anchor_data.copy()\nelse:\n# Check columns\ntry:\nanchor_prep = anchor_data[self.model.x_col]\nexcept:\nraise ValueError(\"Provided anchor data (already preprocessed) do not match model's inputs columns\")\n# Create the explainer\nself.explainer = shap.Explainer(fn_output, anchor_prep)\ndef classifier_fn(self, content_prep: pd.DataFrame) -&gt; np.ndarray:\n'''Function to get probabilities from a dataset (already preprocessed) - classifiers\n        Args:\n            content_prep (pd.DataFrame): dataset (already preprocessed) to be considered\n        Returns:\n            np.array: probabilities\n        '''\n# Get probabilities\n# Mypy raises a false error here, needs to be ignored\nreturn self.model.predict_proba(content_prep)[:, self.current_class_or_label_index]  # type: ignore\ndef regressor_fn(self, content_prep: pd.DataFrame) -&gt; np.ndarray:\n'''Function to get predictions from a dataset (already preprocessed) - regressors\n        Args:\n            content_prep (pd.DataFrame): dataset (already preprocessed) to be considered\n        Returns:\n            np.array: predictions\n        '''\n# Get predictions\n# Mypy raises a false error here, needs to be ignored\nreturn self.model.predict(content_prep)  # type: ignore\ndef explain_instance(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; shap.Explanation:\n'''Explains predictions by returning a shap.Explanation object\n        This function calls the Shap module.\n        Args:\n            content (pd.DataFrame): Entries to be explained\n        Kwargs:\n            class_or_label_index (int): for classification only. Class or label index to be considered.\n        Returns:\n            shap.Explanation: Shap Explanation object\n        '''\n# Apply preprocessing\nif self.model.preprocess_pipeline is not None:\ndf_prep = utils_models.apply_pipeline(content, self.model.preprocess_pipeline)\nelse:\ndf_prep = content.copy()\nlogger.warning(\"No preprocessing pipeline found - we consider no preprocessing, but it should not be so !\")\n# Set index (if needed)\nif class_or_label_index is not None:\nself.current_class_or_label_index = class_or_label_index\n# Get explanations\nreturn self.explainer(df_prep)  # Shap values\ndef explain_instance_as_html(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n        Args:\n            content (pd.DataFrame): Single entry to be explained\n        Kwargs:\n            class_or_label_index (int): for classification only. Class or label index to be considered.\n        Returns:\n            str: An HTML code with the explanation\n        '''\nshap_values = self.explain_instance(content, class_or_label_index=class_or_label_index)\n# Waterfall figure\nplt.clf()\nwaterfall_fig = shap.plots.waterfall(shap_values[0], show=False)\nwith tempfile.TemporaryFile('w+b') as plt_file:\nwaterfall_fig.savefig(plt_file, format='png', bbox_inches='tight')\nplt_file.seek(0)\nencoded = base64.b64encode(plt_file.read())\nplt.clf()\nhtml_waterfall = f\"&lt;img src=\\'data:image/png;base64, {encoded.decode('utf-8')}\\' class=\\\"shap_fig\\\" &gt;\"  # Class name is used in the demonstrator\n# Force figure\nhtml_force = shap.plots.force(shap_values[0]).html()\n# Combine &amp; return\nfinal_html = f\"&lt;head&gt;{shap.getjs()}&lt;/head&gt;{html_waterfall}&lt;br&gt;&lt;body&gt;{html_force}&lt;/body&gt;\"\nreturn final_html\ndef explain_instance_as_json(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; Union[dict, list]:\n'''Explains predictions by returning a JSON serializable object\n        This function calls the Shap module.\n        Args:\n            content (pd.DataFrame): entries to be explained\n        Kwargs:\n            class_or_label_index (int): for classification only. Class or label index to be considered.\n        Returns:\n            (Union[dict, list]): Shap values\n        '''\nreturn [\n{\n\"features\": explanation.feature_names, \n\"preprocessed_values\": explanation.data, \n\"shap_values\": explanation.values, \n\"shap_base_values\": explanation.base_values,\n}\nfor explanation in self.explain_instance(content, class_or_label_index=class_or_label_index, **kwargs)\n]\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.__init__","title":"<code>__init__(model, anchor_data, anchor_preprocessed=False)</code>","text":"<p>Initialization</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[ModelClass]</code> <p>A model instance with predict (regressors) or predict_proba (classifiers) functions</p> required <code>anchor_data</code> <code>pd.DataFrame</code> <p>data anchor needed by shap (usually 100 data points)</p> required Kwargs <p>anchor_preprocessed (bool): If the anchor data has already been preprocessed</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the provided model is a regressor and does not implement a <code>predict</code> function</p> <code>TypeError</code> <p>If the provided model is a classifier and does not implement a <code>predict_proba</code> function</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def __init__(self, model: Type[ModelClass], anchor_data: pd.DataFrame, anchor_preprocessed: bool = False) -&gt; None:\n''' Initialization\n    Args:\n        model: A model instance with predict (regressors) or predict_proba (classifiers) functions\n        anchor_data (pd.DataFrame): data anchor needed by shap (usually 100 data points)\n    Kwargs:\n        anchor_preprocessed (bool): If the anchor data has already been preprocessed\n    Raises:\n        TypeError: If the provided model is a regressor and does not implement a `predict` function\n        TypeError: If the provided model is a classifier and does not implement a `predict_proba` function\n    '''\nsuper().__init__()\npred_op = getattr(model, \"predict\", None)\npred_proba_op = getattr(model, \"predict_proba\", None)\nif model.model_type == 'regressor':\nif pred_op is None or not callable(pred_op):\nraise TypeError(\"The supplied model must implement a predict() function\")\n# Check classifier\nif model.model_type == 'classifier':\nif pred_proba_op is None or not callable(pred_proba_op):\nraise TypeError(\"The supplied model must implement a predict_proba() function\")\n# Set attributes\nself.model = model\nself.model_type = model.model_type\n# Our explainers will explain a prediction for a given class / label\n# These atributes are set on the fly and will change the proba function used by the explainer\nself.current_class_or_label_index = 0\nfn_output = self.classifier_fn if self.model_type == 'classifier' else self.regressor_fn\n# Preprocess the anchor data\nif not anchor_preprocessed:\nif self.model.preprocess_pipeline is not None:\nanchor_prep = utils_models.apply_pipeline(anchor_data, self.model.preprocess_pipeline)\nelse:\nanchor_prep = anchor_data.copy()\nelse:\n# Check columns\ntry:\nanchor_prep = anchor_data[self.model.x_col]\nexcept:\nraise ValueError(\"Provided anchor data (already preprocessed) do not match model's inputs columns\")\n# Create the explainer\nself.explainer = shap.Explainer(fn_output, anchor_prep)\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.classifier_fn","title":"<code>classifier_fn(content_prep)</code>","text":"<p>Function to get probabilities from a dataset (already preprocessed) - classifiers</p> <p>Parameters:</p> Name Type Description Default <code>content_prep</code> <code>pd.DataFrame</code> <p>dataset (already preprocessed) to be considered</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.array: probabilities</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def classifier_fn(self, content_prep: pd.DataFrame) -&gt; np.ndarray:\n'''Function to get probabilities from a dataset (already preprocessed) - classifiers\n    Args:\n        content_prep (pd.DataFrame): dataset (already preprocessed) to be considered\n    Returns:\n        np.array: probabilities\n    '''\n# Get probabilities\n# Mypy raises a false error here, needs to be ignored\nreturn self.model.predict_proba(content_prep)[:, self.current_class_or_label_index]  # type: ignore\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.explain_instance","title":"<code>explain_instance(content, class_or_label_index=None, **kwargs)</code>","text":"<p>Explains predictions by returning a shap.Explanation object</p> <p>This function calls the Shap module.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>pd.DataFrame</code> <p>Entries to be explained</p> required Kwargs <p>class_or_label_index (int): for classification only. Class or label index to be considered.</p> <p>Returns:</p> Type Description <code>shap.Explanation</code> <p>shap.Explanation: Shap Explanation object</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; shap.Explanation:\n'''Explains predictions by returning a shap.Explanation object\n    This function calls the Shap module.\n    Args:\n        content (pd.DataFrame): Entries to be explained\n    Kwargs:\n        class_or_label_index (int): for classification only. Class or label index to be considered.\n    Returns:\n        shap.Explanation: Shap Explanation object\n    '''\n# Apply preprocessing\nif self.model.preprocess_pipeline is not None:\ndf_prep = utils_models.apply_pipeline(content, self.model.preprocess_pipeline)\nelse:\ndf_prep = content.copy()\nlogger.warning(\"No preprocessing pipeline found - we consider no preprocessing, but it should not be so !\")\n# Set index (if needed)\nif class_or_label_index is not None:\nself.current_class_or_label_index = class_or_label_index\n# Get explanations\nreturn self.explainer(df_prep)  # Shap values\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, class_or_label_index=None, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>pd.DataFrame</code> <p>Single entry to be explained</p> required Kwargs <p>class_or_label_index (int): for classification only. Class or label index to be considered.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>An HTML code with the explanation</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n    Args:\n        content (pd.DataFrame): Single entry to be explained\n    Kwargs:\n        class_or_label_index (int): for classification only. Class or label index to be considered.\n    Returns:\n        str: An HTML code with the explanation\n    '''\nshap_values = self.explain_instance(content, class_or_label_index=class_or_label_index)\n# Waterfall figure\nplt.clf()\nwaterfall_fig = shap.plots.waterfall(shap_values[0], show=False)\nwith tempfile.TemporaryFile('w+b') as plt_file:\nwaterfall_fig.savefig(plt_file, format='png', bbox_inches='tight')\nplt_file.seek(0)\nencoded = base64.b64encode(plt_file.read())\nplt.clf()\nhtml_waterfall = f\"&lt;img src=\\'data:image/png;base64, {encoded.decode('utf-8')}\\' class=\\\"shap_fig\\\" &gt;\"  # Class name is used in the demonstrator\n# Force figure\nhtml_force = shap.plots.force(shap_values[0]).html()\n# Combine &amp; return\nfinal_html = f\"&lt;head&gt;{shap.getjs()}&lt;/head&gt;{html_waterfall}&lt;br&gt;&lt;body&gt;{html_force}&lt;/body&gt;\"\nreturn final_html\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, class_or_label_index=None, **kwargs)</code>","text":"<p>Explains predictions by returning a JSON serializable object</p> <p>This function calls the Shap module.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>pd.DataFrame</code> <p>entries to be explained</p> required Kwargs <p>class_or_label_index (int): for classification only. Class or label index to be considered.</p> <p>Returns:</p> Type Description <code>Union[dict, list]</code> <p>Shap values</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: pd.DataFrame, class_or_label_index: Union[int, None] = None, **kwargs) -&gt; Union[dict, list]:\n'''Explains predictions by returning a JSON serializable object\n    This function calls the Shap module.\n    Args:\n        content (pd.DataFrame): entries to be explained\n    Kwargs:\n        class_or_label_index (int): for classification only. Class or label index to be considered.\n    Returns:\n        (Union[dict, list]): Shap values\n    '''\nreturn [\n{\n\"features\": explanation.feature_names, \n\"preprocessed_values\": explanation.data, \n\"shap_values\": explanation.values, \n\"shap_base_values\": explanation.base_values,\n}\nfor explanation in self.explain_instance(content, class_or_label_index=class_or_label_index, **kwargs)\n]\n</code></pre>"},{"location":"reference/template_num/monitoring/model_explainer/#template_num.monitoring.model_explainer.ShapExplainer.regressor_fn","title":"<code>regressor_fn(content_prep)</code>","text":"<p>Function to get predictions from a dataset (already preprocessed) - regressors</p> <p>Parameters:</p> Name Type Description Default <code>content_prep</code> <code>pd.DataFrame</code> <p>dataset (already preprocessed) to be considered</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.array: predictions</p> Source code in <code>template_num/monitoring/model_explainer.py</code> <pre><code>def regressor_fn(self, content_prep: pd.DataFrame) -&gt; np.ndarray:\n'''Function to get predictions from a dataset (already preprocessed) - regressors\n    Args:\n        content_prep (pd.DataFrame): dataset (already preprocessed) to be considered\n    Returns:\n        np.array: predictions\n    '''\n# Get predictions\n# Mypy raises a false error here, needs to be ignored\nreturn self.model.predict(content_prep)  # type: ignore\n</code></pre>"},{"location":"reference/template_num/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/template_num/preprocessing/column_preprocessors/","title":"Column preprocessors","text":""},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoBinner","title":"<code>AutoBinner</code>","text":"<p>         Bases: <code>Estimator</code></p> <p>Automatically creates a \"other\" category when the categories are heavily unbalanced /! Replaces the values of some categories /!\\</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>class AutoBinner(Estimator):\n'''Automatically creates a \"other\" category when the categories are heavily unbalanced\n    /!\\\\ Replaces the values of some categories /!\\\\\n    '''\ndef __init__(self, strategy: str = \"auto\", min_cat_count: int = 3, threshold: float = 0.05) -&gt; None:\n'''Initialization of the class\n        Kwargs:\n            strategy (str): 'auto' or 'threshold'\n                - 'auto': Aggregates all categories as long as their cumulated frequence is less than threshold\n                - 'threshold': Aggregates all category whose frequence is less than threshold\n            min_cat_count (int): Minimal number of category to keep\n            threshold (float): The threshold to consider\n        Raises:\n            ValueError: The object strategy must be in the list of allowed strategies\n            ValueError: The object min_cat_count must be non negative\n            ValueError: The object threshold must be in ]0,1[\n        '''\nsuper().__init__(None)\nallowed_strategies = [\"threshold\", \"auto\"]\nself.strategy = strategy\nif self.strategy not in allowed_strategies:\nraise ValueError(f\"Can only use these strategies: {allowed_strategies}. \"\nf\"Got strategy={strategy}\")\nif min_cat_count &lt; 0:\nraise ValueError(\"min_cat_count must be non negative\")\nif not (0 &lt; threshold &lt; 1):\nraise ValueError(f\"threshold must be in ]0,1[, not {threshold}\")\n# Set attributes\nself.min_cat_count = min_cat_count\nself.threshold = threshold\nself.kept_cat_by_index: Dict[int, list] = {}\ndef fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n'''Fit the AutoBinner on X.\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n        Kwargs:\n            y: Not used here\n        Returns:\n            self: AutoBinner\n        '''\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\nself.input_length = X.shape[1]\n# Fits column one by one\nfor col_index in range(self.input_length):\n# Get col serie\nX_tmp_ser = X.iloc[:, col_index]\n# Get unique vals\nunique_cat = list(X_tmp_ser.unique())\n# If less vals than min threshold, set this column allowed values with all uniques values\nif len(unique_cat) &lt;= self.min_cat_count:\nself.kept_cat_by_index[col_index] = unique_cat\ncontinue\n# If more vals than min threshold, keep values based on strategy\ntable = X_tmp_ser.value_counts() / X_tmp_ser.count()\ntable = table.sort_values()\nif self.strategy == 'auto':\ntable = np.cumsum(table)\n# If only one category is less than the threshold, we do not need to transform it\nif table[1] &gt; self.threshold:\nself.kept_cat_by_index[col_index] = unique_cat\ncontinue\n# Otherwise, we get rid of the superfluous categories\nelse:\nto_remove = list(table[table &lt; self.threshold].index)\nfor item in to_remove:\nunique_cat.remove(item)\nself.kept_cat_by_index[col_index] = unique_cat\nself.fitted_ = True\nreturn self\ndef transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n'''Imputes all missing values in X.\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n                The input data to complete.\n        '''\ncheck_is_fitted(self, 'fitted_')\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\nfor col_index in range(self.input_length):\nX.iloc[:, col_index] = X.iloc[:, col_index].apply(lambda x: x if x in self.kept_cat_by_index[col_index] else 'other_')\nreturn X.to_numpy()  # Compatibility, returns a numpy array\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoBinner.__init__","title":"<code>__init__(strategy='auto', min_cat_count=3, threshold=0.05)</code>","text":"<p>Initialization of the class</p> Kwargs <p>strategy (str): 'auto' or 'threshold'     - 'auto': Aggregates all categories as long as their cumulated frequence is less than threshold     - 'threshold': Aggregates all category whose frequence is less than threshold min_cat_count (int): Minimal number of category to keep threshold (float): The threshold to consider</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>The object strategy must be in the list of allowed strategies</p> <code>ValueError</code> <p>The object min_cat_count must be non negative</p> <code>ValueError</code> <p>The object threshold must be in ]0,1[</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def __init__(self, strategy: str = \"auto\", min_cat_count: int = 3, threshold: float = 0.05) -&gt; None:\n'''Initialization of the class\n    Kwargs:\n        strategy (str): 'auto' or 'threshold'\n            - 'auto': Aggregates all categories as long as their cumulated frequence is less than threshold\n            - 'threshold': Aggregates all category whose frequence is less than threshold\n        min_cat_count (int): Minimal number of category to keep\n        threshold (float): The threshold to consider\n    Raises:\n        ValueError: The object strategy must be in the list of allowed strategies\n        ValueError: The object min_cat_count must be non negative\n        ValueError: The object threshold must be in ]0,1[\n    '''\nsuper().__init__(None)\nallowed_strategies = [\"threshold\", \"auto\"]\nself.strategy = strategy\nif self.strategy not in allowed_strategies:\nraise ValueError(f\"Can only use these strategies: {allowed_strategies}. \"\nf\"Got strategy={strategy}\")\nif min_cat_count &lt; 0:\nraise ValueError(\"min_cat_count must be non negative\")\nif not (0 &lt; threshold &lt; 1):\nraise ValueError(f\"threshold must be in ]0,1[, not {threshold}\")\n# Set attributes\nself.min_cat_count = min_cat_count\nself.threshold = threshold\nself.kept_cat_by_index: Dict[int, list] = {}\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoBinner.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit the AutoBinner on X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or pd.DataFrame</code> <p>Shape (n_samples, n_features)</p> required Kwargs <p>y: Not used here</p> <p>Returns:</p> Name Type Description <code>self</code> <code>Any</code> <p>AutoBinner</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n'''Fit the AutoBinner on X.\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n    Kwargs:\n        y: Not used here\n    Returns:\n        self: AutoBinner\n    '''\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\nself.input_length = X.shape[1]\n# Fits column one by one\nfor col_index in range(self.input_length):\n# Get col serie\nX_tmp_ser = X.iloc[:, col_index]\n# Get unique vals\nunique_cat = list(X_tmp_ser.unique())\n# If less vals than min threshold, set this column allowed values with all uniques values\nif len(unique_cat) &lt;= self.min_cat_count:\nself.kept_cat_by_index[col_index] = unique_cat\ncontinue\n# If more vals than min threshold, keep values based on strategy\ntable = X_tmp_ser.value_counts() / X_tmp_ser.count()\ntable = table.sort_values()\nif self.strategy == 'auto':\ntable = np.cumsum(table)\n# If only one category is less than the threshold, we do not need to transform it\nif table[1] &gt; self.threshold:\nself.kept_cat_by_index[col_index] = unique_cat\ncontinue\n# Otherwise, we get rid of the superfluous categories\nelse:\nto_remove = list(table[table &lt; self.threshold].index)\nfor item in to_remove:\nunique_cat.remove(item)\nself.kept_cat_by_index[col_index] = unique_cat\nself.fitted_ = True\nreturn self\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoBinner.transform","title":"<code>transform(X)</code>","text":"<p>Imputes all missing values in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or pd.DataFrame</code> <p>Shape (n_samples, n_features) The input data to complete.</p> required Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n'''Imputes all missing values in X.\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n            The input data to complete.\n    '''\ncheck_is_fitted(self, 'fitted_')\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\nfor col_index in range(self.input_length):\nX.iloc[:, col_index] = X.iloc[:, col_index].apply(lambda x: x if x in self.kept_cat_by_index[col_index] else 'other_')\nreturn X.to_numpy()  # Compatibility, returns a numpy array\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoLogTransform","title":"<code>AutoLogTransform</code>","text":"<p>         Bases: <code>Estimator</code></p> <p>Automatically applies a log transformation on numerical data if the distribution of the variables    is skewed (abs(skew) &gt; min_skewness) and if there is an amplitude superior to min_amplitude between    the 10th and 90th percentiles</p> <p>WARNING: YOUR DATA MUST BE POSITIVE IN ORDER FOR EVERYTHING TO WORK CORRECTLY</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>class AutoLogTransform(Estimator):\n'''Automatically applies a log transformation on numerical data if the distribution of the variables\n       is skewed (abs(skew) &gt; min_skewness) and if there is an amplitude superior to min_amplitude between\n       the 10th and 90th percentiles\n    WARNING: YOUR DATA MUST BE POSITIVE IN ORDER FOR EVERYTHING TO WORK CORRECTLY\n    '''\ndef __init__(self, min_skewness: float = 2, min_amplitude: float = 10E3) -&gt; None:\n'''Initialization of the class\n        Kwargs:\n            min_skewness (float): Absolute value of the required skewness to apply a log transformation\n            min_amplitude (float): Minimal value of the amplitude between the 10th and 90th percentiles\n                                   required to apply a log transformation\n        '''\nsuper().__init__(None)\n# Set attributes\nself.min_skewness = min_skewness\nself.min_amplitude = min_amplitude\n# Columns on which to apply the transformation\n# Set on fit\n# Warning: sklearn does not support columns name, so we can only use indexes\n# Hence, X input must expose same columns order (this won't be checked)\nself.applicable_columns_index: Optional[List[Any]] = None\ndef fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n'''Fit transformer\n        Args:\n            X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        Kwargs:\n            y (None): Not used here\n        Returns:\n            self\n        '''\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\n# Otherwise, we reset the columns name because sklearn can't manage them\nelse:\nX = X.rename(columns={col: i for i, col in enumerate(X.columns)})\nself.input_length = X.shape[1]\n# Get applicable columns\nskew = X.skew()\ncandidates = list(skew[abs(skew) &gt; self.min_skewness].index)\nif len(candidates) &gt; 0:\nq10 = X.iloc[:, candidates].quantile(q=0.1)\nq90 = X.iloc[:, candidates].quantile(q=0.9)\namp = q90 - q10\n# Update applicable_columns_index\nself.applicable_columns_index = list(amp[amp &gt; self.min_amplitude].index)\nself.fitted_ = True\nreturn self\ndef transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n'''Transforms X - apply log on applicable columns\n        Args:\n            X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        Returns:\n            X_out: Array-like, shape [n_samples, n_features]\n                        Transformed input.\n        '''\n# Validate input\ncheck_is_fitted(self, 'fitted_')\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\n# Log transformation on the applicable columns\nif len(self.applicable_columns_index) &gt; 0:\nX.iloc[:, self.applicable_columns_index] = np.log(X.iloc[:, self.applicable_columns_index])\n# Compatibility -&gt; returns numpy array\nreturn X.to_numpy()\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoLogTransform.__init__","title":"<code>__init__(min_skewness=2, min_amplitude=10000.0)</code>","text":"<p>Initialization of the class</p> Kwargs <p>min_skewness (float): Absolute value of the required skewness to apply a log transformation min_amplitude (float): Minimal value of the amplitude between the 10th and 90th percentiles                        required to apply a log transformation</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def __init__(self, min_skewness: float = 2, min_amplitude: float = 10E3) -&gt; None:\n'''Initialization of the class\n    Kwargs:\n        min_skewness (float): Absolute value of the required skewness to apply a log transformation\n        min_amplitude (float): Minimal value of the amplitude between the 10th and 90th percentiles\n                               required to apply a log transformation\n    '''\nsuper().__init__(None)\n# Set attributes\nself.min_skewness = min_skewness\nself.min_amplitude = min_amplitude\n# Columns on which to apply the transformation\n# Set on fit\n# Warning: sklearn does not support columns name, so we can only use indexes\n# Hence, X input must expose same columns order (this won't be checked)\nself.applicable_columns_index: Optional[List[Any]] = None\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoLogTransform.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit transformer</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or pd.DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required Kwargs <p>y (None): Not used here</p> <p>Returns:</p> Type Description <code>Any</code> <p>self</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n'''Fit transformer\n    Args:\n        X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n    Kwargs:\n        y (None): Not used here\n    Returns:\n        self\n    '''\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\n# Otherwise, we reset the columns name because sklearn can't manage them\nelse:\nX = X.rename(columns={col: i for i, col in enumerate(X.columns)})\nself.input_length = X.shape[1]\n# Get applicable columns\nskew = X.skew()\ncandidates = list(skew[abs(skew) &gt; self.min_skewness].index)\nif len(candidates) &gt; 0:\nq10 = X.iloc[:, candidates].quantile(q=0.1)\nq90 = X.iloc[:, candidates].quantile(q=0.9)\namp = q90 - q10\n# Update applicable_columns_index\nself.applicable_columns_index = list(amp[amp &gt; self.min_amplitude].index)\nself.fitted_ = True\nreturn self\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.AutoLogTransform.transform","title":"<code>transform(X)</code>","text":"<p>Transforms X - apply log on applicable columns</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or pd.DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Returns:</p> Name Type Description <code>X_out</code> <code>np.ndarray</code> <p>Array-like, shape [n_samples, n_features]         Transformed input.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n'''Transforms X - apply log on applicable columns\n    Args:\n        X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n    Returns:\n        X_out: Array-like, shape [n_samples, n_features]\n                    Transformed input.\n    '''\n# Validate input\ncheck_is_fitted(self, 'fitted_')\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\n# Log transformation on the applicable columns\nif len(self.applicable_columns_index) &gt; 0:\nX.iloc[:, self.applicable_columns_index] = np.log(X.iloc[:, self.applicable_columns_index])\n# Compatibility -&gt; returns numpy array\nreturn X.to_numpy()\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer","title":"<code>EmbeddingTransformer</code>","text":"<p>         Bases: <code>Estimator</code></p> <p>Constructs a transformer that apply an embedding mapping to Categorical columns</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>class EmbeddingTransformer(Estimator):\n'''Constructs a transformer that apply an embedding mapping to Categorical columns'''\ndef __init__(self, embedding: Union[str, dict], none_strategy: str = 'zeros') -&gt; None:\n'''Initialization of the class\n        Args:\n            embedding (str or dict): Embedding to use\n                - If dict -&gt; ok, ready to go\n                - If str -&gt; path to the file to load (json)\n        Kwargs:\n            none_strategy (str): Strategy to fill elements not in embedding\n                - Zeros: only 0s\n        Raises:\n            ValueError: If strategy is not in the allowed strategies\n            ValueError: If the embedding is of type str but does not end in .json\n            FileNotFoundError: If the path to the embedding does not exist\n        '''\nsuper().__init__(None)\n# Check none strategy\nallowed_strategies = [\"zeros\"]\nself.none_strategy = none_strategy\nif self.none_strategy not in allowed_strategies:\nraise ValueError(f\"Can only use these strategies: {allowed_strategies}, got strategy={self.none_strategy}\")\n# If str, loads the embedding\nif isinstance(embedding, str):\nif not embedding.endswith('.json'):\nraise ValueError(f\"The file {embedding} must be a .json file\")\nif not os.path.exists(embedding):\nraise FileNotFoundError(f\"The file {embedding} does not exist\")\nwith open(embedding, 'r', encoding='utf-8') as f:\nself.embedding = json.load(f)\nelse:\nself.embedding = embedding\n# Get embedding size\nself.embedding_size = len(self.embedding[list(self.embedding.keys())[0]])\n# Other params\nself.n_missed = 0\ndef fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n'''Fit transformer\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n        Kwargs:\n            y: Not used here\n        Returns\n            self (EmbeddingTransformer)\n        '''\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\nself.input_length = X.shape[1]\n# Nothing to do\nself.fitted_ = True\nreturn self\ndef transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n'''Transform X - embedding mapping\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n        Raises:\n            ValueError: If there are missing columns\n        Returns:\n            X_out (np.ndarray): Shape (n_samples, n_features) transformed input.\n        '''\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\nn_rows = X.shape[0]\n# Apply mapping\nnew_df = pd.DataFrame()\nfor col in X.columns:\nself.n_missed = 0  # Counts the number of missing elements in the embedding\ntmp_serie = X[col].apply(self.apply_embedding)  # Updates self.n_missed\nnew_df = pd.concat([new_df, pd.DataFrame(tmp_serie.to_list())], axis=1)\nperc_missed = self.n_missed / n_rows * 100\nif perc_missed != 0:\nlogger.warning(f\"Warning, {self.n_missed} ({perc_missed} %) missing elements in the embedding for column {col}\")\nreturn new_df.to_numpy()  # Compatibility, returns a numpy array\ndef apply_embedding(self, content) -&gt; list:\n'''Applies embedding mapping\n        Args:\n            content: Content on which to apply embedding mapping\n        Raises:\n            ValueError: If the strategy is not recognized\n        Returns:\n            list: Applied embedding\n        '''\nif content in self.embedding.keys():\nreturn self.embedding[content]\nelse:\nself.n_missed += 1\nif self.none_strategy == 'zeros':\nreturn [0] * self.embedding_size\nelse:\nraise ValueError(f\"Strategy {self.none_strategy} not recognized\")\ndef get_feature_names(self, features_in: list, *args, **kwargs) -&gt; np.ndarray:\n'''Returns feature names for output features.\n        Args:\n            features_in (list): list of features\n        Returns:\n            output_feature_names: ndarray of shape (n_output_features,)\n                Array of feature names.\n        '''\ncheck_is_fitted(self, 'fitted_')\nnew_features = [f\"emb_{feat}_{i}\" for feat in features_in for i in range(self.embedding_size)]\nreturn np.array(new_features, dtype=object)\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer.__init__","title":"<code>__init__(embedding, none_strategy='zeros')</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>embedding</code> <code>str or dict</code> <p>Embedding to use - If dict -&gt; ok, ready to go - If str -&gt; path to the file to load (json)</p> required Kwargs <p>none_strategy (str): Strategy to fill elements not in embedding     - Zeros: only 0s</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If strategy is not in the allowed strategies</p> <code>ValueError</code> <p>If the embedding is of type str but does not end in .json</p> <code>FileNotFoundError</code> <p>If the path to the embedding does not exist</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def __init__(self, embedding: Union[str, dict], none_strategy: str = 'zeros') -&gt; None:\n'''Initialization of the class\n    Args:\n        embedding (str or dict): Embedding to use\n            - If dict -&gt; ok, ready to go\n            - If str -&gt; path to the file to load (json)\n    Kwargs:\n        none_strategy (str): Strategy to fill elements not in embedding\n            - Zeros: only 0s\n    Raises:\n        ValueError: If strategy is not in the allowed strategies\n        ValueError: If the embedding is of type str but does not end in .json\n        FileNotFoundError: If the path to the embedding does not exist\n    '''\nsuper().__init__(None)\n# Check none strategy\nallowed_strategies = [\"zeros\"]\nself.none_strategy = none_strategy\nif self.none_strategy not in allowed_strategies:\nraise ValueError(f\"Can only use these strategies: {allowed_strategies}, got strategy={self.none_strategy}\")\n# If str, loads the embedding\nif isinstance(embedding, str):\nif not embedding.endswith('.json'):\nraise ValueError(f\"The file {embedding} must be a .json file\")\nif not os.path.exists(embedding):\nraise FileNotFoundError(f\"The file {embedding} does not exist\")\nwith open(embedding, 'r', encoding='utf-8') as f:\nself.embedding = json.load(f)\nelse:\nself.embedding = embedding\n# Get embedding size\nself.embedding_size = len(self.embedding[list(self.embedding.keys())[0]])\n# Other params\nself.n_missed = 0\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer.apply_embedding","title":"<code>apply_embedding(content)</code>","text":"<p>Applies embedding mapping</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <p>Content on which to apply embedding mapping</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the strategy is not recognized</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Applied embedding</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def apply_embedding(self, content) -&gt; list:\n'''Applies embedding mapping\n    Args:\n        content: Content on which to apply embedding mapping\n    Raises:\n        ValueError: If the strategy is not recognized\n    Returns:\n        list: Applied embedding\n    '''\nif content in self.embedding.keys():\nreturn self.embedding[content]\nelse:\nself.n_missed += 1\nif self.none_strategy == 'zeros':\nreturn [0] * self.embedding_size\nelse:\nraise ValueError(f\"Strategy {self.none_strategy} not recognized\")\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fit transformer</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or pd.DataFrame</code> <p>Shape (n_samples, n_features)</p> required Kwargs <p>y: Not used here</p> <p>Returns     self (EmbeddingTransformer)</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n'''Fit transformer\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n    Kwargs:\n        y: Not used here\n    Returns\n        self (EmbeddingTransformer)\n    '''\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\nself.input_length = X.shape[1]\n# Nothing to do\nself.fitted_ = True\nreturn self\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer.get_feature_names","title":"<code>get_feature_names(features_in, *args, **kwargs)</code>","text":"<p>Returns feature names for output features.</p> <p>Parameters:</p> Name Type Description Default <code>features_in</code> <code>list</code> <p>list of features</p> required <p>Returns:</p> Name Type Description <code>output_feature_names</code> <code>np.ndarray</code> <p>ndarray of shape (n_output_features,) Array of feature names.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def get_feature_names(self, features_in: list, *args, **kwargs) -&gt; np.ndarray:\n'''Returns feature names for output features.\n    Args:\n        features_in (list): list of features\n    Returns:\n        output_feature_names: ndarray of shape (n_output_features,)\n            Array of feature names.\n    '''\ncheck_is_fitted(self, 'fitted_')\nnew_features = [f\"emb_{feat}_{i}\" for feat in features_in for i in range(self.embedding_size)]\nreturn np.array(new_features, dtype=object)\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.EmbeddingTransformer.transform","title":"<code>transform(X)</code>","text":"<p>Transform X - embedding mapping</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or pd.DataFrame</code> <p>Shape (n_samples, n_features)</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If there are missing columns</p> <p>Returns:</p> Name Type Description <code>X_out</code> <code>np.ndarray</code> <p>Shape (n_samples, n_features) transformed input.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n'''Transform X - embedding mapping\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n    Raises:\n        ValueError: If there are missing columns\n    Returns:\n        X_out (np.ndarray): Shape (n_samples, n_features) transformed input.\n    '''\nX = self._validate_input(X)\n# If x is a numpy array, casts it in pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\nn_rows = X.shape[0]\n# Apply mapping\nnew_df = pd.DataFrame()\nfor col in X.columns:\nself.n_missed = 0  # Counts the number of missing elements in the embedding\ntmp_serie = X[col].apply(self.apply_embedding)  # Updates self.n_missed\nnew_df = pd.concat([new_df, pd.DataFrame(tmp_serie.to_list())], axis=1)\nperc_missed = self.n_missed / n_rows * 100\nif perc_missed != 0:\nlogger.warning(f\"Warning, {self.n_missed} ({perc_missed} %) missing elements in the embedding for column {col}\")\nreturn new_df.to_numpy()  # Compatibility, returns a numpy array\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.Estimator","title":"<code>Estimator</code>","text":"<p>         Bases: <code>BaseEstimator</code></p> <p>Base class for the classes defined below. Implements _validate_input and fit_transform.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>class Estimator(BaseEstimator):\n'''Base class for the classes defined below. Implements _validate_input and fit_transform.'''\ndef __init__(self, input_length: Union[int, None]) -&gt; None:\n'''Initialization of the class\n        Args:\n            input_length (int): The number of columns of the input (used in _validate_input())\n        '''\nself.input_length = input_length\ndef _validate_input(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; Union[np.ndarray, pd.DataFrame]:\n'''Validates input format\n        Args:\n            X (np.ndarray or pd.DataFrame): Input to validate\n        Raises:\n            ValueError: If the shape of the input does not correspond to self.input_length\n        Returns:\n            np.ndarray or pd.DataFrame: A copy of X\n        '''\nif self.input_length is not None and X.shape[1] != self.input_length:\nraise ValueError(f\"Bad shape: ({X.shape[1]} != {self.input_length})\")\n# Mandatory copy in order not to modify the original !\nif isinstance(X, pd.DataFrame):\nreturn X.copy(deep=True)\nelse:\nreturn X.copy()\ndef fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series, pd.DataFrame]) -&gt; Any:\n'''Fit transformer\n        Args:\n            X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n            y (np.ndarray or pd.Series or pd.DataFrame): Array-like, shape = [n_samples, n_targets]\n        Returns:\n            self\n        '''\nraise NotImplementedError(\"'fit' needs to be overridden\")\ndef transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n'''Transforms X\n        Args:\n            X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        Returns:\n            Transformed X\n        '''\nraise NotImplementedError(\"'transform' needs to be overridden\")\ndef fit_transform(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series, pd.DataFrame, None] = None) -&gt; np.ndarray:\n'''Applies both fit &amp; transform.\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape = [n_samples, n_features]\n        Kwargs:\n            y: Array-like, shape = [n_samples]\n        Returns:\n            X_out: Array-like, shape [n_samples, n_features]\n                        Transformed input.\n        '''\nself.fit(X, y)\nreturn self.transform(X)\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.Estimator.__init__","title":"<code>__init__(input_length)</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>input_length</code> <code>int</code> <p>The number of columns of the input (used in _validate_input())</p> required Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def __init__(self, input_length: Union[int, None]) -&gt; None:\n'''Initialization of the class\n    Args:\n        input_length (int): The number of columns of the input (used in _validate_input())\n    '''\nself.input_length = input_length\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.Estimator.fit","title":"<code>fit(X, y)</code>","text":"<p>Fit transformer</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or pd.DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required <code>y</code> <code>np.ndarray or pd.Series or pd.DataFrame</code> <p>Array-like, shape = [n_samples, n_targets]</p> required <p>Returns:</p> Type Description <code>Any</code> <p>self</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series, pd.DataFrame]) -&gt; Any:\n'''Fit transformer\n    Args:\n        X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n        y (np.ndarray or pd.Series or pd.DataFrame): Array-like, shape = [n_samples, n_targets]\n    Returns:\n        self\n    '''\nraise NotImplementedError(\"'fit' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.Estimator.fit_transform","title":"<code>fit_transform(X, y=None)</code>","text":"<p>Applies both fit &amp; transform.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or pd.DataFrame</code> <p>Shape = [n_samples, n_features]</p> required Kwargs <p>y: Array-like, shape = [n_samples]</p> <p>Returns:</p> Name Type Description <code>X_out</code> <code>np.ndarray</code> <p>Array-like, shape [n_samples, n_features]         Transformed input.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit_transform(self, X: Union[np.ndarray, pd.DataFrame], y: Union[np.ndarray, pd.Series, pd.DataFrame, None] = None) -&gt; np.ndarray:\n'''Applies both fit &amp; transform.\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape = [n_samples, n_features]\n    Kwargs:\n        y: Array-like, shape = [n_samples]\n    Returns:\n        X_out: Array-like, shape [n_samples, n_features]\n                    Transformed input.\n    '''\nself.fit(X, y)\nreturn self.transform(X)\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.Estimator.transform","title":"<code>transform(X)</code>","text":"<p>Transforms X</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or pd.DataFrame</code> <p>Array-like, shape = [n_samples, n_features]</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Transformed X</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n'''Transforms X\n    Args:\n        X (np.ndarray or pd.DataFrame): Array-like, shape = [n_samples, n_features]\n    Returns:\n        Transformed X\n    '''\nraise NotImplementedError(\"'transform' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.ThresholdingTransform","title":"<code>ThresholdingTransform</code>","text":"<p>         Bases: <code>Estimator</code></p> <p>Applies a threshold on columns. If min and max values are given, the threshold is manual; otherwise it is statistical.</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>class ThresholdingTransform(Estimator):\n'''Applies a threshold on columns.\n    If min and max values are given, the threshold is manual; otherwise it is statistical.\n    '''\ndef __init__(self, thresholds: List[Tuple], quantiles: tuple = (0.05, 0.95)) -&gt; None:\n'''Initialization of the class\n        Args:\n            tresholds (list of tuple): Each tuple contains (min_val, max_val).\n        Kwargs:\n            quantiles (tuple): Tuple containing (quantile_min, quantile_max)\n        Raises:\n            ValueError: If quantiles values are not between 0 and 1 and if quantiles[0] &gt;= quantiles[1]\n        '''\nif not 0 &lt; quantiles[0] &lt; 1 or not 0 &lt; quantiles[1] &lt; 1 or not quantiles[0] &lt; quantiles[1]:\nraise ValueError(f\"The values contained in quantiles should verify quantile_min &lt; quantile_max and both &gt; 0 and &lt; 1. quantiles = {quantiles} is not supported.\")\nsuper().__init__(len(thresholds))\n# Set attributes\nself.thresholds = thresholds\nself.fitted_thresholds: List[tuple] = []\nself.quantiles = quantiles\ndef fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n'''Fits the ThresholdingTransform on X.\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape [n_samples, n_features]\n        Kwargs:\n            y (None): Not used here.\n        Returns:\n            self: ThresholdingTransform\n        '''\nX = self._validate_input(X)\n# If X is a numpy array, casts it as a pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\n# Fits column one by one\nfor col_index, item in enumerate(self.thresholds):\nval_min, val_max = item\nif val_min is None:\nval_min = X.iloc[:, col_index].quantile(q=self.quantiles[0])\nif val_max is None:\nval_max = X.iloc[:, col_index].quantile(q=self.quantiles[1])\nself.fitted_thresholds.append((col_index, val_min, val_max))\nself.fitted_ = True\nreturn self\ndef transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n'''Impute all missing values in X.\n        Args:\n            X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n                The input data to complete.\n        '''\ncheck_is_fitted(self, 'fitted_')\nX = self._validate_input(X)\n# If X is a numpy array, casts it to pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\nfor col_index, val_min, val_max in self.fitted_thresholds:\nX.iloc[:, col_index][X.iloc[:, col_index] &lt; val_min] = val_min\nX.iloc[:, col_index][X.iloc[:, col_index] &gt; val_max] = val_max\nreturn X.to_numpy()  # Compatibility -&gt; return a numpy array\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.ThresholdingTransform.__init__","title":"<code>__init__(thresholds, quantiles=(0.05, 0.95))</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>tresholds</code> <code>list of tuple</code> <p>Each tuple contains (min_val, max_val).</p> required Kwargs <p>quantiles (tuple): Tuple containing (quantile_min, quantile_max)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If quantiles values are not between 0 and 1 and if quantiles[0] &gt;= quantiles[1]</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def __init__(self, thresholds: List[Tuple], quantiles: tuple = (0.05, 0.95)) -&gt; None:\n'''Initialization of the class\n    Args:\n        tresholds (list of tuple): Each tuple contains (min_val, max_val).\n    Kwargs:\n        quantiles (tuple): Tuple containing (quantile_min, quantile_max)\n    Raises:\n        ValueError: If quantiles values are not between 0 and 1 and if quantiles[0] &gt;= quantiles[1]\n    '''\nif not 0 &lt; quantiles[0] &lt; 1 or not 0 &lt; quantiles[1] &lt; 1 or not quantiles[0] &lt; quantiles[1]:\nraise ValueError(f\"The values contained in quantiles should verify quantile_min &lt; quantile_max and both &gt; 0 and &lt; 1. quantiles = {quantiles} is not supported.\")\nsuper().__init__(len(thresholds))\n# Set attributes\nself.thresholds = thresholds\nself.fitted_thresholds: List[tuple] = []\nself.quantiles = quantiles\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.ThresholdingTransform.fit","title":"<code>fit(X, y=None)</code>","text":"<p>Fits the ThresholdingTransform on X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or pd.DataFrame</code> <p>Shape [n_samples, n_features]</p> required Kwargs <p>y (None): Not used here.</p> <p>Returns:</p> Name Type Description <code>self</code> <code>Any</code> <p>ThresholdingTransform</p> Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def fit(self, X: Union[np.ndarray, pd.DataFrame], y: Any = None) -&gt; Any:\n'''Fits the ThresholdingTransform on X.\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape [n_samples, n_features]\n    Kwargs:\n        y (None): Not used here.\n    Returns:\n        self: ThresholdingTransform\n    '''\nX = self._validate_input(X)\n# If X is a numpy array, casts it as a pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\n# Fits column one by one\nfor col_index, item in enumerate(self.thresholds):\nval_min, val_max = item\nif val_min is None:\nval_min = X.iloc[:, col_index].quantile(q=self.quantiles[0])\nif val_max is None:\nval_max = X.iloc[:, col_index].quantile(q=self.quantiles[1])\nself.fitted_thresholds.append((col_index, val_min, val_max))\nself.fitted_ = True\nreturn self\n</code></pre>"},{"location":"reference/template_num/preprocessing/column_preprocessors/#template_num.preprocessing.column_preprocessors.ThresholdingTransform.transform","title":"<code>transform(X)</code>","text":"<p>Impute all missing values in X.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>np.ndarray or pd.DataFrame</code> <p>Shape (n_samples, n_features) The input data to complete.</p> required Source code in <code>template_num/preprocessing/column_preprocessors.py</code> <pre><code>def transform(self, X: Union[np.ndarray, pd.DataFrame]) -&gt; np.ndarray:\n'''Impute all missing values in X.\n    Args:\n        X (np.ndarray or pd.DataFrame): Shape (n_samples, n_features)\n            The input data to complete.\n    '''\ncheck_is_fitted(self, 'fitted_')\nX = self._validate_input(X)\n# If X is a numpy array, casts it to pd.DataFrame\nif isinstance(X, np.ndarray):\nX = pd.DataFrame(X)\nfor col_index, val_min, val_max in self.fitted_thresholds:\nX.iloc[:, col_index][X.iloc[:, col_index] &lt; val_min] = val_min\nX.iloc[:, col_index][X.iloc[:, col_index] &gt; val_max] = val_max\nreturn X.to_numpy()  # Compatibility -&gt; return a numpy array\n</code></pre>"},{"location":"reference/template_num/preprocessing/outlier_detection/","title":"Outlier detection","text":""},{"location":"reference/template_num/preprocessing/outlier_detection/#template_num.preprocessing.outlier_detection.check_for_outliers","title":"<code>check_for_outliers(X, n_estimators=100, n_neighbors=20)</code>","text":"<p>Agreggates two results of outliers detection and warns the user if some were detected</p> Args <p>X (np.ndarray, pd.DataFrame): Shape = [n_samples, n_features]</p> Kwargs <p>n_estimators (int): number of estimators for the IsolationForest     If 0, do not IsolationForest n_neighbors (int): number of neighbors for the LocalOutlierFactor     If 0, do not use LocalOutlierFactor</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n_estimators &lt; 0</p> <code>ValueError</code> <p>If n_neighbors &lt; 0</p> <code>ValueError</code> <p>If both n_estimators and n_neighbors are equal to 0</p> <p>Returns:</p> Name Type Description <code>outliers</code> <code>np.ndarray</code> <p>1d array of n_samples containing -1 if outlier, 1 otherwise</p> Source code in <code>template_num/preprocessing/outlier_detection.py</code> <pre><code>def check_for_outliers(X: Union[pd.DataFrame, np.ndarray], n_estimators: int = 100, n_neighbors: int = 20) -&gt; np.ndarray:\n'''Agreggates two results of outliers detection and warns the user if some were detected\n    Args :\n        X (np.ndarray, pd.DataFrame): Shape = [n_samples, n_features]\n    Kwargs:\n        n_estimators (int): number of estimators for the IsolationForest\n            If 0, do not IsolationForest\n        n_neighbors (int): number of neighbors for the LocalOutlierFactor\n            If 0, do not use LocalOutlierFactor\n    Raises:\n        ValueError: If n_estimators &lt; 0\n        ValueError: If n_neighbors &lt; 0\n        ValueError: If both n_estimators and n_neighbors are equal to 0\n    Returns:\n        outliers (np.ndarray): 1d array of n_samples containing -1 if outlier, 1 otherwise\n    '''\n# Manage errors\nif n_estimators &lt; 0:\nraise ValueError(\"n_estimators must be positive\")\nif n_neighbors &lt; 0:\nraise ValueError(\"n_neighbors must be positive\")\nif n_estimators + n_neighbors == 0:\nraise ValueError(\"n_neighbors and n_estimators can't both be equal to 0\")\n# Init. outliers (1 = not an outlier, -1 = outlier)\noutliers = np.ones(X.shape[0], dtype=int)\n# Get outliers from IsolationForest\nif not n_estimators == 0:\nrun_forest = IsolationForest(n_estimators=n_estimators)\noutliers |= run_forest.fit_predict(X)  # In-place union\nelse:\nlogger.info(\"IsolationForest is skipped (n_estimators == 0)\")\n# Get outliers from LocalOutlierFactor\nif not n_neighbors == 0:\nlof = LocalOutlierFactor(n_neighbors=n_neighbors)\noutliers |= lof.fit_predict(X)  # In-place union\nelse:\nlogger.info(\"LocalOutlierFactor is skipped (n_neighbors == 0)\")\n# Logger\nif int(cmath.exp(1j * integrate.quad(lambda x: math.sqrt(1 - pow(x, 2)), -1, 1)[0] * 2).real) in outliers:\nlogger.warning(\"The dataset seems to contain outliers at indices:\")\nlogger.warning(\", \".join(str(v) for v in list(np.where(outliers == -1)[0])))\n# Return outliers\nreturn outliers\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/","title":"Preprocess","text":""},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.get_ct_feature_names","title":"<code>get_ct_feature_names(ct)</code>","text":"<p>Gets the names of the columns when considering a fitted ColumnTransfomer From: https://stackoverflow.com/questions/57528350/can-you-consistently-keep-track-of-column-labels-using-sklearns-transformer-api</p> <p>Parameters:</p> Name Type Description Default <code>ColumnTransformer</code> <p>Column tranformer to be processed</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of new feature names</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def get_ct_feature_names(ct: ColumnTransformer) -&gt; list:\n'''Gets the names of the columns when considering a fitted ColumnTransfomer\n    From: https://stackoverflow.com/questions/57528350/can-you-consistently-keep-track-of-column-labels-using-sklearns-transformer-api\n    Args:\n        ColumnTransformer: Column tranformer to be processed\n    Returns:\n        list: List of new feature names\n    '''\n# Handles all estimators, pipelines inside ColumnTransfomer\n# does not work when remainder =='passthrough'\n# which requires the input column names.\noutput_features = []\nfor name, estimator, features in ct.transformers_:\nif name != 'remainder':\nif isinstance(estimator, Pipeline):\ncurrent_features = features\nfor step in estimator:\nif type(step) == tuple:\nstep = step[1]\ncurrent_features = get_feature_out(step, current_features)\nfeatures_out = current_features\nelse:\nfeatures_out = get_feature_out(estimator, features)\nif hasattr(ct, 'verbose_feature_names_out') and ct.verbose_feature_names_out == False:\noutput_features.extend(features_out)\nelse:\noutput_features.extend([f'{name}__{feat}' for feat in features_out])\nelif estimator == 'passthrough':\n# features is indexes in case of passthrough\nif hasattr(ct, 'verbose_feature_names_out') and ct.verbose_feature_names_out == False:\noutput_features.extend(ct.feature_names_in_[features])\nelse:\noutput_features.extend([f'remainder__{feat}' for feat in ct.feature_names_in_[features]])\nreturn output_features\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.get_feature_out","title":"<code>get_feature_out(estimator, features_in)</code>","text":"<p>Gets the name of a column when considering a fitted estimator From: https://stackoverflow.com/questions/57528350/can-you-consistently-keep-track-of-column-labels-using-sklearns-transformer-api</p> <p>Parameters:</p> Name Type Description Default <code>(?)</code> <p>Estimator to be processed</p> required <code>(list)</code> <p>Input columns</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of new feature names</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def get_feature_out(estimator, features_in: list) -&gt; list:\n'''Gets the name of a column when considering a fitted estimator\n    From: https://stackoverflow.com/questions/57528350/can-you-consistently-keep-track-of-column-labels-using-sklearns-transformer-api\n    Args:\n        (?): Estimator to be processed\n        (list): Input columns\n    Returns:\n        list: List of new feature names\n    '''\nif hasattr(estimator, 'get_feature_names'):\nif isinstance(estimator, _VectorizerMixin):\n# handling all vectorizers\nreturn estimator.get_feature_names()\nelse:\nreturn estimator.get_feature_names(features_in)\nelif isinstance(estimator, SelectorMixin):\nreturn np.array(features_in)[estimator.get_support()]\nelse:\nreturn features_in\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.get_pipeline","title":"<code>get_pipeline(pipeline_str)</code>","text":"<p>Gets a pipeline from its name</p> <p>Parameters:</p> Name Type Description Default <code>pipeline_str</code> <code>str</code> <p>Name of the pipeline</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the name of the pipeline is not known</p> <p>Returns:</p> Name Type Description <code>ColumnTransfomer</code> <code>ColumnTransformer</code> <p>Pipeline to be used for the preprocessing</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def get_pipeline(pipeline_str: str) -&gt; ColumnTransformer:\n'''Gets a pipeline from its name\n    Args:\n        pipeline_str (str): Name of the pipeline\n    Raises:\n        ValueError: If the name of the pipeline is not known\n    Returns:\n        ColumnTransfomer: Pipeline to be used for the preprocessing\n    '''\n# Process\npipelines_dict = get_pipelines_dict()\nif pipeline_str not in pipelines_dict.keys():\nraise ValueError(f\"The pipeline {pipeline_str} is not known.\")\n# Get pipeline\npipeline = pipelines_dict[pipeline_str]\n# Return\nreturn pipeline\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.get_pipelines_dict","title":"<code>get_pipelines_dict()</code>","text":"<p>Gets a dictionary of available preprocessing pipelines</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of preprocessing pipelines</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def get_pipelines_dict() -&gt; dict:\n'''Gets a dictionary of available preprocessing pipelines\n    Returns:\n        dict: Dictionary of preprocessing pipelines\n    '''\npipelines_dict = {\n# - /!\\ DO NOT DELETE no_preprocess -&gt; necessary for compatibility /!\\ -\n# Identity transformer, hence we specify verbose_feature_names_out to False to not change columns names\n'no_preprocess': ColumnTransformer([('identity', FunctionTransformer(lambda x: x),\nmake_column_selector())], verbose_feature_names_out=False),\n'preprocess_P1': preprocess_P1(),  # Example of a pipeline\n# 'preprocess_AUTO': preprocess_auto(), # Automatic preprocessing based on statistics on data\n# 'preprocess_P2': preprocess_P2 , ETC ...\n}\nreturn pipelines_dict\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.preprocess_P1","title":"<code>preprocess_P1()</code>","text":"<p>Gets \"default\" preprocessing pipeline</p> <p>Returns:</p> Name Type Description <code>ColumnTransformer</code> <code>ColumnTransformer</code> <p>The pipeline</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def preprocess_P1() -&gt; ColumnTransformer:\n'''Gets \"default\" preprocessing pipeline\n    Returns:\n        ColumnTransformer: The pipeline\n    '''\nnumeric_pipeline = make_pipeline(SimpleImputer(strategy='median'), StandardScaler())\n# cat_pipeline = make_pipeline(SimpleImputer(strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore'))\n# text_pipeline = make_pipeline(CountVectorizer(), SelectKBest(k=5))\n# Check https://scikit-learn.org/stable/modules/generated/sklearn.compose.make_column_selector.html\n# and https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html#pandas.DataFrame.select_dtypes\n# to understand make_column_selector\n# /!\\\n# BE VERY CAUTIOUS WHEN USING FunctionTransformer !\n# A pickled pipeline can still depends on a module definition. Hence, even when pickled, you may not have consistent results !\n# Please try to use lambdas or local function, without any dependency. It reduces risk of changes.\n# If you still have to use a module function, try to never change it later on.\n# https://stackoverflow.com/questions/73788824/how-can-i-save-reload-a-functiontransformer-object-and-expect-it-to-always-wor\n# https://github.com/OSS-Pole-Emploi/gabarit/issues/63\n# /!\\\n# /!\\ EXEMPLE HERE /!\\\n# Good practice: Use directly the names of the columns instead of a \"selector\"\n# WARNING: The text pipeline is supposed to work on a column 'text' -&gt; Please adapt it to your project if you want to use it\n# By default, we only keep the preprocess on numerical columns\ntransformers = [\n('num', numeric_pipeline, make_column_selector(dtype_include='number')),\n# ('cat', cat_pipeline, make_column_selector(dtype_include='category')), # To convert a column in a column with dtype category: df[\"A\"].astype(\"category\")\n# ('text', text_pipeline, 'text'), # CountVectorizer possible one column at a time\n]\n# TODO: add sparse compatibility !\n# Use somethings like this :\n# - After applying a pipeline ...\n# if scipy.sparse.issparse(preprocessed_x):\n#     preprocessed_df = pd.DataFrame.sparse.from_spmatrix(preprocessed_x)\n# - Before training ...\n# x_train = x_train.sparse.to_coo().tocsr()\n# x_valid = x_valid.sparse.to_coo().tocsr()\n# ...\npipeline = ColumnTransformer(transformers, sparse_threshold=0, remainder='drop')  # Use remainder='passthrough' to keep all other columns (not recommended)\nreturn pipeline\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.preprocess_auto","title":"<code>preprocess_auto()</code>","text":"<p>Gets an \"automatic\" pipeline. Different functions are applied depending on stats calculated on the data</p> <p>Returns:</p> Name Type Description <code>ColumnTransformer</code> <code>ColumnTransformer</code> <p>The automatic pipeline</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def preprocess_auto() -&gt; ColumnTransformer:\n'''Gets an \"automatic\" pipeline. Different functions are applied depending on stats calculated on the data\n    Returns:\n        ColumnTransformer: The automatic pipeline\n    '''\n# Numeric :\n# 1) SimpleImputer()\n# 2) If abs(skew) &gt; 2 &amp;&amp; pctl(90) - pctl(10) &gt; 10^3 =&gt; logtransform\n# 3) StandardScaler()\n# Categorical :\n# 1) SimpleImputer()\n# 2) If #cat &gt; 5; We accumulate the less represented instances in a meta-category \"other\"\n# 3) OneHot\npass\n</code></pre>"},{"location":"reference/template_num/preprocessing/preprocess/#template_num.preprocessing.preprocess.retrieve_columns_from_pipeline","title":"<code>retrieve_columns_from_pipeline(df, pipeline)</code>","text":"<p>Retrieves columns name after preprocessing</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe after preprocessing (without target)</p> required <code>pipeline</code> <code>ColumnTransformer</code> <p>Used pipeline</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: Dataframe with columns' name</p> Source code in <code>template_num/preprocessing/preprocess.py</code> <pre><code>def retrieve_columns_from_pipeline(df: pd.DataFrame, pipeline: ColumnTransformer) -&gt; pd.DataFrame:\n'''Retrieves columns name after preprocessing\n    Args:\n        df (pd.DataFrame): Dataframe after preprocessing (without target)\n        pipeline (ColumnTransformer): Used pipeline\n    Returns:\n        pd.DataFrame: Dataframe with columns' name\n    '''\n# Use deepcopy !\nnew_df = df.copy(deep=True)\n# Check if fitted:\nif not hasattr(pipeline, 'transformers_'):\nraise AttributeError(\"The pipeline must be fitted to use the function retrieve_columns_from_pipeline\")\n# EXPERIMENTAL: We do a try... except... if we can't get the names\n# First try : use sklearn get_feature_names_out function (might crash)\n# Second try : backup on old custom method\n# Third solution : ['x0', 'x1', ...]\ntry:\ntry:\nnew_columns = pipeline.get_feature_names_out()\n# Backup on old custom method\nexcept:\nnew_columns = get_ct_feature_names(pipeline)\nassert len(new_columns) == new_df.shape[1], \"There is a discrepancy in the number of columns\" +\\\n                                                f\" between the preprocessed DataFrame ({new_df.shape[1]})\" +\\\n                                                f\" and the pipeline ({len(new_columns)}).\"\n# No solution\nexcept Exception as e:\nlogger.error(\"Can't get the names of the columns. Backup on ['x0', 'x1', ...]\")\nlogger.error(repr(e))\nnew_columns = [f'x{i}' for i in range(len(new_df.columns))]\n# TODO : check for duplicates in new_columns ???\nnew_df.columns = new_columns\nreturn new_df\n</code></pre>"},{"location":"reference/template_vision/","title":"Template vision","text":""},{"location":"reference/template_vision/utils/","title":"Utils","text":""},{"location":"reference/template_vision/utils/#template_vision.utils.DownloadProgressBar","title":"<code>DownloadProgressBar</code>","text":"<p>         Bases: <code>tqdm</code></p> <p>Displays a progress bar</p> Source code in <code>template_vision/utils.py</code> <pre><code>class DownloadProgressBar(tqdm):\n'''Displays a progress bar'''\ndef update_to(self, b: int = 1, bsize: int = 1, tsize: Any = None) -&gt; None:\nif tsize is not None:\nself.total = tsize\nself.update(b * bsize - self.n)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.HiddenPrints","title":"<code>HiddenPrints</code>","text":"<p>Hides all prints</p> Source code in <code>template_vision/utils.py</code> <pre><code>class HiddenPrints:\n'''Hides all prints'''\ndef __enter__(self) -&gt; None:\nself._original_stdout = sys.stdout\nsys.stdout = open(os.devnull, 'w')\ndef __exit__(self, exc_type, exc_val, exc_tb) -&gt; None:\nsys.stdout.close()\nsys.stdout = self._original_stdout\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.NpEncoder","title":"<code>NpEncoder</code>","text":"<p>         Bases: <code>json.JSONEncoder</code></p> <p>JSON encoder to manage numpy objects</p> Source code in <code>template_vision/utils.py</code> <pre><code>class NpEncoder(json.JSONEncoder):\n'''JSON encoder to manage numpy objects'''\ndef default(self, obj) -&gt; Any:\nif is_ndarray_convertable(obj):\nreturn ndarray_to_builtin_object(obj)\nelif isinstance(obj, set):\nreturn list(obj)\nelse:\nreturn super(NpEncoder, self).default(obj)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.data_agnostic_str_to_list","title":"<code>data_agnostic_str_to_list(function)</code>","text":"<p>Decorator to transform a string into a list of one element. DO NOT CAST BACK TO ONE ELEMENT.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>func</code> <p>Function to decorate</p> required <p>Returns:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The decorated function</p> Source code in <code>template_vision/utils.py</code> <pre><code>def data_agnostic_str_to_list(function: Callable) -&gt; Callable:\n'''Decorator to transform a string into a list of one element.\n    DO NOT CAST BACK TO ONE ELEMENT.\n    Args:\n        function (func): Function to decorate\n    Returns:\n        function: The decorated function\n    '''\n# Get wrapper\ndef wrapper(x, *args, **kwargs):\n'''Wrapper'''\nif type(x) == str:\n# Cast str into a single element list\nmy_list = [x]\n# Call function\nresults = function(my_list, *args, **kwargs)\nelse:\nresults = function(x, *args, **kwargs)\n# Return\nreturn results\nreturn wrapper\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.display_shape","title":"<code>display_shape(df)</code>","text":"<p>Displays the number of line and of column of a table.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Table to parse</p> required Source code in <code>template_vision/utils.py</code> <pre><code>def display_shape(df: pd.DataFrame) -&gt; None:\n'''Displays the number of line and of column of a table.\n    Args:\n        df (pd.DataFrame): Table to parse\n    '''\n# Display\nlogger.info(f\"Number of lines : {df.shape[0]}. Number of columns : {df.shape[1]}.\")\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.download_url","title":"<code>download_url(urls, output_path)</code>","text":"<p>Downloads an object from a list of URLs. This function will try every URL until it find an available one.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list</code> <p>List of URL to try</p> required <code>output_path</code> <code>str</code> <p>Where to save the downloaded object</p> required <p>Raises:</p> Type Description <code>ConnectionError</code> <p>If no URL is available</p> Source code in <code>template_vision/utils.py</code> <pre><code>@data_agnostic_str_to_list\ndef download_url(urls: list, output_path: str) -&gt; None:\n'''Downloads an object from a list of URLs.\n    This function will try every URL until it find an available one.\n    Args:\n        urls (list): List of URL to try\n        output_path (str): Where to save the downloaded object\n    Raises:\n        ConnectionError: If no URL is available\n    '''\n# Start by creating output directory if does not exists\noutput_dir = os.path.dirname(output_path)\nif not os.path.exists(output_dir):\nos.makedirs(output_dir)\n# Test each url\nis_downloaded = False\nfor url in urls:\nif not is_downloaded:\ntry:\n# From https://stackoverflow.com/questions/15644964/python-progress-bar-and-downloads\nwith DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=url.split('/')[-1]) as t:\nrequest.urlretrieve(url, filename=output_path, reporthook=t.update_to)\nis_downloaded = True  # Download ok\nexcept Exception:\nlogger.warning(f\"Can't download from URL {url}.\")\nif not is_downloaded:\nraise ConnectionError(\"Couldn't find a working URL\")\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.find_folder_path","title":"<code>find_folder_path(folder_name, base_folder=None)</code>","text":"<p>Find a folder in a base folder and its subfolders. If base_folder is None, considers folder_name as a path and check it exists</p> <p>i.e., with the following structure : - C:/     - base_folder/         - folderA/             - folderB/         - folderC/ find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC find_folder_path(folderB, None) raises an error</p> <p>Parameters:</p> Name Type Description Default <code>folder_name</code> <code>str</code> <p>name of the folder to find. If base_folder is None, consider a path instead.</p> required Kwargs <p>base_folder (str): path of the base folder. If None, consider folder_name as a path.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>if we can't find folder_name in base_folder</p> <code>FileNotFoundError</code> <p>if folder_name is not a valid path (case where base_folder is None)</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>path to the wanted folder</p> Source code in <code>template_vision/utils.py</code> <pre><code>def find_folder_path(folder_name: str, base_folder: Union[str, None] = None) -&gt; str:\n'''Find a folder in a base folder and its subfolders.\n    If base_folder is None, considers folder_name as a path and check it exists\n    i.e., with the following structure :\n    - C:/\n        - base_folder/\n            - folderA/\n                - folderB/\n            - folderC/\n    find_folder_path(folderA, C:/base_folder) == C:/base_folder/folderA\n    find_folder_path(folderB, C:/base_folder) == C:/base_folder/folderA/folderB\n    find_folder_path(C:/base_folder/folderC, None) == C:/base_folder/folderC\n    find_folder_path(folderB, None) raises an error\n    Args:\n        folder_name (str): name of the folder to find. If base_folder is None, consider a path instead.\n    Kwargs:\n        base_folder (str): path of the base folder. If None, consider folder_name as a path.\n    Raises:\n        FileNotFoundError: if we can't find folder_name in base_folder\n        FileNotFoundError: if folder_name is not a valid path (case where base_folder is None)\n    Returns:\n        str: path to the wanted folder\n    '''\nif base_folder is not None:\nfolder_path = None\nfor path, subdirs, files in os.walk(base_folder):\nfor name in subdirs:\nif name == folder_name:\nfolder_path = os.path.join(path, name)\nif folder_path is None:\nraise FileNotFoundError(f\"Can't find folder {folder_name} inside {base_folder} and its subfolders\")\nelse:\nfolder_path = folder_name\nif not os.path.exists(folder_path):\nraise FileNotFoundError(f\"Can't find folder {folder_path} (considered as a path)\")\nreturn folder_path\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.get_chunk_limits","title":"<code>get_chunk_limits(x, chunksize=10000)</code>","text":"<p>Gets chunk limits from a pandas series or dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>pd.Series or pd.DataFrame</code> <p>Documents to consider</p> required Kwargs <p>chunksize (int): The chunk size</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the chunk size is negative</p> <p>Returns:</p> Type Description <code>List[Tuple[int]]</code> <p>list: the chunk limits Source code in <code>template_vision/utils.py</code> <pre><code>def get_chunk_limits(x: Union[pd.DataFrame, pd.Series], chunksize: int = 10000) -&gt; List[Tuple[int]]:\n'''Gets chunk limits from a pandas series or dataframe.\n    Args:\n        x (pd.Series or pd.DataFrame): Documents to consider\n    Kwargs:\n        chunksize (int): The chunk size\n    Raises:\n        ValueError: If the chunk size is negative\n    Returns:\n        list&lt;tuple&gt;: the chunk limits\n    '''\nif chunksize &lt; 0:\nraise ValueError('The object chunksize must not be negative.')\n# Processs\nif chunksize == 0 or chunksize &gt;= x.shape[0]:\nchunks_limits = [(0, x.shape[0])]\nelse:\nchunks_limits = [(i * chunksize, min((i + 1) * chunksize, x.shape[0]))\nfor i in range(1 + ((x.shape[0] - 1) // chunksize))]\nreturn chunks_limits  # type: ignore\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.get_data_path","title":"<code>get_data_path()</code>","text":"<p>Returns the path to the data folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the data folder</p> Source code in <code>template_vision/utils.py</code> <pre><code>def get_data_path() -&gt; str:\n'''Returns the path to the data folder\n    Returns:\n        str: Path of the data folder\n    '''\nif DIR_PATH is None:\ndir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_vision-data')\nelse:\ndir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_vision-data')\nif not os.path.isdir(dir_path):\nos.mkdir(dir_path)\nreturn os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.get_models_path","title":"<code>get_models_path()</code>","text":"<p>Returns the path to the models folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the models folder</p> Source code in <code>template_vision/utils.py</code> <pre><code>def get_models_path() -&gt; str:\n'''Returns the path to the models folder\n    Returns:\n        str: Path of the models folder\n    '''\nif DIR_PATH is None:\ndir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_vision-models')\nelse:\ndir_path = os.path.join(os.path.abspath(DIR_PATH), 'template_vision-models')\nif not os.path.isdir(dir_path):\nos.mkdir(dir_path)\nreturn os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.get_package_version","title":"<code>get_package_version()</code>","text":"<p>Returns the current version of the package</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>version of the package</p> Source code in <code>template_vision/utils.py</code> <pre><code>def get_package_version() -&gt; str:\n'''Returns the current version of the package\n    Returns:\n        str: version of the package\n    '''\nversion = pkg_resources.get_distribution('template_vision').version\nreturn version\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.get_ressources_path","title":"<code>get_ressources_path()</code>","text":"<p>Returns the path to the ressources folder</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>Path of the ressources folder</p> Source code in <code>template_vision/utils.py</code> <pre><code>def get_ressources_path() -&gt; str:\n'''Returns the path to the ressources folder\n    Returns:\n        str: Path of the ressources folder\n    '''\ndir_path = os.path.join(os.path.dirname(os.path.dirname(os.path.realpath(__file__))), 'template_vision-ressources')\nif not os.path.isdir(dir_path):\nos.mkdir(dir_path)\nreturn os.path.abspath(dir_path)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.is_ndarray_convertable","title":"<code>is_ndarray_convertable(obj)</code>","text":"<p>Returns True if the object is covertable to a builtin type in the same way a np.ndarray is</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>an object to test</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the object is covertable to a list as a np.ndarray is</p> Source code in <code>template_vision/utils.py</code> <pre><code>def is_ndarray_convertable(obj: Any) -&gt; bool:\n'''Returns True if the object is covertable to a builtin type in the same way a np.ndarray is\n    Args:\n        obj (Any): an object to test\n    Returns:\n        bool: True if the object is covertable to a list as a np.ndarray is\n    '''\nreturn hasattr(obj, \"dtype\") and hasattr(obj, \"astype\") and hasattr(obj, \"tolist\")\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.ndarray_to_builtin_object","title":"<code>ndarray_to_builtin_object(obj)</code>","text":"<p>Transform a numpy.ndarray like object to a builtin type like int, float or list</p> <p>Parameters:</p> Name Type Description Default <code>obj</code> <code>Any</code> <p>An object</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>Raise a ValueError when obj is not ndarray convertable</p> <p>Returns:</p> Name Type Description <code>Any</code> <code>Any</code> <p>The object converted to a builtin type like int, float or list</p> Source code in <code>template_vision/utils.py</code> <pre><code>def ndarray_to_builtin_object(obj: Any) -&gt; Any:\n'''Transform a numpy.ndarray like object to a builtin type like int, float or list\n    Args:\n        obj (Any): An object\n    Raises:\n        ValueError: Raise a ValueError when obj is not ndarray convertable\n    Returns:\n        Any: The object converted to a builtin type like int, float or list\n    '''\nif is_ndarray_convertable(obj):\nif np.issubdtype(obj.dtype, np.integer):\nreturn obj.astype(int).tolist()\nelif np.issubdtype(obj.dtype, np.number):\nreturn obj.astype(float).tolist()\nelse:\nreturn obj.tolist()\nelse:\nraise ValueError(f\"{obj} is not ndarray convertable\")\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.read_folder","title":"<code>read_folder(folder_path, images_ext=('.jpg', '.jpeg', '.png'), sep=';', encoding='utf-8', accept_no_metadata=False)</code>","text":"<p>Loads images and classes / bboxes from a directory of images</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>Directory with the images to be loaded - abs path</p> required Kwargs <p>images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file) sep (str): Separator of the metadata file - if exists encoding (str): Encoding of the metadata file - if exists accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions)</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of images path</p> <code>list</code> <code>list</code> <p>List of classes associated with images if classification task, bboxes if objet detection task</p> <code>str</code> <code>str</code> <p>Name of the prerprocessing pipeline used</p> <code>str</code> <code>str</code> <p>Task type ('classification' or 'object_detection')</p> Source code in <code>template_vision/utils.py</code> <pre><code>def read_folder(folder_path: str, images_ext: tuple = ('.jpg', '.jpeg', '.png'),\nsep: str = ';', encoding: str = 'utf-8',\naccept_no_metadata: bool = False) -&gt; Tuple[list, list, str, str]:\n'''Loads images and classes / bboxes from a directory of images\n    Args:\n        folder_path (str): Directory with the images to be loaded - abs path\n    Kwargs:\n        images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file)\n        sep (str): Separator of the metadata file - if exists\n        encoding (str): Encoding of the metadata file - if exists\n        accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions)\n    Returns:\n        list: List of images path\n        list: List of classes associated with images if classification task, bboxes if objet detection task\n        str: Name of the prerprocessing pipeline used\n        str: Task type ('classification' or 'object_detection')\n    '''\nmetadata_object_detection = os.path.join(folder_path, 'metadata_bboxes.csv')\n# Object detection\nif os.path.exists(metadata_object_detection):\nlogger.info(\"Object detection task - Loading folder ...\")\npath_list, bboxes_list, preprocess_str = read_folder_object_detection(folder_path, images_ext=images_ext,\nsep=sep, encoding=encoding,\naccept_no_metadata=accept_no_metadata)\nreturn path_list, bboxes_list, preprocess_str, 'object_detection'\n# Classifier\nelse:\nlogger.info(\"Classification task - Loading folder ...\")\npath_list, classes_list, preprocess_str = read_folder_classification(folder_path, images_ext=images_ext,\nsep=sep, encoding=encoding,\naccept_no_metadata=accept_no_metadata)\nreturn path_list, classes_list, preprocess_str, 'classification'\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.read_folder_classification","title":"<code>read_folder_classification(folder_path, images_ext=('.jpg', '.jpeg', '.png'), sep=';', encoding='utf-8', accept_no_metadata=False)</code>","text":"<p>Loads images and classes from a directory of images - classification task</p> <p>Solution 1: usage of a metadata file (metadata.csv) Solution 2: all images at root directory, and prefixed with class names (e.g. class_filename.ext) Solution 3: all images saved in class subdirectories Solution 4: read images from root directory - no targets</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>Directory with the images to be loaded - abs path</p> required Kwargs <p>images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file) sep (str): Separator of the metadata file - if exists encoding (str): Encoding of the metadata file - if exists accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions)</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If folder path does not exists</p> <code>NotADirectoryError</code> <p>If the provided path is not a directory</p> <code>ValueError</code> <p>If column 'filename' does not exists in the metadata file</p> <code>ValueError</code> <p>If column 'class' does not exists in the metadata file and accept_no_metadata is False</p> <code>RuntimeError</code> <p>If no loading solution found</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of images path</p> <code>list</code> <code>list</code> <p>List of classes associated with images</p> <code>str</code> <code>str</code> <p>Name of the prerprocessing pipeline used</p> Source code in <code>template_vision/utils.py</code> <pre><code>def read_folder_classification(folder_path: str, images_ext: tuple = ('.jpg', '.jpeg', '.png'),\nsep: str = ';', encoding: str = 'utf-8',\naccept_no_metadata: bool = False) -&gt; Tuple[list, list, str]:\n'''Loads images and classes from a directory of images - classification task\n    Solution 1: usage of a metadata file (metadata.csv)\n    Solution 2: all images at root directory, and prefixed with class names (e.g. class_filename.ext)\n    Solution 3: all images saved in class subdirectories\n    Solution 4: read images from root directory - no targets\n    Args:\n        folder_path (str): Directory with the images to be loaded - abs path\n    Kwargs:\n        images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file)\n        sep (str): Separator of the metadata file - if exists\n        encoding (str): Encoding of the metadata file - if exists\n        accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions)\n    Raises:\n        FileNotFoundError: If folder path does not exists\n        NotADirectoryError: If the provided path is not a directory\n        ValueError: If column 'filename' does not exists in the metadata file\n        ValueError: If column 'class' does not exists in the metadata file and accept_no_metadata is False\n        RuntimeError: If no loading solution found\n    Returns:\n        list: List of images path\n        list: List of classes associated with images\n        str: Name of the prerprocessing pipeline used\n    '''\nlogger.info(f\"Loading folder {folder_path} ...\")\n# Check path exists and it's a directory\nif not os.path.exists(folder_path):\nraise FileNotFoundError(f\"The path {folder_path} does not exist\")\nif not os.path.isdir(folder_path):\nraise NotADirectoryError(f\"{folder_path} is not a valid directory\")\n# We first check for a preprocessing file\npreprocess_file = os.path.join(folder_path, 'preprocess_pipeline.conf')\nif os.path.exists(preprocess_file):\nlogger.info(\"Found a preprocessing file\")\nwith open(preprocess_file, 'r', encoding=encoding) as f:\npreprocess_str = f.readline()\nelse:\nlogger.info(\"Can't find a preprocessing file, backup on 'no_preprocess'\")\npreprocess_str = 'no_preprocess'\n# Solution 1: we try to load the directory by reading a metadata file\n# This file must be named metadata.csv and contain a column `filename`\nmetadata_file = os.path.join(folder_path, 'metadata.csv')\nif os.path.exists(metadata_file):\nlogger.info(\"Found a metadata file\")\n# Loading metadata file\nmetadata_df = pd.read_csv(metadata_file, sep=sep, encoding=encoding)\nif 'filename' not in metadata_df.columns:\nraise ValueError(\"The metadata file must contain a column 'filename'\")\n# Retrieving information (path &amp; classes)\npath_list = list(metadata_df['filename'].values)\npath_list = [os.path.join(folder_path, f) for f in path_list]\nif 'class' in metadata_df.columns:\nclasses_list = [str(cl) for cl in metadata_df['class'].values]\nelif accept_no_metadata:\nlogger.info(\"Can't retrieve classes (missing 'class' column in metadata file)\")\nclasses_list = None\nelse:\nraise ValueError(\"The metadata file must contain a column 'class' with argument `accept_no_metadata` at False\")\n# Return here\nreturn path_list, classes_list, preprocess_str\n# Solution 2: we check if all files are inside the root directory and if they are all prefixed (i.e. prefix_filename.ext)\nfolder_list = os.listdir(folder_path)\nfolder_list = [f for f in folder_list if f != 'preprocess_pipeline.conf']  # Do not consider preprocessing file\n# Check if all files are images\nif all([f.endswith(images_ext) for f in folder_list]):\nlogger.info(\"Try to load images from root directory\")\npath_list = [os.path.join(folder_path, f) for f in folder_list]\n# Check prefixes\nif all([len(f.split('_')) &gt; 1 for f in folder_list]) and all([len(f.split('_')[0]) &gt; 0 for f in folder_list]):\nclasses_list = [f.split('_')[0] for f in folder_list]\nelse:\nlogger.info(\"Can't retrieve classes (files are not prefixed)\")\nclasses_list = None\n# Return here\nreturn path_list, classes_list, preprocess_str\n# Solution 3: check if images are saved in class subdirectories\nfolders_elements = os.listdir(folder_path)\nfolders_elements = [f for f in folders_elements if f != 'preprocess_pipeline.conf']  # Do not consider preprocessing file\n# Check if only subdirectories\nif all([os.path.isdir(os.path.join(folder_path, f)) for f in folders_elements]):\n# Check if each subdirectories contain images\nif all([all([f2.endswith(images_ext) for f2 in os.listdir(os.path.join(folder_path, f))]) for f in folders_elements]):\nlogger.info(\"Try to load images from class subdirectories\")\n# Retrieving information (path &amp; classes)\npath_list = []\nclasses_list = []\nfor folder in folders_elements:\ntmp_path_list = [os.path.join(folder_path, folder, f) for f in os.listdir(os.path.join(folder_path, folder))]\ntmp_classes_list = [folder] * len(tmp_path_list)\npath_list += tmp_path_list\nclasses_list += tmp_classes_list\nreturn path_list, classes_list, preprocess_str\n# Solution 4: if accept no metadata, we retrieve all images inside the root directory\nelif accept_no_metadata:\nfolder_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\nfolder_list = [f for f in folder_list if f != 'preprocess_pipeline.conf']  # Do not consider preprocessing file\nfolder_list = [f for f in folder_list if f.endswith(images_ext)]  # Keep only images\npath_list = [os.path.join(folder_path, f) for f in folder_list]  # Get abs paths\nreturn path_list, None, preprocess_str  # No targets\n# No more solution, raise error\nraise RuntimeError(f\"No loading solution found for folder ({folder_path})\")\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.read_folder_object_detection","title":"<code>read_folder_object_detection(folder_path, images_ext=('.jpg', '.jpeg', '.png'), sep=';', encoding='utf-8', accept_no_metadata=False)</code>","text":"<p>Loads images and bboxes from a directory of images - object detection task</p> <p>Solution 1: usage of a metadata file (metadata_bboxes.csv) Solution 2: read images from root directory - no targets</p> <p>Parameters:</p> Name Type Description Default <code>folder_path</code> <code>str</code> <p>Directory with the images to be loaded - abs path</p> required Kwargs <p>images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file) sep (str): Separator of the metadata file - if exists encoding (str): Encoding of the metadata file - if exists accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions)</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If folder path does not exists</p> <code>NotADirectoryError</code> <p>If the provided path is not a directory</p> <code>ValueError</code> <p>If column 'filename' does not exists in the metadata file</p> <code>RuntimeError</code> <p>If no loading solution found</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>List of images path</p> <code>list</code> <code>list</code> <p>List of bboxes associated with images</p> <code>str</code> <code>str</code> <p>Name of the prerprocessing pipeline used</p> Source code in <code>template_vision/utils.py</code> <pre><code>def read_folder_object_detection(folder_path: str, images_ext: tuple = ('.jpg', '.jpeg', '.png'),\nsep: str = ';', encoding: str = 'utf-8',\naccept_no_metadata: bool = False) -&gt; Tuple[list, list, str]:\n'''Loads images and bboxes from a directory of images - object detection task\n    Solution 1: usage of a metadata file (metadata_bboxes.csv)\n    Solution 2: read images from root directory - no targets\n    Args:\n        folder_path (str): Directory with the images to be loaded - abs path\n    Kwargs:\n        images_ext (tuple): Accepted images extensions if automatic detection (i.e. no metadata file)\n        sep (str): Separator of the metadata file - if exists\n        encoding (str): Encoding of the metadata file - if exists\n        accept_no_metadata (bool): If we allow no targets metadata (i.e. returns only file paths, useful for predictions)\n    Raises:\n        FileNotFoundError: If folder path does not exists\n        NotADirectoryError: If the provided path is not a directory\n        ValueError: If column 'filename' does not exists in the metadata file\n        RuntimeError: If no loading solution found\n    Returns:\n        list: List of images path\n        list: List of bboxes associated with images\n        str: Name of the prerprocessing pipeline used\n    '''\nlogger.info(f\"Loading folder {folder_path} ...\")\n# Check path exists and it's a directory\nif not os.path.exists(folder_path):\nraise FileNotFoundError(f\"The path {folder_path} does not exist\")\nif not os.path.isdir(folder_path):\nraise NotADirectoryError(f\"{folder_path} is not a valid directory\")\n# We first check for a preprocessing file\npreprocess_file = os.path.join(folder_path, 'preprocess_pipeline.conf')\nif os.path.exists(preprocess_file):\nlogger.info(\"Found a preprocessing file\")\nwith open(preprocess_file, 'r', encoding=encoding) as f:\npreprocess_str = f.readline()\nelse:\nlogger.info(\"Can't find a preprocessing file, backup on 'no_preprocess'\")\npreprocess_str = 'no_preprocess'\n# Solution 1: we try to load the directory by reading a metadata file\n# This file must be named metadata_bboxes.csv and contain a column `filename`\nmetadata_file = os.path.join(folder_path, 'metadata_bboxes.csv')\nif os.path.exists(metadata_file):\nlogger.info(\"Found a metadata file\")\n# Loading metadata file\nmetadata_df = pd.read_csv(metadata_file, sep=sep, encoding=encoding)\nif 'filename' not in metadata_df.columns:\nraise ValueError(\"The metadata file must contain a column 'filename'\")\n# Retrieving information (path &amp; bboxes)\nfilenames = list(metadata_df['filename'].unique())\npath_list = [os.path.join(folder_path, f) for f in filenames]\n# Try to read bboxes\nif all([_ in metadata_df.columns for _ in ['class', 'x1', 'x2', 'y1', 'y2']]):\nbboxes_list = []\nfor filename in filenames:\nfiltered_bboxes = metadata_df[metadata_df.filename == filename]\ntmp_bboxes_list = []\nfor i, row in filtered_bboxes.iterrows():\ntmp_bboxes_list.append({\n'class': str(row['class']),  # We ensure all classes are strings\n'x1': row['x1'],\n'x2': row['x2'],\n'y1': row['y1'],\n'y2': row['y2'],\n})\nbboxes_list.append(tmp_bboxes_list)\nelse:\nlogger.info(\"Can't retrieve bboxes\")\nbboxes_list = None\n# Solution 2: if accept no metadata, we retrieve all images inside the root directory\nelif accept_no_metadata:\nfolder_list = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\nfolder_list = [f for f in folder_list if f != 'preprocess_pipeline.conf']  # Do not consider preprocessing file\nfolder_list = [f for f in folder_list if f.endswith(images_ext)]  # Keep only images\npath_list = [os.path.join(folder_path, f) for f in folder_list]  # Get abs paths\nbboxes_list = None  # No targets\n# No solution found, raise error\nelse:\nraise RuntimeError(f\"No loading solution found for folder ({folder_path})\")\n# Return\nreturn path_list, bboxes_list, preprocess_str\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.rebuild_metadata_classification","title":"<code>rebuild_metadata_classification(filenames_list, classes_list)</code>","text":"<p>Rebuilds a metadata file from files names and associated classes - classification task</p> <p>Parameters:</p> Name Type Description Default <code>filenames_list</code> <code>list</code> <p>List of files names (actually a path relative to files parent directory)</p> required <code>classes_list</code> <code>list</code> <p>List of classes</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>Both list must be of same length</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The new metadata dataframe</p> Source code in <code>template_vision/utils.py</code> <pre><code>def rebuild_metadata_classification(filenames_list: list, classes_list: list) -&gt; pd.DataFrame:\n'''Rebuilds a metadata file from files names and associated classes - classification task\n    Args:\n        filenames_list (list): List of files names (actually a path relative to files parent directory)\n        classes_list (list): List of classes\n    Raises:\n        AssertionError: Both list must be of same length\n    Returns:\n        pd.DataFrame: The new metadata dataframe\n    '''\nassert len(filenames_list) == len(classes_list), \"Both list 'filenames_list' &amp; 'classes_list' must be of same length\"\nreturn pd.DataFrame({'filename': filenames_list, 'class': classes_list})\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.rebuild_metadata_object_detection","title":"<code>rebuild_metadata_object_detection(filenames_list, bboxes_list)</code>","text":"<p>Rebuilds a metadata file from files names and associated bboxes - object detection task</p> <p>Parameters:</p> Name Type Description Default <code>filenames_list</code> <code>list</code> <p>List of files names (actually a path relative to files parent directory)</p> required <code>bboxes_list</code> <code>list</code> <p>List of bboxes</p> required <p>Raises:</p> Type Description <code>AssertionError</code> <p>Both list must be of same length</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The new metadata dataframe</p> Source code in <code>template_vision/utils.py</code> <pre><code>def rebuild_metadata_object_detection(filenames_list: list, bboxes_list: list) -&gt; pd.DataFrame:\n'''Rebuilds a metadata file from files names and associated bboxes - object detection task\n    Args:\n        filenames_list (list): List of files names (actually a path relative to files parent directory)\n        bboxes_list (list): List of bboxes\n    Raises:\n        AssertionError: Both list must be of same length\n    Returns:\n        pd.DataFrame: The new metadata dataframe\n    '''\nassert len(filenames_list) == len(bboxes_list), \"Both list 'filenames_list' &amp; 'bboxes_list' must be of same length\"\nrows = []\nfor filename, bboxes in zip(filenames_list, bboxes_list):\nfor bbox in bboxes:\nnew_row = {'filename': filename, 'class': bbox['class'], 'x1': bbox['x1'], 'x2': bbox['x2'], 'y1': bbox['y1'], 'y2': bbox['y2']}\nrows.append(new_row)\nreturn pd.DataFrame(rows)\n</code></pre>"},{"location":"reference/template_vision/utils/#template_vision.utils.trained_needed","title":"<code>trained_needed(function)</code>","text":"<p>Decorator to ensure that a model has been trained.</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>func</code> <p>Function to decorate</p> required <p>Returns:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The decorated function</p> Source code in <code>template_vision/utils.py</code> <pre><code>def trained_needed(function: Callable) -&gt; Callable:\n'''Decorator to ensure that a model has been trained.\n    Args:\n        function (func): Function to decorate\n    Returns:\n        function: The decorated function\n    '''\n# Get wrapper\ndef wrapper(self, *args, **kwargs):\n'''Wrapper'''\nif not self.trained:\nraise AttributeError(f\"The function {function.__name__} can't be called as long as the model hasn't been fitted\")\nelse:\nreturn function(self, *args, **kwargs)\nreturn wrapper\n</code></pre>"},{"location":"reference/template_vision/models_training/","title":"Models training","text":""},{"location":"reference/template_vision/models_training/model_class/","title":"Model class","text":""},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass","title":"<code>ModelClass</code>","text":"<p>Parent class for the models</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>class ModelClass:\n'''Parent class for the models'''\n_default_name = 'none'\n# Variable annotation : https://www.python.org/dev/peps/pep-0526/\n# Solves lots of typing errors, cf mypy\nlist_classes: list\ndict_classes: dict\n# Not implemented :\n# -&gt; fit\n# -&gt; predict\n# -&gt; predict_proba\n# -&gt; inverse_transform\n# -&gt; get_and_save_metrics\ndef __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None,\nlevel_save: str = 'HIGH', **kwargs) -&gt; None:\n'''Initialization of the parent class.\n        Kwargs:\n            model_dir (str): Folder where to save the model\n                If None, creates a directory based on the model's name and the date (most common usage)\n            model_name (str): The name of the model\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOW + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n            NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n        '''\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type -&gt; 'classifier' or 'object_detector' depending on the model\nself.model_type = None\n# Model name\nself.model_name = self._default_name if model_name is None else model_name\n# Model folder\nif model_dir is None:\nself.model_dir = self._get_model_dir()\nelse:\nif not os.path.exists(model_dir):\nos.makedirs(model_dir)\nif not os.path.isdir(model_dir):\nraise NotADirectoryError(f\"{model_dir} is not a valid directory\")\nself.model_dir = os.path.abspath(model_dir)\n# Other options\nself.level_save = level_save\n# is trained ?\nself.trained = False\nself.nb_fit = 0\n# Configuration dict. to be logged. Set on save.\nself.json_dict: Dict[Any, Any] = {}\ndef fit(self, df_train, **kwargs) -&gt; dict:\n'''Trains the model\n        Args:\n            df_train (pd.DataFrame): Train dataset\n                Must contain file_path &amp; file_class columns if classifier\n                Must contain file_path &amp; bboxes columns if object detector\n        Returns:\n            dict: Fit arguments, to be used with transfer learning fine-tuning\n        '''\nraise NotImplementedError(\"'fit' needs to be overridden\")\ndef predict(self, df_test: pd.DataFrame, **kwargs) -&gt; Union[np.ndarray, list]:\n'''Predictions on test set\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Returns:\n            (np.ndarray | list): Array, shape = [n_samples, n_classes] or List of n_samples elements\n        '''\nraise NotImplementedError(\"'predict' needs to be overridden\")\ndef predict_proba(self, df_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nraise NotImplementedError(\"'predict_proba' needs to be overridden\")\ndef inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Gets the final format of prediction\n            - Classification : classes from predictions\n            - Object detections : list of bboxes per image\n        Args:\n            y (list | np.ndarray): Array-like\n        Returns:\n            List of classes if classifier\n            List of bboxes if object detector\n        '''\nraise NotImplementedError(\"'inverse_transform' needs to be overridden\")\ndef get_and_save_metrics(self, y_true, y_pred, list_files_x: Union[list, None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n        Args:\n            y_true (?): Array-like [n_samples, 1] if classifier\n                # If classifier, class of each image\n                # If object detector, list of list of bboxes per image\n                    bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n            y_pred (?): Array-like [n_samples, 1] if classifier\n                # If classifier, class of each image\n                # If object detector, list of list of bboxes per image\n                    bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n        Kwargs:\n            list_files_x (list): Input images file paths\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\nraise NotImplementedError(\"'get_and_save_metrics' needs to be overridden\")\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Manage paths\npkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\nconf_path = os.path.join(self.model_dir, \"configurations.json\")\n# Save model &amp; pipeline preprocessing si level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nwith open(pkl_path, 'wb') as f:\npickle.dump(self, f)\n# Save configuration JSON\njson_dict = {\n'maintainers': 'Agence DataServices',\n'gabarit_version': '1.2.5-dev-local',\n'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n'package_version': utils.get_package_version(),\n'model_name': self.model_name,\n'model_dir': self.model_dir,\n'model_type': self.model_type,\n'trained': self.trained,\n'nb_fit': self.nb_fit,\n'level_save': self.level_save,\n'librairie': None,\n}\n# Merge json_data if not None\nif json_data is not None:\n# Priority given to json_data !\njson_dict = {**json_dict, **json_data}\n# Add conf to attributes\nself.json_dict = json_dict\n# Save conf\nwith open(conf_path, 'w', encoding='utf-8') as json_file:\njson.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n# Now, save a proprietes file for the model upload\nself._save_upload_properties(json_dict)\ndef _save_upload_properties(self, json_dict: Union[dict, None] = None) -&gt; None:\n'''Prepares a configuration file for a future export (e.g on an artifactory)\n        Kwargs:\n            json_dict: Configurations to save\n        '''\nif json_dict is None:\njson_dict = {}\n# Manage paths\nproprietes_path = os.path.join(self.model_dir, \"proprietes.json\")\nvanilla_model_upload_instructions = os.path.join(utils.get_ressources_path(), 'model_upload_instructions.md')\nspecific_model_upload_instructions = os.path.join(self.model_dir, \"model_upload_instructions.md\")\n# First, we define a list of \"allowed\" properties\nallowed_properties = [\"maintainers\", \"gabarit_version\", \"date\", \"package_version\", \"model_name\", \"list_classes\",\n\"librairie\", \"fit_time\"]\n# Now we filter these properties\nfinal_dict = {k: v for k, v in json_dict.items() if k in allowed_properties}\n# Save\nwith open(proprietes_path, 'w', encoding='utf-8') as f:\njson.dump(final_dict, f, indent=4, cls=utils.NpEncoder)\n# Add instructions to upload a model to a storage solution (e.g. Artifactory)\nwith open(vanilla_model_upload_instructions, 'r', encoding='utf-8') as f:\ncontent = f.read()\n# TODO: to be improved\nnew_content = content.replace('model_dir_path_identifier', os.path.abspath(self.model_dir))\nwith open(specific_model_upload_instructions, 'w', encoding='utf-8') as f:\nf.write(new_content)\ndef _get_model_dir(self) -&gt; str:\n'''Gets a folder where to save the model\n        Returns:\n            str: Path to the folder\n        '''\nmodels_dir = utils.get_models_path()\nsubfolder = os.path.join(models_dir, self.model_name)\nfolder_name = datetime.now().strftime(f\"{self.model_name}_%Y_%m_%d-%H_%M_%S\")\nmodel_dir = os.path.join(subfolder, folder_name)\nif os.path.isdir(model_dir):\ntime.sleep(1)  # Wait 1 second so that the 'date' changes...\nreturn self._get_model_dir()  # Get new directory name\nelse:\nos.makedirs(model_dir)\nreturn model_dir\ndef display_if_gpu_activated(self) -&gt; None:\n'''Displays if a GPU is being used'''\nif self._is_gpu_activated():\nself.logger.info(\"GPU activated\")\ndef _is_gpu_activated(self) -&gt; bool:\n'''Checks if we use a GPU\n        Returns:\n            bool: whether GPU is available or not\n        '''\n# By default, no GPU\nreturn False\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.__init__","title":"<code>__init__(model_dir=None, model_name=None, level_save='HIGH', **kwargs)</code>","text":"<p>Initialization of the parent class.</p> Kwargs <p>model_dir (str): Folder where to save the model     If None, creates a directory based on the model's name and the date (most common usage) model_name (str): The name of the model level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOW + hdf5 + pkl + plots     HIGH: MEDIUM + predictions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])</p> <code>NotADirectoryError</code> <p>If a provided model directory is not a directory (i.e. it's a file)</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def __init__(self, model_dir: Union[str, None] = None, model_name: Union[str, None] = None,\nlevel_save: str = 'HIGH', **kwargs) -&gt; None:\n'''Initialization of the parent class.\n    Kwargs:\n        model_dir (str): Folder where to save the model\n            If None, creates a directory based on the model's name and the date (most common usage)\n        model_name (str): The name of the model\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOW + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        NotADirectoryError: If a provided model directory is not a directory (i.e. it's a file)\n    '''\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type -&gt; 'classifier' or 'object_detector' depending on the model\nself.model_type = None\n# Model name\nself.model_name = self._default_name if model_name is None else model_name\n# Model folder\nif model_dir is None:\nself.model_dir = self._get_model_dir()\nelse:\nif not os.path.exists(model_dir):\nos.makedirs(model_dir)\nif not os.path.isdir(model_dir):\nraise NotADirectoryError(f\"{model_dir} is not a valid directory\")\nself.model_dir = os.path.abspath(model_dir)\n# Other options\nself.level_save = level_save\n# is trained ?\nself.trained = False\nself.nb_fit = 0\n# Configuration dict. to be logged. Set on save.\nself.json_dict: Dict[Any, Any] = {}\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.display_if_gpu_activated","title":"<code>display_if_gpu_activated()</code>","text":"<p>Displays if a GPU is being used</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def display_if_gpu_activated(self) -&gt; None:\n'''Displays if a GPU is being used'''\nif self._is_gpu_activated():\nself.logger.info(\"GPU activated\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.fit","title":"<code>fit(df_train, **kwargs)</code>","text":"<p>Trains the model</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>pd.DataFrame</code> <p>Train dataset Must contain file_path &amp; file_class columns if classifier Must contain file_path &amp; bboxes columns if object detector</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Fit arguments, to be used with transfer learning fine-tuning</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def fit(self, df_train, **kwargs) -&gt; dict:\n'''Trains the model\n    Args:\n        df_train (pd.DataFrame): Train dataset\n            Must contain file_path &amp; file_class columns if classifier\n            Must contain file_path &amp; bboxes columns if object detector\n    Returns:\n        dict: Fit arguments, to be used with transfer learning fine-tuning\n    '''\nraise NotImplementedError(\"'fit' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, list_files_x=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like [n_samples, 1] if classifier</p> required <code>y_pred</code> <code>?</code> <p>Array-like [n_samples, 1] if classifier</p> required Kwargs <p>list_files_x (list): Input images file paths type_data (str): Type of dataset (validation, test, ...)</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, list_files_x: Union[list, None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n    Args:\n        y_true (?): Array-like [n_samples, 1] if classifier\n            # If classifier, class of each image\n            # If object detector, list of list of bboxes per image\n                bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n        y_pred (?): Array-like [n_samples, 1] if classifier\n            # If classifier, class of each image\n            # If object detector, list of list of bboxes per image\n                bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n    Kwargs:\n        list_files_x (list): Input images file paths\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\nraise NotImplementedError(\"'get_and_save_metrics' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.get_and_save_metrics--if-classifier-class-of-each-image","title":"If classifier, class of each image","text":""},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.get_and_save_metrics--if-object-detector-list-of-list-of-bboxes-per-image","title":"If object detector, list of list of bboxes per image","text":"<pre><code>bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.get_and_save_metrics--if-classifier-class-of-each-image","title":"If classifier, class of each image","text":""},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.get_and_save_metrics--if-object-detector-list-of-list-of-bboxes-per-image","title":"If object detector, list of list of bboxes per image","text":"<pre><code>bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets the final format of prediction     - Classification : classes from predictions     - Object detections : list of bboxes per image</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>list | np.ndarray</code> <p>Array-like</p> required <p>Returns:</p> Type Description <code>Union[list, tuple]</code> <p>List of classes if classifier</p> <code>Union[list, tuple]</code> <p>List of bboxes if object detector</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Gets the final format of prediction\n        - Classification : classes from predictions\n        - Object detections : list of bboxes per image\n    Args:\n        y (list | np.ndarray): Array-like\n    Returns:\n        List of classes if classifier\n        List of bboxes if object detector\n    '''\nraise NotImplementedError(\"'inverse_transform' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.predict","title":"<code>predict(df_test, **kwargs)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>pd.DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required <p>Returns:</p> Type Description <code>np.ndarray | list</code> <p>Array, shape = [n_samples, n_classes] or List of n_samples elements</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def predict(self, df_test: pd.DataFrame, **kwargs) -&gt; Union[np.ndarray, list]:\n'''Predictions on test set\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n    Returns:\n        (np.ndarray | list): Array, shape = [n_samples, n_classes] or List of n_samples elements\n    '''\nraise NotImplementedError(\"'predict' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.predict_proba","title":"<code>predict_proba(df_test, **kwargs)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>pd.DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def predict_proba(self, df_test: pd.DataFrame, **kwargs) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\nraise NotImplementedError(\"'predict_proba' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_class/#template_vision.models_training.model_class.ModelClass.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/model_class.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Manage paths\npkl_path = os.path.join(self.model_dir, f\"{self.model_name}.pkl\")\nconf_path = os.path.join(self.model_dir, \"configurations.json\")\n# Save model &amp; pipeline preprocessing si level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nwith open(pkl_path, 'wb') as f:\npickle.dump(self, f)\n# Save configuration JSON\njson_dict = {\n'maintainers': 'Agence DataServices',\n'gabarit_version': '1.2.5-dev-local',\n'date': datetime.now().strftime(\"%d/%m/%Y - %H:%M:%S\"),  # Not the same as the folder's name\n'package_version': utils.get_package_version(),\n'model_name': self.model_name,\n'model_dir': self.model_dir,\n'model_type': self.model_type,\n'trained': self.trained,\n'nb_fit': self.nb_fit,\n'level_save': self.level_save,\n'librairie': None,\n}\n# Merge json_data if not None\nif json_data is not None:\n# Priority given to json_data !\njson_dict = {**json_dict, **json_data}\n# Add conf to attributes\nself.json_dict = json_dict\n# Save conf\nwith open(conf_path, 'w', encoding='utf-8') as json_file:\njson.dump(json_dict, json_file, indent=4, cls=utils.NpEncoder)\n# Now, save a proprietes file for the model upload\nself._save_upload_properties(json_dict)\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/","title":"Model keras","text":""},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras","title":"<code>ModelKeras</code>","text":"<p>         Bases: <code>ModelClass</code></p> <p>Generic model for Keras NN</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>class ModelKeras(ModelClass):\n'''Generic model for Keras NN'''\n_default_name = 'model_keras'\n# Not implemented :\n# -&gt; _get_model\n# -&gt; reload_from_standalone\n# Should pby be overridden :\n# -&gt; _get_preprocess_input\ndef __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\nwidth: int = 224, height: int = 224, depth: int = 3, color_mode: str = 'rgb',\nin_memory: bool = False, data_augmentation_params: dict = {},\nnb_train_generator_images_to_save: int = 20,\nkeras_params: dict = {}, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n        Kwargs:\n            batch_size (int): Batch size\n            epochs (int): Number of epochs\n            validation_split (float): Percentage for the validation set split\n                Only used if no input validation set when fitting\n            patience (int): Early stopping patience\n            width (int): NN input width (images are resized)\n            height (int): NN input height (images are resized)\n            depth (int): NN input depth\n            color_mode (str): NN input color mode\n            in_memory (bool): If all images should be loaded in memory, otherwise it uses a generator\n                /!\\\\ OOM errors can happen really quickly (depends on the dataset size)\n                /!\\\\ Data augmentation impossible if `in_memory` is set to True\n            data_augmentation_params (dict): Dictionnary of parameters to be used with the data augmentation\n                cf. https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n                /!\\\\ Not used if `in_memory` is set to True\n            nb_train_generator_images_to_save (int): If &gt; 0, save some input generated images\n                If helps with to understand what goes in your NN\n            keras_params (dict): Parameters used by keras models.\n                e.g. learning_rate, nb_lstm_units, etc...\n                The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n                This parameter was initially added in order to do an hyperparameters search\n        Raises:\n            ValueError: If `in_memory` is set to True and `data_augmentation_params` is not empty\n        '''\n# TODO: learning rate should be an attribute !\n# Check for errors\nif in_memory and len(data_augmentation_params) &gt; 0:\nraise ValueError(\"Data augmentation is impossible for 'in_memory' mode\")\n# Init.\nsuper().__init__(**kwargs)\n# Fix tensorflow GPU\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Param. model\nself.batch_size = batch_size\nself.epochs = epochs\nself.validation_split = validation_split\nself.patience = patience\n# Params. generator\nself.width = width\nself.height = height\nself.depth = depth\nself.color_mode = color_mode\nself.in_memory = in_memory\nself.data_augmentation_params = data_augmentation_params.copy()\n# Warnings if depth does not match with color_mode\nif self.color_mode == 'rgb' and self.depth != 3:\nself.logger.warning(f\"`color_mode` parameter is 'rgb', but `depth` parameteris not equal to 3 ({self.depth})\")\nself.logger.warning(\"We continue, but this can lead to errors during the training\")\nif self.color_mode == 'rgba' and self.depth != 4:\nself.logger.warning(f\"`color_mode` parameter is 'rgba', but `depth` parameteris not equal to 4 ({self.depth})\")\nself.logger.warning(\"We continue, but this can lead to errors during the training\")\n# TODO: add Test time augmentation ?\n# Misc.\nself.nb_train_generator_images_to_save = nb_train_generator_images_to_save\n# Model set on fit\nself.model: Any = None\n# Set preprocess input\nself.preprocess_input = self._get_preprocess_input()\n# Keras params\nself.keras_params = keras_params.copy()\n# Keras custom objects : we get the ones specified in utils_deep_keras\nself.custom_objects = utils_deep_keras.custom_objects\ndef fit(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None, with_shuffle: bool = True, **kwargs) -&gt; dict:\n'''Fits the model\n        Args:\n            df_train (pd.DataFrame): Train dataset\n                Must contain file_path &amp; file_class columns if classifier\n                Must contain file_path &amp; bboxes columns if object detector\n        Kwargs:\n            df_valid (pd.DataFrame): Validation dataset\n                Must contain file_path &amp; file_class columns if classifier\n                Must contain file_path &amp; bboxes columns if object detector\n            with_shuffle (boolean): If the train dataset must be shuffled\n                This should be used if the input dataset is not shuffled &amp; no validation set as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            NotImplementedError: If the model is not `classifier` nor `object_detector`\n        Returns:\n            dict: Fit arguments, to be used with transfer learning fine-tuning\n        '''\nif self.model_type == 'classifier':\nreturn self._fit_classifier(df_train, df_valid=df_valid, with_shuffle=with_shuffle, **kwargs)\nelif self.model_type == 'object_detector':\nreturn self._fit_object_detector(df_train, df_valid=df_valid, with_shuffle=with_shuffle, **kwargs)\nelse:\nraise NotImplementedError(\"Only `classifier` and `object_detector` model type are supported.\")\ndef _fit_classifier(self, df_train: pd.DataFrame, df_valid: pd.DataFrame = None, with_shuffle: bool = True, **kwargs) -&gt; dict:\n'''Fits the model - classifier\n        Args:\n            df_train (pd.DataFrame): Train dataset\n                Must contain file_path &amp; file_class columns\n        Kwargs:\n            df_valid (pd.DataFrame): Validation dataset\n                Must contain file_path &amp; file_class columns\n            with_shuffle (boolean): If the train dataset must be shuffled\n                This should be used if the input dataset is not shuffled &amp; no validation set as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            ValueError: If the model is not of type `classifier`\n            AssertionError: If already trained and new dataset does not match model's classes\n        Returns:\n            dict: Fit arguments, to be used with transfer learning fine-tuning\n        '''\nif self.model_type != 'classifier':\nraise ValueError(f\"`_fit_classifier` function does not support model type {self.model_type}\")\n##############################################\n# Manage retrain\n##############################################\n# If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n# And we save the old conf\nif self.trained:\n# Get src files to save\nsrc_files = [os.path.join(self.model_dir, \"configurations.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\nsrc_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Change model dir\nself.model_dir = self._get_model_dir()\n# Get dst files\ndst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\ndst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Copies\nfor src, dst in zip(src_files, dst_files):\ntry:\nshutil.copyfile(src, dst)\nexcept Exception as e:\nself.logger.error(f\"Impossible to copy {src} to {dst}\")\nself.logger.error(\"We still continue ...\")\nself.logger.error(repr(e))\n##############################################\n# Prepare dataset\n# Also extract list of classes\n##############################################\n# Extract list of classes from df_train\nlist_classes = sorted(list(df_train['file_class'].unique()))\n# Also set dict_classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# Validate classes if already trained, else set them\nif self.trained:\nassert self.list_classes == list_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nassert self.dict_classes == dict_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nelse:\nself.list_classes = list_classes\nself.dict_classes = dict_classes\n# Shuffle training dataset if wanted\n# It is advised as validation_split from keras does not shufle the data\n# Hence, for classificationt task, we might have classes in the validation data that we never met in the training data\nif with_shuffle:\ndf_train = df_train.sample(frac=1.).reset_index(drop=True)\nif df_valid is None:\nself.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\n##############################################\n# We save some preprocessed / augmented input images examples\n##############################################\n# Save some examples\nif self.nb_train_generator_images_to_save &gt; 0:\nself.logger.info(\"Retrieving a generator to save some preprocessed / augmented input images examples\")\n# 1. Retrieve a generator (if in_memory, use 'valid' to avoid augmentation)\nif not self.in_memory:\ntmp_gen = self._get_generator(df_train, data_type='train', batch_size=1)\nelse:\ntmp_gen = self._get_generator(df_train, data_type='valid', batch_size=1)\n# 2. Retrieve generated images one by one\nimages = [tmp_gen.next()[0][0] for i in range(self.nb_train_generator_images_to_save)]\n# 3. Remove negative pixels\nmin_pixel = min([np.min(_) for _ in images])\nif min_pixel &lt; 0:\nimages = [arr - min_pixel for arr in images]\n# 4. Rescale and scale uint8\nmax_pixel = max([np.max(_) for _ in images])\nimages = [(arr * 255 / max_pixel).astype('uint8') for arr in images]\n# 5. Cast back to image format\nimages = [Image.fromarray(arr, 'RGBA' if arr.shape[-1] == 4 else 'RGB') for arr in images]\n# 6. Save\nsave_dir = os.path.join(self.model_dir, f'examples_fit_{self.nb_fit + 1}')\nif not os.path.exists(save_dir):\nos.makedirs(save_dir)\nfor i, im in enumerate(images):\nim_path = os.path.join(save_dir, f'example_{i}.png')\nim.save(im_path, format='PNG')\n##############################################\n# Get generators if not in_memory, else get full data\n# Finally fit the model\n##############################################\nif not self.in_memory:\nself.logger.info(\"Loading data via generators\")\n# Create generators\nif df_valid is not None:\nself.logger.info(\"Retrieving a generator for the training set\")\ntrain_generator = self._get_generator(df_train, data_type='train', batch_size=min(self.batch_size, len(df_train)))\nself.logger.info(\"Retrieving a generator for the validation set\")\nvalid_generator = self._get_generator(df_valid, data_type='valid', batch_size=min(self.batch_size, len(df_valid)))\n# Set dataset related args\nsteps_per_epoch_arg = len(df_train) // min(self.batch_size, len(df_train))\nvalidation_steps_arg = len(df_valid) // min(self.batch_size, len(df_valid))\nelse:\n# If no validation, we'll split the training set using validation_split attribute\ndf_train_split, df_valid_split = train_test_split(df_train, test_size=self.validation_split)\nself.logger.info(\"Retrieving a generator for the training set\")\ntrain_generator = self._get_generator(df_train_split, data_type='train', batch_size=min(self.batch_size, len(df_train_split)))\nself.logger.info(\"Retrieving a generator for the validation set\")\nvalid_generator = self._get_generator(df_valid_split, data_type='valid', batch_size=min(self.batch_size, len(df_valid_split)))\n# Set dataset related args\nsteps_per_epoch_arg = len(df_train_split) // min(self.batch_size, len(df_train_split))\nvalidation_steps_arg = len(df_valid_split) // min(self.batch_size, len(df_valid_split))\n# Get fit arguments\nx_arg = train_generator\ny_arg = None\nbatch_size_arg = None\n# validation_data does work with generators (TensorFlow doc is not up to date)\nvalidation_data_arg = valid_generator\nvalidation_split_arg = None\n# Load in memory - Can easily lead to OOM issues\nelse:\nself.logger.info(\"Loading data in memory\")\n# We retrieve all the data\n# Trick: we still use generators to have the correct preprocessing\n# -&gt; data_type = valid (no shuffle, no data augmentation)\ntrain_generator = self._get_generator(df_train, data_type='valid', batch_size=len(df_train))\nx_train, y_train = train_generator.next()\nif df_valid is not None:\nvalid_generator = self._get_generator(df_valid, data_type='valid', batch_size=min(self.batch_size, len(df_valid)))\nx_val, y_val = valid_generator.next()\nvalidation_data = (x_val, y_val)\nelse:\nvalidation_data = None\n# Get fit arguments\nx_arg = x_train\ny_arg = y_train\nbatch_size_arg = self.batch_size\nsteps_per_epoch_arg = None\nvalidation_data_arg = validation_data  # Can be None if no validation set\nvalidation_steps_arg = None\nvalidation_split_arg = self.validation_split if validation_data is None else None\n# Get model (if already fitted, _get_model returns instance model)\nself.model = self._get_model()\n# Get callbacks (early stopping &amp; checkpoint)\ncallbacks = self._get_callbacks()\n# Fit\n# We use a try...except in order to save the model if an error arises\n# after more than a minute into training\nstart_time = time.time()\ntry:\nfit_arguments = {\n'x': x_arg,\n'y': y_arg,\n'batch_size': batch_size_arg,\n'steps_per_epoch': steps_per_epoch_arg,\n'validation_data': validation_data_arg,\n'validation_split': validation_split_arg,\n'validation_steps': validation_steps_arg,\n}\nfit_history = self.model.fit(  # type: ignore\nepochs=self.epochs,\ncallbacks=callbacks,\nverbose=1,\n**fit_arguments,\n)\nexcept (RuntimeError, SystemError, SystemExit, EnvironmentError, KeyboardInterrupt, tf.errors.ResourceExhaustedError, tf.errors.InternalError,\ntf.errors.UnavailableError, tf.errors.UnimplementedError, tf.errors.UnknownError, Exception) as e:\n# Steps:\n# 1. Display tensorflow error\n# 2. Check if more than one minute elapsed &amp; not several iterations &amp; existence best.hdf5\n# 3. Reload best model\n# 4. We consider that a fit occured (trained = True, nb_fit += 1)\n# 5. Save &amp; create a warning file\n# 6. Display error messages\n# 7. Raise an error\n# 1.\nself.logger.error(repr(e))\n# 2.\nbest_path = os.path.join(self.model_dir, 'best.hdf5')\ntime_spent = time.time() - start_time\nif time_spent &gt;= 60 and os.path.exists(best_path):\n# 3.\nself.model = load_model(best_path, custom_objects=self.custom_objects)\n# 4.\nself.trained = True\nself.nb_fit += 1\n# 5.\nself.save()\nwith open(os.path.join(self.model_dir, \"0_MODEL_INCOMPLETE\"), 'w'):\npass\nwith open(os.path.join(self.model_dir, \"1_TRAINING_NEEDS_TO_BE_RESUMED\"), 'w'):\npass\n# 6.\nself.logger.error(\"[EXPERIMENTAL] Error during model training\")\nself.logger.error(f\"[EXPERIMENTAL] The error happened after {round(time_spent, 2)}s of training\")\nself.logger.error(\"[EXPERIMENTAL] A saving of the model is done but this model won't be usable as is.\")\nself.logger.error(f\"[EXPERIMENTAL] In order to resume the training, we have to specify this model ({ntpath.basename(self.model_dir)}) in the file 2_training.py\")\nself.logger.error(\"[EXPERIMENTAL] Warning, the preprocessing is not saved in the configuration file\")\nself.logger.error(\"[EXPERIMENTAL] Warning, the best model might be corrupted in some cases\")\n# 7.\nraise RuntimeError(\"Error during model training\")\n# Print accuracy &amp; loss if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._plot_metrics_and_loss(fit_history)\n# Reload best model\nself.model = load_model(\nos.path.join(self.model_dir, 'best.hdf5'),\ncustom_objects=self.custom_objects\n)\n# Set trained\nself.trained = True\nself.nb_fit += 1\n# Return fit arguments. This is useful for transfer learning algorithms\nreturn fit_arguments\ndef _fit_object_detector(self, df_train: pd.DataFrame, df_valid: pd.DataFrame = None, with_shuffle: bool = True, **kwargs) -&gt; dict:\n'''Fits the model - object detector\n        Args:\n            df_train (pd.DataFrame): Train dataset\n                Must contain file_path &amp; bboxes columns\n        Kwargs:\n            df_valid (pd.DataFrame): Validation dataset\n                Must contain file_path &amp; bboxes columns\n            with_shuffle (boolean): If the train dataset must be shuffled\n        Raises:\n            ValueError: If the model is not of type `object_detector`\n        '''\nraise NotImplementedError(\"'_fit_object_detector' needs to be overridden\")\n@utils.trained_needed\ndef predict(self, df_test: pd.DataFrame, return_proba: bool = False, batch_size: Union[int, None] = None) -&gt; Union[np.ndarray, list]:\n'''Predictions on test set\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes -- classifier only\n            batch_size (int): Batch size to be used -- classifier only\n        Raises:\n            NotImplementedError: If the model is not `classifier` nor `object_detector`\n        Returns:\n            (np.ndarray | list): Array, shape = [n_samples, n_classes] or List of n_samples elements\n        '''\nif self.model_type == 'classifier':\nreturn self._predict_classifier(df_test, return_proba=return_proba, batch_size=batch_size)\nelif self.model_type == 'object_detector':\nreturn self._predict_object_detector(df_test)\nelse:\nraise NotImplementedError(\"Only 'classifier' and 'object_detector' model type are supported\")\n@utils.trained_needed\ndef _predict_classifier(self, df_test, return_proba: bool = False, batch_size: int = None) -&gt; np.ndarray:\n'''Predictions on test set\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Kwargs:\n            return_proba (bool): If the function should return the probabilities instead of the classes\n            batch_size (int): Batch size to be used\n        Raises:\n            ValueError: If the model is not a classifier\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nif self.model_type != 'classifier':\nraise ValueError(f\"`_predict_classifier` function does not support model type {self.model_type}\")\n# Backup on training batch size if no batch size defined\nif batch_size is None:\nbatch_size = self.batch_size\n# Get generator or fulldata if in_memory\nif not self.in_memory:\nself.logger.info(\"Retrieving a generator for test data\")\ntest_generator = self._get_generator(df_test, data_type='test', batch_size=min(batch_size, len(df_test)))\n# Get predict arguments\nx_arg = test_generator\nbatch_size_arg = None\nelse:\nself.logger.info(\"Retrieving a all test data in memory\")\ntest_generator = self._get_generator(df_test, data_type='test', batch_size=len(df_test))\nx_test, _ = test_generator.next()\n# Get predict arguments\nx_arg = x_test\nbatch_size_arg = batch_size\n# Predict\npredicted_proba = self.model.predict(  # type: ignore\nx_arg,\nbatch_size=batch_size_arg,\nsteps=None,\nworkers=8,  # TODO : Check if this is ok if there are less CPUs\nverbose=1\n)\n# We return the probabilities if wanted\nif return_proba:\nreturn predicted_proba\n# Finally, we get the classes predictions\nreturn self.get_classes_from_proba(predicted_proba)  # type: ignore\n@utils.trained_needed\ndef _predict_object_detector(self, df_test: pd.DataFrame, **kwargs) -&gt; list:\n'''Predictions on test set - works only with batch size = 1\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Raises:\n            ValueError: If the model is not an object detector\n        Returns:\n            list: List of list of bboxes (one list per image)\n        '''\nraise NotImplementedError(\"'_predict_object_detector' needs to be overridden\")\n@utils.trained_needed\ndef predict_proba(self, df_test, batch_size: int = None) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Kwargs:\n            batch_size (int): Batch size to be used\n        Raises:\n            ValueError: If the model is not a classifier\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\nif self.model_type != 'classifier':\nraise ValueError(f\"`predict_proba` function does not support model type {self.model_type}\")\n# We reuse the predict function\nreturn self.predict(df_test, return_proba=True, batch_size=batch_size)\ndef _get_generator(self, df: pd.DataFrame, data_type: str, batch_size: int, **kwargs) -&gt; ImageDataGenerator:\n'''Gets image generator from a list of files\n        Args:\n            df (pd.DataFrame): DataFrame with files to be loaded\n            data_type (str): 'train', 'valid' or 'test'\n            batch_size (int): Batch size to be used\n        Raises:\n            NotImplementedError: If the model type is not supported\n        '''\nif self.model_type == 'classifier':\nreturn self._get_generator_classifier(df, data_type, batch_size)\nelse:\nraise NotImplementedError(f\"`_get_generator` needs to be overridden for model type {self.model_type}\")\ndef _get_generator_classifier(self, df: pd.DataFrame, data_type: str, batch_size: int, **kwarg) -&gt; ImageDataGenerator:\n'''Gets image generator from a list of files - classifier version\n        Args:\n            df (pd.DataFrame): DataFrame with files to be loaded\n            data_type (str): 'train', 'valid' or 'test'\n            batch_size (int): Batch size to be used\n        Raises:\n            ValueError: If the model is not a classifier\n            ValueError: If data_type is not in ['train', 'valid', 'test']\n            AttributeError: If list_classes attribute is not defined\n        '''\nif self.model_type != 'classifier':\nraise ValueError(f\"`_get_generator_classifier` function does not support model type {self.model_type}\")\nif data_type not in ['train', 'valid', 'test']:\nraise ValueError(f\"{data_type} is not a valid option for argument data_type (['train', 'valid', 'test'])\")\nif self.list_classes is None:\nraise AttributeError(\"Cannot get an image generator if list_classes is not set.\")\n# Copy\ndf = df.copy(deep=True)\n# Set data_gen (no augmentation if validation/test)\nif data_type == 'train':\ndata_generator = ImageDataGenerator(preprocessing_function=self.preprocess_input, **self.data_augmentation_params)\nelse:\ndata_generator = ImageDataGenerator(preprocessing_function=self.preprocess_input)\n# Get generator\nshuffle = True if data_type == 'train' else False  # DO NOT SHUFFLE IF VALID OR TEST !\nif data_type != 'test':\ngenerator = data_generator.flow_from_dataframe(df, directory=None, x_col='file_path', y_col='file_class', classes=self.list_classes,\ntarget_size=(self.width, self.height), color_mode=self.color_mode, class_mode='categorical',\nbatch_size=batch_size, shuffle=shuffle)\n# For the test dataset, we create a fake DataFrame with a unique class\nelse:\ndf['fake_class_col'] = 'all_classes'\ngenerator = data_generator.flow_from_dataframe(df, directory=None, x_col='file_path', y_col='fake_class_col', classes=['all_classes'],\ntarget_size=(self.width, self.height), color_mode=self.color_mode, class_mode='categorical',\nbatch_size=batch_size, shuffle=False)\nreturn generator\ndef _get_preprocess_input(self) -&gt; Union[Callable, None]:\n'''Gets the preprocessing to be used before feeding images to the NN\n        Needs to be overridden by child classes\n        Returns:\n            (Callable | None): Preprocessing function\n        '''\nreturn None\ndef _get_model(self) -&gt; Any:\n'''Gets a model structure - returns the instance model instead if already defined\n        Returns:\n            (Model): a Keras model\n        '''\nraise NotImplementedError(\"'_get_model' needs to be overridden\")\ndef _get_callbacks(self, *args) -&gt; list:\n'''Gets model callbacks\n        Returns:\n            list: List of callbacks\n        '''\n# Get classic callbacks\ncallbacks = [EarlyStopping(monitor='val_loss', patience=self.patience, restore_best_weights=True)]\nif self.level_save in ['MEDIUM', 'HIGH']:\ncallbacks.append(\nModelCheckpoint(\nfilepath=os.path.join(self.model_dir, 'best.hdf5'), monitor='val_loss', save_best_only=True, mode='auto'\n)\n)\ncallbacks.append(CSVLogger(filename=os.path.join(self.model_dir, 'logger.csv'), separator=';', append=False))\ncallbacks.append(TerminateOnNaN())\n# Get LearningRateScheduler\nscheduler = self._get_learning_rate_scheduler()\nif scheduler is not None:\ncallbacks.append(LearningRateScheduler(scheduler))\n# Manage tensorboard\nif self.level_save in ['HIGH']:\n# Get log directory\nmodels_path = utils.get_models_path()\ntensorboard_dir = os.path.join(models_path, 'tensorboard_logs')\n# We add a prefix so that the function load_model works correctly (it looks for a sub-folder with model name)\nlog_dir = os.path.join(tensorboard_dir, f\"tensorboard_{ntpath.basename(self.model_dir)}\")\nif not os.path.exists(log_dir):\nos.makedirs(log_dir)\n# TODO: check if this class does not slow proccesses\n# -&gt; For now: comment\n# Create custom class to monitore LR changes\n# https://stackoverflow.com/questions/49127214/keras-how-to-output-learning-rate-onto-tensorboard\n# class LRTensorBoard(TensorBoard):\n#     def __init__(self, log_dir, **kwargs) -&gt; None:  # add other arguments to __init__ if you need\n#         super().__init__(log_dir=log_dir, **kwargs)\n#\n#     def on_epoch_end(self, epoch, logs=None):\n#         logs.update({'lr': K.eval(self.model.optimizer.lr)})\n#         super().on_epoch_end(epoch, logs)\ncallbacks.append(TensorBoard(log_dir=log_dir, write_grads=False, write_images=False))\nself.logger.info(f\"To start tensorboard: python -m tensorboard.main --logdir {tensorboard_dir} --samples_per_plugin images=10\")\n# We use samples_per_plugin to avoid a rare issue between matplotlib and tensorboard\n# https://stackoverflow.com/questions/27147300/matplotlib-tcl-asyncdelete-async-handler-deleted-by-the-wrong-thread\nreturn callbacks\ndef _get_learning_rate_scheduler(self) -&gt; Union[Callable, None]:\n'''Fonction to define a Learning Rate Scheduler\n           -&gt; if it returns None, no scheduler will be used. (def.)\n           -&gt; This function will be save directly in the model configuration file\n           -&gt; This can be overridden at runing time\n        Returns:\n            (Callable | None): A learning rate Scheduler\n        '''\n# e.g.\n# def scheduler(epoch):\n#     lim_epoch = 75\n#     if epoch &lt; lim_epoch:\n#         return 0.01\n#     else:\n#         return max(0.001, 0.01 * math.exp(0.01 * (lim_epoch - epoch)))\nscheduler = None\nreturn scheduler\ndef _plot_metrics_and_loss(self, fit_history, **kwargs) -&gt; None:\n'''Plots available metrics and losses\n        Args:\n            fit_history (?) : fit history\n        '''\n# Manage dir\nplots_path = os.path.join(self.model_dir, 'plots')\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\n# Get a dictionnary of possible metrics/loss plots\nmetrics_dir = {\n'acc': ['Accuracy', 'accuracy'],\n'loss': ['Loss', 'loss'],\n'categorical_accuracy': ['Categorical accuracy', 'categorical_accuracy'],\n'f1': ['F1-score', 'f1_score'],\n'precision': ['Precision', 'precision'],\n'recall': ['Recall', 'recall'],\n}\n# Plot each available metric\nfor metric in fit_history.history.keys():\nif metric in metrics_dir.keys():\ntitle = metrics_dir[metric][0]\nfilename = metrics_dir[metric][1]\nplt.figure(figsize=(10, 8))\nplt.plot(fit_history.history[metric])\nplt.plot(fit_history.history[f'val_{metric}'])\nplt.title(f\"Model {title}\")\nplt.ylabel(title)\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n# Save\nfilename == f\"{filename}.jpeg\"\nplt.savefig(os.path.join(plots_path, filename))\n# Close figures\nplt.close('all')\ndef _save_model_png(self, model) -&gt; None:\n'''Tries to save the structure of the model in png format\n        Graphviz necessary\n        Args:\n            model (?): model to plot\n        '''\n# Check if graphiz is intalled\n# TODO : to be improved !\ngraphiz_path = 'C:/Program Files (x86)/Graphviz2.38/bin/'\nif os.path.isdir(graphiz_path):\nos.environ[\"PATH\"] += os.pathsep + 'C:/Program Files (x86)/Graphviz2.38/bin/'\nimg_path = os.path.join(self.model_dir, 'model.png')\nplot_model(model, to_file=img_path)\n@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'keras'\njson_data['batch_size'] = self.batch_size\njson_data['epochs'] = self.epochs\njson_data['validation_split'] = self.validation_split\njson_data['patience'] = self.patience\njson_data['width'] = self.width\njson_data['height'] = self.height\njson_data['depth'] = self.depth\njson_data['color_mode'] = self.color_mode\njson_data['in_memory'] = self.in_memory\njson_data['data_augmentation_params'] = self.data_augmentation_params\njson_data['nb_train_generator_images_to_save'] = self.nb_train_generator_images_to_save\njson_data['keras_params'] = self.keras_params\nif self.model is not None:\njson_data['keras_model'] = json.loads(self.model.to_json())\nelse:\njson_data['keras_model'] = None\n# Add _get_model code if not in json_data\nif '_get_model' not in json_data.keys():\njson_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n# Add _get_preprocess_input code if not in json_data\nif '_get_preprocess_input' not in json_data.keys():\njson_data['_get_preprocess_input'] = pickle.source.getsourcelines(self._get_preprocess_input)[0]\n# Save preprocess_input to a .pkl file if level_save &gt; LOW\npkl_path = os.path.join(self.model_dir, \"preprocess_input.pkl\")\nif self.level_save in ['MEDIUM', 'HIGH']:\nwith open(pkl_path, 'wb') as f:\npickle.dump(self.preprocess_input, f)\n# Add _get_learning_rate_scheduler code if not in json_data\nif '_get_learning_rate_scheduler' not in json_data.keys():\njson_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n# Add custom_objects code if not in json_data\nif 'custom_objects' not in json_data.keys():\ncustom_objects_str = self.custom_objects.copy()\nfor key in custom_objects_str.keys():\nif callable(custom_objects_str[key]):\n# Nominal case\nif not type(custom_objects_str[key]) == functools.partial:\ncustom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n# Manage partials\nelse:\ncustom_objects_str[key] = {\n'type': 'partial',\n'args': custom_objects_str[key].args,\n'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n}\njson_data['custom_objects'] = custom_objects_str\n# Save strategy :\n# - best.hdf5 already saved in fit()\n# - can't pickle keras model, so we drop it, save, and reload it\nkeras_model = self.model\nself.model = None\nsuper().save(json_data=json_data)\nself.model = keras_model\ndef reload_model(self, hdf5_path: str) -&gt; Any:\n'''Loads a Keras model from a HDF5 file\n        Args:\n            hdf5_path (str): Path to the hdf5 file\n        Returns:\n            ?: Keras model\n        '''\n# Fix tensorflow GPU if not already done (useful if we reload a model)\ntry:\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\nexcept Exception:\npass\n# We check if we already have the custom objects\nif hasattr(self, 'custom_objects') and self.custom_objects is not None:\ncustom_objects = self.custom_objects\nelse:\nself.logger.warning(\"Can't find the attribute 'custom_objects' in the model to be reloaded\")\nself.logger.warning(\"Backup on the default custom_objects of utils_deep_keras\")\ncustom_objects = utils_deep_keras.custom_objects\n# Loading of the model\nkeras_model = load_model(hdf5_path, custom_objects=custom_objects)\n# Set trained to true if not already true\nif not self.trained:\nself.trained = True\nself.nb_fit = 1\n# Return\nreturn keras_model\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Needs to be overridden /!\\\\ -\n        '''\nraise NotImplementedError(\"'reload' needs to be overridden\")\ndef _is_gpu_activated(self) -&gt; bool:\n'''Checks if a GPU is used\n        Returns:\n            bool: whether GPU is available or not\n        '''\n# Check for available GPU devices\nphysical_devices = tf.config.list_physical_devices('GPU')\nif len(physical_devices) &gt; 0:\nreturn True\nelse:\nreturn False\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.__init__","title":"<code>__init__(batch_size=64, epochs=99, validation_split=0.2, patience=5, width=224, height=224, depth=3, color_mode='rgb', in_memory=False, data_augmentation_params={}, nb_train_generator_images_to_save=20, keras_params={}, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass for more arguments)</p> Kwargs <p>batch_size (int): Batch size epochs (int): Number of epochs validation_split (float): Percentage for the validation set split     Only used if no input validation set when fitting patience (int): Early stopping patience width (int): NN input width (images are resized) height (int): NN input height (images are resized) depth (int): NN input depth color_mode (str): NN input color mode in_memory (bool): If all images should be loaded in memory, otherwise it uses a generator     /! OOM errors can happen really quickly (depends on the dataset size)     /! Data augmentation impossible if <code>in_memory</code> is set to True data_augmentation_params (dict): Dictionnary of parameters to be used with the data augmentation     cf. https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator     /! Not used if <code>in_memory</code> is set to True nb_train_generator_images_to_save (int): If &gt; 0, save some input generated images     If helps with to understand what goes in your NN keras_params (dict): Parameters used by keras models.     e.g. learning_rate, nb_lstm_units, etc...     The purpose of this dictionary is for the user to use it as they wants in the _get_model function     This parameter was initially added in order to do an hyperparameters search</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If <code>in_memory</code> is set to True and <code>data_augmentation_params</code> is not empty</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>def __init__(self, batch_size: int = 64, epochs: int = 99, validation_split: float = 0.2, patience: int = 5,\nwidth: int = 224, height: int = 224, depth: int = 3, color_mode: str = 'rgb',\nin_memory: bool = False, data_augmentation_params: dict = {},\nnb_train_generator_images_to_save: int = 20,\nkeras_params: dict = {}, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass for more arguments)\n    Kwargs:\n        batch_size (int): Batch size\n        epochs (int): Number of epochs\n        validation_split (float): Percentage for the validation set split\n            Only used if no input validation set when fitting\n        patience (int): Early stopping patience\n        width (int): NN input width (images are resized)\n        height (int): NN input height (images are resized)\n        depth (int): NN input depth\n        color_mode (str): NN input color mode\n        in_memory (bool): If all images should be loaded in memory, otherwise it uses a generator\n            /!\\\\ OOM errors can happen really quickly (depends on the dataset size)\n            /!\\\\ Data augmentation impossible if `in_memory` is set to True\n        data_augmentation_params (dict): Dictionnary of parameters to be used with the data augmentation\n            cf. https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/image/ImageDataGenerator\n            /!\\\\ Not used if `in_memory` is set to True\n        nb_train_generator_images_to_save (int): If &gt; 0, save some input generated images\n            If helps with to understand what goes in your NN\n        keras_params (dict): Parameters used by keras models.\n            e.g. learning_rate, nb_lstm_units, etc...\n            The purpose of this dictionary is for the user to use it as they wants in the _get_model function\n            This parameter was initially added in order to do an hyperparameters search\n    Raises:\n        ValueError: If `in_memory` is set to True and `data_augmentation_params` is not empty\n    '''\n# TODO: learning rate should be an attribute !\n# Check for errors\nif in_memory and len(data_augmentation_params) &gt; 0:\nraise ValueError(\"Data augmentation is impossible for 'in_memory' mode\")\n# Init.\nsuper().__init__(**kwargs)\n# Fix tensorflow GPU\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Param. model\nself.batch_size = batch_size\nself.epochs = epochs\nself.validation_split = validation_split\nself.patience = patience\n# Params. generator\nself.width = width\nself.height = height\nself.depth = depth\nself.color_mode = color_mode\nself.in_memory = in_memory\nself.data_augmentation_params = data_augmentation_params.copy()\n# Warnings if depth does not match with color_mode\nif self.color_mode == 'rgb' and self.depth != 3:\nself.logger.warning(f\"`color_mode` parameter is 'rgb', but `depth` parameteris not equal to 3 ({self.depth})\")\nself.logger.warning(\"We continue, but this can lead to errors during the training\")\nif self.color_mode == 'rgba' and self.depth != 4:\nself.logger.warning(f\"`color_mode` parameter is 'rgba', but `depth` parameteris not equal to 4 ({self.depth})\")\nself.logger.warning(\"We continue, but this can lead to errors during the training\")\n# TODO: add Test time augmentation ?\n# Misc.\nself.nb_train_generator_images_to_save = nb_train_generator_images_to_save\n# Model set on fit\nself.model: Any = None\n# Set preprocess input\nself.preprocess_input = self._get_preprocess_input()\n# Keras params\nself.keras_params = keras_params.copy()\n# Keras custom objects : we get the ones specified in utils_deep_keras\nself.custom_objects = utils_deep_keras.custom_objects\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.fit","title":"<code>fit(df_train, df_valid=None, with_shuffle=True, **kwargs)</code>","text":"<p>Fits the model</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>pd.DataFrame</code> <p>Train dataset Must contain file_path &amp; file_class columns if classifier Must contain file_path &amp; bboxes columns if object detector</p> required Kwargs <p>df_valid (pd.DataFrame): Validation dataset     Must contain file_path &amp; file_class columns if classifier     Must contain file_path &amp; bboxes columns if object detector with_shuffle (boolean): If the train dataset must be shuffled     This should be used if the input dataset is not shuffled &amp; no validation set as the split_validation takes the lines in order.     Thus, the validation set might get classes which are not in the train set ...</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the model is not <code>classifier</code> nor <code>object_detector</code></p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Fit arguments, to be used with transfer learning fine-tuning</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>def fit(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None, with_shuffle: bool = True, **kwargs) -&gt; dict:\n'''Fits the model\n    Args:\n        df_train (pd.DataFrame): Train dataset\n            Must contain file_path &amp; file_class columns if classifier\n            Must contain file_path &amp; bboxes columns if object detector\n    Kwargs:\n        df_valid (pd.DataFrame): Validation dataset\n            Must contain file_path &amp; file_class columns if classifier\n            Must contain file_path &amp; bboxes columns if object detector\n        with_shuffle (boolean): If the train dataset must be shuffled\n            This should be used if the input dataset is not shuffled &amp; no validation set as the split_validation takes the lines in order.\n            Thus, the validation set might get classes which are not in the train set ...\n    Raises:\n        NotImplementedError: If the model is not `classifier` nor `object_detector`\n    Returns:\n        dict: Fit arguments, to be used with transfer learning fine-tuning\n    '''\nif self.model_type == 'classifier':\nreturn self._fit_classifier(df_train, df_valid=df_valid, with_shuffle=with_shuffle, **kwargs)\nelif self.model_type == 'object_detector':\nreturn self._fit_object_detector(df_train, df_valid=df_valid, with_shuffle=with_shuffle, **kwargs)\nelse:\nraise NotImplementedError(\"Only `classifier` and `object_detector` model type are supported.\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.predict","title":"<code>predict(df_test, return_proba=False, batch_size=None)</code>","text":"<p>Predictions on test set</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>pd.DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required Kwargs <p>return_proba (bool): If the function should return the probabilities instead of the classes -- classifier only batch_size (int): Batch size to be used -- classifier only</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the model is not <code>classifier</code> nor <code>object_detector</code></p> <p>Returns:</p> Type Description <code>np.ndarray | list</code> <p>Array, shape = [n_samples, n_classes] or List of n_samples elements</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>@utils.trained_needed\ndef predict(self, df_test: pd.DataFrame, return_proba: bool = False, batch_size: Union[int, None] = None) -&gt; Union[np.ndarray, list]:\n'''Predictions on test set\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n    Kwargs:\n        return_proba (bool): If the function should return the probabilities instead of the classes -- classifier only\n        batch_size (int): Batch size to be used -- classifier only\n    Raises:\n        NotImplementedError: If the model is not `classifier` nor `object_detector`\n    Returns:\n        (np.ndarray | list): Array, shape = [n_samples, n_classes] or List of n_samples elements\n    '''\nif self.model_type == 'classifier':\nreturn self._predict_classifier(df_test, return_proba=return_proba, batch_size=batch_size)\nelif self.model_type == 'object_detector':\nreturn self._predict_object_detector(df_test)\nelse:\nraise NotImplementedError(\"Only 'classifier' and 'object_detector' model type are supported\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.predict_proba","title":"<code>predict_proba(df_test, batch_size=None)</code>","text":"<p>Predicts probabilities on the test dataset</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>pd.DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required Kwargs <p>batch_size (int): Batch size to be used</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model is not a classifier</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>@utils.trained_needed\ndef predict_proba(self, df_test, batch_size: int = None) -&gt; np.ndarray:\n'''Predicts probabilities on the test dataset\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n    Kwargs:\n        batch_size (int): Batch size to be used\n    Raises:\n        ValueError: If the model is not a classifier\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\nif self.model_type != 'classifier':\nraise ValueError(f\"`predict_proba` function does not support model type {self.model_type}\")\n# We reuse the predict function\nreturn self.predict(df_test, return_proba=True, batch_size=batch_size)\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Needs to be overridden /! -</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Needs to be overridden /!\\\\ -\n    '''\nraise NotImplementedError(\"'reload' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.reload_model","title":"<code>reload_model(hdf5_path)</code>","text":"<p>Loads a Keras model from a HDF5 file</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to the hdf5 file</p> required <p>Returns:</p> Type Description <code>Any</code> <p>?: Keras model</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>def reload_model(self, hdf5_path: str) -&gt; Any:\n'''Loads a Keras model from a HDF5 file\n    Args:\n        hdf5_path (str): Path to the hdf5 file\n    Returns:\n        ?: Keras model\n    '''\n# Fix tensorflow GPU if not already done (useful if we reload a model)\ntry:\ngpu_devices = tf.config.experimental.list_physical_devices('GPU')\nfor device in gpu_devices:\ntf.config.experimental.set_memory_growth(device, True)\nexcept Exception:\npass\n# We check if we already have the custom objects\nif hasattr(self, 'custom_objects') and self.custom_objects is not None:\ncustom_objects = self.custom_objects\nelse:\nself.logger.warning(\"Can't find the attribute 'custom_objects' in the model to be reloaded\")\nself.logger.warning(\"Backup on the default custom_objects of utils_deep_keras\")\ncustom_objects = utils_deep_keras.custom_objects\n# Loading of the model\nkeras_model = load_model(hdf5_path, custom_objects=custom_objects)\n# Set trained to true if not already true\nif not self.trained:\nself.trained = True\nself.nb_fit = 1\n# Return\nreturn keras_model\n</code></pre>"},{"location":"reference/template_vision/models_training/model_keras/#template_vision.models_training.model_keras.ModelKeras.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/model_keras.py</code> <pre><code>@no_type_check  # We do not check the type, because it is complicated with managing custom_objects_str\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['librairie'] = 'keras'\njson_data['batch_size'] = self.batch_size\njson_data['epochs'] = self.epochs\njson_data['validation_split'] = self.validation_split\njson_data['patience'] = self.patience\njson_data['width'] = self.width\njson_data['height'] = self.height\njson_data['depth'] = self.depth\njson_data['color_mode'] = self.color_mode\njson_data['in_memory'] = self.in_memory\njson_data['data_augmentation_params'] = self.data_augmentation_params\njson_data['nb_train_generator_images_to_save'] = self.nb_train_generator_images_to_save\njson_data['keras_params'] = self.keras_params\nif self.model is not None:\njson_data['keras_model'] = json.loads(self.model.to_json())\nelse:\njson_data['keras_model'] = None\n# Add _get_model code if not in json_data\nif '_get_model' not in json_data.keys():\njson_data['_get_model'] = pickle.source.getsourcelines(self._get_model)[0]\n# Add _get_preprocess_input code if not in json_data\nif '_get_preprocess_input' not in json_data.keys():\njson_data['_get_preprocess_input'] = pickle.source.getsourcelines(self._get_preprocess_input)[0]\n# Save preprocess_input to a .pkl file if level_save &gt; LOW\npkl_path = os.path.join(self.model_dir, \"preprocess_input.pkl\")\nif self.level_save in ['MEDIUM', 'HIGH']:\nwith open(pkl_path, 'wb') as f:\npickle.dump(self.preprocess_input, f)\n# Add _get_learning_rate_scheduler code if not in json_data\nif '_get_learning_rate_scheduler' not in json_data.keys():\njson_data['_get_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_learning_rate_scheduler)[0]\n# Add custom_objects code if not in json_data\nif 'custom_objects' not in json_data.keys():\ncustom_objects_str = self.custom_objects.copy()\nfor key in custom_objects_str.keys():\nif callable(custom_objects_str[key]):\n# Nominal case\nif not type(custom_objects_str[key]) == functools.partial:\ncustom_objects_str[key] = pickle.source.getsourcelines(custom_objects_str[key])[0]\n# Manage partials\nelse:\ncustom_objects_str[key] = {\n'type': 'partial',\n'args': custom_objects_str[key].args,\n'function': pickle.source.getsourcelines(custom_objects_str[key].func)[0],\n}\njson_data['custom_objects'] = custom_objects_str\n# Save strategy :\n# - best.hdf5 already saved in fit()\n# - can't pickle keras model, so we drop it, save, and reload it\nkeras_model = self.model\nself.model = None\nsuper().save(json_data=json_data)\nself.model = keras_model\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/","title":"Utils deep keras","text":""},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.f1","title":"<code>f1(y_true, y_pred)</code>","text":"<p>f1 score, to use as custom metrics</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def f1(y_true, y_pred) -&gt; float:\n'''f1 score, to use as custom metrics\n    - /!\\\\ To use with a big batch size /!\\\\ -\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n# Round pred to 0 &amp; 1\ny_pred = K.round(y_pred)\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ny_pred = K.round(y_pred)\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n# tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\np = tp / (tp + fp + K.epsilon())\nr = tp / (tp + fn + K.epsilon())\nf1 = 2 * p * r / (p + r + K.epsilon())\nf1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\nweighted_f1 = f1 * ground_positives / K.sum(ground_positives)\nweighted_f1 = K.sum(weighted_f1)\nreturn weighted_f1\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.f1_loss","title":"<code>f1_loss(y_true, y_pred)</code>","text":"<p>f1 loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def f1_loss(y_true, y_pred) -&gt; float:\n'''f1 loss, to use as custom loss\n    - /!\\\\ To use with a big batch size /!\\\\ -\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n# TODO : Find a mean of rounding y_pred\n# TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n# We can't round here :(\n# Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n# Common ops without gradient: K.argmax, K.round, K.eval.\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n# tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\np = tp / (tp + fp + K.epsilon())\nr = tp / (tp + fn + K.epsilon())\nf1 = 2 * p * r / (p + r + K.epsilon())\nf1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\nweighted_f1 = f1 * ground_positives / K.sum(ground_positives)\nweighted_f1 = K.sum(weighted_f1)\nreturn 1 - weighted_f1\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.fb_loss","title":"<code>fb_loss(b, y_true, y_pred)</code>","text":"<p>fB loss, to use as custom loss</p> <ul> <li>/! To use with a big batch size /! -</li> </ul> From <p>https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> required <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def fb_loss(b: float, y_true, y_pred) -&gt; float:\n'''fB loss, to use as custom loss\n    - /!\\\\ To use with a big batch size /!\\\\ -\n    From:\n        https://www.kaggle.com/rejpalcz/best-loss-function-for-f1-score-metric\n        https://stackoverflow.com/questions/59963911/how-to-write-a-custom-f1-loss-function-with-weighted-average-for-keras\n    Args:\n        b (float): importance recall in the calculation of the fB score\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\n# TODO : Find a mean of rounding y_pred\n# TODO : Problem : models will quickly converge on probabilities 1.0 &amp; 0.0 to optimize this loss....\n# We can't round here :(\n# Please make sure that all of your ops have a gradient defined (i.e. are differentiable).\n# Common ops without gradient: K.argmax, K.round, K.eval.\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n# tn = K.sum(K.cast((1 - y_true) * (1 - y_pred), 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\np = tp / (tp + fp + K.epsilon())\nr = tp / (tp + fn + K.epsilon())\nfb = (1 + b**2) * p * r / ((p * b**2) + r + K.epsilon())\nfb = tf.where(tf.math.is_nan(fb), tf.zeros_like(fb), fb)\nweighted_fb = fb * ground_positives / K.sum(ground_positives)\nweighted_fb = K.sum(weighted_fb)\nreturn 1 - weighted_fb\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.get_fb_loss","title":"<code>get_fb_loss(b=2.0)</code>","text":"<p>Gets a fB-score loss</p> <p>Parameters:</p> Name Type Description Default <code>b</code> <code>float</code> <p>importance recall in the calculation of the fB score</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>fb_loss</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def get_fb_loss(b: float = 2.0) -&gt; Callable:\n''' Gets a fB-score loss\n    Args:\n        b (float): importance recall in the calculation of the fB score\n    Returns:\n        Callable: fb_loss\n    '''\n# - /!\\ Utilisation partial obligatoire pour pouvoir pickle des fonctions dynamiques ! /!\\ -\nfn = partial(fb_loss, b)\n# FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\nfn.__name__ = 'fb_loss'  # type: ignore\nreturn fn\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.get_weighted_binary_crossentropy","title":"<code>get_weighted_binary_crossentropy(pos_weight=10.0)</code>","text":"<p>Gets a \"weighted binary crossentropy\" loss From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>Weight of the positive class, to be tuned</p> <code>10.0</code> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Weighted binary crossentropy loss</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def get_weighted_binary_crossentropy(pos_weight: float = 10.0) -&gt; Callable:\n''' Gets a \"weighted binary crossentropy\" loss\n    From https://stats.stackexchange.com/questions/261128/neural-network-for-multi-label-classification-with-large-number-of-classes-outpu\n    TO BE ADDED IN custom_objects : 'weighted_binary_crossentropy': utils_deep_keras.get_weighted_binary_crossentropy(pos_weight=...)\n    Args:\n        pos_weight (float): Weight of the positive class, to be tuned\n    Returns:\n        Callable: Weighted binary crossentropy loss\n    '''\n# - /!\\ Use of partial mandatory in order to be able to pickle dynamical functions ! /!\\ -\nfn = partial(weighted_binary_crossentropy, pos_weight)\n# FIX:  AttributeError: 'functools.partial' object has no attribute '__name__'\nfn.__name__ = 'weighted_binary_crossentropy'  # type: ignore\nreturn fn\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.precision","title":"<code>precision(y_true, y_pred)</code>","text":"<p>Precision, to use as custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def precision(y_true, y_pred) -&gt; float:\n'''Precision, to use as custom metrics\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\ny_pred = K.round(y_pred)\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\nfp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\nprecision = tp / (tp + fp + K.epsilon())\nprecision = tf.where(tf.math.is_nan(precision), tf.zeros_like(precision), precision)\nweighted_precision = precision * ground_positives / K.sum(ground_positives)\nweighted_precision = K.sum(weighted_precision)\nreturn weighted_precision\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.recall","title":"<code>recall(y_true, y_pred)</code>","text":"<p>Recall to use as a custom metrics</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Ground truth values</p> required <code>y_pred</code> <p>The predicted values</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def recall(y_true, y_pred) -&gt; float:\n'''Recall to use as a custom metrics\n    Args:\n        y_true: Ground truth values\n        y_pred: The predicted values\n    Returns:\n        float: metric\n    '''\ny_pred = K.round(y_pred)\ny_true = K.cast(y_true, 'float32')  # Fix : TypeError: Input 'y' of 'Mul' Op has type float32 that does not match type int32 of argument 'x'.\nground_positives = K.sum(y_true, axis=0) + K.epsilon()  # We add an epsilon -&gt; manage the case where a class is absent in the batch\ntp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\nfn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\nrecall = tp / (tp + fn + K.epsilon())\nrecall = tf.where(tf.math.is_nan(recall), tf.zeros_like(recall), recall)\nweighted_recall = recall * ground_positives / K.sum(ground_positives)\nweighted_recall = K.sum(weighted_recall)\nreturn weighted_recall\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_deep_keras/#template_vision.models_training.utils_deep_keras.weighted_binary_crossentropy","title":"<code>weighted_binary_crossentropy(pos_weight, target, output)</code>","text":"<p>Weighted binary crossentropy between an output tensor and a target tensor. pos_weight is used as a multiplier for the positive targets.</p> <p>Combination of the following functions: * keras.losses.binary_crossentropy * keras.backend.tensorflow_backend.binary_crossentropy * tf.nn.weighted_cross_entropy_with_logits</p> <p>Parameters:</p> Name Type Description Default <code>pos_weight</code> <code>float</code> <p>poid classe positive, to be tuned</p> required <code>target</code> <p>Target tensor</p> required <code>output</code> <p>Output tensor</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>metric</p> Source code in <code>template_vision/models_training/utils_deep_keras.py</code> <pre><code>def weighted_binary_crossentropy(pos_weight: float, target, output) -&gt; float:\n'''Weighted binary crossentropy between an output tensor\n    and a target tensor. pos_weight is used as a multiplier\n    for the positive targets.\n    Combination of the following functions:\n    * keras.losses.binary_crossentropy\n    * keras.backend.tensorflow_backend.binary_crossentropy\n    * tf.nn.weighted_cross_entropy_with_logits\n    Args:\n        pos_weight (float): poid classe positive, to be tuned\n        target: Target tensor\n        output: Output tensor\n    Returns:\n        float: metric\n    '''\ntarget = K.cast(target, 'float32')\noutput = K.cast(output, 'float32')\n# transform back to logits\n_epsilon = tf.convert_to_tensor(K.epsilon(), output.dtype.base_dtype)\noutput = tf.clip_by_value(output, _epsilon, 1 - _epsilon)\noutput = tf.math.log(output / (1 - output))\n# compute weighted loss\nloss = tf.nn.weighted_cross_entropy_with_logits(target, output, pos_weight=pos_weight)\nreturn tf.reduce_mean(loss, axis=-1)\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/","title":"Utils models","text":""},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.display_train_test_shape","title":"<code>display_train_test_shape(df_train, df_test, df_shape=None)</code>","text":"<p>Displays the size of a train/test split</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>pd.DataFrame</code> <p>Train dataset</p> required <code>df_test</code> <code>pd.DataFrame</code> <p>Test dataset</p> required Kwargs <p>df_shape (int): Size of the initial dataset</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object df_shape is not positive</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def display_train_test_shape(df_train: pd.DataFrame, df_test: pd.DataFrame, df_shape: Union[int, None] = None) -&gt; None:\n'''Displays the size of a train/test split\n    Args:\n        df_train (pd.DataFrame): Train dataset\n        df_test (pd.DataFrame): Test dataset\n    Kwargs:\n        df_shape (int): Size of the initial dataset\n    Raises:\n        ValueError: If the object df_shape is not positive\n    '''\nif df_shape is not None and df_shape &lt; 1:\nraise ValueError(\"The object df_shape must be positive\")\n# Process\nif df_shape is None:\ndf_shape = df_train.shape[0] + df_test.shape[0]\nlogger.info(f\"There are {df_train.shape[0]} lines in the train dataset and {df_test.shape[0]} in the test dataset.\")\nlogger.info(f\"{round(100 * df_train.shape[0] / df_shape, 2)}% of data are in the train set\")\nlogger.info(f\"{round(100 * df_test.shape[0] / df_shape, 2)}% of data are in the test set\")\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.load_model","title":"<code>load_model(model_dir, is_path=False)</code>","text":"<p>Loads a model from a path</p> <p>Parameters:</p> Name Type Description Default <code>model_dir</code> <code>str</code> <p>Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19)</p> required Kwargs <p>is_path (bool): If folder path instead of name (permits to load model from elsewhere)</p> <p>Returns:</p> Name Type Description <code>Any</code> <p>?: Model</p> <code>dict</code> <code>dict</code> <p>Model configurations</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def load_model(model_dir: str, is_path: bool = False) -&gt; Tuple[Any, dict]:\n'''Loads a model from a path\n    Args:\n        model_dir (str): Name of the folder containing the model (e.g. model_autres_2019_11_07-13_43_19)\n    Kwargs:\n        is_path (bool): If folder path instead of name (permits to load model from elsewhere)\n    Returns:\n        ?: Model\n        dict: Model configurations\n    '''\n# Find model path\nbase_folder = None if is_path else utils.get_models_path()\nmodel_path = utils.find_folder_path(model_dir, base_folder)\n# Get configs\nconfiguration_path = os.path.join(model_path, 'configurations.json')\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys() and configs['dict_classes'] is not None:\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\n# Load model\npkl_path = os.path.join(model_path, f\"{configs['model_name']}.pkl\")\nwith open(pkl_path, 'rb') as f:\nmodel = pickle.load(f)\n# Change model_dir if diff\nif model_path != model.model_dir:\nmodel.model_dir = model_path\nconfigs['model_dir'] = model_path\n# Load specifics\nhdf5_path = os.path.join(model_path, 'best.hdf5')\n# TODO : we should probably have a single function `load_self` and let the model manage it's reload\n# Check for keras model\nif os.path.exists(hdf5_path):\n# If a specific reload function has been defined (e.g. faster RCNN), we use it\nif hasattr(model, 'reload_models_from_hdf5'):\nmodel.reload_models_from_hdf5(hdf5_path)\nelse:\nmodel.model = model.reload_model(hdf5_path)\n# Display if GPU is being used\nmodel.display_if_gpu_activated()\n# Return model &amp; configs\nreturn model, configs\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.normal_split","title":"<code>normal_split(df, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe containing the data</p> required Kwargs <p>test_size (float): Proportion representing the size of the expected test set seed (int): random seed</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object test_size is not between 0 and 1</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>Train dataframe</p> <code>DataFrame</code> <code>pd.DataFrame</code> <p>Test dataframe</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def normal_split(df: pd.DataFrame, test_size: float = 0.25, seed: Union[int, None] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n'''Splits a DataFrame into train and test sets\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\nif not 0 &lt;= test_size &lt;= 1:\nraise ValueError('The object test_size must be between 0 and 1')\n# Normal split\nlogger.info(\"Normal split\")\ndf_train, df_test = train_test_split(df, test_size=test_size, random_state=seed)\n# Display\ndisplay_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n# Return\nreturn df_train, df_test\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.predict","title":"<code>predict(data_input, model, model_conf, return_proba=False, **kwargs)</code>","text":"<p>Gets predictions of a model on images</p> <p>Parameters:</p> Name Type Description Default <code>data_input</code> <code>str | list&lt;str&gt; | np.ndarray</code> <p>New content to be predicted - str: abs. path to an image - list: list of abs. path to an image - np.ndarray: an already loaded image     Possibility to have several images if 4 dim (i.e (nb_images, width, height, channels)) - pd.DataFrame: Dataframe with a column file_path (abs. paths to images) required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <code>model_conf</code> <code>dict</code> <p>Model configurations</p> required Kwargs <p>return_proba (bool): If probabilities must be return instead</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If model is object detection task</p> <code>FileNotFoundError</code> <p>If the input file does not exist (input type == str)</p> <code>FileNotFoundError</code> <p>If one of the input files does not exist (input type == list)</p> <code>ValueError</code> <p>If the input image format is not compatible (input type == np.ndarray)</p> <code>ValueError</code> <p>If the input array is not compatible (input type == np.ndarray)</p> <code>ValueError</code> <p>If the input DataFrame does not contains a 'file_path' column (input type == pd.DataFrame)</p> <code>ValueError</code> <p>If the input type is not a valid type option</p> <p>Returns:</p> Type Description <code>Union[List[str], np.ndarray]</code> <p>List[str], np.ndarray: predictions or probabilities - If return_proba -&gt; np.ndarray - Else List[str]</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def predict(data_input: Union[str, List[str], np.ndarray, pd.DataFrame], model, model_conf: dict,\nreturn_proba: bool = False, **kwargs) -&gt; Union[List[str], np.ndarray]:\n'''Gets predictions of a model on images\n    Args:\n        data_input (str | list&lt;str&gt; | np.ndarray): New content to be predicted\n            - str: abs. path to an image\n            - list&lt;str&gt;: list of abs. path to an image\n            - np.ndarray: an already loaded image\n                Possibility to have several images if 4 dim (i.e (nb_images, width, height, channels))\n            - pd.DataFrame: Dataframe with a column file_path (abs. paths to images)\n        model (ModelClass): Model to use\n        model_conf (dict): Model configurations\n    Kwargs:\n        return_proba (bool): If probabilities must be return instead\n    Raises:\n        NotImplementedError: If model is object detection task\n        FileNotFoundError: If the input file does not exist (input type == str)\n        FileNotFoundError: If one of the input files does not exist (input type == list)\n        ValueError: If the input image format is not compatible (input type == np.ndarray)\n        ValueError: If the input array is not compatible (input type == np.ndarray)\n        ValueError: If the input DataFrame does not contains a 'file_path' column (input type == pd.DataFrame)\n        ValueError: If the input type is not a valid type option\n    Returns:\n        List[str], np.ndarray: predictions or probabilities\n            - If return_proba -&gt; np.ndarray\n            - Else List[str]\n    '''\n# TODO\n# TODO\n# TODO: Make this works with object_detector !!!\n# TODO\n# TODO\nif model.model_type == 'object_detector':\nraise NotImplementedError(\"`predict` is not yet implemented for object detection task\")\n##############################################\n# Retrieve data - PIL format (list)\n##############################################\n# Type 1: absolute path\nif isinstance(data_input, str):\nif not os.path.exists(data_input):\nraise FileNotFoundError(f\"The file {data_input} does not exist\")\nimages = [Image.open(data_input)]\n# Type 2: list of absolute paths\nelif isinstance(data_input, list):\nif not all([os.path.exists(_) for _ in data_input]):\nraise FileNotFoundError(\"At least one of the input path does not exist\")\nimages = [Image.open(_) for _ in data_input]\n# Type 3: numpy array\nelif isinstance(data_input, np.ndarray):\n# If only one image (shape = 3), exapnd a 4th image\nif len(data_input.shape) == 3:\ndata_input = np.expand_dims(data_input, 0)\n# Consider input as image list\nif len(data_input.shape) == 4:\nimages = []\nfor i in range(data_input.shape[0]):\nnp_image = data_input[i]\n# RGB\nif np_image.shape[-1] == 3:\nimages.append(Image.fromarray(np_image, 'RGB'))\nelif np_image.shape[-1] == 4:\nimages.append(Image.fromarray(np_image, 'RGBA'))\nelse:\nraise ValueError(f\"Input image format ({np_image.shape}) is not compatible\")\nelse:\nraise ValueError(f\"Input array format ({type(data_input)}) is not valid\")\n# Type 4: pd.DataFrame\nelif isinstance(data_input, pd.DataFrame):\nif 'file_path' not in data_input.columns:\nraise ValueError(\"The input DataFrame does not contains a 'file_path' column (mandatory)\")\nfile_paths = list(data_input['file_path'].values)\nif not all([os.path.exists(_) for _ in file_paths]):\nraise FileNotFoundError(\"At least one of the input path does not exist\")\nimages = [Image.open(_) for _ in file_paths]\n# No solution\nelse:\nraise ValueError(f\"Input type ({type(data_input)}) is not a valid type option.\")\n##############################################\n# Apply preprocessing\n##############################################\n# Get preprocessor\nif 'preprocess_str' in model_conf.keys():\npreprocess_str = model_conf['preprocess_str']\nelse:\npreprocess_str = \"no_preprocess\"\npreprocessor = preprocess.get_preprocessor(preprocess_str)\n# Preprocess\nimages_preprocessed = preprocessor(images)\n##############################################\n# Save all preprocessed images in a temporary directory\n##############################################\n# We'll create a temporary folder to save preprocessed images\nwith tempfile.TemporaryDirectory(dir=utils.get_data_path()) as tmp_folder:\n# Save images\nimages_path = []\nfor i, img in enumerate(images_preprocessed):\nimg_path = os.path.join(tmp_folder, f\"image_{i}.png\")\nimg.save(img_path, format='PNG')\nimages_path.append(img_path)\n# Get predictions\ndf = pd.DataFrame({'file_path': images_path})\npredictions, probas = model.predict_with_proba(df)\n# Getting out of the context, all temporary data is deleted\n##############################################\n# Return result\n##############################################\nif return_proba:\nreturn probas\nelse:\nreturn model.inverse_transform(predictions)\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.predict_with_proba","title":"<code>predict_with_proba(data_input, model, model_conf)</code>","text":"<p>Gets probabilities predictions of a model on a dataset</p> <p>Parameters:</p> Name Type Description Default <code>data_input</code> <code>str | list&lt;str&gt; | np.ndarray</code> <p>New content to be predicted - str: abs. path to an image - list: list of abs. path to an image - np.ndarray: an already loaded image     Possibility to have several images if 4 dim (i.e (nb_images, width, height, channels)) - pd.DataFrame: Dataframe with a column file_path (abs. paths to images) required <code>model</code> <code>ModelClass</code> <p>Model to use</p> required <code>model_conf</code> <code>dict</code> <p>Model configurations</p> required <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If model is object detection task</p> <code>ValueError</code> <p>If predict does not return an np.ndarray</p> <p>Returns:</p> Type Description <code>Tuple[List[str], List[float]]</code> <p>Union[List[str], List[float]]: predictions, probabilities</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def predict_with_proba(data_input: Union[str, List[str], np.ndarray, pd.DataFrame], model,\nmodel_conf: dict) -&gt; Tuple[List[str], List[float]]:\n'''Gets probabilities predictions of a model on a dataset\n    Args:\n        data_input (str | list&lt;str&gt; | np.ndarray): New content to be predicted\n            - str: abs. path to an image\n            - list&lt;str&gt;: list of abs. path to an image\n            - np.ndarray: an already loaded image\n                Possibility to have several images if 4 dim (i.e (nb_images, width, height, channels))\n            - pd.DataFrame: Dataframe with a column file_path (abs. paths to images)\n        model (ModelClass): Model to use\n        model_conf (dict): Model configurations\n    Raises:\n        NotImplementedError: If model is object detection task\n        ValueError : If predict does not return an np.ndarray\n    Returns:\n        Union[List[str], List[float]]: predictions, probabilities\n    '''\nif model.model_type == 'object_detector':\nraise NotImplementedError(\"`predict_with_proba` is not yet implemented for object detection task\")\n# Get probas\nprobas = predict(data_input, model, model_conf, return_proba=True)\n# Check type\nif type(probas) != np.ndarray:\nraise ValueError(\"Internal error - probas should be an np.ndarray.\")\n# Manage cases with only one element\npredictions = model.get_classes_from_proba(probas)\npredictions = model.inverse_transform(predictions)\nmax_probas = list(probas.max(axis=1))\n# Return\nreturn predictions, max_probas\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.remove_small_classes","title":"<code>remove_small_classes(df, col, min_rows=2)</code>","text":"<p>Deletes the classes with small numbers of elements</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str | int</code> <p>Columns containing the classes</p> required Kwargs <p>min_rows (int): Minimal number of lines in the training set (default: 2)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object min_rows is not positive</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: New dataset</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def remove_small_classes(df: pd.DataFrame, col: Union[str, int], min_rows: int = 2) -&gt; pd.DataFrame:\n'''Deletes the classes with small numbers of elements\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str | int): Columns containing the classes\n    Kwargs:\n        min_rows (int): Minimal number of lines in the training set (default: 2)\n    Raises:\n        ValueError: If the object min_rows is not positive\n    Returns:\n        pd.DataFrame: New dataset\n    '''\nif min_rows &lt; 1:\nraise ValueError(\"The object min_rows must be positive\")\n# Looking for classes with less than min_rows lines\nv_count = df[col].value_counts()\nclasses_to_remove = list(v_count[v_count &lt; min_rows].index.values)\nfor cl in classes_to_remove:\nlogger.warning(f\"/!\\\\ /!\\\\ /!\\\\ Class {cl} has less than {min_rows} lines in the training set.\")\nlogger.warning(\"/!\\\\ /!\\\\ /!\\\\ This class is automatically removed from the dataset.\")\nreturn df[~df[col].isin(classes_to_remove)]\n</code></pre>"},{"location":"reference/template_vision/models_training/utils_models/#template_vision.models_training.utils_models.stratified_split","title":"<code>stratified_split(df, col, test_size=0.25, seed=None)</code>","text":"<p>Splits a DataFrame into train and test sets - Stratified strategy</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>pd.DataFrame</code> <p>Dataframe containing the data</p> required <code>col</code> <code>str or int</code> <p>column on which to do the stratified split</p> required Kwargs <p>test_size (float): Proportion representing the size of the expected test set seed (int): Random seed</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object test_size is not between 0 and 1</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>pd.DataFrame</code> <p>Train dataframe</p> <code>DataFrame</code> <code>pd.DataFrame</code> <p>Test dataframe</p> Source code in <code>template_vision/models_training/utils_models.py</code> <pre><code>def stratified_split(df: pd.DataFrame, col: Union[str, int], test_size: float = 0.25,\nseed: Union[int, None] = None) -&gt; Tuple[pd.DataFrame, pd.DataFrame]:\n'''Splits a DataFrame into train and test sets - Stratified strategy\n    Args:\n        df (pd.DataFrame): Dataframe containing the data\n        col (str or int): column on which to do the stratified split\n    Kwargs:\n        test_size (float): Proportion representing the size of the expected test set\n        seed (int): Random seed\n    Raises:\n        ValueError: If the object test_size is not between 0 and 1\n    Returns:\n        DataFrame: Train dataframe\n        DataFrame: Test dataframe\n    '''\nif not 0 &lt;= test_size &lt;= 1:\nraise ValueError('The object test_size must be between 0 and 1')\n# Stratified split\nlogger.info(\"Stratified split\")\ndf = remove_small_classes(df, col, min_rows=math.ceil(1 / test_size))  # minimum lines number per category to split\ndf_train, df_test = train_test_split(df, stratify=df[col], test_size=test_size, random_state=seed)\n# Display\ndisplay_train_test_shape(df_train, df_test, df_shape=df.shape[0])\n# Return\nreturn df_train, df_test\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/","title":"Classifiers","text":""},{"location":"reference/template_vision/models_training/classifiers/model_classifier/","title":"Model classifier","text":""},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin","title":"<code>ModelClassifierMixin</code>","text":"<p>Parent class (Mixin) for classifier models</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>class ModelClassifierMixin:\n'''Parent class (Mixin) for classifier models'''\n# Not implemented :\n# -&gt; predict : To be implementd by the parent class when using this mixin\ndef __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n'''Initialization of the class\n        Kwargs:\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOWlevel_save + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        '''\nsuper().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type\nself.model_type = 'classifier'\n# Classes list to use (set on fit)\nself.list_classes = None\nself.dict_classes = None\n# Other options\nself.level_save = level_save\n@utils.trained_needed\ndef predict_with_proba(self, df_test: pd.DataFrame) -&gt; Tuple[np.ndarray, np.ndarray]:\n'''Predictions on test set with probabilities\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        Returns:\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n            (np.ndarray): Array, shape = [n_samples, n_classes]\n        '''\n# Process\npredicted_proba = self.predict(df_test, return_proba=True)\npredicted_class = self.get_classes_from_proba(predicted_proba)\nreturn predicted_class, predicted_proba\n@utils.trained_needed\ndef get_predict_position(self, df_test: pd.DataFrame, y_true) -&gt; np.ndarray:\n'''Gets the order of predictions of y_true.\n        Positions start at 1 (not 0)\n        Args:\n            df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n            y_true (?): Array-like, shape = [n_samples, n_features] - Classes\n        Returns:\n            (?): Array, shape = [n_samples]\n        '''\n# Process\n# Cast as pd.Series\ny_true = pd.Series(y_true)\n# Get predicted probabilities\npredicted_proba = self.predict(df_test, return_proba=True)\n# Get position\norder = predicted_proba.argsort()\nranks = len(self.list_classes) - order.argsort()\ndf_probas = pd.DataFrame(ranks, columns=self.list_classes)\npredict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\nreturn predict_positions\ndef get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n'''Gets the classes from probabilities\n        Args:\n            predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n        Returns:\n            predicted_class (np.ndarray): Shape = [n_samples]\n        '''\npredicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\nreturn predicted_class\ndef get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n'''Gets the Top n predictions from probabilities\n        Args:\n            predicted_proba (np.ndarray): Predicted probabilities = [n_samples, n_classes]\n        kwargs:\n            n (int): Number of classes to return\n        Raises:\n            ValueError: If the number of classes to return is greater than the number of classes of the model\n        Returns:\n            top_n (list): Top n predicted classes\n            top_n_proba (list): Top n probabilities (corresponding to the top_n list of classes)\n        '''\nif self.list_classes is not None and n &gt; len(self.list_classes):\nraise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n# Process\nidx = predicted_proba.argsort()[:, -n:][:, ::-1]\ntop_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\ntop_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))\nreturn top_n, top_n_proba\ndef inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Gets a list of classes from the predictions (mainly useful for multi-labels)\n        Args:\n            y (list | np.ndarray): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n        Returns:\n            (?): List of classes\n        '''\nreturn list(y) if type(y) == np.ndarray else y\ndef get_and_save_metrics(self, y_true, y_pred, list_files_x: Union[list, None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n        Args:\n            y_true (?): Array-like [n_samples, 1] if classifier\n                # If classifier, class of each image\n                # If object detector, list of list of bboxes per image\n                    bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n            y_pred (?): Array-like [n_samples, 1] if classifier\n                # If classifier, class of each image\n                # If object detector, list of list of bboxes per image\n                    bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n        Kwargs:\n            list_files_x (list): Input images file paths\n            type_data (str): Type of dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The d\n        '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Save a predictionn file if wanted\nif self.level_save == 'HIGH':\n# Inverse transform\ny_true_df = list(self.inverse_transform(y_true))\ny_pred_df = list(self.inverse_transform(y_pred))\n# Concat in a dataframe\ndf = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n# Ajout colonne file_path si possible\nif list_files_x is not None:\ndf['file_path'] = list_files_x\n# Add a matched column\ndf['matched'] = (df['y_true'] == df['y_pred']).astype(int)\n#  Save predictions\nfile_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\ndf.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.] * len(self.list_classes) + [1.0]\nfor i, cl in enumerate(self.list_classes):\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# Global Statistics\nself.logger.info('-- * * * * * * * * * * * * * * --')\nself.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\nself.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\nself.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\nself.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\nself.logger.info('--------------------------------')\n# Metrics file\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Add metrics\nlabels = self.list_classes\n# Plot confusion matrices if level_save &gt; LOW\nif self.level_save in ['MEDIUM', 'HIGH']:\nif len(labels) &gt; 50:\nself.logger.warning(\nf\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n\"Heavy chances of slowness/display bugs/crashes...\\n\"\n\"SKIP the plots\"\n)\nelse:\n# Global statistics\nc_mat = confusion_matrix(y_true, y_pred, labels=labels)\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)\n# Get statistics per class\nfor label in labels:\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Save .csv\nfile_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\ndf_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n# Save accuracy\nacc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\nwith open(acc_path, 'w'):\npass\nreturn df_stats\ndef get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on mono-label predictions\n        Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n        Args:\n            y_true (?): Array-like, shape = [n_samples,]\n            y_pred (?): Array-like, shape = [n_samples,]\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.] * len(self.list_classes) + [1.0]\nfor i, cl in enumerate(self.list_classes):\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# DataFrame metrics\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Get statistics per class\nlabels = self.list_classes\nfor label in labels:\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Return dataframe\nreturn df_stats\ndef _update_info_from_c_mat(self, c_mat: np.ndarray, label: str, log_info: bool = True) -&gt; dict:\n'''Updates a dataframe for the method get_and_save_metrics, given a confusion matrix\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            label (str): Label to use\n        Kwargs:\n            log_info (bool): If the statistics must be logged\n        Returns:\n            dict: Dictionary with the information for the update of the dataframe\n        '''\n# Extract all needed info from c_mat\ntrue_negative = c_mat[0][0]\ntrue_positive = c_mat[1][1]\nfalse_negative = c_mat[1][0]\nfalse_positive = c_mat[0][1]\ncondition_positive = false_negative + true_positive\ncondition_negative = false_positive + true_negative\npredicted_positive = false_positive + true_positive\npredicted_negative = false_negative + true_negative\ntrues_cat = true_negative + true_positive\nfalses_cat = false_negative + false_positive\naccuracy = (true_negative + true_positive) / (true_negative + true_positive + false_negative + false_positive)\nprecision = 0 if predicted_positive == 0 else true_positive / predicted_positive\nrecall = 0 if condition_positive == 0 else true_positive / condition_positive\nf1 = 0 if precision + recall == 0 else 2 * precision * recall / (precision + recall)\n# Display some info\nif log_info:\nself.logger.info(\nf\"F1-score: {round(f1, 5)}  \\t Precision: {round(100 * precision, 2)}% \\t\"\nf\"Recall: {round(100 * recall, 2)}% \\t Trues: {trues_cat} \\t Falses: {falses_cat} \\t\\t --- {label} \"\n)\n# Return result\nreturn {\n'Label': f'{label}',\n'F1-Score': f1,\n'Accuracy': accuracy,\n'Precision': precision,\n'Recall': recall,\n'Trues': trues_cat,\n'Falses': falses_cat,\n'True positive': true_positive,\n'True negative': true_negative,\n'False positive': false_positive,\n'False negative': false_negative,\n'Condition positive': condition_positive,\n'Condition negative': condition_negative,\n'Predicted positive': predicted_positive,\n'Predicted negative': predicted_negative,\n}\ndef _plot_confusion_matrix(self, c_mat: np.ndarray, labels: list, type_data: str = '',\nnormalized: bool = False, subdir: Union[str, None] = None) -&gt; None:\n'''Plots a confusion matrix\n        Args:\n            c_mat (np.ndarray): Confusion matrix\n            labels (list): Labels to plot\n        Kwargs:\n            type_data (str): Type of dataset (validation, test, ...)\n            normalized (bool): If the confusion matrix should be normalized\n            subdir (str): Sub-directory for writing the plot\n        '''\n# Get title\nif normalized:\ntitle = f\"Normalized confusion matrix{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\nelse:\ntitle = f\"Confusion matrix, without normalization{' - ' + type_data if len(type_data) &gt; 0 else ''}\"\n# Init. plot\nwidth = round(10 + 0.5 * len(c_mat))\nheight = round(4 / 5 * width)\nfig, ax = plt.subplots(figsize=(width, height))\n# Plot\nif normalized:\nc_mat = c_mat.astype('float') / c_mat.sum(axis=1)[:, np.newaxis]\nsns.heatmap(c_mat, annot=True, fmt=\".2f\", cmap=plt.cm.Blues, ax=ax)\nelse:\nsns.heatmap(c_mat, annot=True, fmt=\"d\", cmap=plt.cm.Blues, ax=ax)\n# labels, title and ticks\nax.set_xlabel('Predicted classes', fontsize=height * 2)\nax.set_ylabel('Real classes', fontsize=height * 2)\nax.set_title(title, fontsize=width * 2)\nax.xaxis.set_ticklabels(labels)\nax.yaxis.set_ticklabels(labels)\nplt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right')\nplt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right')\nplt.tight_layout()\n# Save\nplots_path = os.path.join(self.model_dir, 'plots')\nif subdir is not None:  # Add subdir\nplots_path = os.path.join(plots_path, subdir)\nfile_name = f\"{type_data + '_' if len(type_data) &gt; 0 else ''}confusion_matrix{'_normalized' if normalized else ''}.png\"\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\nplt.savefig(os.path.join(plots_path, file_name))\n# Close figures\nplt.close('all')\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['list_classes'] = self.list_classes\njson_data['dict_classes'] = self.dict_classes\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.__init__","title":"<code>__init__(level_save='HIGH', **kwargs)</code>","text":"<p>Initialization of the class</p> Kwargs <p>level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOWlevel_save + hdf5 + pkl + plots     HIGH: MEDIUM + predictions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n'''Initialization of the class\n    Kwargs:\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOWlevel_save + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n    '''\nsuper().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type\nself.model_type = 'classifier'\n# Classes list to use (set on fit)\nself.list_classes = None\nself.dict_classes = None\n# Other options\nself.level_save = level_save\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, list_files_x=None, type_data='')</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like [n_samples, 1] if classifier</p> required <code>y_pred</code> <code>?</code> <p>Array-like [n_samples, 1] if classifier</p> required Kwargs <p>list_files_x (list): Input images file paths type_data (str): Type of dataset (validation, test, ...)</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The d</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def get_and_save_metrics(self, y_true, y_pred, list_files_x: Union[list, None] = None,\ntype_data: str = '') -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n    Args:\n        y_true (?): Array-like [n_samples, 1] if classifier\n            # If classifier, class of each image\n            # If object detector, list of list of bboxes per image\n                bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n        y_pred (?): Array-like [n_samples, 1] if classifier\n            # If classifier, class of each image\n            # If object detector, list of list of bboxes per image\n                bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n    Kwargs:\n        list_files_x (list): Input images file paths\n        type_data (str): Type of dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The d\n    '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Save a predictionn file if wanted\nif self.level_save == 'HIGH':\n# Inverse transform\ny_true_df = list(self.inverse_transform(y_true))\ny_pred_df = list(self.inverse_transform(y_pred))\n# Concat in a dataframe\ndf = pd.DataFrame({'y_true': y_true_df, 'y_pred': y_pred_df})\n# Ajout colonne file_path si possible\nif list_files_x is not None:\ndf['file_path'] = list_files_x\n# Add a matched column\ndf['matched'] = (df['y_true'] == df['y_pred']).astype(int)\n#  Save predictions\nfile_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\ndf.sort_values('matched', ascending=True).to_csv(file_path, sep=';', index=None, encoding='utf-8')\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.] * len(self.list_classes) + [1.0]\nfor i, cl in enumerate(self.list_classes):\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# Global Statistics\nself.logger.info('-- * * * * * * * * * * * * * * --')\nself.logger.info(f\"Statistics f1-score{' ' + type_data if len(type_data) &gt; 0 else ''}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"Total accuracy : {round(acc_tot * 100, 2)}% \\t Trues: {trues} \\t Falses: {falses}\")\nself.logger.info(f\"F1-score (weighted) : {round(f1_weighted, 5)}\")\nself.logger.info(f\"Precision (weighted) : {round(precision_weighted, 5)}\")\nself.logger.info(f\"Recall (weighted) : {round(recall_weighted, 5)}\")\nself.logger.info('--------------------------------')\n# Metrics file\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Add metrics\nlabels = self.list_classes\n# Plot confusion matrices if level_save &gt; LOW\nif self.level_save in ['MEDIUM', 'HIGH']:\nif len(labels) &gt; 50:\nself.logger.warning(\nf\"Warning, there are {len(labels)} categories to plot in the confusion matrix.\\n\"\n\"Heavy chances of slowness/display bugs/crashes...\\n\"\n\"SKIP the plots\"\n)\nelse:\n# Global statistics\nc_mat = confusion_matrix(y_true, y_pred, labels=labels)\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=False)\nself._plot_confusion_matrix(c_mat, labels, type_data=type_data, normalized=True)\n# Get statistics per class\nfor label in labels:\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Save .csv\nfile_path = os.path.join(self.model_dir, f\"f1{'_' + type_data if len(type_data) &gt; 0 else ''}@{f1_weighted}.csv\")\ndf_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n# Save accuracy\nacc_path = os.path.join(self.model_dir, f\"acc{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(acc_tot, 5)}\")\nwith open(acc_path, 'w'):\npass\nreturn df_stats\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics--if-classifier-class-of-each-image","title":"If classifier, class of each image","text":""},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics--if-object-detector-list-of-list-of-bboxes-per-image","title":"If object detector, list of list of bboxes per image","text":"<pre><code>bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics--if-classifier-class-of-each-image","title":"If classifier, class of each image","text":""},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_and_save_metrics--if-object-detector-list-of-list-of-bboxes-per-image","title":"If object detector, list of list of bboxes per image","text":"<pre><code>bbox format : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_classes_from_proba","title":"<code>get_classes_from_proba(predicted_proba)</code>","text":"<p>Gets the classes from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>np.ndarray</code> <p>The probabilities predicted by the model, shape = [n_samples, n_classes]</p> required <p>Returns:</p> Name Type Description <code>predicted_class</code> <code>np.ndarray</code> <p>Shape = [n_samples]</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def get_classes_from_proba(self, predicted_proba: np.ndarray) -&gt; np.ndarray:\n'''Gets the classes from probabilities\n    Args:\n        predicted_proba (np.ndarray): The probabilities predicted by the model, shape = [n_samples, n_classes]\n    Returns:\n        predicted_class (np.ndarray): Shape = [n_samples]\n    '''\npredicted_class = np.vectorize(lambda x: self.dict_classes[x])(predicted_proba.argmax(axis=-1))\nreturn predicted_class\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_metrics_simple_monolabel","title":"<code>get_metrics_simple_monolabel(y_true, y_pred)</code>","text":"<p>Gets metrics on mono-label predictions Same as the method get_and_save_metrics but without all the fluff (save, etc.)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <code>y_pred</code> <code>?</code> <p>Array-like, shape = [n_samples,]</p> required <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def get_metrics_simple_monolabel(self, y_true, y_pred) -&gt; pd.DataFrame:\n'''Gets metrics on mono-label predictions\n    Same as the method get_and_save_metrics but without all the fluff (save, etc.)\n    Args:\n        y_true (?): Array-like, shape = [n_samples,]\n        y_pred (?): Array-like, shape = [n_samples,]\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\n# Cast to np.array\ny_true = np.array(y_true)\ny_pred = np.array(y_pred)\n# Check shapes\nif len(y_true.shape) == 2 and y_true.shape[1] == 1:\ny_true = np.ravel(y_true)\nif len(y_pred.shape) == 2 and y_pred.shape[1] == 1:\ny_pred = np.ravel(y_pred)\n# Gets global f1 score / acc_tot / trues / falses / precision / recall / support\nf1_weighted = f1_score(y_true, y_pred, average='weighted', zero_division=0)\ntrues = np.sum(y_true == y_pred)\nfalses = np.sum(y_true != y_pred)\nacc_tot = accuracy_score(y_true, y_pred)\nprecision_weighted = precision_score(y_true, y_pred, average='weighted', zero_division=0)\nrecall_weighted = recall_score(y_true, y_pred, average='weighted', zero_division=0)\nlabels_tmp, counts_tmp = np.unique(y_true, return_counts=True)\nsupport = [0.] * len(self.list_classes) + [1.0]\nfor i, cl in enumerate(self.list_classes):\nif cl in labels_tmp:\nidx_tmp = list(labels_tmp).index(cl)\nsupport[i] = counts_tmp[idx_tmp] / y_pred.shape[0]\n# DataFrame metrics\ndf_stats = pd.DataFrame(columns=['Label', 'F1-Score', 'Accuracy',\n'Precision', 'Recall', 'Trues', 'Falses',\n'True positive', 'True negative',\n'False positive', 'False negative',\n'Condition positive', 'Condition negative',\n'Predicted positive', 'Predicted negative'])\n# Get statistics per class\nlabels = self.list_classes\nfor label in labels:\nlabel_str = str(label)  # Fix : If label is an int, can cause some problems (e.g. only zeroes in the confusion matrix)\nnone_class = 'None' if label_str != 'None' else 'others'  # Check that the class is not already 'None'\ny_true_tmp = [label_str if _ == label else none_class for _ in y_true]\ny_pred_tmp = [label_str if _ == label else none_class for _ in y_pred]\nc_mat_tmp = confusion_matrix(y_true_tmp, y_pred_tmp, labels=[none_class, label_str])\ndf_stats = df_stats.append(self._update_info_from_c_mat(c_mat_tmp, label, log_info=False), ignore_index=True)\n# Add global statistics\nglobal_stats = {\n'Label': 'All',\n'F1-Score': f1_weighted,\n'Accuracy': acc_tot,\n'Precision': precision_weighted,\n'Recall': recall_weighted,\n'Trues': trues,\n'Falses': falses,\n'True positive': None,\n'True negative': None,\n'False positive': None,\n'False negative': None,\n'Condition positive': None,\n'Condition negative': None,\n'Predicted positive': None,\n'Predicted negative': None,\n}\ndf_stats = df_stats.append(global_stats, ignore_index=True)\n# Add support\ndf_stats['Support'] = support\n# Return dataframe\nreturn df_stats\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_predict_position","title":"<code>get_predict_position(df_test, y_true)</code>","text":"<p>Gets the order of predictions of y_true. Positions start at 1 (not 0)</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>pd.DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required <code>y_true</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features] - Classes</p> required <p>Returns:</p> Type Description <code>?</code> <p>Array, shape = [n_samples]</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>@utils.trained_needed\ndef get_predict_position(self, df_test: pd.DataFrame, y_true) -&gt; np.ndarray:\n'''Gets the order of predictions of y_true.\n    Positions start at 1 (not 0)\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n        y_true (?): Array-like, shape = [n_samples, n_features] - Classes\n    Returns:\n        (?): Array, shape = [n_samples]\n    '''\n# Process\n# Cast as pd.Series\ny_true = pd.Series(y_true)\n# Get predicted probabilities\npredicted_proba = self.predict(df_test, return_proba=True)\n# Get position\norder = predicted_proba.argsort()\nranks = len(self.list_classes) - order.argsort()\ndf_probas = pd.DataFrame(ranks, columns=self.list_classes)\npredict_positions = np.array([df_probas.loc[i, cl] if cl in df_probas.columns else -1 for i, cl in enumerate(y_true)])\nreturn predict_positions\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.get_top_n_from_proba","title":"<code>get_top_n_from_proba(predicted_proba, n=5)</code>","text":"<p>Gets the Top n predictions from probabilities</p> <p>Parameters:</p> Name Type Description Default <code>predicted_proba</code> <code>np.ndarray</code> <p>Predicted probabilities = [n_samples, n_classes]</p> required kwargs <p>n (int): Number of classes to return</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of classes to return is greater than the number of classes of the model</p> <p>Returns:</p> Name Type Description <code>top_n</code> <code>list</code> <p>Top n predicted classes</p> <code>top_n_proba</code> <code>list</code> <p>Top n probabilities (corresponding to the top_n list of classes)</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def get_top_n_from_proba(self, predicted_proba: np.ndarray, n: int = 5) -&gt; Tuple[list, list]:\n'''Gets the Top n predictions from probabilities\n    Args:\n        predicted_proba (np.ndarray): Predicted probabilities = [n_samples, n_classes]\n    kwargs:\n        n (int): Number of classes to return\n    Raises:\n        ValueError: If the number of classes to return is greater than the number of classes of the model\n    Returns:\n        top_n (list): Top n predicted classes\n        top_n_proba (list): Top n probabilities (corresponding to the top_n list of classes)\n    '''\nif self.list_classes is not None and n &gt; len(self.list_classes):\nraise ValueError(\"The number of classes to return is greater than the number of classes of the model\")\n# Process\nidx = predicted_proba.argsort()[:, -n:][:, ::-1]\ntop_n_proba = list(np.take_along_axis(predicted_proba, idx, axis=1))\ntop_n = list(np.vectorize(lambda x: self.dict_classes[x])(idx))\nreturn top_n, top_n_proba\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets a list of classes from the predictions (mainly useful for multi-labels)</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>list | np.ndarray</code> <p>Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s</p> required <p>Returns:</p> Type Description <code>?</code> <p>List of classes</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def inverse_transform(self, y: Union[list, np.ndarray]) -&gt; Union[list, tuple]:\n'''Gets a list of classes from the predictions (mainly useful for multi-labels)\n    Args:\n        y (list | np.ndarray): Array-like, shape = [n_samples, n_classes], arrays of 0s and 1s\n    Returns:\n        (?): List of classes\n    '''\nreturn list(y) if type(y) == np.ndarray else y\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.predict_with_proba","title":"<code>predict_with_proba(df_test)</code>","text":"<p>Predictions on test set with probabilities</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>pd.DataFrame</code> <p>DataFrame to be predicted, with column file_path</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> <code>np.ndarray</code> <p>Array, shape = [n_samples, n_classes]</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>@utils.trained_needed\ndef predict_with_proba(self, df_test: pd.DataFrame) -&gt; Tuple[np.ndarray, np.ndarray]:\n'''Predictions on test set with probabilities\n    Args:\n        df_test (pd.DataFrame): DataFrame to be predicted, with column file_path\n    Returns:\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n        (np.ndarray): Array, shape = [n_samples, n_classes]\n    '''\n# Process\npredicted_proba = self.predict(df_test, return_proba=True)\npredicted_class = self.get_classes_from_proba(predicted_proba)\nreturn predicted_class, predicted_proba\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_classifier/#template_vision.models_training.classifiers.model_classifier.ModelClassifierMixin.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/classifiers/model_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['list_classes'] = self.list_classes\njson_data['dict_classes'] = self.dict_classes\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_cnn_classifier/","title":"Model cnn classifier","text":""},{"location":"reference/template_vision/models_training/classifiers/model_cnn_classifier/#template_vision.models_training.classifiers.model_cnn_classifier.ModelCnnClassifier","title":"<code>ModelCnnClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelKeras</code></p> <p>CNN model for classification</p> Source code in <code>template_vision/models_training/classifiers/model_cnn_classifier.py</code> <pre><code>class ModelCnnClassifier(ModelClassifierMixin, ModelKeras):\n'''CNN model for classification'''\n_default_name = 'model_cnn_classifier'\ndef __init__(self, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)'''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\ndef _get_model(self) -&gt; Model:\n'''Gets a model structure - returns the instance model instead if already defined\n        Returns:\n            (Model): a Keras model\n        '''\n# Return model if already set\nif self.model is not None:\nreturn self.model\n# Get input/output dimensions\ninput_shape = (self.width, self.height, self.depth)\nnum_classes = len(self.list_classes)\n# Process\ninput_layer = Input(shape=input_shape)\n# Feature extraction\nx = Conv2D(16, 3, padding='same', activation=None, kernel_initializer=\"he_uniform\")(input_layer)\nx = BatchNormalization(momentum=0.99)(x)\nx = ELU(alpha=1.0)(x)\nx = AveragePooling2D(2, strides=2, padding='valid')(x)\nx = Conv2D(32, 3, padding='same', activation=None, kernel_initializer=\"he_uniform\")(x)\nx = BatchNormalization(momentum=0.99)(x)\nx = ELU(alpha=1.0)(x)\nx = AveragePooling2D(2, strides=2, padding='valid')(x)\nx = Conv2D(48, 3, padding='same', activation=None, kernel_initializer=\"he_uniform\")(x)\nx = BatchNormalization(momentum=0.99)(x)\nx = ELU(alpha=1.0)(x)\nx = AveragePooling2D(2, strides=2, padding='valid')(x)\nx = Conv2D(32, 3, padding='same', activation=None, kernel_initializer=\"he_uniform\")(x)\nx = BatchNormalization(momentum=0.99)(x)\nx = ELU(alpha=1.0)(x)\nx = AveragePooling2D(2, strides=2, padding='valid')(x)\n# Flatten\nx = Flatten()(x)\n# Classification\nx = Dense(64, activation=None, kernel_initializer=\"he_uniform\")(x)\nx = BatchNormalization(momentum=0.99)(x)\nx = ELU(alpha=1.0)(x)\nx = Dropout(0.2)(x)\nx = Dense(128, activation=None, kernel_initializer=\"he_uniform\")(x)\nx = BatchNormalization(momentum=0.99)(x)\nx = ELU(alpha=1.0)(x)\nx = Dropout(0.2)(x)\nx = Dense(128, activation=None, kernel_initializer=\"he_uniform\")(x)\nx = BatchNormalization(momentum=0.99)(x)\nx = ELU(alpha=1.0)(x)\nx = Dropout(0.2)(x)\n# Last layer\nactivation = 'softmax'\nout = Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform')(x)\n# Set model\nmodel = Model(inputs=input_layer, outputs=[out])\n# Set optimizer\nlr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.001\ndecay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\nself.logger.info(f\"Learning rate: {lr}\")\nself.logger.info(f\"Decay: {decay}\")\noptimizer = Adam(lr=lr, decay=decay)\n# Set loss &amp; metrics\nloss = 'categorical_crossentropy'\nmetrics: List[Union[str, Callable]] = ['accuracy']\n# Compile model\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nmodel.summary()\n# Try to save model as png if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._save_model_png(model)\n# Return\nreturn model\ndef _get_preprocess_input(self) -&gt; Union[Callable, None]:\n'''Gets the preprocessing to be used before feeding images to the NN\n        Returns:\n            (Callable | None): Preprocessing function\n        '''\nreturn preprocess_input\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            preprocess_input_path (str): Path to preprocess input file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If preprocess_input_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object preprocess_input_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\npreprocess_input_path = kwargs.get('preprocess_input_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif preprocess_input_path is None:\nraise ValueError(\"The argument preprocess_input_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(preprocess_input_path):\nraise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'width', 'height', 'depth', 'color_mode', 'in_memory',\n'data_augmentation_params', 'nb_train_generator_images_to_save',\n'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Reload preprocess_input\nwith open(preprocess_input_path, 'rb') as f:\nself.preprocess_input = pickle.load(f)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_cnn_classifier/#template_vision.models_training.classifiers.model_cnn_classifier.ModelCnnClassifier.__init__","title":"<code>__init__(**kwargs)</code>","text":"<p>Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)</p> Source code in <code>template_vision/models_training/classifiers/model_cnn_classifier.py</code> <pre><code>def __init__(self, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)'''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_cnn_classifier/#template_vision.models_training.classifiers.model_cnn_classifier.ModelCnnClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file preprocess_input_path (str): Path to preprocess input file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hdf5_path is None</p> <code>ValueError</code> <p>If preprocess_input_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_input_path is not an existing file</p> Source code in <code>template_vision/models_training/classifiers/model_cnn_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        preprocess_input_path (str): Path to preprocess input file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If preprocess_input_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object preprocess_input_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\npreprocess_input_path = kwargs.get('preprocess_input_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif preprocess_input_path is None:\nraise ValueError(\"The argument preprocess_input_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(preprocess_input_path):\nraise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'width', 'height', 'depth', 'color_mode', 'in_memory',\n'data_augmentation_params', 'nb_train_generator_images_to_save',\n'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Reload preprocess_input\nwith open(preprocess_input_path, 'rb') as f:\nself.preprocess_input = pickle.load(f)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_cnn_classifier/#template_vision.models_training.classifiers.model_cnn_classifier.ModelCnnClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/classifiers/model_cnn_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_transfer_learning_classifier/","title":"Model transfer learning classifier","text":""},{"location":"reference/template_vision/models_training/classifiers/model_transfer_learning_classifier/#template_vision.models_training.classifiers.model_transfer_learning_classifier.ModelTransferLearningClassifier","title":"<code>ModelTransferLearningClassifier</code>","text":"<p>         Bases: <code>ModelClassifierMixin</code>, <code>ModelKeras</code></p> <p>CNN model with transfer learning for classification</p> Source code in <code>template_vision/models_training/classifiers/model_transfer_learning_classifier.py</code> <pre><code>class ModelTransferLearningClassifier(ModelClassifierMixin, ModelKeras):\n'''CNN model with transfer learning for classification'''\n_default_name = 'model_transfer_learning_classifier'\ndef __init__(self, with_fine_tune: bool = True, second_epochs: int = 10, second_lr: float = 1e-5, second_patience: int = 5, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)\n        Kwargs:\n            with_fine_tune (bool): If a fine-tuning step should be performed after first training\n            second_epochs (int): Number of epochs for the fine-tuning step\n            second_lr (float): Learning rate for the fine-tuning step\n            second_patience (int): Patience for the fine-tuning step\n        '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Params\nself.with_fine_tune = with_fine_tune\nself.second_epochs = second_epochs\nself.second_lr = second_lr\nself.second_patience = second_patience\ndef _fit_classifier(self, df_train: pd.DataFrame, df_valid: pd.DataFrame = None, with_shuffle: bool = True, **kwargs) -&gt; dict:\n'''Fits the model - overrides parent function\n        Args:\n            df_train (pd.DataFrame): Train dataset\n                Must contain file_path &amp; file_class columns\n        Kwargs:\n            df_valid (pd.DataFrame): Validation dataset\n                Must contain file_path &amp; file_class columns\n            with_shuffle (boolean): If the train dataset must be shuffled\n                This should be used if the input dataset is not shuffled &amp; no validation set as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Returns:\n            dict: fit arguments\n        '''\n# First fit\nself.logger.info(\"Transfer Learning - Premier entrainement\")\nfit_arguments = super()._fit_classifier(df_train, df_valid=df_valid, with_shuffle=with_shuffle, **kwargs)\n# Fine tune if wanted\nif self.with_fine_tune and self.second_epochs &gt; 0:\n# Unfreeze all layers\nfor layer in self.model.layers:  # type: ignore\nlayer.trainable = True\n# /!\\ Recompile, otherwise unfreeze is not taken into account ! /!\\\n# Cf. https://keras.io/guides/transfer_learning/#fine-tuning\nself._compile_model(self.model, lr=self.second_lr)  # Use new LR !\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nself.model.summary()  # type: ignore\n# Get new callbacks\nnew_callbacks = self._get_second_callbacks()\n# Second fit\nself.logger.info(\"Transfer Learning - Fine-tuning\")\nfit_history = self.model.fit(  # type: ignore\nepochs=self.second_epochs,\ncallbacks=new_callbacks,\nverbose=1,\nworkers=8,  # TODO : Check if this is ok if there are less CPUs\n**fit_arguments,\n)\n# Print accuracy &amp; loss if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\n# Rename first fit plots dir\noriginal_plots_path = os.path.join(self.model_dir, 'plots')\nrenamed_plots_path = os.path.join(self.model_dir, 'plots_initial_fit')\nshutil.move(original_plots_path, renamed_plots_path)\n# Plot new fit graphs\nself._plot_metrics_and_loss(fit_history)\n# Reload best model\nself.model = load_model_keras(\nos.path.join(self.model_dir, 'best.hdf5'),\ncustom_objects=self.custom_objects\n)\nreturn fit_arguments\ndef _get_model(self) -&gt; Model:\n'''Gets a model structure - returns the instance model instead if already defined\n        Returns:\n            (Model): a Keras model\n        '''\n# Return model if already set\nif self.model is not None:\nreturn self.model\n# The base model will be loaded by keras's internal functions\n# Keras uses the `get_file` function to load all files from a cache directory (or from the internet)\n# Per default, all keras's application try to load models files from the keras's cache directory (.keras)\n# However, these application do not have a parameter to change the default directory, but we want all data\n# inside the project's data directory (especially as we do not always have access to the internet).\n# To do so, we change keras's internal function `get_file` to use a directory inside our package as the cache dir.\n# This allows us to predownload a model from anysource.\n# IMPORTANT : we need to reset the `get_file` function at the end of this function\n# Monkey patching : https://stackoverflow.com/questions/5626193/what-is-monkey-patching\ncache_dir = os.path.join(utils.get_data_path(), 'cache_keras')\nif not os.path.exists(cache_dir):\nos.makedirs(cache_dir)\nold_get_file = data_utils.get_file\ndata_utils.get_file = partial(data_utils.get_file, cache_dir=cache_dir)\n# Check if the base model exists, otherwise try to download it\n# VGG 16\n# base_model_name = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\n# base_model_path = os.path.join(utils.get_data_path(), 'cache_keras', 'models', base_model_name)\n# base_model_backup_urls = [\n# 'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n# ]\n# EfficientNetB6\nbase_model_name = 'efficientnetb6_notop.h5'\nbase_model_path = os.path.join(utils.get_data_path(), 'cache_keras', 'models', base_model_name)\nbase_model_backup_urls = [\n'https://storage.googleapis.com/keras-applications/efficientnetb6_notop.h5',\n]\n# Check model presence\nif not os.path.exists(base_model_path):\ntry:\nutils.download_url(base_model_backup_urls, base_model_path)\nexcept Exception:\n# If we can't download it, we let the function crash alone\nself.logger.warning(\"Can't find / download the base model for transfer learning application.\")\n# Get input/output dimensions\ninput_shape = (self.width, self.height, self.depth)\nnum_classes = len(self.list_classes)\n# Process\ninput_layer = Input(shape=input_shape)\n# Example VGG16 - to be used with tensorflow.keras.applications.vgg16.preprocess_input - cf _get_preprocess_input\n# We must use training=False to use the batch norm layers in inference mode\n# (cf. https://keras.io/guides/transfer_learning/)\n# base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n# base_model.trainable = False  # We disable the first layers\n# x = base_model(input_layer, training=False)\n# x = Flatten()(x)\n# Example EfficientNetB6 - to be used with tensorflow.keras.applications.efficientnet.preprocess_input - cf _get_preprocess_input\n# We must use training=False to use the batch norm layers in inference mode\n# (cf. https://keras.io/guides/transfer_learning/)\nbase_model = EfficientNetB6(weights='imagenet', include_top=False, input_shape=input_shape)\nbase_model.trainable = False  # We disable the first layers\nx = base_model(input_layer, training=False)\nx = GlobalAveragePooling2D()(x)\n# Last layer\nactivation = 'softmax'\nout = Dense(num_classes, activation=activation, kernel_initializer='glorot_uniform')(x)\n# Set model\nmodel = Model(inputs=input_layer, outputs=[out])\n# Get lr &amp; compile\nlr = self.keras_params['learning_rate'] if 'learning_rate' in self.keras_params.keys() else 0.001\nself._compile_model(model, lr=lr)\n# Display model\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nmodel.summary()\n# Try to save model as png if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._save_model_png(model)\n# We reset the `get_file` function (cf. explanations)\ndata_utils.get_file = old_get_file\n# Return\nreturn model\ndef _compile_model(self, model: Model, lr: float) -&gt; None:\n'''Compiles the model. This is usually done in _get_model, but adding a function here\n        helps to simplify the fine-tuning step.\n        Args:\n            model (Model): Model to be compiled\n            lr (float): Learning rate to be used\n        '''\n# Set optimizer\ndecay = self.keras_params['decay'] if 'decay' in self.keras_params.keys() else 0.0\nself.logger.info(f\"Learning rate: {lr}\")\nself.logger.info(f\"Decay: {decay}\")\noptimizer = Adam(lr=lr, decay=decay)\n# Set loss &amp; metrics\nloss = 'categorical_crossentropy'\nmetrics: List[Union[str, Callable]] = ['accuracy']\n# Compile model\nmodel.compile(optimizer=optimizer, loss=loss, metrics=metrics)\ndef _get_preprocess_input(self) -&gt; Union[Callable, None]:\n'''Gets the preprocessing to be used before feeding images to the NN\n        Returns:\n            (Callable | None): Preprocessing function\n        '''\n# Preprocessing VGG 16\n# return vgg16_preprocess_input\n# Preprocessing efficient net\nreturn enet_preprocess_input\ndef _get_second_callbacks(self) -&gt; list:\n'''Gets model callbacks - second fit\n        Returns:\n            list: List of callbacks\n        '''\n# We start by renaming 'best.hdf5' &amp; 'logger.csv'\nif os.path.exists(os.path.join(self.model_dir, 'best.hdf5')):\nos.rename(os.path.join(self.model_dir, 'best.hdf5'), os.path.join(self.model_dir, 'best_initial_fit.hdf5'))\nif os.path.exists(os.path.join(self.model_dir, 'logger.csv')):\nos.rename(os.path.join(self.model_dir, 'logger.csv'), os.path.join(self.model_dir, 'logger_initial_fit.csv'))\n# Get classic callbacks\ncallbacks = [EarlyStopping(monitor='val_loss', patience=self.second_patience, restore_best_weights=True)]\nif self.level_save in ['MEDIUM', 'HIGH']:\ncallbacks.append(\nModelCheckpoint(\nfilepath=os.path.join(self.model_dir, 'best.hdf5'), monitor='val_loss', save_best_only=True, mode='auto'\n)\n)\ncallbacks.append(CSVLogger(filename=os.path.join(self.model_dir, 'logger.csv'), separator=';', append=False))\ncallbacks.append(TerminateOnNaN())\n# Get LearningRateScheduler\nscheduler = self._get_second_learning_rate_scheduler()\nif scheduler is not None:\ncallbacks.append(LearningRateScheduler(scheduler))\n# Manage tensorboard\nif self.level_save in ['HIGH']:\n# Get log directory\nmodels_path = utils.get_models_path()\ntensorboard_dir = os.path.join(models_path, 'tensorboard_logs_second_fit')\n# We add a prefix so that the function load_model works correctly (it looks for a sub-folder with model name)\nlog_dir = os.path.join(tensorboard_dir, f\"tensorboard_{ntpath.basename(self.model_dir)}\")\nif not os.path.exists(log_dir):\nos.makedirs(log_dir)\n# TODO: check if this class does not slow proccesses\n# -&gt; For now: comment\n# Create custom class to monitore LR changes\n# https://stackoverflow.com/questions/49127214/keras-how-to-output-learning-rate-onto-tensorboard\n# class LRTensorBoard(TensorBoard):\n#     def __init__(self, log_dir, **kwargs) -&gt; None:  # add other arguments to __init__ if you need\n#         super().__init__(log_dir=log_dir, **kwargs)\n#\n#     def on_epoch_end(self, epoch, logs=None):\n#         logs.update({'lr': K.eval(self.model.optimizer.lr)})\n#         super().on_epoch_end(epoch, logs)\ncallbacks.append(TensorBoard(log_dir=log_dir, write_grads=False, write_images=False))\nself.logger.info(f\"To start tensorboard: python -m tensorboard.main --logdir {tensorboard_dir} --samples_per_plugin images=10\")\n# We use samples_per_plugin to avoid a rare issue between matplotlib and tensorboard\n# https://stackoverflow.com/questions/27147300/matplotlib-tcl-asyncdelete-async-handler-deleted-by-the-wrong-thread\nreturn callbacks\ndef _get_second_learning_rate_scheduler(self) -&gt; Union[Callable, None]:\n'''Fonction to define a Learning Rate Scheduler - second fit\n           -&gt; if it returns None, no scheduler will be used. (def.)\n           -&gt; This function will be save directly in the model configuration file\n           -&gt; This can be overridden at runing time\n        Returns:\n            (Callable | None): A learning rate Scheduler\n        '''\n# e.g.\n# def scheduler(epoch):\n#     lim_epoch = 75\n#     if epoch &lt; lim_epoch:\n#         return 0.01\n#     else:\n#         return max(0.001, 0.01 * math.exp(0.01 * (lim_epoch - epoch)))\nscheduler = None\nreturn scheduler\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['with_fine_tune'] = self.with_fine_tune\njson_data['second_epochs'] = self.second_epochs\njson_data['second_lr'] = self.second_lr\njson_data['second_patience'] = self.second_patience\n# Add _compile_model code if not in json_data\nif '_compile_model' not in json_data.keys():\njson_data['_compile_model'] = pickle.source.getsourcelines(self._compile_model)[0]\n# Add _get_second_learning_rate_scheduler code if not in json_data\nif '_get_second_learning_rate_scheduler' not in json_data.keys():\njson_data['_get_second_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_second_learning_rate_scheduler)[0]\n# Save\nsuper().save(json_data=json_data)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            preprocess_input_path (str): Path to preprocess input file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If hdf5_path is None\n            ValueError: If preprocess_input_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object hdf5_path is not an existing file\n            FileNotFoundError: If the object preprocess_input_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\npreprocess_input_path = kwargs.get('preprocess_input_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif preprocess_input_path is None:\nraise ValueError(\"The argument preprocess_input_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(preprocess_input_path):\nraise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'width', 'height', 'depth', 'color_mode', 'in_memory',\n'data_augmentation_params', 'nb_train_generator_images_to_save',\n'with_fine_tune', 'second_epochs', 'second_lr', 'second_patience',\n'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Reload preprocess_input\nwith open(preprocess_input_path, 'rb') as f:\nself.preprocess_input = pickle.load(f)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_transfer_learning_classifier/#template_vision.models_training.classifiers.model_transfer_learning_classifier.ModelTransferLearningClassifier.__init__","title":"<code>__init__(with_fine_tune=True, second_epochs=10, second_lr=1e-05, second_patience=5, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)</p> Kwargs <p>with_fine_tune (bool): If a fine-tuning step should be performed after first training second_epochs (int): Number of epochs for the fine-tuning step second_lr (float): Learning rate for the fine-tuning step second_patience (int): Patience for the fine-tuning step</p> Source code in <code>template_vision/models_training/classifiers/model_transfer_learning_classifier.py</code> <pre><code>def __init__(self, with_fine_tune: bool = True, second_epochs: int = 10, second_lr: float = 1e-5, second_patience: int = 5, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass, ModelKeras &amp; ModelClassifierMixin for more arguments)\n    Kwargs:\n        with_fine_tune (bool): If a fine-tuning step should be performed after first training\n        second_epochs (int): Number of epochs for the fine-tuning step\n        second_lr (float): Learning rate for the fine-tuning step\n        second_patience (int): Patience for the fine-tuning step\n    '''\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Params\nself.with_fine_tune = with_fine_tune\nself.second_epochs = second_epochs\nself.second_lr = second_lr\nself.second_patience = second_patience\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_transfer_learning_classifier/#template_vision.models_training.classifiers.model_transfer_learning_classifier.ModelTransferLearningClassifier.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration and \"standalones\" files - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file preprocess_input_path (str): Path to preprocess input file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hdf5_path is None</p> <code>ValueError</code> <p>If preprocess_input_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_input_path is not an existing file</p> Source code in <code>template_vision/models_training/classifiers/model_transfer_learning_classifier.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration and \"standalones\" files\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        preprocess_input_path (str): Path to preprocess input file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If hdf5_path is None\n        ValueError: If preprocess_input_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object hdf5_path is not an existing file\n        FileNotFoundError: If the object preprocess_input_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\npreprocess_input_path = kwargs.get('preprocess_input_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif preprocess_input_path is None:\nraise ValueError(\"The argument preprocess_input_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(preprocess_input_path):\nraise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save',\n'batch_size', 'epochs', 'validation_split', 'patience',\n'width', 'height', 'depth', 'color_mode', 'in_memory',\n'data_augmentation_params', 'nb_train_generator_images_to_save',\n'with_fine_tune', 'second_epochs', 'second_lr', 'second_patience',\n'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Reload model\nself.model = load_model_keras(hdf5_path, custom_objects=self.custom_objects)\n# Reload preprocess_input\nwith open(preprocess_input_path, 'rb') as f:\nself.preprocess_input = pickle.load(f)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/classifiers/model_transfer_learning_classifier/#template_vision.models_training.classifiers.model_transfer_learning_classifier.ModelTransferLearningClassifier.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/classifiers/model_transfer_learning_classifier.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['with_fine_tune'] = self.with_fine_tune\njson_data['second_epochs'] = self.second_epochs\njson_data['second_lr'] = self.second_lr\njson_data['second_patience'] = self.second_patience\n# Add _compile_model code if not in json_data\nif '_compile_model' not in json_data.keys():\njson_data['_compile_model'] = pickle.source.getsourcelines(self._compile_model)[0]\n# Add _get_second_learning_rate_scheduler code if not in json_data\nif '_get_second_learning_rate_scheduler' not in json_data.keys():\njson_data['_get_second_learning_rate_scheduler'] = pickle.source.getsourcelines(self._get_second_learning_rate_scheduler)[0]\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/","title":"Object detectors","text":""},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/","title":"Model detectron faster rcnn","text":""},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.LossEvalHook","title":"<code>LossEvalHook</code>","text":"<p>         Bases: <code>HookBase</code></p> <p>Hook to save the metrics and losses on the validation dataset</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>class LossEvalHook(HookBase):\n'''Hook to save the metrics and losses on the validation dataset'''\ndef __init__(self, eval_period: int, model, data_loader) -&gt; None:\n'''Initialization of the class\n        Args:\n            eval_period (int) : Number of iteration between two losses calculation\n            model : Considered model\n            data_loader : A dataloader containing the validation data\n        '''\nself._model = model\nself._period = eval_period\nself._data_loader = data_loader\ndef _do_loss_eval(self) -&gt; dict:\n'''Calculates the losses on the validation dataset. Saves them in the storage and\n        returns them.\n        Return:\n            the dict containing the losses\n        '''\n# Name of the considered losses\nlist_losses = ['loss_cls', 'loss_box_reg', 'loss_rpn_cls', 'loss_rpn_loc', 'total_loss']\nlosses = {name_loss: [] for name_loss in list_losses}\n# For each batch in the data_loader...\nfor inputs in self._data_loader:\nif torch.cuda.is_available():\ntorch.cuda.synchronize()  # Waits for all kernels in all streams on a CUDA device to complete\n# ... we calculates the losses...\nloss_batch = self._get_loss(inputs)\n# ... and we add them to the dictionary which save the results for each batch\nfor name_loss in list_losses:\nlosses[name_loss].append(loss_batch[name_loss])\n# We get the mean of the losses on the batches\nmean_losses = {name_loss: np.mean(list_losses_batch) for name_loss, list_losses_batch in losses.items()}\n# We save the losses in the storage of the trainer\nfor name_loss, value_loss in mean_losses.items():\nself.trainer.storage.put_scalar(f'validation_{name_loss}', value_loss)\ncomm.synchronize()\nreturn losses\ndef _get_loss(self, data) -&gt; dict:\n'''Calculates the losses corresponding to data\n        Args:\n            data: The data on which to calculate the losses\n        Returns:\n            dict: A dictionary containing all the losses\n        '''\n# Calculates the losses\nmetrics_dict = self._model(data)\n# Cast the losses to float and put them in a dictionary\nmetrics_dict = {\nk: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)\nfor k, v in metrics_dict.items()\n}\n# Calculate the total loss and add it to the dictionary of losses\ntotal_loss = sum(loss for loss in metrics_dict.values())\nmetrics_dict['total_loss'] = total_loss\nreturn metrics_dict\ndef after_step(self) -&gt; None:\n'''After the training step, check if we are at an iteration where we\n        should calculates the losses. If it is the case, calculate them and save\n        them in the storage.\n        '''\nnext_iter = self.trainer.iter + 1\nis_final = next_iter == self.trainer.max_iter\nif is_final or (self._period &gt; 0 and next_iter % self._period == 0):\nself._do_loss_eval()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.LossEvalHook.__init__","title":"<code>__init__(eval_period, model, data_loader)</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>eval_period</code> <code>int) </code> <p>Number of iteration between two losses calculation</p> required <code>model</code> <p>Considered model</p> required <code>data_loader</code> <p>A dataloader containing the validation data</p> required Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def __init__(self, eval_period: int, model, data_loader) -&gt; None:\n'''Initialization of the class\n    Args:\n        eval_period (int) : Number of iteration between two losses calculation\n        model : Considered model\n        data_loader : A dataloader containing the validation data\n    '''\nself._model = model\nself._period = eval_period\nself._data_loader = data_loader\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.LossEvalHook.after_step","title":"<code>after_step()</code>","text":"<p>After the training step, check if we are at an iteration where we should calculates the losses. If it is the case, calculate them and save them in the storage.</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def after_step(self) -&gt; None:\n'''After the training step, check if we are at an iteration where we\n    should calculates the losses. If it is the case, calculate them and save\n    them in the storage.\n    '''\nnext_iter = self.trainer.iter + 1\nis_final = next_iter == self.trainer.max_iter\nif is_final or (self._period &gt; 0 and next_iter % self._period == 0):\nself._do_loss_eval()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector","title":"<code>ModelDetectronFasterRcnnObjectDetector</code>","text":"<p>         Bases: <code>ModelObjectDetectorMixin</code>, <code>ModelClass</code></p> <p>Faster RCNN model (detectron2) for object detection</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>class ModelDetectronFasterRcnnObjectDetector(ModelObjectDetectorMixin, ModelClass):\n'''Faster RCNN model (detectron2) for object detection'''\n_default_name = 'model_detectron_faster_rcnn_object_detector'\ndef __init__(self, epochs: int = 99, batch_size: int = 1, validation_split: float = 0.2, lr: float = 0.00025,\nmin_delta_es: float = 0., patience: int = 5, restore_best_weights: bool = True,\ndata_augmentation_params: Union[dict, None] = None,\nrpn_min_overlap: float = 0.3, rpn_max_overlap: float = 0.7, rpn_restrict_num_regions: int = 128,\nroi_nms_overlap_threshold: float = 0.7, pred_bbox_proba_threshold: float = 0.5,\npred_nms_overlap_threshold: float = 0.5, nb_log_write_per_epoch: int = 1, nb_log_display_per_epoch: int = 10,\n**kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelObjectDetectorMixin for more arguments)\n        Args:\n            epochs (float): Maximal number of epochs\n            batch_size (int): Number of images in a batch when training\n            validation_split (float): Validation split fraction\n                Only used if there is no validation dataset as input when fitting\n            lr (float): Base (because we can use a lr scheduler) learning rate to use\n            min_delta_es (float): Minimal change in losses to be considered an amelioration for early stopping\n            patience (int): Early stopping patience. Put to 0 to disable early stopping\n            restore_best_weights (bool): If True, when the training is done, save the model with the best\n                loss on the validation dataset instead of the last model (even if early stopping is disabled)\n            data_augmentation_params (dict): Set of allowed data augmentation\n            rpn_min_overlap (float): Under this threshold a region is classified as background (RPN model)\n            rpn_max_overlap (float): Above this threshold a region is classified as object (RPN model)\n            rpn_restrict_num_regions (int): Maximal number of regions to keep as target for the RPN\n            roi_nms_overlap_threshold (float): The NMS deletes overlapping ROIs whose IOU is above this threshold\n            pred_bbox_proba_threshold (float): Above this threshold (for probabilities), a ROI is considered to be a match\n            pred_nms_overlap_threshold (float): When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold\n            nb_log_write_per_epoch (int): Number of metrics logs written during one epoch (losses for the train and the valid)\n            nb_log_display_per_epoch (int): Number of metrics logs displayed during one epoch (losses for the train only)\n        Raises:\n            ValueError: If rpn_min_overlap is not in [0, 1]\n            ValueError: If rpn_max_overlap is not in [0, 1]\n            ValueError: If rpn_min_overlap &gt; rpn_max_overlap\n            ValueError: If rpn_restrict_num_regions is not positive\n            ValueError: If roi_nms_overlap_threshold is not in [0, 1]\n            ValueError: If pred_bbox_proba_threshold is not in [0, 1]\n            ValueError: If pred_nms_overlap_threshold is not in [0, 1]\n        '''\n# Check errors\nif not 0 &lt;= rpn_min_overlap &lt;= 1:\nraise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) must be between 0 and 1, included\")\nif not 0 &lt;= rpn_max_overlap &lt;= 1:\nraise ValueError(f\"The argument rpn_max_overlap ({rpn_max_overlap}) must be between 0 and 1, included\")\nif rpn_min_overlap &gt; rpn_max_overlap:\nraise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) can't be bigger than rpn_max_overlap ({rpn_max_overlap})\")\nif rpn_restrict_num_regions &lt; 1:\nraise ValueError(f\"The argument rpn_restrict_num_regions ({rpn_restrict_num_regions}) must be positive\")\nif not 0 &lt;= roi_nms_overlap_threshold &lt;= 1:\nraise ValueError(f\"The argument roi_nms_overlap_threshold ({roi_nms_overlap_threshold}) must be between 0 and 1, included\")\nif not 0 &lt;= pred_bbox_proba_threshold &lt;= 1:\nraise ValueError(f\"The argument pred_bbox_proba_threshold ({pred_bbox_proba_threshold}) must be between 0 and 1, included\")\nif not 0 &lt;= pred_nms_overlap_threshold &lt;= 1:\nraise ValueError(f\"The argument pred_nms_overlap_threshold ({pred_nms_overlap_threshold}) must be between 0 and 1, included\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Attributes\nself.validation_split = validation_split\n# Early stopping parameters\nself.min_delta_es = min_delta_es\nself.patience = patience\nself.restore_best_weights = restore_best_weights\n# Data augmentation\nif data_augmentation_params is None:\ndata_augmentation_params = {'horizontal_flip': True, 'vertical_flip': True, 'rot_90': True}\nself.data_augmentation_params = data_augmentation_params\n# Parameters to \"convert\" iterations to epochs\n# Detectron works with a number of iterations (ie. number of batch to use during training)\n# In order to be more uniform with other models, we will use \"epochs\" rather than iterations\nself.nb_log_write_per_epoch = nb_log_write_per_epoch\nself.epochs = epochs\nself.nb_log_display_per_epoch = nb_log_display_per_epoch\n# Load config &amp; pre-trained model\nself.detectron_config_base_filename = 'Base-RCNN-FPN.yaml'\nself.detectron_config_filename = 'faster_rcnn_R_50_FPN_3x.yaml'\nself.detectron_model_filename = 'model_final_280758.pkl'\ndetectron_config_base_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_config_base_filename)\ndetectron_config_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_config_filename)\ndetectron_model_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_model_filename)\n# Backup URLs if the files do not exist\ndetectron_config_base_backup_urls = [\n'https://raw.githubusercontent.com/facebookresearch/detectron2/main/configs/Base-RCNN-FPN.yaml',\n]\ndetectron_config_backup_urls = [\n'https://raw.githubusercontent.com/facebookresearch/detectron2/main/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml',\n]\ndetectron_model_backup_urls = [\n'https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl',\n]\n# Check files availability\nfiles_available = True\n# For each file, we try to download it if does not exists in the projet\nif not os.path.exists(detectron_config_base_path):\ntry:\nself.logger.warning(\"The base configuration file of the faster RCNN of detectron2 is not present in your data folder.\")\nself.logger.warning(\"Trying to download the file.\")\nutils.download_url(detectron_config_base_backup_urls, detectron_config_base_path)\nexcept ConnectionError:\nself.logger.warning(\"Can't download the file. You can try to get it manually.\")\nself.logger.warning(\"You can find it here https://github.com/facebookresearch/detectron2/blob/main/configs/Base-RCNN-FPN.yaml\")\nself.logger.warning(\"The model won't work, except if it is 'reloaded'\")\nself.cfg = None\nfiles_available = False\n# /!\\ WARNING, the key _BASE_ of the configuration file of the RCNN must point to the base configuration file /!\\\n# /!\\ It won't work if it is not the case !!! /!\\\nif not os.path.exists(detectron_config_path):\ntry:\nself.logger.warning(\"The configuration file of the faster RCNN of detectron2 is not present in your data folder.\")\nself.logger.warning(\"Trying to download the file.\")\nutils.download_url(detectron_config_backup_urls, detectron_config_path)\n# Configure _BASE_ keyword from detectron_config_path\nwith open(detectron_config_path, \"r\") as f:\ndetectron_config = yaml.load(f, yaml.CLoader)\ndetectron_config[\"_BASE_\"] = self.detectron_config_base_filename\nwith open(detectron_config_path, \"w\") as f:\nyaml.dump(detectron_config, f)\nexcept ConnectionError:\nself.logger.warning(\"Can't download the file. You can try to get it manually.\")\nself.logger.warning(\"You can find it here https://github.com/facebookresearch/detectron2/blob/main/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\nself.logger.warning(\"You will have to modify the key _BASE_ to point to the base configuration file\")\nself.logger.warning(\"The model won't work, except if it is 'reloaded'\")\nself.cfg = None\nfiles_available = False\nif not os.path.exists(detectron_model_path):\ntry:\nself.logger.warning(\"The weights file of the faster RCNN of detectron2 is not present in your data folder.\")\nself.logger.warning(\"Trying to download the file.\")\nutils.download_url(detectron_model_backup_urls, detectron_model_path)\nexcept ConnectionError:\nself.logger.warning(\"Can't download the file. You can try to get it manually.\")\nself.logger.warning(\"You can download the weights here : https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\")\nself.logger.warning(\"The model won't work, except if it is 'reloaded'\")\nself.cfg = None\nfiles_available = False\n# Load if ok\nif files_available:\ncfg = get_cfg()  # Get base config\ntry:\ncfg.merge_from_file(detectron_config_path)  # Merge faster RCNN config\nexcept Exception:\nself.logger.error(\"Error when reading model configurations\")\nself.logger.error(\"A common issue is that the key _BASE_ of the configuration file of the RCNN must point to the base configuration file\")\nself.logger.error(\"Check your file 'faster_rcnn_R_50_FPN_3x.yaml'\")\nraise\nself.cfg = cfg\n# Weights\nself.cfg.MODEL.WEIGHTS = detectron_model_path\n# Training parameters\nself.cfg.DATALOADER.NUM_WORKERS = 2\nself.cfg.SOLVER.IMS_PER_BATCH = batch_size\nself.cfg.SOLVER.BASE_LR = lr\nself.cfg.MODEL.RPN.IOU_THRESHOLDS = [rpn_min_overlap, rpn_max_overlap]\nself.cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE = rpn_restrict_num_regions\nself.cfg.MODEL.RPN.NMS_THRESH = roi_nms_overlap_threshold\nself.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = pred_bbox_proba_threshold\nself.cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = pred_nms_overlap_threshold\n# We put outputs in the folder of the model\nself.cfg.OUTPUT_DIR = self.model_dir\n# Check that the GPU is available. Otherwise, CPU\nif not torch.cuda.is_available():\nself.logger.warning(\"Warning, no GPU detected, the model will use CPU\")\nself.cfg.MODEL.DEVICE = \"cpu\"\n#####################\n# Register datasets\n#####################\ndef _register_dataset(self, df: pd.DataFrame, data_type: str) -&gt; None:\n'''Registers a dataset in the global variables used by detectron2\n        Args:\n            df (pd.DataFrame): Dataset to use\n                Must contain the column 'file_path' with the path to an image\n                Must contain the column 'bboxes' containing the list of bboxes of the image\n            data_type (str): Data type, 'train' or 'valid'\n        Raises:\n            ValueError: If data_type not in ['train', 'valid']\n            ValueError: If the dataframe has no 'file_path' column\n            ValueError: If the dataframe has no 'bboxes' column\n        '''\nif data_type not in ['train', 'valid']:\nraise ValueError(f\"The value {data_type} is not a suitable value for the argument data_type.\")\nif 'file_path' not in df.columns:\nraise ValueError(\"The column 'file_path' is mandatory in the input dataframe\")\nif 'bboxes' not in df.columns:\nraise ValueError(\"The column 'bboxes' is mandatory in the input dataframe\")\n# Name of the dataset\nname_dataset = f\"dataset_{data_type}\"\n# Deletes the dataset in the catalog if already present\nif name_dataset in DatasetCatalog:\nDatasetCatalog.pop(name_dataset)\nif name_dataset in MetadataCatalog:\nMetadataCatalog.pop(name_dataset)\n# Register the dataset in the catalogues\ninv_dict_classes = {value: key for key, value in self.dict_classes.items()}\nDatasetCatalog.register(name_dataset, lambda: self._prepare_dataset_format(df, inv_dict_classes))  # register format : (str, func)\nMetadataCatalog.get(name_dataset).set(thing_classes=self.list_classes)\ndef _prepare_dataset_format(self, df: pd.DataFrame, inv_dict_classes: dict) -&gt; list:\n'''Puts the dataframe containing the file paths and the bboxes in the suitable format for detectron2\n        Args:\n            df (pd.DataFrame): Dataset to use\n                Must contain the column 'file_path' with the path to an image\n                Must contain the column 'bboxes' containing the list of bboxes of the image\n        Returns:\n            A list of dictionaris each corresponding to an image (and the associated bboxes)\n        '''\n# Dictionary mapping of the classes\ninv_dict_classes = {value: key for key, value in self.dict_classes.items()}\n# Get info from the dataset\npath_list = list(df['file_path'])\nbboxes_list = list(df['bboxes'])\ndataset_dicts = []\n# For each image, we will create a dictionary with the elements expected by detectron2\nfor idx, (path, bboxes) in enumerate(zip(path_list, bboxes_list)):\n# Get the height and width of the image\ntry:\nheight, width = cv2.imread(path).shape[:2]\nexcept Exception:\nself.logger.warning(f\"Can't read image {path}. We will skip it when training.\")\ncontinue\nrecord = {}\nrecord[\"file_name\"] = path\nrecord[\"image_id\"] = idx\nrecord[\"height\"] = height\nrecord[\"width\"] = width\n# Creation of the list of the associated bboxes (annotations)\nobjs = []\nfor bbox in bboxes:\nbbox_coordinates = [bbox[coord] for coord in ['x1', 'y1', 'x2', 'y2']]\n# We put it in the format expected by detectron2\nobj = {\n\"bbox\": bbox_coordinates,\n\"bbox_mode\": BoxMode.XYXY_ABS,\n\"category_id\": inv_dict_classes[bbox[\"class\"]],\n\"iscrowd\": 0\n}\nobjs.append(obj)\n# We register all the bboxes in \"annotations\" ...\nrecord[\"annotations\"] = objs\n# ... and we append to the dictionary of the dataset\ndataset_dicts.append(record)\nreturn dataset_dicts\n#####################\n# Fit\n#####################\ndef fit(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None, with_shuffle: bool = True) -&gt; None:\n'''Trains the model\n        Args:\n            df_train (pd.DataFrame): Training dataset with columns file_path &amp; bboxes\n        Kwargs:\n            df_valid (pd.DataFrame): Validation dataset with columns file_path &amp; bboxes\n            with_shuffle (boolean): If data must be shuffled before fitting\n                This should be used if the target is not shuffled as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            AssertionError: If the same classes are not present when comparing an already trained model\n                and a new dataset\n        '''\n##############################################\n# Manage retrain\n##############################################\n# If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n# And we save the old conf\nif self.trained:\n# Get src files to save\nsrc_files = [os.path.join(self.model_dir, \"configurations.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\nsrc_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Change model dir\nself.model_dir = self._get_model_dir()\nself.cfg.OUTPUT_DIR = self.model_dir\n# Get dst files\ndst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\ndst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Copies\nfor src, dst in zip(src_files, dst_files):\ntry:\nshutil.copyfile(src, dst)\nexcept Exception as e:\nself.logger.error(f\"Impossible to copy {src} to {dst}\")\nself.logger.error(\"We still continue ...\")\nself.logger.error(repr(e))\n##############################################\n# Prepare dataset\n# Also extract list of classes\n##############################################\n# Extract list of classes from df_train\nset_classes = set()\nfor bboxes in df_train['bboxes'].to_dict().values():\nset_classes = set_classes.union({bbox['class'] for bbox in bboxes})\nlist_classes = sorted(list(set_classes))\n# Also set dict_classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# We make sure that we have str for all classes\n# We do not raise an error, detectron2 will do it\nclasses_not_string = {cl for cl in set_classes if not isinstance(cl, str)}\nif len(classes_not_string):\nself.logger.warning(f\"Warning, the following classes are not strings : {classes_not_string}. Detectron2 requires that all classes are strings.\")\n# Validate classes if already trained, else set them\nif self.trained:\nassert self.list_classes == list_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nassert self.dict_classes == dict_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nelse:\nself.list_classes = list_classes\nself.dict_classes = dict_classes\n# Shuffle training dataset if wanted\n# If not, if no validation is provided, the train_test_split could stay in order\n# Hence, for classification task, we might have classes in the validation data that we never met in the training data\nif with_shuffle:\ndf_train = df_train.sample(frac=1.).reset_index(drop=True)\n# Manage the absence of a validation dataset\nif df_valid is None:\nself.logger.warning(f\"Attention, pas de jeu de validation. On va donc split le jeu de training (fraction valid = {self.validation_split})\")\ndf_train, df_valid = train_test_split(df_train, test_size=self.validation_split)\n# We register the train and validation datasets\nself._register_dataset(df=df_train, data_type='train')\nself._register_dataset(df=df_valid, data_type='valid')\n# We give the number of classes to the model\nself.cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(self.list_classes)\n# We give the datasets to use to the model (that we have previously registered in the catalogues)\nself.cfg.DATASETS.TRAIN = (\"dataset_train\", )\nself.cfg.DATASETS.TEST = (\"dataset_valid\", )\n# \"Translate\" the number of iterations to \"epoch\"\nnb_iter_per_epoch = max(int(len(df_train) / self.cfg.SOLVER.IMS_PER_BATCH), 1)\n# Number of iterations between two log writes\nnb_iter_log_write = max(int(nb_iter_per_epoch / self.nb_log_write_per_epoch), 1)\n# Number of iterations between two log displays\nnb_iter_log_display = max(int(nb_iter_per_epoch / self.nb_log_display_per_epoch), 1)\n# Maximal number of iterations\nnb_max_iter = self.epochs * nb_iter_per_epoch - 1\nself.cfg.SOLVER.MAX_ITER = nb_max_iter\n# We have to change the class attribute because it is used in a class method BEFORE instanciation\nTrainerRCNN.data_augmentation_params.update(self.data_augmentation_params)\n# We train\ntrainer = TrainerRCNN(self.cfg,\nlength_epoch=len(df_train),\nnb_iter_per_epoch=nb_iter_per_epoch,\nnb_iter_log_write=nb_iter_log_write,\nnb_iter_log_display=nb_iter_log_display,\nnb_log_write_per_epoch=self.nb_log_write_per_epoch,\nmin_delta_es=self.min_delta_es,\npatience=self.patience,\nrestore_best_weights=self.restore_best_weights)\n# Resume to False because we automatically save the best weights in a file,\n# and then we point self.cfg.MODEL.WEIGHTS to this file\ntrainer.resume_or_load(resume=False)\ntrainer.train()\n# Update train status\nself.trained = True\nself.nb_fit += 1\n# We change the weights path to the post-training weights\nself.cfg.MODEL.WEIGHTS = os.path.join(self.cfg.OUTPUT_DIR, 'best.pth')\n# Plots losses &amp; metrics\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._plot_metrics_and_loss()\n#####################\n# Predict\n#####################\n@utils.trained_needed\ndef predict(self, df_test: pd.DataFrame, write_images: bool = False,\noutput_dir_image: Union[str, None] = None, **kwargs) -&gt; List[List[dict]]:\n'''Predictions on test set - batch size must be equal to 1\n        Args:\n            df_test (pd.DataFrame): Data to predict, with a column 'file_path'\n            write_images (bool): If True, we write images with the predicted bboxes\n            output_dir_image (str): Path to which we want to write the predicted images (if write_images is True)\n        Returns:\n            (list&lt;list&lt;dict&gt;&gt;): list (one entry per image) of list of bboxes\n        '''\n# First we take care of the case where we want to write images\nif write_images:\n# Metadata used by the Visualizer to draw bboxes\nmetadata = Metadata(name='metadata_for_predict')\nmetadata.set(thing_classes=self.list_classes)\n# Prepare the folders to write the images\n# Manage case where output_dir_image is None\nif output_dir_image is None:\noutput_dir_image = os.path.join(self.cfg.OUTPUT_DIR, 'inference', 'images')\n# Create folder if it does not exist\nif not os.path.exists(output_dir_image):\nos.makedirs(output_dir_image)\n# We define a predictor\npredictor = DefaultPredictor(self.cfg)\n# For each image...\nlist_bbox = []\nfor file_path in df_test['file_path']:\nlist_bboxes_img = []\n# We open the image\nim = cv2.imread(file_path)\nif im is not None:\n# We predict\noutputs = predictor(im)\nif write_images:\n# We draw bboxes and we write the image\nfilename = os.path.split(file_path)[-1]\nv = Visualizer(im[:, :, ::-1], metadata, scale=1.0)\nv = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\ncv2.imwrite(os.path.join(output_dir_image, filename), v.get_image()[:, :, ::-1])\n# We get the bboxes, the scores and the classes\nboxes = np.array(outputs['instances'].get('pred_boxes').tensor.cpu())\nscores = np.array(outputs['instances'].get('scores').cpu())\nclasses = np.array(outputs['instances'].get('pred_classes').cpu())\n# For each bbox predicted\nfor idx in range(len(boxes)):\n# We put it in bbox format\ncoordinates = boxes[idx]\nbbox = {'x1': coordinates[0], 'y1': coordinates[1], 'x2': coordinates[2],\n'y2': coordinates[3], 'proba': scores[idx],\n'class': self.dict_classes[classes[idx]]}\n# An we append it\nlist_bboxes_img.append(bbox.copy())\nlist_bbox.append(list_bboxes_img.copy())\nreturn list_bbox\n#####################\n# Misc.\n#####################\ndef _plot_metrics_and_loss(self, **kwargs) -&gt; None:\n'''Plots interesting metrics from training and saves them in files'''\n# Get metrics from detectron2 info\npath_json_metrics = os.path.join(self.cfg.OUTPUT_DIR, 'metrics.json')\nmetrics = self._load_metrics_from_json(path_json_metrics)\n# Manage plots\nplots_path = os.path.join(self.cfg.OUTPUT_DIR, 'plots')\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\ndict_plots = {'total_loss': {'title': 'Total loss', 'output_filename': 'total_loss'},\n'loss_cls': {'title': 'Classifier classification loss', 'output_filename': 'loss_cls_classifier'},\n'loss_box_reg': {'title': 'Classifier regression loss', 'output_filename': 'loss_reg_classifier'},\n'loss_rpn_cls': {'title': 'RPN classification loss', 'output_filename': 'loss_cls_rpn'},\n'loss_rpn_loc': {'title': 'RPN regression loss', 'output_filename': 'loss_reg_rpn'}}\n# Plot each metric one by one\nfor name_metric, char_metric in dict_plots.items():\nself._plot_one_metric(metrics=metrics,\nname_metric=name_metric,\ntitle=char_metric['title'],\noutput_filename=char_metric['output_filename'],\nplots_path=plots_path)\ndef _load_metrics_from_json(self, json_path: str) -&gt; pd.DataFrame:\n'''Reads the .json written by the training and puts it in a dataframe\n        Args:\n            json_path (str) : Path to the .json file\n        Returns:\n            pd.DataFrame: A dataframe containing all the metrics saved during the training\n        '''\nlines = []\nwith open(json_path, 'r') as f:\nfor line in f:\nlines.append(json.loads(line))\n# We get rid of the lines which do not contain validation_total_loss\nlines = [line for line in lines if 'validation_total_loss' in line]\nmetrics = pd.DataFrame(lines)\nmetrics = metrics.drop_duplicates()\nreturn metrics\ndef _plot_one_metric(self, metrics: pd.DataFrame, name_metric: str, title: str,\noutput_filename: str, plots_path: str) -&gt; None:\n'''Plots the figure of a metric for the train and validation datasets and saves it.\n        Args:\n            metrics (pd.DataFrame): The dataframe containing the metrics\n            name_metric (str): The name of the metric we want to plot\n            title (str): The name we want to give to the plot\n            output_filename (str): The name of the file we want to save (without the extension)\n            plots_path (str): The path to the plot folder\n        '''\n# Get the lists of metrics for the train and validation datasets\nlist_train = list(metrics[name_metric])\nlist_valid = list(metrics[f'validation_{name_metric}'])\nlist_iteration = list(metrics['iteration'])\n# Plot\nplt.figure(figsize=(10, 8))\nplt.plot(list_iteration, list_train)\nplt.plot(list_iteration, list_valid)\nplt.title(title)\nplt.ylabel(title)\nplt.xlabel(\"Number of iterations\")\nplt.legend(['Train', 'Validation'], loc='upper left')\n# Save\nfilename = f\"{output_filename}.jpeg\"\nplt.savefig(os.path.join(plots_path, filename))\nplt.close('all')\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Save attributes &amp; cfg (contains all params)\njson_data['librairie'] = 'detectron2'\njson_data['validation_split'] = self.validation_split\njson_data['min_delta_es'] = self.min_delta_es\njson_data['patience'] = self.patience\njson_data['restore_best_weights'] = self.restore_best_weights\njson_data['data_augmentation_params'] = self.data_augmentation_params\njson_data['nb_log_write_per_epoch'] = self.nb_log_write_per_epoch\njson_data['epochs'] = self.epochs\njson_data['nb_log_display_per_epoch'] = self.nb_log_display_per_epoch\njson_data['detectron_config_base_filename'] = self.detectron_config_base_filename\njson_data['detectron_config_filename'] = self.detectron_config_filename\njson_data['detectron_model_filename'] = self.detectron_model_filename\njson_data['cfg'] = self.cfg\n# We save le model with CPU so that there is no problem later\n# when we use the model (with streamlit for example)\ndevice = self.cfg.MODEL.DEVICE\nself.cfg.MODEL.DEVICE = \"cpu\"\nsuper().save(json_data=json_data)\n# We undo what we just did\nself.cfg.MODEL.DEVICE = device\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Loads a model from its configuration and the weights of the network\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            pth_path (str): Path to pth file\n        Raises:\n            ValueError: If configuration_path is None\n            ValueError: If pth_path is None\n            FileNotFoundError: If the object configuration_path is not an existing file\n            FileNotFoundError: If the object pth_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\npth_path = kwargs.get('pth_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif pth_path is None:\nraise ValueError(\"The argument pth_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(pth_path):\nraise FileNotFoundError(f\"The file {pth_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'validation_split', 'min_delta_es', 'patience', 'restore_best_weights',\n'list_classes', 'dict_classes', 'data_augmentation_params', 'level_save',\n'nb_log_write_per_epoch', 'epochs', 'nb_log_display_per_epoch', 'detectron_config_base_filename',\n'detectron_config_filename', 'detectron_model_filename', 'cfg']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Transform cfg into CfgNode\nself.cfg = CfgNode(init_dict=self.cfg)\n# Save best pth in new folder\nnew_pth_path = os.path.join(self.model_dir, 'best.pth')\nshutil.copyfile(pth_path, new_pth_path)\n# Reload model\nself.cfg.MODEL.WEIGHTS = new_pth_path\n# Change output path\nself.cfg.OUTPUT_DIR = self.model_dir\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector.__init__","title":"<code>__init__(epochs=99, batch_size=1, validation_split=0.2, lr=0.00025, min_delta_es=0.0, patience=5, restore_best_weights=True, data_augmentation_params=None, rpn_min_overlap=0.3, rpn_max_overlap=0.7, rpn_restrict_num_regions=128, roi_nms_overlap_threshold=0.7, pred_bbox_proba_threshold=0.5, pred_nms_overlap_threshold=0.5, nb_log_write_per_epoch=1, nb_log_display_per_epoch=10, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass &amp; ModelObjectDetectorMixin for more arguments)</p> <p>Parameters:</p> Name Type Description Default <code>epochs</code> <code>float</code> <p>Maximal number of epochs</p> <code>99</code> <code>batch_size</code> <code>int</code> <p>Number of images in a batch when training</p> <code>1</code> <code>validation_split</code> <code>float</code> <p>Validation split fraction Only used if there is no validation dataset as input when fitting</p> <code>0.2</code> <code>lr</code> <code>float</code> <p>Base (because we can use a lr scheduler) learning rate to use</p> <code>0.00025</code> <code>min_delta_es</code> <code>float</code> <p>Minimal change in losses to be considered an amelioration for early stopping</p> <code>0.0</code> <code>patience</code> <code>int</code> <p>Early stopping patience. Put to 0 to disable early stopping</p> <code>5</code> <code>restore_best_weights</code> <code>bool</code> <p>If True, when the training is done, save the model with the best loss on the validation dataset instead of the last model (even if early stopping is disabled)</p> <code>True</code> <code>data_augmentation_params</code> <code>dict</code> <p>Set of allowed data augmentation</p> <code>None</code> <code>rpn_min_overlap</code> <code>float</code> <p>Under this threshold a region is classified as background (RPN model)</p> <code>0.3</code> <code>rpn_max_overlap</code> <code>float</code> <p>Above this threshold a region is classified as object (RPN model)</p> <code>0.7</code> <code>rpn_restrict_num_regions</code> <code>int</code> <p>Maximal number of regions to keep as target for the RPN</p> <code>128</code> <code>roi_nms_overlap_threshold</code> <code>float</code> <p>The NMS deletes overlapping ROIs whose IOU is above this threshold</p> <code>0.7</code> <code>pred_bbox_proba_threshold</code> <code>float</code> <p>Above this threshold (for probabilities), a ROI is considered to be a match</p> <code>0.5</code> <code>pred_nms_overlap_threshold</code> <code>float</code> <p>When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold</p> <code>0.5</code> <code>nb_log_write_per_epoch</code> <code>int</code> <p>Number of metrics logs written during one epoch (losses for the train and the valid)</p> <code>1</code> <code>nb_log_display_per_epoch</code> <code>int</code> <p>Number of metrics logs displayed during one epoch (losses for the train only)</p> <code>10</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If rpn_min_overlap is not in [0, 1]</p> <code>ValueError</code> <p>If rpn_max_overlap is not in [0, 1]</p> <code>ValueError</code> <p>If rpn_min_overlap &gt; rpn_max_overlap</p> <code>ValueError</code> <p>If rpn_restrict_num_regions is not positive</p> <code>ValueError</code> <p>If roi_nms_overlap_threshold is not in [0, 1]</p> <code>ValueError</code> <p>If pred_bbox_proba_threshold is not in [0, 1]</p> <code>ValueError</code> <p>If pred_nms_overlap_threshold is not in [0, 1]</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def __init__(self, epochs: int = 99, batch_size: int = 1, validation_split: float = 0.2, lr: float = 0.00025,\nmin_delta_es: float = 0., patience: int = 5, restore_best_weights: bool = True,\ndata_augmentation_params: Union[dict, None] = None,\nrpn_min_overlap: float = 0.3, rpn_max_overlap: float = 0.7, rpn_restrict_num_regions: int = 128,\nroi_nms_overlap_threshold: float = 0.7, pred_bbox_proba_threshold: float = 0.5,\npred_nms_overlap_threshold: float = 0.5, nb_log_write_per_epoch: int = 1, nb_log_display_per_epoch: int = 10,\n**kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass &amp; ModelObjectDetectorMixin for more arguments)\n    Args:\n        epochs (float): Maximal number of epochs\n        batch_size (int): Number of images in a batch when training\n        validation_split (float): Validation split fraction\n            Only used if there is no validation dataset as input when fitting\n        lr (float): Base (because we can use a lr scheduler) learning rate to use\n        min_delta_es (float): Minimal change in losses to be considered an amelioration for early stopping\n        patience (int): Early stopping patience. Put to 0 to disable early stopping\n        restore_best_weights (bool): If True, when the training is done, save the model with the best\n            loss on the validation dataset instead of the last model (even if early stopping is disabled)\n        data_augmentation_params (dict): Set of allowed data augmentation\n        rpn_min_overlap (float): Under this threshold a region is classified as background (RPN model)\n        rpn_max_overlap (float): Above this threshold a region is classified as object (RPN model)\n        rpn_restrict_num_regions (int): Maximal number of regions to keep as target for the RPN\n        roi_nms_overlap_threshold (float): The NMS deletes overlapping ROIs whose IOU is above this threshold\n        pred_bbox_proba_threshold (float): Above this threshold (for probabilities), a ROI is considered to be a match\n        pred_nms_overlap_threshold (float): When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold\n        nb_log_write_per_epoch (int): Number of metrics logs written during one epoch (losses for the train and the valid)\n        nb_log_display_per_epoch (int): Number of metrics logs displayed during one epoch (losses for the train only)\n    Raises:\n        ValueError: If rpn_min_overlap is not in [0, 1]\n        ValueError: If rpn_max_overlap is not in [0, 1]\n        ValueError: If rpn_min_overlap &gt; rpn_max_overlap\n        ValueError: If rpn_restrict_num_regions is not positive\n        ValueError: If roi_nms_overlap_threshold is not in [0, 1]\n        ValueError: If pred_bbox_proba_threshold is not in [0, 1]\n        ValueError: If pred_nms_overlap_threshold is not in [0, 1]\n    '''\n# Check errors\nif not 0 &lt;= rpn_min_overlap &lt;= 1:\nraise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) must be between 0 and 1, included\")\nif not 0 &lt;= rpn_max_overlap &lt;= 1:\nraise ValueError(f\"The argument rpn_max_overlap ({rpn_max_overlap}) must be between 0 and 1, included\")\nif rpn_min_overlap &gt; rpn_max_overlap:\nraise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) can't be bigger than rpn_max_overlap ({rpn_max_overlap})\")\nif rpn_restrict_num_regions &lt; 1:\nraise ValueError(f\"The argument rpn_restrict_num_regions ({rpn_restrict_num_regions}) must be positive\")\nif not 0 &lt;= roi_nms_overlap_threshold &lt;= 1:\nraise ValueError(f\"The argument roi_nms_overlap_threshold ({roi_nms_overlap_threshold}) must be between 0 and 1, included\")\nif not 0 &lt;= pred_bbox_proba_threshold &lt;= 1:\nraise ValueError(f\"The argument pred_bbox_proba_threshold ({pred_bbox_proba_threshold}) must be between 0 and 1, included\")\nif not 0 &lt;= pred_nms_overlap_threshold &lt;= 1:\nraise ValueError(f\"The argument pred_nms_overlap_threshold ({pred_nms_overlap_threshold}) must be between 0 and 1, included\")\n# Init.\nsuper().__init__(**kwargs)\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Attributes\nself.validation_split = validation_split\n# Early stopping parameters\nself.min_delta_es = min_delta_es\nself.patience = patience\nself.restore_best_weights = restore_best_weights\n# Data augmentation\nif data_augmentation_params is None:\ndata_augmentation_params = {'horizontal_flip': True, 'vertical_flip': True, 'rot_90': True}\nself.data_augmentation_params = data_augmentation_params\n# Parameters to \"convert\" iterations to epochs\n# Detectron works with a number of iterations (ie. number of batch to use during training)\n# In order to be more uniform with other models, we will use \"epochs\" rather than iterations\nself.nb_log_write_per_epoch = nb_log_write_per_epoch\nself.epochs = epochs\nself.nb_log_display_per_epoch = nb_log_display_per_epoch\n# Load config &amp; pre-trained model\nself.detectron_config_base_filename = 'Base-RCNN-FPN.yaml'\nself.detectron_config_filename = 'faster_rcnn_R_50_FPN_3x.yaml'\nself.detectron_model_filename = 'model_final_280758.pkl'\ndetectron_config_base_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_config_base_filename)\ndetectron_config_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_config_filename)\ndetectron_model_path = os.path.join(utils.get_data_path(), 'detectron2_conf_files', self.detectron_model_filename)\n# Backup URLs if the files do not exist\ndetectron_config_base_backup_urls = [\n'https://raw.githubusercontent.com/facebookresearch/detectron2/main/configs/Base-RCNN-FPN.yaml',\n]\ndetectron_config_backup_urls = [\n'https://raw.githubusercontent.com/facebookresearch/detectron2/main/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml',\n]\ndetectron_model_backup_urls = [\n'https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl',\n]\n# Check files availability\nfiles_available = True\n# For each file, we try to download it if does not exists in the projet\nif not os.path.exists(detectron_config_base_path):\ntry:\nself.logger.warning(\"The base configuration file of the faster RCNN of detectron2 is not present in your data folder.\")\nself.logger.warning(\"Trying to download the file.\")\nutils.download_url(detectron_config_base_backup_urls, detectron_config_base_path)\nexcept ConnectionError:\nself.logger.warning(\"Can't download the file. You can try to get it manually.\")\nself.logger.warning(\"You can find it here https://github.com/facebookresearch/detectron2/blob/main/configs/Base-RCNN-FPN.yaml\")\nself.logger.warning(\"The model won't work, except if it is 'reloaded'\")\nself.cfg = None\nfiles_available = False\n# /!\\ WARNING, the key _BASE_ of the configuration file of the RCNN must point to the base configuration file /!\\\n# /!\\ It won't work if it is not the case !!! /!\\\nif not os.path.exists(detectron_config_path):\ntry:\nself.logger.warning(\"The configuration file of the faster RCNN of detectron2 is not present in your data folder.\")\nself.logger.warning(\"Trying to download the file.\")\nutils.download_url(detectron_config_backup_urls, detectron_config_path)\n# Configure _BASE_ keyword from detectron_config_path\nwith open(detectron_config_path, \"r\") as f:\ndetectron_config = yaml.load(f, yaml.CLoader)\ndetectron_config[\"_BASE_\"] = self.detectron_config_base_filename\nwith open(detectron_config_path, \"w\") as f:\nyaml.dump(detectron_config, f)\nexcept ConnectionError:\nself.logger.warning(\"Can't download the file. You can try to get it manually.\")\nself.logger.warning(\"You can find it here https://github.com/facebookresearch/detectron2/blob/main/configs/COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")\nself.logger.warning(\"You will have to modify the key _BASE_ to point to the base configuration file\")\nself.logger.warning(\"The model won't work, except if it is 'reloaded'\")\nself.cfg = None\nfiles_available = False\nif not os.path.exists(detectron_model_path):\ntry:\nself.logger.warning(\"The weights file of the faster RCNN of detectron2 is not present in your data folder.\")\nself.logger.warning(\"Trying to download the file.\")\nutils.download_url(detectron_model_backup_urls, detectron_model_path)\nexcept ConnectionError:\nself.logger.warning(\"Can't download the file. You can try to get it manually.\")\nself.logger.warning(\"You can download the weights here : https://dl.fbaipublicfiles.com/detectron2/COCO-Detection/faster_rcnn_R_50_FPN_3x/137849458/model_final_280758.pkl\")\nself.logger.warning(\"The model won't work, except if it is 'reloaded'\")\nself.cfg = None\nfiles_available = False\n# Load if ok\nif files_available:\ncfg = get_cfg()  # Get base config\ntry:\ncfg.merge_from_file(detectron_config_path)  # Merge faster RCNN config\nexcept Exception:\nself.logger.error(\"Error when reading model configurations\")\nself.logger.error(\"A common issue is that the key _BASE_ of the configuration file of the RCNN must point to the base configuration file\")\nself.logger.error(\"Check your file 'faster_rcnn_R_50_FPN_3x.yaml'\")\nraise\nself.cfg = cfg\n# Weights\nself.cfg.MODEL.WEIGHTS = detectron_model_path\n# Training parameters\nself.cfg.DATALOADER.NUM_WORKERS = 2\nself.cfg.SOLVER.IMS_PER_BATCH = batch_size\nself.cfg.SOLVER.BASE_LR = lr\nself.cfg.MODEL.RPN.IOU_THRESHOLDS = [rpn_min_overlap, rpn_max_overlap]\nself.cfg.MODEL.RPN.BATCH_SIZE_PER_IMAGE = rpn_restrict_num_regions\nself.cfg.MODEL.RPN.NMS_THRESH = roi_nms_overlap_threshold\nself.cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = pred_bbox_proba_threshold\nself.cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = pred_nms_overlap_threshold\n# We put outputs in the folder of the model\nself.cfg.OUTPUT_DIR = self.model_dir\n# Check that the GPU is available. Otherwise, CPU\nif not torch.cuda.is_available():\nself.logger.warning(\"Warning, no GPU detected, the model will use CPU\")\nself.cfg.MODEL.DEVICE = \"cpu\"\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector.fit","title":"<code>fit(df_train, df_valid=None, with_shuffle=True)</code>","text":"<p>Trains the model</p> <p>Parameters:</p> Name Type Description Default <code>df_train</code> <code>pd.DataFrame</code> <p>Training dataset with columns file_path &amp; bboxes</p> required Kwargs <p>df_valid (pd.DataFrame): Validation dataset with columns file_path &amp; bboxes with_shuffle (boolean): If data must be shuffled before fitting     This should be used if the target is not shuffled as the split_validation takes the lines in order.     Thus, the validation set might get classes which are not in the train set ...</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If the same classes are not present when comparing an already trained model and a new dataset</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def fit(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None, with_shuffle: bool = True) -&gt; None:\n'''Trains the model\n    Args:\n        df_train (pd.DataFrame): Training dataset with columns file_path &amp; bboxes\n    Kwargs:\n        df_valid (pd.DataFrame): Validation dataset with columns file_path &amp; bboxes\n        with_shuffle (boolean): If data must be shuffled before fitting\n            This should be used if the target is not shuffled as the split_validation takes the lines in order.\n            Thus, the validation set might get classes which are not in the train set ...\n    Raises:\n        AssertionError: If the same classes are not present when comparing an already trained model\n            and a new dataset\n    '''\n##############################################\n# Manage retrain\n##############################################\n# If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n# And we save the old conf\nif self.trained:\n# Get src files to save\nsrc_files = [os.path.join(self.model_dir, \"configurations.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\nsrc_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Change model dir\nself.model_dir = self._get_model_dir()\nself.cfg.OUTPUT_DIR = self.model_dir\n# Get dst files\ndst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\ndst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Copies\nfor src, dst in zip(src_files, dst_files):\ntry:\nshutil.copyfile(src, dst)\nexcept Exception as e:\nself.logger.error(f\"Impossible to copy {src} to {dst}\")\nself.logger.error(\"We still continue ...\")\nself.logger.error(repr(e))\n##############################################\n# Prepare dataset\n# Also extract list of classes\n##############################################\n# Extract list of classes from df_train\nset_classes = set()\nfor bboxes in df_train['bboxes'].to_dict().values():\nset_classes = set_classes.union({bbox['class'] for bbox in bboxes})\nlist_classes = sorted(list(set_classes))\n# Also set dict_classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# We make sure that we have str for all classes\n# We do not raise an error, detectron2 will do it\nclasses_not_string = {cl for cl in set_classes if not isinstance(cl, str)}\nif len(classes_not_string):\nself.logger.warning(f\"Warning, the following classes are not strings : {classes_not_string}. Detectron2 requires that all classes are strings.\")\n# Validate classes if already trained, else set them\nif self.trained:\nassert self.list_classes == list_classes, \\\n            \"Error: the new dataset does not match with the already fitted model\"\nassert self.dict_classes == dict_classes, \\\n            \"Error: the new dataset does not match with the already fitted model\"\nelse:\nself.list_classes = list_classes\nself.dict_classes = dict_classes\n# Shuffle training dataset if wanted\n# If not, if no validation is provided, the train_test_split could stay in order\n# Hence, for classification task, we might have classes in the validation data that we never met in the training data\nif with_shuffle:\ndf_train = df_train.sample(frac=1.).reset_index(drop=True)\n# Manage the absence of a validation dataset\nif df_valid is None:\nself.logger.warning(f\"Attention, pas de jeu de validation. On va donc split le jeu de training (fraction valid = {self.validation_split})\")\ndf_train, df_valid = train_test_split(df_train, test_size=self.validation_split)\n# We register the train and validation datasets\nself._register_dataset(df=df_train, data_type='train')\nself._register_dataset(df=df_valid, data_type='valid')\n# We give the number of classes to the model\nself.cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(self.list_classes)\n# We give the datasets to use to the model (that we have previously registered in the catalogues)\nself.cfg.DATASETS.TRAIN = (\"dataset_train\", )\nself.cfg.DATASETS.TEST = (\"dataset_valid\", )\n# \"Translate\" the number of iterations to \"epoch\"\nnb_iter_per_epoch = max(int(len(df_train) / self.cfg.SOLVER.IMS_PER_BATCH), 1)\n# Number of iterations between two log writes\nnb_iter_log_write = max(int(nb_iter_per_epoch / self.nb_log_write_per_epoch), 1)\n# Number of iterations between two log displays\nnb_iter_log_display = max(int(nb_iter_per_epoch / self.nb_log_display_per_epoch), 1)\n# Maximal number of iterations\nnb_max_iter = self.epochs * nb_iter_per_epoch - 1\nself.cfg.SOLVER.MAX_ITER = nb_max_iter\n# We have to change the class attribute because it is used in a class method BEFORE instanciation\nTrainerRCNN.data_augmentation_params.update(self.data_augmentation_params)\n# We train\ntrainer = TrainerRCNN(self.cfg,\nlength_epoch=len(df_train),\nnb_iter_per_epoch=nb_iter_per_epoch,\nnb_iter_log_write=nb_iter_log_write,\nnb_iter_log_display=nb_iter_log_display,\nnb_log_write_per_epoch=self.nb_log_write_per_epoch,\nmin_delta_es=self.min_delta_es,\npatience=self.patience,\nrestore_best_weights=self.restore_best_weights)\n# Resume to False because we automatically save the best weights in a file,\n# and then we point self.cfg.MODEL.WEIGHTS to this file\ntrainer.resume_or_load(resume=False)\ntrainer.train()\n# Update train status\nself.trained = True\nself.nb_fit += 1\n# We change the weights path to the post-training weights\nself.cfg.MODEL.WEIGHTS = os.path.join(self.cfg.OUTPUT_DIR, 'best.pth')\n# Plots losses &amp; metrics\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._plot_metrics_and_loss()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector.predict","title":"<code>predict(df_test, write_images=False, output_dir_image=None, **kwargs)</code>","text":"<p>Predictions on test set - batch size must be equal to 1</p> <p>Parameters:</p> Name Type Description Default <code>df_test</code> <code>pd.DataFrame</code> <p>Data to predict, with a column 'file_path'</p> required <code>write_images</code> <code>bool</code> <p>If True, we write images with the predicted bboxes</p> <code>False</code> <code>output_dir_image</code> <code>str</code> <p>Path to which we want to write the predicted images (if write_images is True)</p> <code>None</code> <p>Returns:</p> Type Description <code>list&lt;list&lt;dict&gt;&gt;</code> <p>list (one entry per image) of list of bboxes</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>@utils.trained_needed\ndef predict(self, df_test: pd.DataFrame, write_images: bool = False,\noutput_dir_image: Union[str, None] = None, **kwargs) -&gt; List[List[dict]]:\n'''Predictions on test set - batch size must be equal to 1\n    Args:\n        df_test (pd.DataFrame): Data to predict, with a column 'file_path'\n        write_images (bool): If True, we write images with the predicted bboxes\n        output_dir_image (str): Path to which we want to write the predicted images (if write_images is True)\n    Returns:\n        (list&lt;list&lt;dict&gt;&gt;): list (one entry per image) of list of bboxes\n    '''\n# First we take care of the case where we want to write images\nif write_images:\n# Metadata used by the Visualizer to draw bboxes\nmetadata = Metadata(name='metadata_for_predict')\nmetadata.set(thing_classes=self.list_classes)\n# Prepare the folders to write the images\n# Manage case where output_dir_image is None\nif output_dir_image is None:\noutput_dir_image = os.path.join(self.cfg.OUTPUT_DIR, 'inference', 'images')\n# Create folder if it does not exist\nif not os.path.exists(output_dir_image):\nos.makedirs(output_dir_image)\n# We define a predictor\npredictor = DefaultPredictor(self.cfg)\n# For each image...\nlist_bbox = []\nfor file_path in df_test['file_path']:\nlist_bboxes_img = []\n# We open the image\nim = cv2.imread(file_path)\nif im is not None:\n# We predict\noutputs = predictor(im)\nif write_images:\n# We draw bboxes and we write the image\nfilename = os.path.split(file_path)[-1]\nv = Visualizer(im[:, :, ::-1], metadata, scale=1.0)\nv = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\ncv2.imwrite(os.path.join(output_dir_image, filename), v.get_image()[:, :, ::-1])\n# We get the bboxes, the scores and the classes\nboxes = np.array(outputs['instances'].get('pred_boxes').tensor.cpu())\nscores = np.array(outputs['instances'].get('scores').cpu())\nclasses = np.array(outputs['instances'].get('pred_classes').cpu())\n# For each bbox predicted\nfor idx in range(len(boxes)):\n# We put it in bbox format\ncoordinates = boxes[idx]\nbbox = {'x1': coordinates[0], 'y1': coordinates[1], 'x2': coordinates[2],\n'y2': coordinates[3], 'proba': scores[idx],\n'class': self.dict_classes[classes[idx]]}\n# An we append it\nlist_bboxes_img.append(bbox.copy())\nlist_bbox.append(list_bboxes_img.copy())\nreturn list_bbox\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Loads a model from its configuration and the weights of the network - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file pth_path (str): Path to pth file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If pth_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object pth_path is not an existing file</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Loads a model from its configuration and the weights of the network\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        pth_path (str): Path to pth file\n    Raises:\n        ValueError: If configuration_path is None\n        ValueError: If pth_path is None\n        FileNotFoundError: If the object configuration_path is not an existing file\n        FileNotFoundError: If the object pth_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\npth_path = kwargs.get('pth_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif pth_path is None:\nraise ValueError(\"The argument pth_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(pth_path):\nraise FileNotFoundError(f\"The file {pth_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'validation_split', 'min_delta_es', 'patience', 'restore_best_weights',\n'list_classes', 'dict_classes', 'data_augmentation_params', 'level_save',\n'nb_log_write_per_epoch', 'epochs', 'nb_log_display_per_epoch', 'detectron_config_base_filename',\n'detectron_config_filename', 'detectron_model_filename', 'cfg']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\n# Transform cfg into CfgNode\nself.cfg = CfgNode(init_dict=self.cfg)\n# Save best pth in new folder\nnew_pth_path = os.path.join(self.model_dir, 'best.pth')\nshutil.copyfile(pth_path, new_pth_path)\n# Reload model\nself.cfg.MODEL.WEIGHTS = new_pth_path\n# Change output path\nself.cfg.OUTPUT_DIR = self.model_dir\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.ModelDetectronFasterRcnnObjectDetector.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\n# Save attributes &amp; cfg (contains all params)\njson_data['librairie'] = 'detectron2'\njson_data['validation_split'] = self.validation_split\njson_data['min_delta_es'] = self.min_delta_es\njson_data['patience'] = self.patience\njson_data['restore_best_weights'] = self.restore_best_weights\njson_data['data_augmentation_params'] = self.data_augmentation_params\njson_data['nb_log_write_per_epoch'] = self.nb_log_write_per_epoch\njson_data['epochs'] = self.epochs\njson_data['nb_log_display_per_epoch'] = self.nb_log_display_per_epoch\njson_data['detectron_config_base_filename'] = self.detectron_config_base_filename\njson_data['detectron_config_filename'] = self.detectron_config_filename\njson_data['detectron_model_filename'] = self.detectron_model_filename\njson_data['cfg'] = self.cfg\n# We save le model with CPU so that there is no problem later\n# when we use the model (with streamlit for example)\ndevice = self.cfg.MODEL.DEVICE\nself.cfg.MODEL.DEVICE = \"cpu\"\nsuper().save(json_data=json_data)\n# We undo what we just did\nself.cfg.MODEL.DEVICE = device\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValJSONWriter","title":"<code>TrainValJSONWriter</code>","text":"<p>         Bases: <code>EventWriter</code></p> <p>Write scalars to a json file. It saves scalars as one json per line (instead of a big json) for easy parsing.</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>class TrainValJSONWriter(EventWriter):\n'''Write scalars to a json file.\n    It saves scalars as one json per line (instead of a big json) for easy parsing.\n    '''\ndef __init__(self, json_file: str, length_epoch: int, nb_iter_per_epoch: int, nb_iter_log: int = 20) -&gt; None:\n'''Initialization of the class\n        Args:\n            json_file (str): File where we save the results\n            length_epoch (int): Number of images in an \"epoch\"\n            nb_iter_per_epoch (int): Number of iterations in an \"epoch\"\n        Kwargs:\n            nb_iter_log (int): Number of iteration between two writes\n        '''\nself.json_file = json_file\nself.length_epoch = length_epoch\nself.nb_iter_per_epoch = nb_iter_per_epoch\nself.nb_iter_log = nb_iter_log\nself.open()\nself.close()\ndef open(self) -&gt; None:\nself._file_handle = PathManager.open(self.json_file, \"a\")\ndef write(self) -&gt; None:\n'''Saves the results'''\nself.open()\nstorage = get_event_storage()\nto_save = defaultdict(dict)\niteration = storage.iter\nif (iteration + 1) % self.nb_iter_log == 0:\nlosses = [loss for loss in storage.histories() if 'loss' in loss]\nlosses_valid = [loss for loss in losses if 'validation' in loss]\nlosses_train = [loss for loss in losses if loss not in losses_valid]\nfor key in losses_train:\nto_save[iteration][key] = storage.histories()[key].median(self.nb_iter_per_epoch)\nfor key in losses_valid:\nto_save[iteration][key] = storage.histories()[key].latest()\nfor itr, scalars_per_iter in to_save.items():\nscalars_per_iter[\"iteration\"] = itr\nself._file_handle.write(json.dumps(scalars_per_iter, sort_keys=True) + \"\\n\")\nself._file_handle.flush()\ntry:\nos.fsync(self._file_handle.fileno())\nexcept AttributeError:\npass\nself.close()\ndef close(self):\n'''Close the open file'''\nself._file_handle.close()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValJSONWriter.__init__","title":"<code>__init__(json_file, length_epoch, nb_iter_per_epoch, nb_iter_log=20)</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>json_file</code> <code>str</code> <p>File where we save the results</p> required <code>length_epoch</code> <code>int</code> <p>Number of images in an \"epoch\"</p> required <code>nb_iter_per_epoch</code> <code>int</code> <p>Number of iterations in an \"epoch\"</p> required Kwargs <p>nb_iter_log (int): Number of iteration between two writes</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def __init__(self, json_file: str, length_epoch: int, nb_iter_per_epoch: int, nb_iter_log: int = 20) -&gt; None:\n'''Initialization of the class\n    Args:\n        json_file (str): File where we save the results\n        length_epoch (int): Number of images in an \"epoch\"\n        nb_iter_per_epoch (int): Number of iterations in an \"epoch\"\n    Kwargs:\n        nb_iter_log (int): Number of iteration between two writes\n    '''\nself.json_file = json_file\nself.length_epoch = length_epoch\nself.nb_iter_per_epoch = nb_iter_per_epoch\nself.nb_iter_log = nb_iter_log\nself.open()\nself.close()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValJSONWriter.close","title":"<code>close()</code>","text":"<p>Close the open file</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def close(self):\n'''Close the open file'''\nself._file_handle.close()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValJSONWriter.write","title":"<code>write()</code>","text":"<p>Saves the results</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def write(self) -&gt; None:\n'''Saves the results'''\nself.open()\nstorage = get_event_storage()\nto_save = defaultdict(dict)\niteration = storage.iter\nif (iteration + 1) % self.nb_iter_log == 0:\nlosses = [loss for loss in storage.histories() if 'loss' in loss]\nlosses_valid = [loss for loss in losses if 'validation' in loss]\nlosses_train = [loss for loss in losses if loss not in losses_valid]\nfor key in losses_train:\nto_save[iteration][key] = storage.histories()[key].median(self.nb_iter_per_epoch)\nfor key in losses_valid:\nto_save[iteration][key] = storage.histories()[key].latest()\nfor itr, scalars_per_iter in to_save.items():\nscalars_per_iter[\"iteration\"] = itr\nself._file_handle.write(json.dumps(scalars_per_iter, sort_keys=True) + \"\\n\")\nself._file_handle.flush()\ntry:\nos.fsync(self._file_handle.fileno())\nexcept AttributeError:\npass\nself.close()\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValMetricPrinter","title":"<code>TrainValMetricPrinter</code>","text":"<p>         Bases: <code>EventWriter</code></p> <p>Takes care of displaying the metrics on the train (and also on the val)</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>class TrainValMetricPrinter(EventWriter):\n'''Takes care of displaying the metrics on the train (and also on the val)'''\ndef __init__(self, cfg, with_valid: bool, length_epoch: int, nb_iter_per_epoch: int, nb_iter_log: int = 20) -&gt; None:\n'''Initialize the class.\n        Args:\n            cfg: Model configuration\n            with valid (bool): If true, also displays the results on the validation dataset\n            length_epoch (int): Number of images in an \"epoch\"\n            nb_iter_per_epoch (int): Number of iterations in an \"epoch\"\n        Kwargs:\n            nb_iter_log (int): Number of iteration between two displays\n        '''\nself.logger = logging.getLogger(__name__)\nself.with_valid = with_valid\nself.nb_iter_log = nb_iter_log\nself.length_epoch = length_epoch\nself.nb_iter_per_epoch = nb_iter_per_epoch\nself.cfg = cfg\ndef write(self):\n'''Prints the wanted info'''\nstorage = get_event_storage()\niteration = storage.iter\n# Calculates a number of \"epoch\" (not necessarily an integer)\nnb_epoch = ((iteration * self.cfg.SOLVER.IMS_PER_BATCH) + 1) / self.length_epoch\nif (iteration + 1) % self.nb_iter_log == 0:\ntry:\nlr = \"{:.5g}\".format(storage.history(\"lr\").latest())\nexcept KeyError:\nlr = \"N/A\"\nif torch.cuda.is_available():\nmax_mem_mb = torch.cuda.max_memory_allocated() / 1024.0 / 1024.0\nelse:\nmax_mem_mb = None\n# NOTE: max_mem is parsed by grep in \"dev/parse_results.sh\"\nlosses = [loss for loss in storage.histories() if 'loss' in loss]\nlosses_valid = [loss for loss in losses if 'validation' in loss]\nlosses_train = [loss for loss in losses if loss not in losses_valid]\n# Logs train results\nself.logger.info(\n\" iter: {iter}, epoch: {nb_epoch}  {losses}  lr: {lr}  {memory}\".format(\niter=iteration,\nlosses=\"  \".join([\"{}: {:.4g}\".format(k, storage.histories()[k].median(self.nb_iter_per_epoch)) for k in losses_train]),\nlr=lr,\nmemory=\"max_mem: {:.0f}M\".format(max_mem_mb) if max_mem_mb is not None else \"\",\nnb_epoch=nb_epoch\n)\n)\nif self.with_valid:\n# Logs val results\nself.logger.info(\n\"VALIDATION iter: {iter}, epoch: {nb_epoch}  {losses}\".format(\niter=iteration,\nlosses=\"  \".join([\"{}: {:.4g}\".format(k, storage.histories()[k].latest()) for k in losses_valid]),\nnb_epoch=nb_epoch\n)\n)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValMetricPrinter.__init__","title":"<code>__init__(cfg, with_valid, length_epoch, nb_iter_per_epoch, nb_iter_log=20)</code>","text":"<p>Initialize the class.</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Model configuration</p> required <code>with</code> <code>valid (bool</code> <p>If true, also displays the results on the validation dataset</p> required <code>length_epoch</code> <code>int</code> <p>Number of images in an \"epoch\"</p> required <code>nb_iter_per_epoch</code> <code>int</code> <p>Number of iterations in an \"epoch\"</p> required Kwargs <p>nb_iter_log (int): Number of iteration between two displays</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def __init__(self, cfg, with_valid: bool, length_epoch: int, nb_iter_per_epoch: int, nb_iter_log: int = 20) -&gt; None:\n'''Initialize the class.\n    Args:\n        cfg: Model configuration\n        with valid (bool): If true, also displays the results on the validation dataset\n        length_epoch (int): Number of images in an \"epoch\"\n        nb_iter_per_epoch (int): Number of iterations in an \"epoch\"\n    Kwargs:\n        nb_iter_log (int): Number of iteration between two displays\n    '''\nself.logger = logging.getLogger(__name__)\nself.with_valid = with_valid\nself.nb_iter_log = nb_iter_log\nself.length_epoch = length_epoch\nself.nb_iter_per_epoch = nb_iter_per_epoch\nself.cfg = cfg\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainValMetricPrinter.write","title":"<code>write()</code>","text":"<p>Prints the wanted info</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def write(self):\n'''Prints the wanted info'''\nstorage = get_event_storage()\niteration = storage.iter\n# Calculates a number of \"epoch\" (not necessarily an integer)\nnb_epoch = ((iteration * self.cfg.SOLVER.IMS_PER_BATCH) + 1) / self.length_epoch\nif (iteration + 1) % self.nb_iter_log == 0:\ntry:\nlr = \"{:.5g}\".format(storage.history(\"lr\").latest())\nexcept KeyError:\nlr = \"N/A\"\nif torch.cuda.is_available():\nmax_mem_mb = torch.cuda.max_memory_allocated() / 1024.0 / 1024.0\nelse:\nmax_mem_mb = None\n# NOTE: max_mem is parsed by grep in \"dev/parse_results.sh\"\nlosses = [loss for loss in storage.histories() if 'loss' in loss]\nlosses_valid = [loss for loss in losses if 'validation' in loss]\nlosses_train = [loss for loss in losses if loss not in losses_valid]\n# Logs train results\nself.logger.info(\n\" iter: {iter}, epoch: {nb_epoch}  {losses}  lr: {lr}  {memory}\".format(\niter=iteration,\nlosses=\"  \".join([\"{}: {:.4g}\".format(k, storage.histories()[k].median(self.nb_iter_per_epoch)) for k in losses_train]),\nlr=lr,\nmemory=\"max_mem: {:.0f}M\".format(max_mem_mb) if max_mem_mb is not None else \"\",\nnb_epoch=nb_epoch\n)\n)\nif self.with_valid:\n# Logs val results\nself.logger.info(\n\"VALIDATION iter: {iter}, epoch: {nb_epoch}  {losses}\".format(\niter=iteration,\nlosses=\"  \".join([\"{}: {:.4g}\".format(k, storage.histories()[k].latest()) for k in losses_valid]),\nnb_epoch=nb_epoch\n)\n)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN","title":"<code>TrainerRCNN</code>","text":"<p>         Bases: <code>DefaultTrainer</code></p> <p>We overload the class DefaultTraine in order to: - change when we save the metrics - use the COCOevaluator - do data augmentation - save the validation metrics when training - do early stopping</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>class TrainerRCNN(DefaultTrainer):\n'''We overload the class DefaultTraine in order to:\n        - change when we save the metrics\n        - use the COCOevaluator\n        - do data augmentation\n        - save the validation metrics when training\n        - do early stopping\n    '''\n# We define a class attribute because it is used by a class method\ndata_augmentation_params = {}\ndef __init__(self, cfg, length_epoch: int, nb_iter_per_epoch: int,\nnb_iter_log_write: int = 20, nb_iter_log_display: int = 20, nb_log_write_per_epoch: int = 1,\nmin_delta_es: float = 0., patience: int = 0, restore_best_weights: bool = False) -&gt; None:\n'''Initialize the Trainer\n        Args:\n            cfg: Configuration to use\n            length_epoch (int): Number of image in an epoch\n            nb_iter_per_epoch (int): Number of iterations in an epoch\n        Kwargs:\n            nb_iter_log_write (int): Number of iterations between two log writes (losses\n                on train and validation datasets)\n            nb_iter_log_display (int): Number of iterations between two log displays (losses\n                on train dataset only)\n            nb_log_write_per_epoch (int): Number of metrics logs written during\n                one epoch (losses for the train and the valid)\n            min_delta_es (float): Minimal change in losses to be considered an amelioration for early stopping\n            patience (int): Early stopping patience. Put to 0 to disable early stopping\n            restore_best_weights (bool): If True, when the training is done, save the model with the best\n                loss on the validation dataset instead of the last model (even if early stopping is disabled)\n        '''\n# We must add the definition of some attributes before the super() because we redefine the method\n# build_hooks which is called by super().__init__ and uses them\nself.nb_iter_log_write = nb_iter_log_write\nself.nb_iter_log_display = nb_iter_log_display\nself.length_epoch = length_epoch\nself.output_dir = cfg.OUTPUT_DIR\nself.nb_iter_per_epoch = nb_iter_per_epoch\nsuper().__init__(cfg)\n# Params early stopping\nself.min_delta_es = min_delta_es\nself.patience = patience\nself.restore_best_weights = restore_best_weights\nself.best_loss = np.inf\nself.best_epoch = 0\n# Misc.\nself.nb_log_write_per_epoch = nb_log_write_per_epoch\n@classmethod\ndef build_evaluator(self, cfg, dataset_name: str) -&gt; COCOEvaluator:\n'''We redefine the method in order to use the COCOevaluator\n        Args:\n            cfg: Training configuration\n            dataset_name (str): Name of the dataset\n        Returns:\n            Evaluator to use\n        '''\noutput_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\nreturn COCOEvaluator(dataset_name, cfg, True, output_folder)\n@classmethod\ndef build_train_loader(self, cfg):\n'''We redefine the method in order to use our own data augmentation\n        Args:\n            cfg: Training configuration\n        Returns:\n            train loader to use, with our own data augmentation\n        '''\nhorizontal_flip = self.data_augmentation_params.get('horizontal_flip', False)\nvertical_flip = self.data_augmentation_params.get('vertical_flip', False)\nrot_90 = self.data_augmentation_params.get('rot_90', False)\nmapper = partial(data_augmentation_mapper, horizontal_flip=horizontal_flip, vertical_flip=vertical_flip, rot_90=rot_90)\nreturn build_detection_train_loader(cfg, mapper=mapper)\ndef train(self):\n'''Run training.\n        Returns:\n            OrderedDict of results, if evaluation is enabled. Otherwise None.\n        '''\nlogger = logging.getLogger(__name__)\nlogger.info(\"Starting training from iteration {}\".format(self.start_iter))\nself.iter = self.start_iter\n# From https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/train_loop.html#TrainerBase\n# The difference is that we get the result of the early stopping and we stop if triggered\n# We also save the final model if restore_best_weights is set to False\nwith EventStorage(self.start_iter) as self.storage:\ntry:\nself.before_train()\nwhile self.iter &lt; self.max_iter:  # We substitute the for by a while in order to break easily with the early stopping\nself.before_step()\nself.run_step()\ntest_early_stopping = self.after_step()\nif test_early_stopping:\nlogger.info(\"Early stopping\")\n# We change the number of maximum iteration if we want to stop early\n# Warning, some hooks are defined with anoter self.max_iter\n# since build_hooks is called by the __init__ of the trainer\nself.max_iter = self.iter\nelse:\nself.iter += 1\n# self.iter == self.max_iter can be used by `after_train` to\n# tell whether the training successfully finished or failed\n# due to exceptions.\nexcept Exception:\nlogger.exception(\"Exception during training:\")\nraise\nfinally:\nself.after_train()\nself.write_model_final()  # * NEW *\n# From https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/defaults.html#DefaultTrainer\nif len(self.cfg.TEST.EXPECTED_RESULTS) and comm.is_main_process():\nassert hasattr(\nself, \"_last_eval_results\"\n), \"No evaluation results obtained during training!\"\nverify_results(self.cfg, self._last_eval_results)\nreturn self._last_eval_results\ndef write_model_final(self) -&gt; None:\n'''If self.restore_best_weights == False, no model is saved during\n        training. Thus we save the final model with the name best.pth\n        '''\npath_model_best = os.path.join(self.cfg.OUTPUT_DIR, 'best.pth')\nif not os.path.exists(path_model_best):\nself.checkpointer.save(\"best\")\ndef after_step(self) -&gt; bool:\n'''Function triggered after each step\n        Returns:\n            bool: If early stopping has been triggered\n        '''\n# We trigger the hooks\nfor h in self._hooks:\nh.after_step()\n# We add the early stopping to the hooks\ntest_early_stopping = self.early_stopping()\nreturn test_early_stopping\ndef early_stopping(self) -&gt; bool:\n'''Triggers if the condition for early stopping are met. We think in term of epoch. Thus\n        the patience is indeed the number of epochs without amelioration\n        Returns:\n            bool: If early stopping has been triggered\n        '''\n# We get all the validation losses\nval_loss_values = self.storage.histories()['validation_total_loss'].values()\n# We only keep those that are at the end of an epoch\nval_loss_values_epoch = val_loss_values[self.nb_log_write_per_epoch-1::self.nb_log_write_per_epoch]\nval_loss_values_epoch = [x[0] for x in val_loss_values_epoch]\nif len(val_loss_values_epoch):\n# We get the minimal loss and the corresponding epoch\nmin_loss = np.min(val_loss_values_epoch)\nmin_epoch = np.argmin(val_loss_values_epoch)\n# If the loss is better, we save the new minimal loss and the corresponding epoch\nif min_loss &lt; self.best_loss - self.min_delta_es:\nself.best_loss = min_loss\nself.best_epoch = min_epoch\n# We save the model with the best weights\nif self.restore_best_weights:\nself.checkpointer.save(\"best\")\n# If the patience is up, we trigger early stopping\nif self.best_epoch + self.patience + 1 &lt;= len(val_loss_values_epoch) and self.patience &gt; 0:\nreturn True\n# Otherwise we do not trigger it\nreturn False\ndef build_hooks(self) -&gt; list:\n'''Build a list of default hooks, including timing, evaluation,\n        checkpointing, lr scheduling, precise BN, writing events.\n        We rewrite this methos (instead of overloading it) so that we can change\n        when we save metrics\n        From : https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/defaults.html#DefaultTrainer\n        Warning, we deleted the hook on the checkpoint, the early stopping hook takes care of it now !\n        Returns:\n            list[HookBase]:\n        '''\ncfg = self.cfg.clone()\ncfg.defrost()\ncfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\nret = [\nmodule_hooks.IterationTimer(),\nmodule_hooks.LRScheduler()\n]\n# We add our custom hook in order to add validation losses\nret.append(LossEvalHook(\nself.nb_iter_log_write,\nself.model,\nbuild_detection_test_loader(  # Use of our decorator of build_detection_test_loader\nself.cfg,\nself.cfg.DATASETS.TEST[0],\nDatasetMapper(self.cfg, True)  # We keep is_train to true to keep the bboxes (needed for losses calculations)\n)\n))\nif comm.is_main_process():\n# Here we take care of the writers and the printers\n# There is a printer which displays the results of train and val when we reach\n# a particular number of iteration (self.nb_iter_log_write)\n# There is a writer which writes the results of train and val when we reach\n# a particular number of iteration (self.nb_iter_log_write)\nret.append(module_hooks.PeriodicWriter([TrainValMetricPrinter(cfg=self.cfg,\nwith_valid=True,\nlength_epoch=self.length_epoch,\nnb_iter_per_epoch=self.nb_iter_per_epoch,\nnb_iter_log=self.nb_iter_log_write),\nTrainValJSONWriter(os.path.join(self.output_dir, \"metrics.json\"),\nself.length_epoch,\nself.nb_iter_per_epoch,\nself.nb_iter_log_write)\n], period=self.nb_iter_log_write))\n# There is a printer which displays the results of train when we reach\n# a particular number of iteration (self.nb_iter_log_display)\nret.append(module_hooks.PeriodicWriter([TrainValMetricPrinter(cfg=self.cfg,\nwith_valid=False,\nlength_epoch=self.length_epoch,\nnb_iter_per_epoch=self.nb_iter_per_epoch,\nnb_iter_log=self.nb_iter_log_display)], period=1))\nreturn ret\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.__init__","title":"<code>__init__(cfg, length_epoch, nb_iter_per_epoch, nb_iter_log_write=20, nb_iter_log_display=20, nb_log_write_per_epoch=1, min_delta_es=0.0, patience=0, restore_best_weights=False)</code>","text":"<p>Initialize the Trainer</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Configuration to use</p> required <code>length_epoch</code> <code>int</code> <p>Number of image in an epoch</p> required <code>nb_iter_per_epoch</code> <code>int</code> <p>Number of iterations in an epoch</p> required Kwargs <p>nb_iter_log_write (int): Number of iterations between two log writes (losses     on train and validation datasets) nb_iter_log_display (int): Number of iterations between two log displays (losses     on train dataset only) nb_log_write_per_epoch (int): Number of metrics logs written during     one epoch (losses for the train and the valid) min_delta_es (float): Minimal change in losses to be considered an amelioration for early stopping patience (int): Early stopping patience. Put to 0 to disable early stopping restore_best_weights (bool): If True, when the training is done, save the model with the best     loss on the validation dataset instead of the last model (even if early stopping is disabled)</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def __init__(self, cfg, length_epoch: int, nb_iter_per_epoch: int,\nnb_iter_log_write: int = 20, nb_iter_log_display: int = 20, nb_log_write_per_epoch: int = 1,\nmin_delta_es: float = 0., patience: int = 0, restore_best_weights: bool = False) -&gt; None:\n'''Initialize the Trainer\n    Args:\n        cfg: Configuration to use\n        length_epoch (int): Number of image in an epoch\n        nb_iter_per_epoch (int): Number of iterations in an epoch\n    Kwargs:\n        nb_iter_log_write (int): Number of iterations between two log writes (losses\n            on train and validation datasets)\n        nb_iter_log_display (int): Number of iterations between two log displays (losses\n            on train dataset only)\n        nb_log_write_per_epoch (int): Number of metrics logs written during\n            one epoch (losses for the train and the valid)\n        min_delta_es (float): Minimal change in losses to be considered an amelioration for early stopping\n        patience (int): Early stopping patience. Put to 0 to disable early stopping\n        restore_best_weights (bool): If True, when the training is done, save the model with the best\n            loss on the validation dataset instead of the last model (even if early stopping is disabled)\n    '''\n# We must add the definition of some attributes before the super() because we redefine the method\n# build_hooks which is called by super().__init__ and uses them\nself.nb_iter_log_write = nb_iter_log_write\nself.nb_iter_log_display = nb_iter_log_display\nself.length_epoch = length_epoch\nself.output_dir = cfg.OUTPUT_DIR\nself.nb_iter_per_epoch = nb_iter_per_epoch\nsuper().__init__(cfg)\n# Params early stopping\nself.min_delta_es = min_delta_es\nself.patience = patience\nself.restore_best_weights = restore_best_weights\nself.best_loss = np.inf\nself.best_epoch = 0\n# Misc.\nself.nb_log_write_per_epoch = nb_log_write_per_epoch\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.after_step","title":"<code>after_step()</code>","text":"<p>Function triggered after each step</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>If early stopping has been triggered</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def after_step(self) -&gt; bool:\n'''Function triggered after each step\n    Returns:\n        bool: If early stopping has been triggered\n    '''\n# We trigger the hooks\nfor h in self._hooks:\nh.after_step()\n# We add the early stopping to the hooks\ntest_early_stopping = self.early_stopping()\nreturn test_early_stopping\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.build_evaluator","title":"<code>build_evaluator(cfg, dataset_name)</code>  <code>classmethod</code>","text":"<p>We redefine the method in order to use the COCOevaluator</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Training configuration</p> required <code>dataset_name</code> <code>str</code> <p>Name of the dataset</p> required <p>Returns:</p> Type Description <code>COCOEvaluator</code> <p>Evaluator to use</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>@classmethod\ndef build_evaluator(self, cfg, dataset_name: str) -&gt; COCOEvaluator:\n'''We redefine the method in order to use the COCOevaluator\n    Args:\n        cfg: Training configuration\n        dataset_name (str): Name of the dataset\n    Returns:\n        Evaluator to use\n    '''\noutput_folder = os.path.join(cfg.OUTPUT_DIR, \"inference\")\nreturn COCOEvaluator(dataset_name, cfg, True, output_folder)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.build_hooks","title":"<code>build_hooks()</code>","text":"<p>Build a list of default hooks, including timing, evaluation, checkpointing, lr scheduling, precise BN, writing events.</p> <p>We rewrite this methos (instead of overloading it) so that we can change when we save metrics From : https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/defaults.html#DefaultTrainer</p> <p>Warning, we deleted the hook on the checkpoint, the early stopping hook takes care of it now !</p> <p>Returns:</p> Type Description <code>list</code> <p>list[HookBase]:</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def build_hooks(self) -&gt; list:\n'''Build a list of default hooks, including timing, evaluation,\n    checkpointing, lr scheduling, precise BN, writing events.\n    We rewrite this methos (instead of overloading it) so that we can change\n    when we save metrics\n    From : https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/defaults.html#DefaultTrainer\n    Warning, we deleted the hook on the checkpoint, the early stopping hook takes care of it now !\n    Returns:\n        list[HookBase]:\n    '''\ncfg = self.cfg.clone()\ncfg.defrost()\ncfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN\nret = [\nmodule_hooks.IterationTimer(),\nmodule_hooks.LRScheduler()\n]\n# We add our custom hook in order to add validation losses\nret.append(LossEvalHook(\nself.nb_iter_log_write,\nself.model,\nbuild_detection_test_loader(  # Use of our decorator of build_detection_test_loader\nself.cfg,\nself.cfg.DATASETS.TEST[0],\nDatasetMapper(self.cfg, True)  # We keep is_train to true to keep the bboxes (needed for losses calculations)\n)\n))\nif comm.is_main_process():\n# Here we take care of the writers and the printers\n# There is a printer which displays the results of train and val when we reach\n# a particular number of iteration (self.nb_iter_log_write)\n# There is a writer which writes the results of train and val when we reach\n# a particular number of iteration (self.nb_iter_log_write)\nret.append(module_hooks.PeriodicWriter([TrainValMetricPrinter(cfg=self.cfg,\nwith_valid=True,\nlength_epoch=self.length_epoch,\nnb_iter_per_epoch=self.nb_iter_per_epoch,\nnb_iter_log=self.nb_iter_log_write),\nTrainValJSONWriter(os.path.join(self.output_dir, \"metrics.json\"),\nself.length_epoch,\nself.nb_iter_per_epoch,\nself.nb_iter_log_write)\n], period=self.nb_iter_log_write))\n# There is a printer which displays the results of train when we reach\n# a particular number of iteration (self.nb_iter_log_display)\nret.append(module_hooks.PeriodicWriter([TrainValMetricPrinter(cfg=self.cfg,\nwith_valid=False,\nlength_epoch=self.length_epoch,\nnb_iter_per_epoch=self.nb_iter_per_epoch,\nnb_iter_log=self.nb_iter_log_display)], period=1))\nreturn ret\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.build_train_loader","title":"<code>build_train_loader(cfg)</code>  <code>classmethod</code>","text":"<p>We redefine the method in order to use our own data augmentation</p> <p>Parameters:</p> Name Type Description Default <code>cfg</code> <p>Training configuration</p> required <p>Returns:</p> Type Description <p>train loader to use, with our own data augmentation</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>@classmethod\ndef build_train_loader(self, cfg):\n'''We redefine the method in order to use our own data augmentation\n    Args:\n        cfg: Training configuration\n    Returns:\n        train loader to use, with our own data augmentation\n    '''\nhorizontal_flip = self.data_augmentation_params.get('horizontal_flip', False)\nvertical_flip = self.data_augmentation_params.get('vertical_flip', False)\nrot_90 = self.data_augmentation_params.get('rot_90', False)\nmapper = partial(data_augmentation_mapper, horizontal_flip=horizontal_flip, vertical_flip=vertical_flip, rot_90=rot_90)\nreturn build_detection_train_loader(cfg, mapper=mapper)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.early_stopping","title":"<code>early_stopping()</code>","text":"<p>Triggers if the condition for early stopping are met. We think in term of epoch. Thus the patience is indeed the number of epochs without amelioration</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>If early stopping has been triggered</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def early_stopping(self) -&gt; bool:\n'''Triggers if the condition for early stopping are met. We think in term of epoch. Thus\n    the patience is indeed the number of epochs without amelioration\n    Returns:\n        bool: If early stopping has been triggered\n    '''\n# We get all the validation losses\nval_loss_values = self.storage.histories()['validation_total_loss'].values()\n# We only keep those that are at the end of an epoch\nval_loss_values_epoch = val_loss_values[self.nb_log_write_per_epoch-1::self.nb_log_write_per_epoch]\nval_loss_values_epoch = [x[0] for x in val_loss_values_epoch]\nif len(val_loss_values_epoch):\n# We get the minimal loss and the corresponding epoch\nmin_loss = np.min(val_loss_values_epoch)\nmin_epoch = np.argmin(val_loss_values_epoch)\n# If the loss is better, we save the new minimal loss and the corresponding epoch\nif min_loss &lt; self.best_loss - self.min_delta_es:\nself.best_loss = min_loss\nself.best_epoch = min_epoch\n# We save the model with the best weights\nif self.restore_best_weights:\nself.checkpointer.save(\"best\")\n# If the patience is up, we trigger early stopping\nif self.best_epoch + self.patience + 1 &lt;= len(val_loss_values_epoch) and self.patience &gt; 0:\nreturn True\n# Otherwise we do not trigger it\nreturn False\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.train","title":"<code>train()</code>","text":"<p>Run training.</p> <p>Returns:</p> Type Description <p>OrderedDict of results, if evaluation is enabled. Otherwise None.</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def train(self):\n'''Run training.\n    Returns:\n        OrderedDict of results, if evaluation is enabled. Otherwise None.\n    '''\nlogger = logging.getLogger(__name__)\nlogger.info(\"Starting training from iteration {}\".format(self.start_iter))\nself.iter = self.start_iter\n# From https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/train_loop.html#TrainerBase\n# The difference is that we get the result of the early stopping and we stop if triggered\n# We also save the final model if restore_best_weights is set to False\nwith EventStorage(self.start_iter) as self.storage:\ntry:\nself.before_train()\nwhile self.iter &lt; self.max_iter:  # We substitute the for by a while in order to break easily with the early stopping\nself.before_step()\nself.run_step()\ntest_early_stopping = self.after_step()\nif test_early_stopping:\nlogger.info(\"Early stopping\")\n# We change the number of maximum iteration if we want to stop early\n# Warning, some hooks are defined with anoter self.max_iter\n# since build_hooks is called by the __init__ of the trainer\nself.max_iter = self.iter\nelse:\nself.iter += 1\n# self.iter == self.max_iter can be used by `after_train` to\n# tell whether the training successfully finished or failed\n# due to exceptions.\nexcept Exception:\nlogger.exception(\"Exception during training:\")\nraise\nfinally:\nself.after_train()\nself.write_model_final()  # * NEW *\n# From https://detectron2.readthedocs.io/en/latest/_modules/detectron2/engine/defaults.html#DefaultTrainer\nif len(self.cfg.TEST.EXPECTED_RESULTS) and comm.is_main_process():\nassert hasattr(\nself, \"_last_eval_results\"\n), \"No evaluation results obtained during training!\"\nverify_results(self.cfg, self._last_eval_results)\nreturn self._last_eval_results\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.TrainerRCNN.write_model_final","title":"<code>write_model_final()</code>","text":"<p>If self.restore_best_weights == False, no model is saved during training. Thus we save the final model with the name best.pth</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def write_model_final(self) -&gt; None:\n'''If self.restore_best_weights == False, no model is saved during\n    training. Thus we save the final model with the name best.pth\n    '''\npath_model_best = os.path.join(self.cfg.OUTPUT_DIR, 'best.pth')\nif not os.path.exists(path_model_best):\nself.checkpointer.save(\"best\")\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_detectron_faster_rcnn/#template_vision.models_training.object_detectors.model_detectron_faster_rcnn.data_augmentation_mapper","title":"<code>data_augmentation_mapper(dataset_dict, horizontal_flip=False, vertical_flip=False, rot_90=False)</code>","text":"<p>Applies the data augmentation on data</p> <p>Parameters:</p> Name Type Description Default <code>dataset_dict</code> <code>dict) </code> <p>Data dictionary containing the images on which to do data augmentation</p> required <code>horizontal_flip</code> <code>bool) </code> <p>If True, can do horizontal flip (with 0.5 proba)</p> <code>False</code> <code>vertical_flip</code> <code>bool) </code> <p>If True, can do vertical flip (with 0.5 proba)</p> <code>False</code> <code>rot_90</code> <code>bool) </code> <p>If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)</p> <code>False</code> <p>Returns:</p> Type Description <code>dict</code> <p>The dictionary after data augmentation</p> Source code in <code>template_vision/models_training/object_detectors/model_detectron_faster_rcnn.py</code> <pre><code>def data_augmentation_mapper(dataset_dict: dict, horizontal_flip: bool = False,\nvertical_flip: bool = False, rot_90: bool = False) -&gt; dict:\n'''Applies the data augmentation on data\n    Args:\n        dataset_dict (dict) : Data dictionary containing the images on which to do data augmentation\n        horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)\n        vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)\n        rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n    Returns:\n        The dictionary after data augmentation\n    '''\ndataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below\nimage = detection_utils.read_image(dataset_dict[\"file_name\"], format=\"BGR\")  # Reads the image\n# Add transformations\ntransform_list = []\nif rot_90:\nangle = np.random.choice([0, 90, 180, 270], 1)[0]\ntransform_list.append(T.RandomRotation(angle=[angle, angle]))\nif horizontal_flip:\ntransform_list.append(T.RandomFlip(prob=0.5, horizontal=True, vertical=False))\nif vertical_flip:\ntransform_list.append(T.RandomFlip(prob=0.5, horizontal=False, vertical=True))\n# Apply transformations to the image\nimage, transforms = T.apply_transform_gens(transform_list, image)\ndataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1).astype(\"float32\"))\ndataset_dict['height'], dataset_dict['width'] = dataset_dict['image'].shape[1:]\n# Update of the bboxes\nannos = [\ndetection_utils.transform_instance_annotations(obj, transforms, image.shape[:2])\nfor obj in dataset_dict.pop(\"annotations\")\nif obj.get(\"iscrowd\", 0) == 0\n]\n# Transform to \"instance\" (suitable format for detectron2)\ninstances = detection_utils.annotations_to_instances(annos, image.shape[:2])\ndataset_dict[\"instances\"] = detection_utils.filter_empty_instances(instances)\n# Return\nreturn dataset_dict\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/","title":"Model keras faster rcnn","text":""},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.CustomGeneratorClassifier","title":"<code>CustomGeneratorClassifier</code>","text":"<p>         Bases: <code>Iterator</code></p> <p>Classifier generator</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>class CustomGeneratorClassifier(Iterator):\n'''Classifier generator'''\ndef __init__(self, img_data_list: List[dict], batch_size: int, shuffle: bool, seed: Union[int, None],\nmodel, shared_model_trainable: bool = False, horizontal_flip: bool = False, vertical_flip: bool = False,\nrot_90: bool = False, data_type: str = 'train', with_img_data: bool = False, **kwargs) -&gt; None:\n'''Initialization of the generator for the classifier\n        Args:\n            img_data_list (list&lt;dict&gt;): Data list (the dictionaries containing file path and bboxes)\n            batch_size (int): Size of the batches to generate\n            shuffle (bool): If the data should be shuffled\n            seed (int): Random seed to use\n            model: Link to the model (nested structure) to access to the other methods of this script\n            shared_model_trainable (bool): If the shared model is set to trainable, we must clone the RPN\n                in order not to worsen the prediction quality of the ROIs (input of the classifier)\n            data_type (str): Data type 'train', 'valid' or 'test'\n            with_img_data (bool): If True, also gives img_data as output\n        Kwargs:\n            horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)\n            vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)\n            rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n        '''\n# Set is_test\nif data_type == 'test':\nself.is_test = True\nelse:\nself.is_test = False\n# Si test, on force batch size \u00e0 1\nif self.is_test:\nbatch_size = 1\n# Super init.\nsuper().__init__(n=len(img_data_list), batch_size=batch_size, shuffle=shuffle, seed=seed)\n# Set params\nself.img_data_list = img_data_list\nself.model = model\nself.horizontal_flip = horizontal_flip\nself.vertical_flip = vertical_flip\nself.rot_90 = rot_90\nself.with_img_data = with_img_data\n# Manage shared_model_trainable\nself.shared_model_trainable = shared_model_trainable\nif self.shared_model_trainable:\nself.rpn_clone = clone_model(self.model.model_rpn)\nself.rpn_clone.set_weights(self.model.model_rpn.get_weights())\nelse:\nself.rpn_clone = None\n# Manage data augmentation &amp; test\nif self.is_test and any([param for param in [self.horizontal_flip, self.vertical_flip, self.rot_90]]):\nmodel.logger.warning(\"Warning, Data Augmentation detected on the test set! This is certainly not desired!\")\ndef _get_batches_of_transformed_samples(self, index_array: np.ndarray):\n'''Gets a batch of inputs for the classifier model\n        Warning, in order to be sure to have the same shape between the various images, we pad them with black pixels\n        Args:\n            index_array (np.ndarray): List of indices to include in a batch\n        Returns:\n            np.ndarray: Data batch to be used by the classifier model (x)\n                # Shape (bach_size, max_resized_height, max_resized_width, 3)\n            dict&lt;np.ndarray&gt;: Data batch to be used by the classifier model (y)\n                dense_class: Classifier - target of the classifier\n                    # Shape (bach_size, feature_map_height, feature_map_width, 2 * nb_anchors)\n                rpn_class: Classifier - target of the regressor\n                    # Shape (bach_size, feature_map_height, feature_map_width, 2 * 4 * nb_anchors)\n            if self.with_img_data : the metadata of the images\n        '''\n# For each selected index, get image &amp; preprocess it, and retrieve max resized_width &amp; resized_height\nbatch_prepared_img_data = []\nmax_resized_width = 0\nmax_resized_height = 0\nfor ind in index_array:\nimg_data = copy.deepcopy(self.img_data_list[ind])\nprepared_img_data = self.model._generate_images_with_bboxes(img_data, horizontal_flip=self.horizontal_flip,\nvertical_flip=self.vertical_flip, rot_90=self.rot_90)\nbatch_prepared_img_data.append(prepared_img_data)\nmax_resized_width = max(max_resized_width, prepared_img_data['resized_width'])\nmax_resized_height = max(max_resized_height, prepared_img_data['resized_height'])\n# Set format X images\nbatch_img_shape = (len(index_array), max_resized_height, max_resized_width, 3)\nbatch_x_img = np.zeros(batch_img_shape)  # Def. pixels noirs\n# Get input images\nfor ind, prepared_img_data in enumerate(batch_prepared_img_data):\nprepared_img_data['batch_width'] = max_resized_width\nprepared_img_data['batch_height'] = max_resized_height\nimg = prepared_img_data['img']\nbatch_x_img[ind, :img.shape[0], :img.shape[1], :] = img\n# Get the input ROIs - need to be formatted\nif self.shared_model_trainable:\nrpn_predictions_cls, rpn_predictions_regr = self.rpn_clone.predict(batch_x_img)\nelse:\nrpn_predictions_cls, rpn_predictions_regr = self.model.model_rpn.predict(batch_x_img)\nrois_coordinates = utils_object_detectors.get_roi_from_rpn_predictions(self.model, batch_prepared_img_data,\nrpn_predictions_cls, rpn_predictions_regr)\n# If test, we return the images and the associated ROIs\nif self.is_test:\n# Get the ROIs with the right format\nbatch_x_rois = utils_object_detectors.get_classifier_test_inputs(rois_coordinates)\nif self.with_img_data:\nreturn {'input_img': batch_x_img, 'input_rois': batch_x_rois}, batch_prepared_img_data\nelse:  # Usually, this case never happens\nreturn {'input_img': batch_x_img, 'input_rois': batch_x_rois}\n# Otherwise, we also manage the targets y\nelse:\n# Get the inputs and the targets of the classifier\nbatch_x_rois, Y1_classifier, Y2_classifier = utils_object_detectors.get_classifier_train_inputs_and_targets(self.model, batch_prepared_img_data, rois_coordinates)\nif batch_x_rois is None:\nself.model.logger.warning(\"We have an image batch without ROI !!!\")\nreturn next(self)  # We try another batch ...\nif self.with_img_data:\nreturn {'input_img': batch_x_img, 'input_rois': batch_x_rois}, {'dense_class': Y1_classifier, 'dense_regr': Y2_classifier}, batch_prepared_img_data\nelse:\nreturn {'input_img': batch_x_img, 'input_rois': batch_x_rois}, {'dense_class': Y1_classifier, 'dense_regr': Y2_classifier}\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.CustomGeneratorClassifier.__init__","title":"<code>__init__(img_data_list, batch_size, shuffle, seed, model, shared_model_trainable=False, horizontal_flip=False, vertical_flip=False, rot_90=False, data_type='train', with_img_data=False, **kwargs)</code>","text":"<p>Initialization of the generator for the classifier</p> <p>Parameters:</p> Name Type Description Default <code>img_data_list</code> <code>list&lt;dict&gt;</code> <p>Data list (the dictionaries containing file path and bboxes)</p> required <code>batch_size</code> <code>int</code> <p>Size of the batches to generate</p> required <code>shuffle</code> <code>bool</code> <p>If the data should be shuffled</p> required <code>seed</code> <code>int</code> <p>Random seed to use</p> required <code>model</code> <p>Link to the model (nested structure) to access to the other methods of this script</p> required <code>shared_model_trainable</code> <code>bool</code> <p>If the shared model is set to trainable, we must clone the RPN in order not to worsen the prediction quality of the ROIs (input of the classifier)</p> <code>False</code> <code>data_type</code> <code>str</code> <p>Data type 'train', 'valid' or 'test'</p> <code>'train'</code> <code>with_img_data</code> <code>bool</code> <p>If True, also gives img_data as output</p> <code>False</code> Kwargs <p>horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba) vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba) rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def __init__(self, img_data_list: List[dict], batch_size: int, shuffle: bool, seed: Union[int, None],\nmodel, shared_model_trainable: bool = False, horizontal_flip: bool = False, vertical_flip: bool = False,\nrot_90: bool = False, data_type: str = 'train', with_img_data: bool = False, **kwargs) -&gt; None:\n'''Initialization of the generator for the classifier\n    Args:\n        img_data_list (list&lt;dict&gt;): Data list (the dictionaries containing file path and bboxes)\n        batch_size (int): Size of the batches to generate\n        shuffle (bool): If the data should be shuffled\n        seed (int): Random seed to use\n        model: Link to the model (nested structure) to access to the other methods of this script\n        shared_model_trainable (bool): If the shared model is set to trainable, we must clone the RPN\n            in order not to worsen the prediction quality of the ROIs (input of the classifier)\n        data_type (str): Data type 'train', 'valid' or 'test'\n        with_img_data (bool): If True, also gives img_data as output\n    Kwargs:\n        horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)\n        vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)\n        rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n    '''\n# Set is_test\nif data_type == 'test':\nself.is_test = True\nelse:\nself.is_test = False\n# Si test, on force batch size \u00e0 1\nif self.is_test:\nbatch_size = 1\n# Super init.\nsuper().__init__(n=len(img_data_list), batch_size=batch_size, shuffle=shuffle, seed=seed)\n# Set params\nself.img_data_list = img_data_list\nself.model = model\nself.horizontal_flip = horizontal_flip\nself.vertical_flip = vertical_flip\nself.rot_90 = rot_90\nself.with_img_data = with_img_data\n# Manage shared_model_trainable\nself.shared_model_trainable = shared_model_trainable\nif self.shared_model_trainable:\nself.rpn_clone = clone_model(self.model.model_rpn)\nself.rpn_clone.set_weights(self.model.model_rpn.get_weights())\nelse:\nself.rpn_clone = None\n# Manage data augmentation &amp; test\nif self.is_test and any([param for param in [self.horizontal_flip, self.vertical_flip, self.rot_90]]):\nmodel.logger.warning(\"Warning, Data Augmentation detected on the test set! This is certainly not desired!\")\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.CustomGeneratorRpn","title":"<code>CustomGeneratorRpn</code>","text":"<p>         Bases: <code>Iterator</code></p> <p>RPN generator</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>class CustomGeneratorRpn(Iterator):\n'''RPN generator'''\ndef __init__(self, img_data_list: List[dict], batch_size: int, shuffle: bool, seed: Union[int, None],\nmodel, horizontal_flip: bool = False, vertical_flip: bool = False,\nrot_90: bool = False, data_type: str = 'train', with_img_data: bool = False, **kwargs) -&gt; None:\n'''Initialization of the RPN generator\n        Args:\n            img_data_list (list&lt;dict&gt;): Data list (the dictionaries containing file path and bboxes)\n            batch_size (int): Size of the batches to generate\n            shuffle (bool): If the data should be shuffled\n            seed (int): Random seed to use\n            model: Link to the model (nested structure) to access to the other methods of this script\n            data_type (str): Data type 'train', 'valid' or 'test'\n            with_img_data (bool): If True, also gives img_data as output\n        Kwargs:\n            horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)\n            vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)\n            rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n        '''\n# Set is_test\nif data_type == 'test':\nself.is_test = True\nelse:\nself.is_test = False\n# If test, the batch size is set to 1\nif self.is_test:\nbatch_size = 1\n# Super init.\nsuper().__init__(n=len(img_data_list), batch_size=batch_size, shuffle=shuffle, seed=seed)\n# Set params\nself.img_data_list = img_data_list\nself.model = model\nself.horizontal_flip = horizontal_flip\nself.vertical_flip = vertical_flip\nself.rot_90 = rot_90\nself.with_img_data = with_img_data\n# Manage data augmentation &amp; test\nif self.is_test and any([param for param in [self.horizontal_flip, self.vertical_flip, self.rot_90]]):\nmodel.logger.warning(\"Warning, data augmentation on the test dataset ! It is most certainly a mistake !\")\ndef _get_batches_of_transformed_samples(self, index_array: np.ndarray) -&gt; tuple:\n'''Gets a batch of inputs for the RPN model\n        Warning, in order to be sure to have the same shape between the various images, we pad them with black pixels\n        Args:\n            index_array (np.ndarray): List of indices to include in a batch\n        Returns:\n            np.ndarray: Data batch to be used by the RPN model (x)\n                # Shape (bach_size, max_resized_height, max_resized_width, 3)\n            dict&lt;np.ndarray&gt;: Data batch to be used by the RPN model (y)\n                rpn_class: RPN - target of the classifier\n                    # Shape (bach_size, feature_map_height, feature_map_width, 2 * nb_anchors)\n                rpn_class: RPN - target of the regressor\n                    # Shape (bach_size, feature_map_height, feature_map_width, 2 * 4 * nb_anchors)\n            if self.with_img_data : the metadata of the images\n        '''\n# For each selected index, get image &amp; preprocess it, and retrieve max resized_width &amp; resized_height\nbatch_prepared_img_data = []\nmax_resized_width = 0\nmax_resized_height = 0\nfor ind in index_array:\nimg_data = copy.deepcopy(self.img_data_list[ind])\nprepared_img_data = self.model._generate_images_with_bboxes(img_data, horizontal_flip=self.horizontal_flip,\nvertical_flip=self.vertical_flip, rot_90=self.rot_90)\nbatch_prepared_img_data.append(prepared_img_data)\nmax_resized_width = max(max_resized_width, prepared_img_data['resized_width'])\nmax_resized_height = max(max_resized_height, prepared_img_data['resized_height'])\n# Set format X\nbatch_shape = (len(index_array), max_resized_height, max_resized_width, 3)\nbatch_x = np.zeros(batch_shape)  # Def. black pixels\n# Get input images\nfor ind, prepared_img_data in enumerate(batch_prepared_img_data):\nprepared_img_data['batch_width'] = max_resized_width\nprepared_img_data['batch_height'] = max_resized_height\nimg = prepared_img_data['img']\nbatch_x[ind, :img.shape[0], :img.shape[1], :] = img\n# If test, we return the images\nif self.is_test:\nif self.with_img_data:\nreturn {'input_img': batch_x}, batch_prepared_img_data\nelse:\nreturn {'input_img': batch_x}\n# Otherwise, we also manage the targets y\nelse:\nbatch_y_cls, batch_y_regr = utils_object_detectors.get_rpn_targets(self.model, batch_prepared_img_data)\n# Return\nif self.with_img_data:\nreturn {'input_img': batch_x}, {'rpn_class': batch_y_cls, 'rpn_regr': batch_y_regr}, batch_prepared_img_data\nelse:\nreturn {'input_img': batch_x}, {'rpn_class': batch_y_cls, 'rpn_regr': batch_y_regr}\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.CustomGeneratorRpn.__init__","title":"<code>__init__(img_data_list, batch_size, shuffle, seed, model, horizontal_flip=False, vertical_flip=False, rot_90=False, data_type='train', with_img_data=False, **kwargs)</code>","text":"<p>Initialization of the RPN generator</p> <p>Parameters:</p> Name Type Description Default <code>img_data_list</code> <code>list&lt;dict&gt;</code> <p>Data list (the dictionaries containing file path and bboxes)</p> required <code>batch_size</code> <code>int</code> <p>Size of the batches to generate</p> required <code>shuffle</code> <code>bool</code> <p>If the data should be shuffled</p> required <code>seed</code> <code>int</code> <p>Random seed to use</p> required <code>model</code> <p>Link to the model (nested structure) to access to the other methods of this script</p> required <code>data_type</code> <code>str</code> <p>Data type 'train', 'valid' or 'test'</p> <code>'train'</code> <code>with_img_data</code> <code>bool</code> <p>If True, also gives img_data as output</p> <code>False</code> Kwargs <p>horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba) vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba) rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def __init__(self, img_data_list: List[dict], batch_size: int, shuffle: bool, seed: Union[int, None],\nmodel, horizontal_flip: bool = False, vertical_flip: bool = False,\nrot_90: bool = False, data_type: str = 'train', with_img_data: bool = False, **kwargs) -&gt; None:\n'''Initialization of the RPN generator\n    Args:\n        img_data_list (list&lt;dict&gt;): Data list (the dictionaries containing file path and bboxes)\n        batch_size (int): Size of the batches to generate\n        shuffle (bool): If the data should be shuffled\n        seed (int): Random seed to use\n        model: Link to the model (nested structure) to access to the other methods of this script\n        data_type (str): Data type 'train', 'valid' or 'test'\n        with_img_data (bool): If True, also gives img_data as output\n    Kwargs:\n        horizontal_flip (bool) : If True, can do horizontal flip (with 0.5 proba)\n        vertical_flip (bool) : If True, can do vertical flip (with 0.5 proba)\n        rot_90 (bool) : If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n    '''\n# Set is_test\nif data_type == 'test':\nself.is_test = True\nelse:\nself.is_test = False\n# If test, the batch size is set to 1\nif self.is_test:\nbatch_size = 1\n# Super init.\nsuper().__init__(n=len(img_data_list), batch_size=batch_size, shuffle=shuffle, seed=seed)\n# Set params\nself.img_data_list = img_data_list\nself.model = model\nself.horizontal_flip = horizontal_flip\nself.vertical_flip = vertical_flip\nself.rot_90 = rot_90\nself.with_img_data = with_img_data\n# Manage data augmentation &amp; test\nif self.is_test and any([param for param in [self.horizontal_flip, self.vertical_flip, self.rot_90]]):\nmodel.logger.warning(\"Warning, data augmentation on the test dataset ! It is most certainly a mistake !\")\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelCheckpointAll","title":"<code>ModelCheckpointAll</code>","text":"<p>         Bases: <code>ModelCheckpoint</code></p> <p>A Callback to save the whole model and not only the model currently being fitted. In order to do so, we overload the class ModelCheckpoint by redefining its method _save_model</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>class ModelCheckpointAll(ModelCheckpoint):\n'''A Callback to save the whole model and not only the model currently being fitted.\n    In order to do so, we overload the class ModelCheckpoint by redefining its method _save_model\n    '''\ndef __init__(self, model_all, **kwargs) -&gt; None:\n'''Initialization of the class\n        Args:\n            model_all (Model): Whole model (RPN &amp; Classifier) of the Faster RCNN\n        '''\nsuper().__init__(**kwargs)\nself.model_all = model_all\ndef _save_model(self, epoch: int, batch, logs: dict) -&gt; None:\n\"\"\"Saves the model.\n        Small trick : we temporarily set the model to model_all, save it, and then reload model_main\n        Args:\n            epoch: the epoch this iteration is in.\n            batch: the batch this iteration is in. `None` if the `save_freq`\n            is set to `epoch`.\n            logs: the `logs` dict passed in to `on_batch_end` or `on_epoch_end`.\n        \"\"\"\n# Small trick on the models\nmodel_main = self.model\nself.model = self.model_all\n# Save\nsuper()._save_model(epoch, batch, logs)\n# Fix trick\nself.model = model_main\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelCheckpointAll.__init__","title":"<code>__init__(model_all, **kwargs)</code>","text":"<p>Initialization of the class</p> <p>Parameters:</p> Name Type Description Default <code>model_all</code> <code>Model</code> <p>Whole model (RPN &amp; Classifier) of the Faster RCNN</p> required Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def __init__(self, model_all, **kwargs) -&gt; None:\n'''Initialization of the class\n    Args:\n        model_all (Model): Whole model (RPN &amp; Classifier) of the Faster RCNN\n    '''\nsuper().__init__(**kwargs)\nself.model_all = model_all\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelKerasFasterRcnnObjectDetector","title":"<code>ModelKerasFasterRcnnObjectDetector</code>","text":"<p>         Bases: <code>ModelObjectDetectorMixin</code>, <code>ModelKeras</code></p> <p>Faster RCNN model (Keras) for object detection</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>class ModelKerasFasterRcnnObjectDetector(ModelObjectDetectorMixin, ModelKeras):\n'''Faster RCNN model (Keras) for object detection'''\n_default_name = 'model_keras_faster_rcnn_object_detector'\ndef __init__(self, img_min_side_size: int = 300, rpn_min_overlap: float = 0.3, rpn_max_overlap: float = 0.7, rpn_restrict_num_regions: int = 256,\npool_resize_classifier: int = 7, nb_rois_classifier: int = 4, roi_nms_overlap_threshold: float = 0.7, nms_max_boxes: int = 300,\nclassifier_min_overlap: float = 0.1, classifier_max_overlap: float = 0.5,\npred_bbox_proba_threshold: float = 0.6, pred_nms_overlap_threshold: float = 0.2,\ndata_augmentation_params: Union[dict, None] = None,\nbatch_size_rpn_trainable_true: Union[int, None] = None, batch_size_classifier_trainable_true: Union[int, None] = None,\nbatch_size_rpn_trainable_false: Union[int, None] = None, batch_size_classifier_trainable_false: Union[int, None] = None,\nepochs_rpn_trainable_true: Union[int, None] = None, epochs_classifier_trainable_true: Union[int, None] = None,\nepochs_rpn_trainable_false: Union[int, None] = None, epochs_classifier_trainable_false: Union[int, None] = None,\npatience_rpn_trainable_true: Union[int, None] = None, patience_classifier_trainable_true: Union[int, None] = None,\npatience_rpn_trainable_false: Union[int, None] = None, patience_classifier_trainable_false: Union[int, None] = None,\nlr_rpn_trainable_true: float = 1e-5, lr_classifier_trainable_true: float = 1e-5, lr_rpn_trainable_false: float = 1e-5,\nlr_classifier_trainable_false: float = 1e-5, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass, ModelKeras &amp; ModelObjectDetectorMixin for more arguments)\n        Kwargs:\n            img_min_side_size (int): Size to give to the smaller dimension as input of the model\n            rpn_min_overlap (float): Under this threshold a region is classified as background (RPN model)\n            rpn_max_overlap (float): Above this threshold a region is classified as object (RPN model)\n            rpn_restrict_num_regions (int): Maximal number of regions to keep as target for the RPN\n            pool_resize_classifier (int): Size to give to the crops done before the classifier (via ROI)\n            nb_rois_classifier (int): Maximal number of ROIs per image during classifier training (per image of a batch)\n            roi_nms_overlap_threshold (float): The NMS deletes overlapping ROIs whose IOU is above this threshold\n            nms_max_boxes (int): Maximal number of ROIs to be returned by the NMS\n            classifier_min_overlap (float): Above this threshold a ROI is considered to be a target of the classifier (but can be 'bg')\n            classifier_max_overlap (float): Above this threshold a ROI is considered to be matching a bbox (so the target is a class, not 'bg')\n            pred_bbox_proba_threshold (float): Above this threshold (for probabilities), a ROI is considered to be a match\n            pred_nms_overlap_threshold (float): When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold\n            data_augmentation_params (dict): Set of allowed data augmentation\n            batch_size_rpn_trainable_true (int): Batch size for the RPN with for first run with trainable set to True\n            batch_size_classifier_trainable_true (int): Batch size for the classifier for the first run with trainable set to True\n            batch_size_rpn_trainable_false (int): Batch size for the RPN for the second run with trainable set to False\n            batch_size_classifier_trainable_false (int): Batch size for the classifier for the second run with trainable set to False\n            epochs_rpn_trainable_true (int): Number of epochs for the RPN for the first run with trainable set to True\n            epochs_classifier_trainable_true (int): Number of epochs for the classifier for the first run with trainable set to True\n            epochs_rpn_trainable_false (int): Number of epochs for the RPN for the second run with trainable set to False\n            epochs_classifier_trainable_false (int): lNumber of epochs for the classifier for the second run with trainable set to False\n            patience_rpn_trainable_true (int): Patience for the RPN for the first run with trainable set to True\n            patience_classifier_trainable_true (int): Patience for the classifier for the first run with trainable set to True\n            patience_rpn_trainable_false (int): Patience for the RPN for the second run with trainable set to False\n            patience_classifier_trainable_false (int): Patience for the classifier for the second run with trainable set to False\n            lr_rpn_trainable_true (float): Learning rate for the RPN for the first run with trainable set to True\n            lr_classifier_trainable_true (float): Learning rate for the classifier for the first run with trainable set to True\n            lr_rpn_trainable_false (float): Learning rate for the RPN for the second run with trainable set to False\n            lr_classifier_trainable_false (float): Learning rate for the classifier for the second run with trainable set to False\n        Raises:\n            ValueError: If img_min_side_size is not positive\n            ValueError: If rpn_min_overlap is not in [0, 1]\n            ValueError: If rpn_max_overlap is not in [0, 1]\n            ValueError: If rpn_min_overlap &gt; rpn_max_overlap\n            ValueError: If rpn_restrict_num_regions is not positive\n            ValueError: If pool_resize_classifier is not positive\n            ValueError: If nb_rois_classifier is not positive\n            ValueError: If roi_nms_overlap_threshold is not in [0, 1]\n            ValueError: If nms_max_boxes is not positive\n            ValueError: If classifier_min_overlap is not in [0, 1]\n            ValueError: If classifier_max_overlap is not in [0, 1]\n            ValueError: If classifier_min_overlap &gt; classifier_max_overlap\n            ValueError: If pred_bbox_proba_threshold is not in [0, 1]\n            ValueError: If pred_nms_overlap_threshold is not in [0, 1]\n            ValueError: If color_mode is not 'rgb'\n            ValueError: If the minimum size of the image is inferior to twice the subsampling ratio\n        '''\n# Manage errors\nif img_min_side_size &lt; 1:\nraise ValueError(f\"The argument img_min_side_size ({img_min_side_size}) must be positive\")\nif not 0 &lt;= rpn_min_overlap &lt;= 1:\nraise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) must be between 0 and 1, included\")\nif not 0 &lt;= rpn_max_overlap &lt;= 1:\nraise ValueError(f\"The argument rpn_max_overlap ({rpn_max_overlap}) must be between 0 and 1, included\")\nif rpn_min_overlap &gt; rpn_max_overlap:\nraise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) can't be superior to rpn_max_overlap ({rpn_max_overlap})\")\nif rpn_restrict_num_regions &lt; 1:\nraise ValueError(f\"The argument rpn_restrict_num_regions ({rpn_restrict_num_regions}) must be positive\")\nif pool_resize_classifier &lt; 1:\nraise ValueError(f\"The argument pool_resize_classifier ({pool_resize_classifier}) must be positive\")\nif nb_rois_classifier &lt; 1:\nraise ValueError(f\"The argument nb_rois_classifier ({nb_rois_classifier}) must be positive\")\nif not 0 &lt;= roi_nms_overlap_threshold &lt;= 1:\nraise ValueError(f\"The argument roi_nms_overlap_threshold ({roi_nms_overlap_threshold}) must be between 0 and 1, included\")\nif nms_max_boxes &lt; 1:\nraise ValueError(f\"The argument nms_max_boxes ({nms_max_boxes}) must be positive\")\nif not 0 &lt;= classifier_min_overlap &lt;= 1:\nraise ValueError(f\"The argument classifier_min_overlap ({classifier_min_overlap}) must be between 0 and 1, included\")\nif not 0 &lt;= classifier_max_overlap &lt;= 1:\nraise ValueError(f\"The argument classifier_max_overlap ({classifier_max_overlap}) must be between 0 and 1, included\")\nif classifier_min_overlap &gt; classifier_max_overlap:\nraise ValueError(f\"The argument classifier_min_overlap ({classifier_min_overlap}) can't be superior to classifier_max_overlap ({classifier_max_overlap})\")\nif not 0 &lt;= pred_bbox_proba_threshold &lt;= 1:\nraise ValueError(f\"The argument pred_bbox_proba_threshold ({pred_bbox_proba_threshold}) must be between 0 and 1, included\")\nif not 0 &lt;= pred_nms_overlap_threshold &lt;= 1:\nraise ValueError(f\"The argument pred_nms_overlap_threshold ({pred_nms_overlap_threshold}) must be between 0 and 1, included\")\n# Size of the input images (must be defined before the super init because it is used in the method _get_preprocess_input)\nself.img_min_side_size = img_min_side_size  # Default 300, in the paper 600\n# Init. (by default we have some data augmentation)\nif data_augmentation_params is None:\ndata_augmentation_params = {'horizontal_flip': True, 'vertical_flip': True, 'rot_90': True}\nsuper().__init__(data_augmentation_params=data_augmentation_params, **kwargs)\nif self.color_mode != 'rgb':\nraise ValueError(\"Faster RCNN model only accept color_mode equal to 'rgb' (compatibility VGG16).\")\n# Put to None some parameters of model_keras not used by this model\nself.width = None\nself.height = None\nself.depth = None\nself.in_memory = None\nself.nb_train_generator_images_to_save = None\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Models, set on fit\nself.model: Any = None\nself.shared_model = None\nself.model_rpn = None\nself.model_classifier = None\n# Weights\nself.vgg_filename = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nself.vgg_path = os.path.join(utils.get_data_path(), 'transfer_learning_weights', self.vgg_filename)\nvgg16_weights_backup_urls = [\n'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n]\nif not os.path.exists(self.vgg_path):\ntry:\nself.logger.warning(\"The weights file for VGG16 is not present in your data folder.\")\nself.logger.warning(\"Trying to download the file.\")\nutils.download_url(vgg16_weights_backup_urls, self.vgg_path)\nexcept ConnectionError:\nself.logger.warning(\"Can't download. You can try to download it manually and save it on DVC.\")\nself.logger.warning(\"Building this model will return an error.\")\nself.logger.warning(\"You can download the weights here : https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n# We don't raise an error because we may reload a trained model\n### Model configuration\n# Configurations related to the base model\nself.shared_model_subsampling = 16  # VGG 16\n# Error if img_min_side_size &lt; 2 * subsampling rate\nif self.img_min_side_size &lt; 2 * self.shared_model_subsampling:\nraise ValueError(\"Can't have a minimum size of an image inferior to twice the subsampling ratio\")\n# Anchors boxes\nself.anchor_box_sizes = [64, 128, 256]  # In the paper : [128, 256, 512]\nself.anchor_box_ratios = [[1, 1], [1. / math.sqrt(2), 2. / math.sqrt(2)], [2. / math.sqrt(2), 1. / math.sqrt(2)]]  # In the paper : [1, 1], [1, 2], [2, 1]]\nself.nb_anchors = len(self.anchor_box_sizes) * len(self.anchor_box_ratios)\nself.list_anchors = [[anchor_size * anchor_ratio[0], anchor_size * anchor_ratio[1]]\nfor anchor_size in self.anchor_box_sizes for anchor_ratio in self.anchor_box_ratios]\n# Sizes\nself.pool_resize_classifier = pool_resize_classifier  # Def 7\n# Scaling (we could probably do without scaling)\nself.rpn_regr_scaling = 4.0\nself.classifier_regr_scaling = [8.0, 8.0, 4.0, 4.0]\n# Thresholds for the RPN to find positive and negative anchor boxes\nself.rpn_min_overlap = rpn_min_overlap  # Def 0.3\nself.rpn_max_overlap = rpn_max_overlap  # Def 0.7\n# Maximum number of regions targets of the RPN\nself.rpn_restrict_num_regions = rpn_restrict_num_regions\n# Classifier configuration\nself.nb_rois_classifier = nb_rois_classifier  # Def 4\nself.roi_nms_overlap_threshold = roi_nms_overlap_threshold  # Def 0.7\nself.nms_max_boxes = nms_max_boxes  # Def 300\nself.classifier_min_overlap = classifier_min_overlap  # Def 0.1\nself.classifier_max_overlap = classifier_max_overlap  # Def 0.5\n# Prediction Thresholds\nself.pred_bbox_proba_threshold = pred_bbox_proba_threshold  # Def 0.6\nself.pred_nms_overlap_threshold = pred_nms_overlap_threshold  # Def 0.2\n### Misc.\n# We add the custom objects only when fitting because we need the number of classes\n# Manage batch_size, epochs &amp; patience (back up on global values if not specified)\nself.batch_size_rpn_trainable_true = batch_size_rpn_trainable_true if batch_size_rpn_trainable_true is not None else self.batch_size\nself.batch_size_classifier_trainable_true = batch_size_classifier_trainable_true if batch_size_classifier_trainable_true is not None else self.batch_size\nself.batch_size_rpn_trainable_false = batch_size_rpn_trainable_false if batch_size_rpn_trainable_false is not None else self.batch_size\nself.batch_size_classifier_trainable_false = batch_size_classifier_trainable_false if batch_size_classifier_trainable_false is not None else self.batch_size\nself.epochs_rpn_trainable_true = epochs_rpn_trainable_true if epochs_rpn_trainable_true is not None else self.epochs\nself.epochs_classifier_trainable_true = epochs_classifier_trainable_true if epochs_classifier_trainable_true is not None else self.epochs\nself.epochs_rpn_trainable_false = epochs_rpn_trainable_false if epochs_rpn_trainable_false is not None else self.epochs\nself.epochs_classifier_trainable_false = epochs_classifier_trainable_false if epochs_classifier_trainable_false is not None else self.epochs\nself.patience_rpn_trainable_true = patience_rpn_trainable_true if patience_rpn_trainable_true is not None else self.patience\nself.patience_classifier_trainable_true = patience_classifier_trainable_true if patience_classifier_trainable_true is not None else self.patience\nself.patience_rpn_trainable_false = patience_rpn_trainable_false if patience_rpn_trainable_false is not None else self.patience\nself.patience_classifier_trainable_false = patience_classifier_trainable_false if patience_classifier_trainable_false is not None else self.patience\n# Save learning rates in params_keras\nself.keras_params['lr_rpn_trainable_true'] = lr_rpn_trainable_true\nself.keras_params['lr_classifier_trainable_true'] = lr_classifier_trainable_true\nself.keras_params['lr_rpn_trainable_false'] = lr_rpn_trainable_false\nself.keras_params['lr_classifier_trainable_false'] = lr_classifier_trainable_false\n#####################\n# Modelisation\n#####################\ndef _get_model(self) -&gt; Any:\n'''Gets a model structure - returns the instance model instead if already defined\n        Returns:\n            (?): Shared layers of the VGG 16 (not compiled)\n            (?): RPN model\n            (?): Classifier model\n            (?): Global model (for load/save only)\n        '''\n# Return models if already set\nif (self.shared_model is not None) and (self.model_rpn is not None) and (self.model_classifier is not None) and (self.model is not None):\nreturn self.shared_model, self.model_rpn, self.model_classifier, self.model\n# First, we define the inputs\ninput_img = Input(shape=(None, None, 3), name='input_img')\ninput_rois = Input(shape=(None, 4), name='input_rois')\n# Then we get the various parts of the network\nshared_model_layers = self._get_shared_model_structure(input_img)  # List (class &amp; regr)\nrpn_layers = self._add_rpn_layers(shared_model_layers)  # List (class &amp; regr)\nclassifier_layers = self._add_classifier_layers(shared_model_layers, input_rois)\n# Base model (shared layers)\nshared_model = Model(input_img, shared_model_layers)\n# We instanciate our models\nmodel_rpn = Model(input_img, rpn_layers)\nmodel_classifier = Model([input_img, input_rois], classifier_layers)\n# Concatenation of the two models, used to load / save the weights\nmodel_all = Model([input_img, input_rois], rpn_layers + classifier_layers)\n# We load the pre-trained weights\nshared_model.load_weights(self.vgg_path, by_name=True)\n# Compile models\nself._compile_model_rpn(model_rpn, lr=self.keras_params['lr_rpn_trainable_true'])\nself._compile_model_classifier(model_classifier, lr=self.keras_params['lr_classifier_trainable_true'])\n# We also compile model_all\nmodel_all.compile(optimizer='sgd', loss='mae')\n# Display summaries\nif self.logger.getEffectiveLevel() &lt; logging.ERROR:\nmodel_all.summary()\n# Try to save models as png if level_save &gt; 'LOW'\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._save_model_png(model_all)\n# Return models\nreturn shared_model, model_rpn, model_classifier, model_all\ndef _compile_model_rpn(self, model_rpn, lr: float) -&gt; None:\n'''Compiles the RPN model using the specified learning rate\n        Args:\n            model_rpn : RPN model to compile\n            lr (float): Learning rate we want to use\n        '''\n# Set optimizer\ndecay = self.keras_params.get('decay_rpn', 0.0)\nself.logger.info(f\"Learning rate used - RPN : {lr}\")\nself.logger.info(f\"Decay used - RPN : {decay}\")\noptimizer_rpn = Adam(lr=lr, decay=decay)\n# Set loss &amp; metrics\nlosses_rpn = {'rpn_class': self.custom_objects['rpn_loss_cls'], 'rpn_regr': self.custom_objects['rpn_loss_regr']}\nmetrics_rpn = {'rpn_class': 'accuracy'}\n# Compile model\nmodel_rpn.compile(optimizer=optimizer_rpn, loss=losses_rpn, metrics=metrics_rpn)\ndef _compile_model_classifier(self, model_classifier, lr: float) -&gt; None:\n'''Compiles the classifier model using the specified learning rate\n        Args:\n            model_classifier : Classifier to compule\n            lr (float): Learning rate we want to use\n        '''\n# Set optimizer\ndecay = self.keras_params.get('decay_classifier', 0.0)\nself.logger.info(f\"Learning rate used - classifier : {lr}\")\nself.logger.info(f\"Decay used - classifier : {decay}\")\noptimizer_classifier = Adam(lr=lr, decay=decay)\n# Set loss &amp; metrics\nlosses_classifier = {'dense_class': self.custom_objects['class_loss_cls'], 'dense_regr': self.custom_objects['class_loss_regr']}\nmetrics_classifier = {'dense_class': 'accuracy'}\n# Compile model\nmodel_classifier.compile(optimizer=optimizer_classifier, loss=losses_classifier, metrics=metrics_classifier)\ndef _get_shared_model_structure(self, input_img):\n'''We give the VGG 16 structure\n        Args:\n            input_img (?): Input layer for the images\n        Returns:\n            ?: VGG16 structure (without the weights)\n        '''\n# Block 1\nx = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(input_img)\nx = Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(x)\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(x)\n# Block 2\nx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(x)\nx = Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(x)\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(x)\n# Block 3\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(x)\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(x)\nx = Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(x)\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(x)\n# Block 4\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(x)\nx = MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(x)\n# Block 5\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(x)\nx = Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(x)\nreturn x\ndef _add_rpn_layers(self, base_layers) -&gt; List[Conv2D]:\n'''Adds the RPN layers to a base model\n        Args:\n            base_layers: Base model - VGG16\n        Returns:\n            ?: RPN layers\n        '''\n# We add a convolution layer\nx = Conv2D(512, (3, 3), padding='same', activation='relu', kernel_initializer='normal', name='rpn_conv1')(base_layers)\n# Fully convolutional layer for our targets\n# - nb_anchors, feature_map_width, feature_map_height  ---&gt;  Classification\n# - nb_anchors * nb_coordinates, feature_map_width, feature_map_height  ---&gt;  Regression\nx_class = Conv2D(self.nb_anchors, (1, 1), activation='sigmoid', kernel_initializer='uniform', name='rpn_class')(x)\nx_regr = Conv2D(self.nb_anchors * 4, (1, 1), activation='linear', kernel_initializer='zero', name='rpn_regr')(x)\n# We return the results of the two parts (the format will be managed by the losses)\nreturn [x_class, x_regr]\ndef _add_classifier_layers(self, base_layers, input_rois) -&gt; List[TimeDistributed]:\n'''Adds layers for classification to a base model and a ROIs tensor\n        Args:\n            base_layers: Base model - VGG16\n            input_rois: Tensor with some ROIs\n                # Shape (1, num_rois, 4), with coordinates (x, y, w, h)\n        Returns:\n            list: List with the classification and regression outputs\n        '''\n# We get the crops on the features map from the ROIs\nout_roi_pool = utils_faster_rcnn.RoiPoolingLayer(self.pool_resize_classifier, name='roi_pool')([base_layers, input_rois])\n# Add the Dense part (we use TimeDistributed to take care of the ROIs dimension)\nout = TimeDistributed(Flatten(name='flatten'), name='distributed_flatten')(out_roi_pool)\nout = TimeDistributed(Dense(4096, activation='relu', name='fc1'), name='distributed_fc1')(out)\nout = TimeDistributed(Dropout(0.5), name='distributed_dropout_1')(out)\nout = TimeDistributed(Dense(4096, activation='relu', name='fc2'), name='distributed_fc2')(out)\nout = TimeDistributed(Dropout(0.5), name='distributed_dropout_2')(out)\n# We output two parts, classifier and regressor\n# Classifier : ROI class\n# Regressor : ROI coordinates correction\nnb_classes = len(self.list_classes)\nout_class = TimeDistributed(Dense(nb_classes + 1, activation='softmax', kernel_initializer='zero'), name=\"dense_class\")(out)\n# TODO: Do we really need to a regression for each class ?!\n# TODO: Couldn't we simply have a single regression ? The corresponding loss would be on matching an object (whatever the class).\n# TODO: Couldn't we event make without the regression part here? After all, it is already done by the RPN.\nout_regr = TimeDistributed(Dense(4 * nb_classes, activation='linear', kernel_initializer='zero'), name=\"dense_regr\")(out)\nreturn [out_class, out_regr]\n#####################\n# Images generation\n#####################\ndef _get_generator(self, df: pd.DataFrame, data_type: str, batch_size: int, generator_type: str,\nshared_model_trainable: bool = False, with_img_data: bool = False):\n'''Gets image generator from a list of files - object detector version\n        Args:\n            df (pd.DataFrame): Dataset to use must contain :\n                - a column 'file_path' with a path to an image\n                - a column 'bboxes', the list of the bboxes of the image (if train or val)\n            data_type (str): Type of data : 'train', 'valid' or 'test'\n            batch_size (int): Batch size to use\n            generator_type (str): The generator to use, 'rpn' or 'classifier'\n            shared_model_trainable (bool): Classifier &amp; train only - if the shared model is trainable,\n                we must clone the RPN in order not to worsen the quality of the ROIs prediction (which\n                are an input of the classifier)\n            with_img_data (bool): If True, the generator also gives img_data as output\n        Raises:\n            ValueError: If the type of the model is not object_detector\n            ValueError: If data_type is not in ['train', 'valid', 'test']\n            ValueError: If the dataframe has no 'file_path' column\n            ValueError: If 'train' or 'valid' and the dataframe has no 'bboxes' column\n        '''\n# Manage errors\nif self.model_type != 'object_detector':\nraise ValueError(f\"Models of type {self.model_type} do not implement the method _get_generator\")\nif data_type not in ['train', 'valid', 'test']:\nraise ValueError(f\"The value {data_type} is not a suitable value for the argument data_type ['train', 'valid', 'test'].\")\nif generator_type not in ['rpn', 'classifier']:\nraise ValueError(f\"The value {generator_type} is not a suitable value for the generator_type ['rpn', 'classifier'].\")\nif 'file_path' not in df.columns:\nraise ValueError(\"The column 'file_path' is mandatory in the input dataframe\")\nif data_type in ['train', 'valid'] and 'bboxes' not in df.columns:\nraise ValueError(f\"The column 'bboxes' is mandatory in the input dataframe when data_type equal to '{data_type}'\")\n# Copy\ndf = df.copy(deep=True)\n# Extract info\nimg_data_list = []\nfor i, row in df.iterrows():\nfilepath = row['file_path']\nif 'bboxes' in df.columns:\nbboxes = row['bboxes']\nimg_data_list.append({'file_path': filepath, 'bboxes': bboxes})\nelse:\nimg_data_list.append({'file_path': filepath})\n# TODO : Manage incorrect bboxes ?\n# Get the suitable generator class\ncustom_generator = CustomGeneratorRpn if generator_type == 'rpn' else CustomGeneratorClassifier\n# Set data_gen (no augmentation nor shuffle if validation/test)\nif data_type == 'train':\ngenerator = custom_generator(img_data_list=img_data_list, batch_size=batch_size, shuffle=True,\nseed=None, model=self, shared_model_trainable=shared_model_trainable,\ndata_type=data_type, **self.data_augmentation_params, with_img_data=with_img_data)\nelse:\ngenerator = custom_generator(img_data_list=img_data_list, batch_size=batch_size, shuffle=False,\nseed=None, model=self, shared_model_trainable=shared_model_trainable,\ndata_type=data_type, with_img_data=with_img_data)\nreturn generator\ndef _generate_images_with_bboxes(self, img_data: dict, horizontal_flip: bool = False,\nvertical_flip: bool = False, rot_90: bool = False) -&gt; dict:\n'''Generates an image and its bboxes from its info (path, etc.)\n        Can do data augmentation but with a limited choice because some transformations are not\n        compatible with the bboxes (eg. 20 degrees angle)\n        Also preprocesses the image\n        Args:\n            img_data (dict): Data on the image and its bboxes\n                Must contain : 'file_path' &amp; 'bboxes'\n        Kwargs:\n            horizontal_flip (bool): If True, can do horizontal flip (with 0.5 proba)\n            vertical_flip (bool): If True, can do vertical flip (with 0.5 proba)\n            rot_90 (bool): If True, can do a rotation of 0, 90, 180 or 270 degrees (0.25 proba for each)\n                By default the augmentations are not applied (set to False)\n        '''\n# Read the image, as TensorFlow does\nwith open(img_data['file_path'], 'rb') as f:\nimg = Image.open(io.BytesIO(f.read()))\nif img.mode != 'RGB':\nimg = img.convert('RGB')\n# Convert to array\nimg = np.asarray(img)\n# Get bboxes &amp; image size\nbboxes = copy.deepcopy(img_data.get('bboxes', []))  # Empty if test\nh, w = img.shape[:2]\n#####################\n### Augmentations ###\n#####################\n# Horizontal flip\nif horizontal_flip and np.random.randint(0, 2) == 0:\nimg = cv2.flip(img, 1)\nfor bbox in bboxes:\nx1, x2 = bbox['x1'], bbox['x2']\nbbox['x2'] = w - x1\nbbox['x1'] = w - x2\n# Vertical flip\nif vertical_flip and np.random.randint(0, 2) == 0:\nimg = cv2.flip(img, 0)\nfor bbox in bboxes:\ny1, y2 = bbox['y1'], bbox['y2']\nbbox['y2'] = h - y1\nbbox['y1'] = h - y2\n# Rotation 0\u00b0, 90\u00b0, 180\u00b0 or 270\u00b0\nif rot_90:\nangle = np.random.choice([0, 90, 180, 270], 1)[0]\nif angle == 270:\nimg = cv2.flip(np.transpose(img, (1, 0, 2)), 0)\nelif angle == 180:\nimg = cv2.flip(img, -1)\nelif angle == 90:\nimg = cv2.flip(np.transpose(img, (1, 0, 2)), 1)\nfor bbox in bboxes:\nx1, x2, y1, y2 = bbox['x1'], bbox['x2'], bbox['y1'], bbox['y2']\nif angle == 270:\nbbox['x1'] = y1\nbbox['x2'] = y2\nbbox['y1'] = w - x2\nbbox['y2'] = w - x1\nelif angle == 180:\nbbox['x2'] = w - x1\nbbox['x1'] = w - x2\nbbox['y2'] = h - y1\nbbox['y1'] = h - y2\nelif angle == 90:\nbbox['x1'] = h - y2\nbbox['x2'] = h - y1\nbbox['y1'] = x1\nbbox['y2'] = x2\n#####################\n### Preprocessing ###\n#####################\n# Keep original sizes\noriginal_height, original_width = img.shape[0], img.shape[1]\n# Preprocess\nimg = self.preprocess_input(img)\n# Get new sizes\nresized_height, resized_width = img.shape[0], img.shape[1]\n# Resize the bboxes following the preprocessing\n# We could get floats but it is not important at this point\nfor bbox in bboxes:\nbbox['x1'] = bbox['x1'] * (resized_width / original_width)\nbbox['x2'] = bbox['x2'] * (resized_width / original_width)\nbbox['y1'] = bbox['y1'] * (resized_height / original_height)\nbbox['y2'] = bbox['y2'] * (resized_height / original_height)\n#####################\n###  Format Data  ###\n#####################\nprepared_data = {\n'img': img,  # Preprocessed image\n'bboxes': bboxes,  # Bboxes after data augmentation &amp; resizing\n'original_height': original_height,\n'original_width': original_width,\n'resized_height': resized_height,\n'resized_width': resized_width,\n}\nreturn prepared_data\ndef _get_preprocess_input(self) -&gt; Union[Callable, None]:\n'''Gets the preprocessing to be used before feeding images to the NN\n        Returns:\n            (Callable | None): Preprocessing function\n        '''\n# Get preprocessing function (resize + vgg16)\nimg_min_side_size = self.img_min_side_size  # We take care not to have references to self in the function\ndef preprocess_input(x_img: np.ndarray, **kwargs) -&gt; np.ndarray:\n'''Preprocessing of a numpy image\n            Resizes the image + classic VGG 16 preprocessing\n            Args:\n                x_img (np.ndarray): Image to process\n            Returns:\n                np.ndarray: Result of the preprocessing\n            '''\n# Resize\nheight, width = x_img.shape[0], x_img.shape[1]\nresized_height, resized_width = utils_object_detectors.get_new_img_size_from_min_side_size(height, width, img_min_side_size)\nx_img = cv2.resize(x_img, (resized_width, resized_height), interpolation=cv2.INTER_CUBIC)  # Format dimension width, height ...\nreturn preprocess_input_vgg16(x_img)\n# Returns it\nreturn preprocess_input\n#####################\n# Fit\n#####################\ndef _fit_object_detector(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None,\nwith_shuffle: bool = True, **kwargs) -&gt; None:\n'''Training of the model\n        Args:\n            df_train (pd.DataFrame): Training data with file_path &amp; bboxes columns\n            df_valid (pd.DataFrame): Validation data with file_path &amp; bboxes columns\n        Kwargs:\n            with_shuffle (boolean): If data must be shuffled before fitting\n                This should be used if the target is not shuffled as the split_validation takes the lines in order.\n                Thus, the validation set might get classes which are not in the train set ...\n        Raises:\n            ValueError: If the type of the model is not object_detector\n            ValueError: If the class 'bg' is present in the input data\n            AssertionError: If the same classes are not present when comparing an already trained model\n                and a new dataset\n        '''\nif self.model_type != 'object_detector':\nraise ValueError(f\"The models of type {self.model_type} do not implement the method _fit_object_detector\")\n##############################################\n# Manage retrain\n##############################################\n# If a model has already been fitted, we make a new folder in order not to overwrite the existing one !\n# And we save the old conf\nif self.trained:\n# Get src files to save\nsrc_files = [os.path.join(self.model_dir, \"configurations.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\nsrc_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Change model dir\nself.model_dir = self._get_model_dir()\n# Get dst files\ndst_files = [os.path.join(self.model_dir, f\"configurations_fit_{self.nb_fit}.json\")]\nif self.nb_fit &gt; 1:\nfor i in range(1, self.nb_fit):\ndst_files.append(os.path.join(self.model_dir, f\"configurations_fit_{i}.json\"))\n# Copies\nfor src, dst in zip(src_files, dst_files):\ntry:\nshutil.copyfile(src, dst)\nexcept Exception as e:\nself.logger.error(f\"Impossible to copy {src} to {dst}\")\nself.logger.error(\"We still continue ...\")\nself.logger.error(repr(e))\n##############################################\n# Prepare dataset\n# Also extract list of classes\n##############################################\n# Extract list of classes from df_train\nset_classes = set()\nfor bboxes in df_train['bboxes'].to_dict().values():\nset_classes = set_classes.union({bbox['class'] for bbox in bboxes})\nif 'bg' in set_classes:\nraise ValueError(\"The 'bg' class must not be present in the bounding boxes classes\")\nlist_classes = sorted(list(set_classes))\n# Also set dict_classes\ndict_classes = {i: col for i, col in enumerate(list_classes)}\n# Validate classes if already trained, else set them\nif self.trained:\nassert self.list_classes == list_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nassert self.dict_classes == dict_classes, \\\n                \"Error: the new dataset does not match with the already fitted model\"\nelse:\nself.list_classes = list_classes\nself.dict_classes = dict_classes\n# Now that we have the list of the classes, we can define the custom_objects\nself.custom_objects = {**utils_faster_rcnn.get_custom_objects_faster_rcnn(self.nb_anchors, len(self.list_classes)), **self.custom_objects}\n# Shuffle training dataset if wanted\n# It is advised as validation_split from keras does not shufle the data\n# Hence, for classification task, we might have classes in the validation data that we never met in the training data\nif with_shuffle:\ndf_train = df_train.sample(frac=1.).reset_index(drop=True)\n# Manage absence of validation datasets\nif df_valid is None:\nself.logger.warning(f\"Warning, no validation set. The training set will be splitted (validation fraction = {self.validation_split})\")\ndf_train, df_valid = train_test_split(df_train, test_size=self.validation_split)\n##############################################\n# Trainings\n# 4 steps :\n#   - Train RPN - sharable model trainable\n#   - Train classifier - sharable model trainable\n#   - Train RPN - sharable model NOT trainable\n#   - Train classifier - sharable model NOT trainable\n##############################################\n# Get model (if already fitted, _get_model returns instance models)\nself.shared_model, self.model_rpn, self.model_classifier, self.model = self._get_model()\n### Train RPN - sharable model trainable\nself.logger.info(\"RPN training - trainable set to True\")\nself._fit_object_detector_RPN(df_train, df_valid, shared_trainable=True)\n### Train classifier - sharable model trainable\nself.logger.info(\"Classifier training - trainable set to True\")\nself._fit_object_detector_classifier(df_train, df_valid, shared_trainable=True)\n### Train RPN - sharable model NOT trainable\nself.logger.info(\"RPN training - trainable set to False\")\nself._fit_object_detector_RPN(df_train, df_valid, shared_trainable=False)\n### Train classifier - sharable model NOT trainable\nself.logger.info(\"Classifier training - trainable set to False\")\nself._fit_object_detector_classifier(df_train, df_valid, shared_trainable=False)\n# We update trained &amp; nb_fit\nself.trained = True\nself.nb_fit += 1\ndef _fit_object_detector_RPN(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None,\nshared_trainable: bool = True, **kwargs) -&gt; None:\n'''RPN training\n        Args:\n            df_train (pd.DataFrame): Training data with file_path &amp; bboxes columns\n            df_valid (pd.DataFrame): Validation data with file_path &amp; bboxes columns\n            shared_trainable (bool): If the shared model is trainable\n        '''\n# Manage trainable\nfor layer in self.shared_model.layers:\nlayer.trainable = shared_trainable\n# We adapt the learning rate\nnew_lr = self.keras_params['lr_rpn_trainable_true'] if shared_trainable else self.keras_params['lr_rpn_trainable_false']\n# /!\\ Recompile, otherwise the unfreeze is not taken into account ! /!\\\n# Cf. https://keras.io/guides/transfer_learning/#fine-tuning\nself._compile_model_rpn(self.model_rpn, lr=new_lr)\n# We adapt the batch_size, the number of epochs and the patience\nbatch_size = self.batch_size_rpn_trainable_true if shared_trainable else self.batch_size_rpn_trainable_false\nepochs = self.epochs_rpn_trainable_true if shared_trainable else self.epochs_rpn_trainable_false\npatience = self.patience_rpn_trainable_true if shared_trainable else self.patience_rpn_trainable_false\n# If the number of epoch is 0, we skip the training\nif epochs == 0:\nself.logger.info(f\"Number of epochs for RPN training - trainable set to {shared_trainable} is 0. We skip it.\")\n# Entrainement\nelse:\n# Create generators for the RPN\nself.logger.info(\"Get a RPN generator for training data.\")\nbatch_size_train = min(batch_size, len(df_train))\ngenerator_rpn_train = self._get_generator(df=df_train, data_type='train', batch_size=batch_size_train, generator_type='rpn')\nself.logger.info(\"Get a RPN generator for validation data.\")\nbatch_size_valid = min(batch_size, len(df_valid))\ngenerator_rpn_valid = self._get_generator(df=df_valid, data_type='valid', batch_size=batch_size_valid, generator_type='rpn')\n# Get callbacks (early stopping &amp; checkpoint)\ncallbacks = self._get_callbacks(patience=patience)\n# Fit !\nfit_history = self.model_rpn.fit(  # type: ignore\nx=generator_rpn_train,\nepochs=epochs,\nvalidation_data=generator_rpn_valid,\ncallbacks=callbacks,\nverbose=1,\nworkers=8,  # TODO : Check if this is ok if there are less CPUs\n)\n# Plots losses &amp; metrics\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._plot_metrics_and_loss(fit_history, model_type='rpn', trainable=shared_trainable)\ndef _fit_object_detector_classifier(self, df_train: pd.DataFrame, df_valid: Union[pd.DataFrame, None] = None,\nshared_trainable: bool = True, **kwargs) -&gt; None:\n'''Training of the classifier\n        Args:\n            df_train (pd.DataFrame): Training data with file_path &amp; bboxes columns\n            df_valid (pd.DataFrame): Validation data with file_path &amp; bboxes columns\n            shared_trainable (bool): If the shared model is trainable\n        '''\n# Manage trainable\nfor layer in self.shared_model.layers:\nlayer.trainable = shared_trainable\n# We adapt the learning rate\nnew_lr = self.keras_params['lr_classifier_trainable_true'] if shared_trainable else self.keras_params['lr_classifier_trainable_false']\n# /!\\ Recompile, otherwise the unfreeze is not taken into account ! /!\\\n# Cf. https://keras.io/guides/transfer_learning/#fine-tuning\nself._compile_model_classifier(self.model_classifier, lr=new_lr)\n# We adapt the batch_size, the number of epochs and the patience\nbatch_size = self.batch_size_classifier_trainable_true if shared_trainable else self.batch_size_classifier_trainable_false\nepochs = self.epochs_classifier_trainable_true if shared_trainable else self.epochs_classifier_trainable_false\npatience = self.patience_classifier_trainable_true if shared_trainable else self.patience_classifier_trainable_false\n# If the number of epoch is 0, we skip the training\nif epochs == 0:\nself.logger.info(f\"Number of epochs for classifier training - trainable set to {shared_trainable} is 0. We skip it.\")\n# Training\nelse:\n# Create generators for the classifier\nself.logger.info(\"Get a classifier generator for training data.\")\nbatch_size_train = min(batch_size, len(df_train))\ngenerator_classifier_train = self._get_generator(df=df_train, data_type='train', batch_size=batch_size_train,\ngenerator_type='classifier', shared_model_trainable=shared_trainable)\nself.logger.info(\"Get a classifier generator for validation data.\")\nbatch_size_valid = min(batch_size, len(df_valid))\ngenerator_classifier_valid = self._get_generator(df=df_valid, data_type='valid', batch_size=batch_size_valid,\ngenerator_type='classifier', shared_model_trainable=shared_trainable)\n# Get callbacks (early stopping &amp; checkpoint)\ncallbacks = self._get_callbacks(patience=patience)\n# Fit !\nfit_history = self.model_classifier.fit(  # type: ignore\nx=generator_classifier_train,\nepochs=epochs,\nvalidation_data=generator_classifier_valid,\ncallbacks=callbacks,\nverbose=1,\nworkers=8,  # TODO : Check if this is ok if there are less CPUs\n)\n# Plots losses &amp; metrics\nif self.level_save in ['MEDIUM', 'HIGH']:\nself._plot_metrics_and_loss(fit_history, model_type='classifier', trainable=shared_trainable)\ndef _get_callbacks(self, patience: int) -&gt; list:\n'''Gets model callbacks\n        Args:\n            patience (int): Early stopping patience\n        Returns:\n            list: List of callbacks\n        '''\n# Get classic callbacks\ncallbacks = [EarlyStopping(monitor='val_loss', patience=patience, restore_best_weights=True)]\nif self.level_save in ['MEDIUM', 'HIGH']:\ncallbacks.append(\nModelCheckpointAll(\nfilepath=os.path.join(self.model_dir, 'best.hdf5'), monitor='val_loss',\nsave_best_only=True, mode='auto', model_all=self.model\n)\n)\ncallbacks.append(CSVLogger(filename=os.path.join(self.model_dir, 'logger.csv'), separator=';', append=False))\ncallbacks.append(TerminateOnNaN())\n# Get LearningRateScheduler\n# FOR NOW, WE DO NOT TAKE INTO ACCOUNT LEARNING RATE SCHEDULERS\n# scheduler = self._get_learning_rate_scheduler()\n# if scheduler is not None:\n#     callbacks.append(LearningRateScheduler(scheduler))\n# Manage tensorboard\nif self.level_save in ['HIGH']:\n# Get log directory\nmodels_path = utils.get_models_path()\ntensorboard_dir = os.path.join(models_path, 'tensorboard_logs')\n# We add a prefix so that the function load_model works correctly (it looks for a sub-folder with model name)\nlog_dir = os.path.join(tensorboard_dir, f\"tensorboard_{ntpath.basename(self.model_dir)}\")\nif not os.path.exists(log_dir):\nos.makedirs(log_dir)\n# TODO: Check if this class slows the process\n# -&gt; For now: comment\n# Create custom class to monitor LR changes\n# https://stackoverflow.com/questions/49127214/keras-how-to-output-learning-rate-onto-tensorboard\n# class LRTensorBoard(TensorBoard):\n#     def __init__(self, log_dir, **kwargs) -&gt; None:  # add other arguments to __init__ if you need\n#         super().__init__(log_dir=log_dir, **kwargs)\n#\n#     def on_epoch_end(self, epoch, logs=None):\n#         logs.update({'lr': K.eval(self.model.optimizer.lr)})\n#         super().on_epoch_end(epoch, logs)\ncallbacks.append(TensorBoard(log_dir=log_dir, write_grads=False, write_images=False))\nself.logger.info(f\"To start tensorboard: python -m tensorboard.main --logdir {tensorboard_dir} --samples_per_plugin images=10\")\n# We use the option samples_per_plugin to avoid a rare problem between matplotlib and tensorboard\n# https://stackoverflow.com/questions/27147300/matplotlib-tcl-asyncdelete-async-handler-deleted-by-the-wrong-thread\nreturn callbacks\n#####################\n# Predict\n#####################\n@utils.trained_needed\ndef _predict_object_detector(self, df_test: pd.DataFrame, **kwargs) -&gt; List[List[dict]]:\n'''Predictions on test set - batch size must is equal to 1\n        Args:\n            df_test (pd.DataFrame): Data to predict, with a column 'file_path'\n        Raises:\n            ValueError: If the model is not of type object_detector\n        Returns:\n            (list&lt;list&lt;dict&gt;&gt;): list (one entry per image) of list of bboxes\n        '''\nif self.model_type != 'object_detector':\nraise ValueError(f\"The models of type {self.model_type} do not implement the method _predict_object_detector\")\n# Instanciate the generator for predictions (batch size must be equal to 1)\ntest_generator = self._get_generator(df=df_test, data_type='test', batch_size=1, generator_type='classifier', with_img_data=True)\nfinal_predictions = []\n# For each image in df_test\nfor index_img in range(len(df_test)):\n# We get the image after preprocessing and some metadata\ninput_data, batch_prepared_img_data = test_generator.next()\ninput_img = input_data['input_img'][0]  # Batch size of 1\ninput_rois = input_data['input_rois'][0]  # Batch size of 1\nimg_data = batch_prepared_img_data[0]  # Batch size of 1\n# We predict thanks to the classifier\npredictions = self.model_classifier.predict(input_data, verbose=0)\nprobas = predictions[0][0]  # Probas match, at the level of the features map\nregr_coordinates = predictions[1][0]  # regressions,  at the level of the features map\n# We only keep the boxes which have a class different from the background and with\n# a high enough probability. At the same time, we get the class and the proba\nfm_boxes_candidates = utils_object_detectors.get_valid_fm_boxes_from_proba(probas, self.pred_bbox_proba_threshold, len(self.list_classes))\n# We apply the regression, then we get back to the level input bbox and we only keep the valid boxes\nboxes_candidates = utils_object_detectors.get_valid_boxes_from_coordinates(input_img, input_rois, fm_boxes_candidates, regr_coordinates,\nself.classifier_regr_scaling, self.shared_model_subsampling,\nself.dict_classes)\n# We apply the NMS algorithm in order to avoid overlaps\nif len(boxes_candidates):\nfinal_boxes = utils_object_detectors.non_max_suppression_fast_on_preds(boxes_candidates, self.pred_nms_overlap_threshold)\n# Finally we resize the boxes depending on the original size of the imaeg and put it in the desired format\npredicted_bboxes = utils_object_detectors.get_final_bboxes(final_boxes, img_data)\nelse:\npredicted_bboxes = []\n# We add the list of bboxes to the total list\nfinal_predictions.append(copy.deepcopy(predicted_bboxes))\n# Return\nreturn final_predictions\n#####################\n# Misc.\n#####################\ndef _plot_metrics_and_loss(self, fit_history, model_type: str = 'rpn', trainable: bool = True, **kwargs) -&gt; None:\n'''Plots some metrics &amp; losses\n        Args:\n            fit_history (?) : Fit history\n        Kwargs:\n            model_type (str): Type of the model (rpn' or 'classifier') used\n            shared_trainable (bool): If the shared model is trainable\n        '''\n# Manage dir\nplots_path = os.path.join(self.model_dir, 'plots')\nif not os.path.exists(plots_path):\nos.makedirs(plots_path)\n# Get a dictionnary of possible metrics/loss plots for both rpn &amp; classifier\nmetrics_dir_rpn = {\n'loss': [f'RPN loss with trainable set to {trainable}', f'loss_{model_type}_trainable_{trainable}'],\n'rpn_class_loss': [f'RPN classification loss with trainable set to {trainable}', f'loss_class_{model_type}_trainable_{trainable}'],\n'rpn_regr_loss': [f'RPN regression loss with trainable set to {trainable}', f'loss_regr_{model_type}_trainable_{trainable}'],\n'rpn_class_accuracy': [f'RPN classification accuracy with trainable set to {trainable}', f'accuracy_class_{model_type}_trainable_{trainable}']\n}\nmetrics_dir_classifier = {\n'loss': [f'Classifier loss with trainable set to {trainable}', f'loss_{model_type}_trainable_{trainable}'],\n'dense_class_loss': [f'Classifier classification loss with trainable set to {trainable}', f'loss_class_{model_type}_trainable_{trainable}'],\n'dense_regr_loss': [f'Classifier regression loss with trainable set to {trainable}', f'loss_regr_{model_type}_trainable_{trainable}'],\n'dense_class_accuracy': [f'Classifier classification accuracy with trainable set to {trainable}', f'accuracy_class_{model_type}_trainable_{trainable}']\n}\n# Get correct metrics dir\nif model_type == 'rpn':\nmetrics_dir = copy.deepcopy(metrics_dir_rpn)\nelse:\nmetrics_dir = copy.deepcopy(metrics_dir_classifier)\n# Plots each available metrics &amp; losses\nfor metric in fit_history.history.keys():\nif metric in metrics_dir.keys():\ntitle = metrics_dir[metric][0]\nfilename = metrics_dir[metric][1]\nplt.figure(figsize=(10, 8))\nplt.plot(fit_history.history[metric])\nplt.plot(fit_history.history[f'val_{metric}'])\nplt.title(f\"Model {title}\")\nplt.ylabel(title)\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper left')\n# Save\nfilename = f\"{filename}.jpeg\"\nplt.savefig(os.path.join(plots_path, filename))\n# Close figures\nplt.close('all')\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['vgg_filename'] = self.vgg_filename\njson_data['shared_model_subsampling'] = self.shared_model_subsampling\njson_data['anchor_box_sizes'] = self.anchor_box_sizes\njson_data['anchor_box_ratios'] = self.anchor_box_ratios\njson_data['nb_anchors'] = self.nb_anchors\njson_data['list_anchors'] = self.list_anchors\njson_data['img_min_side_size'] = self.img_min_side_size\njson_data['pool_resize_classifier'] = self.pool_resize_classifier\njson_data['rpn_regr_scaling'] = self.rpn_regr_scaling\njson_data['classifier_regr_scaling'] = self.classifier_regr_scaling\njson_data['rpn_min_overlap'] = self.rpn_min_overlap\njson_data['rpn_max_overlap'] = self.rpn_max_overlap\njson_data['rpn_restrict_num_regions'] = self.rpn_restrict_num_regions\njson_data['nb_rois_classifier'] = self.nb_rois_classifier\njson_data['roi_nms_overlap_threshold'] = self.roi_nms_overlap_threshold\njson_data['nms_max_boxes'] = self.nms_max_boxes\njson_data['classifier_min_overlap'] = self.classifier_min_overlap\njson_data['classifier_max_overlap'] = self.classifier_max_overlap\njson_data['pred_bbox_proba_threshold'] = self.pred_bbox_proba_threshold\njson_data['pred_nms_overlap_threshold'] = self.pred_nms_overlap_threshold\njson_data['batch_size_rpn_trainable_true'] = self.batch_size_rpn_trainable_true\njson_data['batch_size_classifier_trainable_true'] = self.batch_size_classifier_trainable_true\njson_data['batch_size_rpn_trainable_false'] = self.batch_size_rpn_trainable_false\njson_data['batch_size_classifier_trainable_false'] = self.batch_size_classifier_trainable_false\njson_data['epochs_rpn_trainable_true'] = self.epochs_rpn_trainable_true\njson_data['epochs_classifier_trainable_true'] = self.epochs_classifier_trainable_true\njson_data['epochs_rpn_trainable_false'] = self.epochs_rpn_trainable_false\njson_data['epochs_classifier_trainable_false'] = self.epochs_classifier_trainable_false\njson_data['patience_rpn_trainable_true'] = self.patience_rpn_trainable_true\njson_data['patience_classifier_trainable_true'] = self.patience_classifier_trainable_true\njson_data['patience_rpn_trainable_false'] = self.patience_rpn_trainable_false\njson_data['patience_classifier_trainable_false'] = self.patience_classifier_trainable_false\n# Add some code if not in json_data:\nfor layer_or_compile in ['_add_rpn_layers', '_add_classifier_layers', '_compile_model_rpn', '_compile_model_classifier']:\nif layer_or_compile not in json_data:\njson_data[layer_or_compile] = pickle.source.getsourcelines(getattr(self, layer_or_compile))[0]\n# Save\n# Save strategy :\n# - best.hdf5 already saved in fit() &amp; contains all models\n# - as we can't pickle models, we drop them, save, and reload them\nshared_model = self.shared_model\nmodel_rpn = self.model_rpn\nmodel_classifier = self.model_classifier\nself.shared_model = None\nself.model_rpn = None\nself.model_classifier = None\nsuper().save(json_data=json_data)\nself.shared_model = shared_model\nself.model_rpn = model_rpn\nself.model_classifier = model_classifier\ndef reload_models_from_hdf5(self, hdf5_path: str) -&gt; None:\n'''Reloads all models from a unique hdf5 file. This method is specific to the faster RCNN model.\n        Args:\n            hdf5_path (str): Path to the .hdf5 file with the weightds of model_all\n        Raises:\n            FileNotFoundError: If the object hdf5_path is not an existing file\n        '''\n# Check path exists\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n# Reload model (based on get_models)\n# Set layers\ninput_img = Input(shape=(None, None, 3), name='input_img')  # Warning, 3 channels !\ninput_rois = Input(shape=(None, 4), name='input_rois')\nshared_model_layers = self._get_shared_model_structure(input_img)\nrpn_layers = self._add_rpn_layers(shared_model_layers)\nclassifier_layers = self._add_classifier_layers(shared_model_layers, input_rois)\n# Init. models\nself.shared_model = Model(input_img, shared_model_layers)\nself.model_rpn = Model(input_img, rpn_layers)\nself.model_classifier = Model([input_img, input_rois], classifier_layers)\nself.model = Model([input_img, input_rois], rpn_layers + classifier_layers)\n# Load the weights (loading the weights of model all will also load the weights of the other models since they are linked)\nself.model.load_weights(hdf5_path)\ndef reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration, the network used and its preprocessing\n        - /!\\\\ Experimental /!\\\\ -\n        Kwargs:\n            configuration_path (str): Path to configuration file\n            hdf5_path (str): Path to hdf5 file\n            preprocess_input_path (str): Path to preprocess input file\n        Raises:\n            ValueError : If configuration_path is None\n            ValueError : If hdf5_path is None\n            ValueError : If preprocess_input_path is None\n            FileNotFoundError : If the object configuration_path is not an existing file\n            FileNotFoundError : If the object hdf5_path is not an existing file\n            FileNotFoundError : If the object preprocess_input_path is not an existing file\n        '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\npreprocess_input_path = kwargs.get('preprocess_input_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif preprocess_input_path is None:\nraise ValueError(\"The argument preprocess_input_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(preprocess_input_path):\nraise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save', 'batch_size',\n'epochs', 'validation_split', 'patience', 'color_mode', 'data_augmentation_params',\n'vgg_filename', 'shared_model_subsampling', 'anchor_box_sizes', 'anchor_box_ratios',\n'nb_anchors', 'list_anchors', 'img_min_side_size', 'pool_resize_classifier',\n'rpn_regr_scaling', 'classifier_regr_scaling', 'rpn_min_overlap', 'rpn_max_overlap',\n'rpn_restrict_num_regions', 'nb_rois_classifier', 'roi_nms_overlap_threshold',\n'nms_max_boxes', 'classifier_min_overlap', 'classifier_max_overlap',\n'pred_bbox_proba_threshold', 'pred_nms_overlap_threshold',\n'batch_size_rpn_trainable_true', 'batch_size_classifier_trainable_true',\n'batch_size_rpn_trainable_false', 'batch_size_classifier_trainable_false',\n'epochs_rpn_trainable_true', 'epochs_classifier_trainable_true',\n'epochs_rpn_trainable_false', 'epochs_classifier_trainable_false',\n'patience_rpn_trainable_true', 'patience_classifier_trainable_true',\n'patience_rpn_trainable_false', 'patience_classifier_trainable_false',\n'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\nfor attribute in ['width', 'height', 'depth']:\nsetattr(self, attribute, configs.get(attribute, None))\nself.in_memory = None\nself.nb_train_generator_images_to_save = None\nself.vgg_path = os.path.join(utils.get_data_path(), 'transfer_learning_weights', self.vgg_filename)  # Try to reload from usual path. Not really important if it fails.\n# Set custom objects\nself.custom_objects = {**utils_faster_rcnn.get_custom_objects_faster_rcnn(self.nb_anchors, len(self.list_classes)), **self.custom_objects}\n# Reload model\nself.reload_models_from_hdf5(hdf5_path)\n# Reload preprocess_input\nwith open(preprocess_input_path, 'rb') as f:\nself.preprocess_input = pickle.load(f)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelKerasFasterRcnnObjectDetector.__init__","title":"<code>__init__(img_min_side_size=300, rpn_min_overlap=0.3, rpn_max_overlap=0.7, rpn_restrict_num_regions=256, pool_resize_classifier=7, nb_rois_classifier=4, roi_nms_overlap_threshold=0.7, nms_max_boxes=300, classifier_min_overlap=0.1, classifier_max_overlap=0.5, pred_bbox_proba_threshold=0.6, pred_nms_overlap_threshold=0.2, data_augmentation_params=None, batch_size_rpn_trainable_true=None, batch_size_classifier_trainable_true=None, batch_size_rpn_trainable_false=None, batch_size_classifier_trainable_false=None, epochs_rpn_trainable_true=None, epochs_classifier_trainable_true=None, epochs_rpn_trainable_false=None, epochs_classifier_trainable_false=None, patience_rpn_trainable_true=None, patience_classifier_trainable_true=None, patience_rpn_trainable_false=None, patience_classifier_trainable_false=None, lr_rpn_trainable_true=1e-05, lr_classifier_trainable_true=1e-05, lr_rpn_trainable_false=1e-05, lr_classifier_trainable_false=1e-05, **kwargs)</code>","text":"<p>Initialization of the class (see ModelClass, ModelKeras &amp; ModelObjectDetectorMixin for more arguments)</p> Kwargs <p>img_min_side_size (int): Size to give to the smaller dimension as input of the model rpn_min_overlap (float): Under this threshold a region is classified as background (RPN model) rpn_max_overlap (float): Above this threshold a region is classified as object (RPN model) rpn_restrict_num_regions (int): Maximal number of regions to keep as target for the RPN pool_resize_classifier (int): Size to give to the crops done before the classifier (via ROI) nb_rois_classifier (int): Maximal number of ROIs per image during classifier training (per image of a batch) roi_nms_overlap_threshold (float): The NMS deletes overlapping ROIs whose IOU is above this threshold nms_max_boxes (int): Maximal number of ROIs to be returned by the NMS classifier_min_overlap (float): Above this threshold a ROI is considered to be a target of the classifier (but can be 'bg') classifier_max_overlap (float): Above this threshold a ROI is considered to be matching a bbox (so the target is a class, not 'bg') pred_bbox_proba_threshold (float): Above this threshold (for probabilities), a ROI is considered to be a match pred_nms_overlap_threshold (float): When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold data_augmentation_params (dict): Set of allowed data augmentation batch_size_rpn_trainable_true (int): Batch size for the RPN with for first run with trainable set to True batch_size_classifier_trainable_true (int): Batch size for the classifier for the first run with trainable set to True batch_size_rpn_trainable_false (int): Batch size for the RPN for the second run with trainable set to False batch_size_classifier_trainable_false (int): Batch size for the classifier for the second run with trainable set to False epochs_rpn_trainable_true (int): Number of epochs for the RPN for the first run with trainable set to True epochs_classifier_trainable_true (int): Number of epochs for the classifier for the first run with trainable set to True epochs_rpn_trainable_false (int): Number of epochs for the RPN for the second run with trainable set to False epochs_classifier_trainable_false (int): lNumber of epochs for the classifier for the second run with trainable set to False patience_rpn_trainable_true (int): Patience for the RPN for the first run with trainable set to True patience_classifier_trainable_true (int): Patience for the classifier for the first run with trainable set to True patience_rpn_trainable_false (int): Patience for the RPN for the second run with trainable set to False patience_classifier_trainable_false (int): Patience for the classifier for the second run with trainable set to False lr_rpn_trainable_true (float): Learning rate for the RPN for the first run with trainable set to True lr_classifier_trainable_true (float): Learning rate for the classifier for the first run with trainable set to True lr_rpn_trainable_false (float): Learning rate for the RPN for the second run with trainable set to False lr_classifier_trainable_false (float): Learning rate for the classifier for the second run with trainable set to False</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If img_min_side_size is not positive</p> <code>ValueError</code> <p>If rpn_min_overlap is not in [0, 1]</p> <code>ValueError</code> <p>If rpn_max_overlap is not in [0, 1]</p> <code>ValueError</code> <p>If rpn_min_overlap &gt; rpn_max_overlap</p> <code>ValueError</code> <p>If rpn_restrict_num_regions is not positive</p> <code>ValueError</code> <p>If pool_resize_classifier is not positive</p> <code>ValueError</code> <p>If nb_rois_classifier is not positive</p> <code>ValueError</code> <p>If roi_nms_overlap_threshold is not in [0, 1]</p> <code>ValueError</code> <p>If nms_max_boxes is not positive</p> <code>ValueError</code> <p>If classifier_min_overlap is not in [0, 1]</p> <code>ValueError</code> <p>If classifier_max_overlap is not in [0, 1]</p> <code>ValueError</code> <p>If classifier_min_overlap &gt; classifier_max_overlap</p> <code>ValueError</code> <p>If pred_bbox_proba_threshold is not in [0, 1]</p> <code>ValueError</code> <p>If pred_nms_overlap_threshold is not in [0, 1]</p> <code>ValueError</code> <p>If color_mode is not 'rgb'</p> <code>ValueError</code> <p>If the minimum size of the image is inferior to twice the subsampling ratio</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def __init__(self, img_min_side_size: int = 300, rpn_min_overlap: float = 0.3, rpn_max_overlap: float = 0.7, rpn_restrict_num_regions: int = 256,\npool_resize_classifier: int = 7, nb_rois_classifier: int = 4, roi_nms_overlap_threshold: float = 0.7, nms_max_boxes: int = 300,\nclassifier_min_overlap: float = 0.1, classifier_max_overlap: float = 0.5,\npred_bbox_proba_threshold: float = 0.6, pred_nms_overlap_threshold: float = 0.2,\ndata_augmentation_params: Union[dict, None] = None,\nbatch_size_rpn_trainable_true: Union[int, None] = None, batch_size_classifier_trainable_true: Union[int, None] = None,\nbatch_size_rpn_trainable_false: Union[int, None] = None, batch_size_classifier_trainable_false: Union[int, None] = None,\nepochs_rpn_trainable_true: Union[int, None] = None, epochs_classifier_trainable_true: Union[int, None] = None,\nepochs_rpn_trainable_false: Union[int, None] = None, epochs_classifier_trainable_false: Union[int, None] = None,\npatience_rpn_trainable_true: Union[int, None] = None, patience_classifier_trainable_true: Union[int, None] = None,\npatience_rpn_trainable_false: Union[int, None] = None, patience_classifier_trainable_false: Union[int, None] = None,\nlr_rpn_trainable_true: float = 1e-5, lr_classifier_trainable_true: float = 1e-5, lr_rpn_trainable_false: float = 1e-5,\nlr_classifier_trainable_false: float = 1e-5, **kwargs) -&gt; None:\n'''Initialization of the class (see ModelClass, ModelKeras &amp; ModelObjectDetectorMixin for more arguments)\n    Kwargs:\n        img_min_side_size (int): Size to give to the smaller dimension as input of the model\n        rpn_min_overlap (float): Under this threshold a region is classified as background (RPN model)\n        rpn_max_overlap (float): Above this threshold a region is classified as object (RPN model)\n        rpn_restrict_num_regions (int): Maximal number of regions to keep as target for the RPN\n        pool_resize_classifier (int): Size to give to the crops done before the classifier (via ROI)\n        nb_rois_classifier (int): Maximal number of ROIs per image during classifier training (per image of a batch)\n        roi_nms_overlap_threshold (float): The NMS deletes overlapping ROIs whose IOU is above this threshold\n        nms_max_boxes (int): Maximal number of ROIs to be returned by the NMS\n        classifier_min_overlap (float): Above this threshold a ROI is considered to be a target of the classifier (but can be 'bg')\n        classifier_max_overlap (float): Above this threshold a ROI is considered to be matching a bbox (so the target is a class, not 'bg')\n        pred_bbox_proba_threshold (float): Above this threshold (for probabilities), a ROI is considered to be a match\n        pred_nms_overlap_threshold (float): When predicting, the NMS deletes overlapping predictions whose IOU is above this threshold\n        data_augmentation_params (dict): Set of allowed data augmentation\n        batch_size_rpn_trainable_true (int): Batch size for the RPN with for first run with trainable set to True\n        batch_size_classifier_trainable_true (int): Batch size for the classifier for the first run with trainable set to True\n        batch_size_rpn_trainable_false (int): Batch size for the RPN for the second run with trainable set to False\n        batch_size_classifier_trainable_false (int): Batch size for the classifier for the second run with trainable set to False\n        epochs_rpn_trainable_true (int): Number of epochs for the RPN for the first run with trainable set to True\n        epochs_classifier_trainable_true (int): Number of epochs for the classifier for the first run with trainable set to True\n        epochs_rpn_trainable_false (int): Number of epochs for the RPN for the second run with trainable set to False\n        epochs_classifier_trainable_false (int): lNumber of epochs for the classifier for the second run with trainable set to False\n        patience_rpn_trainable_true (int): Patience for the RPN for the first run with trainable set to True\n        patience_classifier_trainable_true (int): Patience for the classifier for the first run with trainable set to True\n        patience_rpn_trainable_false (int): Patience for the RPN for the second run with trainable set to False\n        patience_classifier_trainable_false (int): Patience for the classifier for the second run with trainable set to False\n        lr_rpn_trainable_true (float): Learning rate for the RPN for the first run with trainable set to True\n        lr_classifier_trainable_true (float): Learning rate for the classifier for the first run with trainable set to True\n        lr_rpn_trainable_false (float): Learning rate for the RPN for the second run with trainable set to False\n        lr_classifier_trainable_false (float): Learning rate for the classifier for the second run with trainable set to False\n    Raises:\n        ValueError: If img_min_side_size is not positive\n        ValueError: If rpn_min_overlap is not in [0, 1]\n        ValueError: If rpn_max_overlap is not in [0, 1]\n        ValueError: If rpn_min_overlap &gt; rpn_max_overlap\n        ValueError: If rpn_restrict_num_regions is not positive\n        ValueError: If pool_resize_classifier is not positive\n        ValueError: If nb_rois_classifier is not positive\n        ValueError: If roi_nms_overlap_threshold is not in [0, 1]\n        ValueError: If nms_max_boxes is not positive\n        ValueError: If classifier_min_overlap is not in [0, 1]\n        ValueError: If classifier_max_overlap is not in [0, 1]\n        ValueError: If classifier_min_overlap &gt; classifier_max_overlap\n        ValueError: If pred_bbox_proba_threshold is not in [0, 1]\n        ValueError: If pred_nms_overlap_threshold is not in [0, 1]\n        ValueError: If color_mode is not 'rgb'\n        ValueError: If the minimum size of the image is inferior to twice the subsampling ratio\n    '''\n# Manage errors\nif img_min_side_size &lt; 1:\nraise ValueError(f\"The argument img_min_side_size ({img_min_side_size}) must be positive\")\nif not 0 &lt;= rpn_min_overlap &lt;= 1:\nraise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) must be between 0 and 1, included\")\nif not 0 &lt;= rpn_max_overlap &lt;= 1:\nraise ValueError(f\"The argument rpn_max_overlap ({rpn_max_overlap}) must be between 0 and 1, included\")\nif rpn_min_overlap &gt; rpn_max_overlap:\nraise ValueError(f\"The argument rpn_min_overlap ({rpn_min_overlap}) can't be superior to rpn_max_overlap ({rpn_max_overlap})\")\nif rpn_restrict_num_regions &lt; 1:\nraise ValueError(f\"The argument rpn_restrict_num_regions ({rpn_restrict_num_regions}) must be positive\")\nif pool_resize_classifier &lt; 1:\nraise ValueError(f\"The argument pool_resize_classifier ({pool_resize_classifier}) must be positive\")\nif nb_rois_classifier &lt; 1:\nraise ValueError(f\"The argument nb_rois_classifier ({nb_rois_classifier}) must be positive\")\nif not 0 &lt;= roi_nms_overlap_threshold &lt;= 1:\nraise ValueError(f\"The argument roi_nms_overlap_threshold ({roi_nms_overlap_threshold}) must be between 0 and 1, included\")\nif nms_max_boxes &lt; 1:\nraise ValueError(f\"The argument nms_max_boxes ({nms_max_boxes}) must be positive\")\nif not 0 &lt;= classifier_min_overlap &lt;= 1:\nraise ValueError(f\"The argument classifier_min_overlap ({classifier_min_overlap}) must be between 0 and 1, included\")\nif not 0 &lt;= classifier_max_overlap &lt;= 1:\nraise ValueError(f\"The argument classifier_max_overlap ({classifier_max_overlap}) must be between 0 and 1, included\")\nif classifier_min_overlap &gt; classifier_max_overlap:\nraise ValueError(f\"The argument classifier_min_overlap ({classifier_min_overlap}) can't be superior to classifier_max_overlap ({classifier_max_overlap})\")\nif not 0 &lt;= pred_bbox_proba_threshold &lt;= 1:\nraise ValueError(f\"The argument pred_bbox_proba_threshold ({pred_bbox_proba_threshold}) must be between 0 and 1, included\")\nif not 0 &lt;= pred_nms_overlap_threshold &lt;= 1:\nraise ValueError(f\"The argument pred_nms_overlap_threshold ({pred_nms_overlap_threshold}) must be between 0 and 1, included\")\n# Size of the input images (must be defined before the super init because it is used in the method _get_preprocess_input)\nself.img_min_side_size = img_min_side_size  # Default 300, in the paper 600\n# Init. (by default we have some data augmentation)\nif data_augmentation_params is None:\ndata_augmentation_params = {'horizontal_flip': True, 'vertical_flip': True, 'rot_90': True}\nsuper().__init__(data_augmentation_params=data_augmentation_params, **kwargs)\nif self.color_mode != 'rgb':\nraise ValueError(\"Faster RCNN model only accept color_mode equal to 'rgb' (compatibility VGG16).\")\n# Put to None some parameters of model_keras not used by this model\nself.width = None\nself.height = None\nself.depth = None\nself.in_memory = None\nself.nb_train_generator_images_to_save = None\n# Get logger (must be done after super init)\nself.logger = logging.getLogger(__name__)\n# Models, set on fit\nself.model: Any = None\nself.shared_model = None\nself.model_rpn = None\nself.model_classifier = None\n# Weights\nself.vgg_filename = 'vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nself.vgg_path = os.path.join(utils.get_data_path(), 'transfer_learning_weights', self.vgg_filename)\nvgg16_weights_backup_urls = [\n'https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',\n]\nif not os.path.exists(self.vgg_path):\ntry:\nself.logger.warning(\"The weights file for VGG16 is not present in your data folder.\")\nself.logger.warning(\"Trying to download the file.\")\nutils.download_url(vgg16_weights_backup_urls, self.vgg_path)\nexcept ConnectionError:\nself.logger.warning(\"Can't download. You can try to download it manually and save it on DVC.\")\nself.logger.warning(\"Building this model will return an error.\")\nself.logger.warning(\"You can download the weights here : https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\")\n# We don't raise an error because we may reload a trained model\n### Model configuration\n# Configurations related to the base model\nself.shared_model_subsampling = 16  # VGG 16\n# Error if img_min_side_size &lt; 2 * subsampling rate\nif self.img_min_side_size &lt; 2 * self.shared_model_subsampling:\nraise ValueError(\"Can't have a minimum size of an image inferior to twice the subsampling ratio\")\n# Anchors boxes\nself.anchor_box_sizes = [64, 128, 256]  # In the paper : [128, 256, 512]\nself.anchor_box_ratios = [[1, 1], [1. / math.sqrt(2), 2. / math.sqrt(2)], [2. / math.sqrt(2), 1. / math.sqrt(2)]]  # In the paper : [1, 1], [1, 2], [2, 1]]\nself.nb_anchors = len(self.anchor_box_sizes) * len(self.anchor_box_ratios)\nself.list_anchors = [[anchor_size * anchor_ratio[0], anchor_size * anchor_ratio[1]]\nfor anchor_size in self.anchor_box_sizes for anchor_ratio in self.anchor_box_ratios]\n# Sizes\nself.pool_resize_classifier = pool_resize_classifier  # Def 7\n# Scaling (we could probably do without scaling)\nself.rpn_regr_scaling = 4.0\nself.classifier_regr_scaling = [8.0, 8.0, 4.0, 4.0]\n# Thresholds for the RPN to find positive and negative anchor boxes\nself.rpn_min_overlap = rpn_min_overlap  # Def 0.3\nself.rpn_max_overlap = rpn_max_overlap  # Def 0.7\n# Maximum number of regions targets of the RPN\nself.rpn_restrict_num_regions = rpn_restrict_num_regions\n# Classifier configuration\nself.nb_rois_classifier = nb_rois_classifier  # Def 4\nself.roi_nms_overlap_threshold = roi_nms_overlap_threshold  # Def 0.7\nself.nms_max_boxes = nms_max_boxes  # Def 300\nself.classifier_min_overlap = classifier_min_overlap  # Def 0.1\nself.classifier_max_overlap = classifier_max_overlap  # Def 0.5\n# Prediction Thresholds\nself.pred_bbox_proba_threshold = pred_bbox_proba_threshold  # Def 0.6\nself.pred_nms_overlap_threshold = pred_nms_overlap_threshold  # Def 0.2\n### Misc.\n# We add the custom objects only when fitting because we need the number of classes\n# Manage batch_size, epochs &amp; patience (back up on global values if not specified)\nself.batch_size_rpn_trainable_true = batch_size_rpn_trainable_true if batch_size_rpn_trainable_true is not None else self.batch_size\nself.batch_size_classifier_trainable_true = batch_size_classifier_trainable_true if batch_size_classifier_trainable_true is not None else self.batch_size\nself.batch_size_rpn_trainable_false = batch_size_rpn_trainable_false if batch_size_rpn_trainable_false is not None else self.batch_size\nself.batch_size_classifier_trainable_false = batch_size_classifier_trainable_false if batch_size_classifier_trainable_false is not None else self.batch_size\nself.epochs_rpn_trainable_true = epochs_rpn_trainable_true if epochs_rpn_trainable_true is not None else self.epochs\nself.epochs_classifier_trainable_true = epochs_classifier_trainable_true if epochs_classifier_trainable_true is not None else self.epochs\nself.epochs_rpn_trainable_false = epochs_rpn_trainable_false if epochs_rpn_trainable_false is not None else self.epochs\nself.epochs_classifier_trainable_false = epochs_classifier_trainable_false if epochs_classifier_trainable_false is not None else self.epochs\nself.patience_rpn_trainable_true = patience_rpn_trainable_true if patience_rpn_trainable_true is not None else self.patience\nself.patience_classifier_trainable_true = patience_classifier_trainable_true if patience_classifier_trainable_true is not None else self.patience\nself.patience_rpn_trainable_false = patience_rpn_trainable_false if patience_rpn_trainable_false is not None else self.patience\nself.patience_classifier_trainable_false = patience_classifier_trainable_false if patience_classifier_trainable_false is not None else self.patience\n# Save learning rates in params_keras\nself.keras_params['lr_rpn_trainable_true'] = lr_rpn_trainable_true\nself.keras_params['lr_classifier_trainable_true'] = lr_classifier_trainable_true\nself.keras_params['lr_rpn_trainable_false'] = lr_rpn_trainable_false\nself.keras_params['lr_classifier_trainable_false'] = lr_classifier_trainable_false\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelKerasFasterRcnnObjectDetector.reload_from_standalone","title":"<code>reload_from_standalone(**kwargs)</code>","text":"<p>Reloads a model from its configuration, the network used and its preprocessing - /! Experimental /! -</p> Kwargs <p>configuration_path (str): Path to configuration file hdf5_path (str): Path to hdf5 file preprocess_input_path (str): Path to preprocess input file</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If configuration_path is None</p> <code>ValueError</code> <p>If hdf5_path is None</p> <code>ValueError</code> <p>If preprocess_input_path is None</p> <code>FileNotFoundError</code> <p>If the object configuration_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> <code>FileNotFoundError</code> <p>If the object preprocess_input_path is not an existing file</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def reload_from_standalone(self, **kwargs) -&gt; None:\n'''Reloads a model from its configuration, the network used and its preprocessing\n    - /!\\\\ Experimental /!\\\\ -\n    Kwargs:\n        configuration_path (str): Path to configuration file\n        hdf5_path (str): Path to hdf5 file\n        preprocess_input_path (str): Path to preprocess input file\n    Raises:\n        ValueError : If configuration_path is None\n        ValueError : If hdf5_path is None\n        ValueError : If preprocess_input_path is None\n        FileNotFoundError : If the object configuration_path is not an existing file\n        FileNotFoundError : If the object hdf5_path is not an existing file\n        FileNotFoundError : If the object preprocess_input_path is not an existing file\n    '''\n# Retrieve args\nconfiguration_path = kwargs.get('configuration_path', None)\nhdf5_path = kwargs.get('hdf5_path', None)\npreprocess_input_path = kwargs.get('preprocess_input_path', None)\n# Checks\nif configuration_path is None:\nraise ValueError(\"The argument configuration_path can't be None\")\nif hdf5_path is None:\nraise ValueError(\"The argument hdf5_path can't be None\")\nif preprocess_input_path is None:\nraise ValueError(\"The argument preprocess_input_path can't be None\")\nif not os.path.exists(configuration_path):\nraise FileNotFoundError(f\"The file {configuration_path} does not exist\")\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\nif not os.path.exists(preprocess_input_path):\nraise FileNotFoundError(f\"The file {preprocess_input_path} does not exist\")\n# Load confs\nwith open(configuration_path, 'r', encoding='utf-8') as f:\nconfigs = json.load(f)\n# Can't set int as keys in json, so need to cast it after reloading\n# dict_classes keys are always ints\nif 'dict_classes' in configs.keys():\nconfigs['dict_classes'] = {int(k): v for k, v in configs['dict_classes'].items()}\nelif 'list_classes' in configs.keys():\nconfigs['dict_classes'] = {i: col for i, col in enumerate(configs['list_classes'])}\n# Set class vars\n# self.model_name = # Keep the created name\n# self.model_dir = # Keep the created folder\nself.nb_fit = configs.get('nb_fit', 1)  # Consider one unique fit by default\nself.trained = configs.get('trained', True)  # Consider trained by default\n# Try to read the following attributes from configs and, if absent, keep the current one\nfor attribute in ['model_type', 'list_classes', 'dict_classes', 'level_save', 'batch_size',\n'epochs', 'validation_split', 'patience', 'color_mode', 'data_augmentation_params',\n'vgg_filename', 'shared_model_subsampling', 'anchor_box_sizes', 'anchor_box_ratios',\n'nb_anchors', 'list_anchors', 'img_min_side_size', 'pool_resize_classifier',\n'rpn_regr_scaling', 'classifier_regr_scaling', 'rpn_min_overlap', 'rpn_max_overlap',\n'rpn_restrict_num_regions', 'nb_rois_classifier', 'roi_nms_overlap_threshold',\n'nms_max_boxes', 'classifier_min_overlap', 'classifier_max_overlap',\n'pred_bbox_proba_threshold', 'pred_nms_overlap_threshold',\n'batch_size_rpn_trainable_true', 'batch_size_classifier_trainable_true',\n'batch_size_rpn_trainable_false', 'batch_size_classifier_trainable_false',\n'epochs_rpn_trainable_true', 'epochs_classifier_trainable_true',\n'epochs_rpn_trainable_false', 'epochs_classifier_trainable_false',\n'patience_rpn_trainable_true', 'patience_classifier_trainable_true',\n'patience_rpn_trainable_false', 'patience_classifier_trainable_false',\n'keras_params']:\nsetattr(self, attribute, configs.get(attribute, getattr(self, attribute)))\nfor attribute in ['width', 'height', 'depth']:\nsetattr(self, attribute, configs.get(attribute, None))\nself.in_memory = None\nself.nb_train_generator_images_to_save = None\nself.vgg_path = os.path.join(utils.get_data_path(), 'transfer_learning_weights', self.vgg_filename)  # Try to reload from usual path. Not really important if it fails.\n# Set custom objects\nself.custom_objects = {**utils_faster_rcnn.get_custom_objects_faster_rcnn(self.nb_anchors, len(self.list_classes)), **self.custom_objects}\n# Reload model\nself.reload_models_from_hdf5(hdf5_path)\n# Reload preprocess_input\nwith open(preprocess_input_path, 'rb') as f:\nself.preprocess_input = pickle.load(f)\n# Save best hdf5 in new folder\nnew_hdf5_path = os.path.join(self.model_dir, 'best.hdf5')\nshutil.copyfile(hdf5_path, new_hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelKerasFasterRcnnObjectDetector.reload_models_from_hdf5","title":"<code>reload_models_from_hdf5(hdf5_path)</code>","text":"<p>Reloads all models from a unique hdf5 file. This method is specific to the faster RCNN model.</p> <p>Parameters:</p> Name Type Description Default <code>hdf5_path</code> <code>str</code> <p>Path to the .hdf5 file with the weightds of model_all</p> required <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the object hdf5_path is not an existing file</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def reload_models_from_hdf5(self, hdf5_path: str) -&gt; None:\n'''Reloads all models from a unique hdf5 file. This method is specific to the faster RCNN model.\n    Args:\n        hdf5_path (str): Path to the .hdf5 file with the weightds of model_all\n    Raises:\n        FileNotFoundError: If the object hdf5_path is not an existing file\n    '''\n# Check path exists\nif not os.path.exists(hdf5_path):\nraise FileNotFoundError(f\"The file {hdf5_path} does not exist\")\n# Reload model (based on get_models)\n# Set layers\ninput_img = Input(shape=(None, None, 3), name='input_img')  # Warning, 3 channels !\ninput_rois = Input(shape=(None, 4), name='input_rois')\nshared_model_layers = self._get_shared_model_structure(input_img)\nrpn_layers = self._add_rpn_layers(shared_model_layers)\nclassifier_layers = self._add_classifier_layers(shared_model_layers, input_rois)\n# Init. models\nself.shared_model = Model(input_img, shared_model_layers)\nself.model_rpn = Model(input_img, rpn_layers)\nself.model_classifier = Model([input_img, input_rois], classifier_layers)\nself.model = Model([input_img, input_rois], rpn_layers + classifier_layers)\n# Load the weights (loading the weights of model all will also load the weights of the other models since they are linked)\nself.model.load_weights(hdf5_path)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_keras_faster_rcnn/#template_vision.models_training.object_detectors.model_keras_faster_rcnn.ModelKerasFasterRcnnObjectDetector.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/object_detectors/model_keras_faster_rcnn.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save configuration JSON\nif json_data is None:\njson_data = {}\njson_data['vgg_filename'] = self.vgg_filename\njson_data['shared_model_subsampling'] = self.shared_model_subsampling\njson_data['anchor_box_sizes'] = self.anchor_box_sizes\njson_data['anchor_box_ratios'] = self.anchor_box_ratios\njson_data['nb_anchors'] = self.nb_anchors\njson_data['list_anchors'] = self.list_anchors\njson_data['img_min_side_size'] = self.img_min_side_size\njson_data['pool_resize_classifier'] = self.pool_resize_classifier\njson_data['rpn_regr_scaling'] = self.rpn_regr_scaling\njson_data['classifier_regr_scaling'] = self.classifier_regr_scaling\njson_data['rpn_min_overlap'] = self.rpn_min_overlap\njson_data['rpn_max_overlap'] = self.rpn_max_overlap\njson_data['rpn_restrict_num_regions'] = self.rpn_restrict_num_regions\njson_data['nb_rois_classifier'] = self.nb_rois_classifier\njson_data['roi_nms_overlap_threshold'] = self.roi_nms_overlap_threshold\njson_data['nms_max_boxes'] = self.nms_max_boxes\njson_data['classifier_min_overlap'] = self.classifier_min_overlap\njson_data['classifier_max_overlap'] = self.classifier_max_overlap\njson_data['pred_bbox_proba_threshold'] = self.pred_bbox_proba_threshold\njson_data['pred_nms_overlap_threshold'] = self.pred_nms_overlap_threshold\njson_data['batch_size_rpn_trainable_true'] = self.batch_size_rpn_trainable_true\njson_data['batch_size_classifier_trainable_true'] = self.batch_size_classifier_trainable_true\njson_data['batch_size_rpn_trainable_false'] = self.batch_size_rpn_trainable_false\njson_data['batch_size_classifier_trainable_false'] = self.batch_size_classifier_trainable_false\njson_data['epochs_rpn_trainable_true'] = self.epochs_rpn_trainable_true\njson_data['epochs_classifier_trainable_true'] = self.epochs_classifier_trainable_true\njson_data['epochs_rpn_trainable_false'] = self.epochs_rpn_trainable_false\njson_data['epochs_classifier_trainable_false'] = self.epochs_classifier_trainable_false\njson_data['patience_rpn_trainable_true'] = self.patience_rpn_trainable_true\njson_data['patience_classifier_trainable_true'] = self.patience_classifier_trainable_true\njson_data['patience_rpn_trainable_false'] = self.patience_rpn_trainable_false\njson_data['patience_classifier_trainable_false'] = self.patience_classifier_trainable_false\n# Add some code if not in json_data:\nfor layer_or_compile in ['_add_rpn_layers', '_add_classifier_layers', '_compile_model_rpn', '_compile_model_classifier']:\nif layer_or_compile not in json_data:\njson_data[layer_or_compile] = pickle.source.getsourcelines(getattr(self, layer_or_compile))[0]\n# Save\n# Save strategy :\n# - best.hdf5 already saved in fit() &amp; contains all models\n# - as we can't pickle models, we drop them, save, and reload them\nshared_model = self.shared_model\nmodel_rpn = self.model_rpn\nmodel_classifier = self.model_classifier\nself.shared_model = None\nself.model_rpn = None\nself.model_classifier = None\nsuper().save(json_data=json_data)\nself.shared_model = shared_model\nself.model_rpn = model_rpn\nself.model_classifier = model_classifier\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/","title":"Model object detector","text":""},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/#template_vision.models_training.object_detectors.model_object_detector.ModelObjectDetectorMixin","title":"<code>ModelObjectDetectorMixin</code>","text":"<p>Parent class (Mixin) for the model of type object detector</p> Source code in <code>template_vision/models_training/object_detectors/model_object_detector.py</code> <pre><code>class ModelObjectDetectorMixin:\n'''Parent class (Mixin) for the model of type object detector'''\n# Not implemented :\n# -&gt; predict : to be implemented by the parent class using this mixin\ndef __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n'''Initialization of the parent class - Object detector\n        Kwargs:\n            level_save (str): Level of saving\n                LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n                MEDIUM: LOW + hdf5 + pkl + plots\n                HIGH: MEDIUM + predictions\n        Raises:\n            ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n        '''\nsuper().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type\nself.model_type = 'object_detector'\n# List of classes to consider (set on fit)\nself.list_classes = None\nself.dict_classes = None\n# Other options\nself.level_save = level_save\ndef inverse_transform(self, y) -&gt; list:\n'''Gets a list of classes from predictions.\n        Useless here, used solely for compatibility.\n        Args:\n            y (?): Array-like, shape = [n_samples, n_features], arrays of 0s and 1s\n        Returns:\n            (list)\n        '''\nreturn list(y) if isinstance(y, np.ndarray) else y\ndef get_and_save_metrics(self, y_true: list, y_pred: list, list_files_x: list = None,\ntype_data: str = '', **kwargs) -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n        Args:\n            y_true (list): Bboxes list, one entry corresponds to the bboxes of one file - truth\n                format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n            y_pred (list): Bboxes list, one entry corresponds to the bboxes of one file - predicted\n                format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ..., 'proba': ...}\n        Kwargs:\n            list_files_x (?): List of input files for the prediction\n            type_data (str): Type of the dataset (validation, test, ...)\n        Returns:\n            pd.DataFrame: The dataframe containing statistics\n        '''\n# Manage errors\nif len(y_true) != len(y_pred):\nraise ValueError(f\"The size of the two lists (y_true et y_pred) must be equal ({len(y_true)} != {len(y_pred)})\")\nif list_files_x is not None and len(y_true) != len(list_files_x):\nraise ValueError(f\"The size of the two lists (y_true et list_files_x) must be equal ({len(y_true)} != {len(list_files_x)})\")\n# Construction dataframe\nif list_files_x is None:\ndf = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\nelse:\ndf = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'file_path': list_files_x})\n# Save a prediction file if wanted\nif self.level_save == 'HIGH':\nfile_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\nif 'file_path' in df.columns:\ndf = df.sort_values('file_path')\ndf.to_csv(file_path, sep=';', index=None, encoding='utf-8')\n# Print info on missing classes and the impact on metrics\ngt_classes = set([bbox['class'] for bboxes in y_true for bbox in bboxes])\ngt_classes_not_in_model = gt_classes.difference(set(self.list_classes))\nmodel_classes_not_in_gt = set(self.list_classes).difference(gt_classes)\n# Prints\nif len(gt_classes_not_in_model):\nself.logger.info(f\"Classes {gt_classes_not_in_model} are not predicted by the model.\")\nself.logger.info(\"We won't take them into account in the calculation of the metrics.\")\nif len(model_classes_not_in_gt):\nself.logger.info(f\"Classes {model_classes_not_in_gt} are not present in the dataset used to calculate the metrics.\")\nself.logger.info(\"Metrics on these classes won't be accurate.\")\n# Get the classes support\ntotal_bbox = sum([1 for image in y_true for bbox in image if bbox['class'] in self.list_classes])\nclasses_support = {}\nif total_bbox == 0:\ntotal_bbox = 1\nfor cl in self.list_classes:\nclasses_support[cl] = sum([bbox['class'] == cl for image in y_true for bbox in image]) / total_bbox\n# Get metrics\n# We use the COCO method to get the Average Precision (AP)\ndict_ap_coco = self._get_coco_ap(y_true, y_pred)\n# Calculate the mean Average Precision (mAP) (weighted or not)\ncoco_map = np.mean([value for value in list(dict_ap_coco.values()) if not np.isnan(value)])\ncoco_wap = sum([dict_ap_coco[cl] * classes_support[cl] for cl in self.list_classes if classes_support[cl] &gt; 0])\n# Global statistics\nself.logger.info('-- * * * * * * * * * * * * * * --')\nself.logger.info(f\"Statistics mAP{' ' + type_data if len(type_data) &gt; 0 else ''}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"mean Average Precision (mAP) - COCO method : {round(coco_map, 4)}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"weighted Average Precision (wAP) - COCO method : {round(coco_wap, 4)}\")\nself.logger.info('--------------------------------')\n# Statistics per classes\nfor cl in self.list_classes:\nself.logger.info(f\"Class {cl}: AP COCO = {round(dict_ap_coco[cl], 4)} /// Support = {round(classes_support[cl], 4)}\")\nself.logger.info('--------------------------------')\n# Construction df_stats\ndf_stats = pd.DataFrame(columns=['Label', 'AP COCO', 'Support'])\ndf_stats = df_stats.append({'Label': 'All', 'AP COCO': coco_map, 'Support': 1.0}, ignore_index=True)\nfor cl in self.list_classes:\ndf_stats = df_stats.append({'Label': cl, 'AP COCO': dict_ap_coco[cl], 'Support': classes_support[cl]}, ignore_index=True)\n# Save csv\nfile_path = os.path.join(self.model_dir, f\"map_coco{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(coco_map, 4)}.csv\")\ndf_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n# Return df_stats\nreturn df_stats\ndef _get_coco_ap(self, y_true: list, y_pred: list) -&gt; dict:\n'''Calculate COCO's AP for each of the class and gives the result in a dictionary\n        where the keys are the classes and the valeus, the corresponding AP value\n         Args:\n            y_true (list): Bboxes list, one entry corresponds to the bboxes of one file - truth\n                format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n            y_pred (list): Bboxes list, one entry corresponds to the bboxes of one file - predicted\n                format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ..., 'proba': ...}\n        Returns:\n            The dictionary containing AP for each class\n        '''\ninv_dict_classes = {value: key for key, value in self.dict_classes.items()}\n# Put the bboxes in COCO format\ncoco_true = self._put_bboxes_in_coco_format(y_true, inv_dict_classes)\ncoco_pred = self._put_bboxes_in_coco_format(y_pred, inv_dict_classes)\nimages = [{'id': i + 1} for i in range(len(y_true))]\ncategories = [{'id': class_id, 'name': class_name, 'supercategory': 'none'}\nfor class_id, class_name in self.dict_classes.items()]\ndataset_coco_true = {'type': 'instances',\n'images': images.copy(),\n'categories': categories.copy(),\n'annotations': coco_true}\ndataset_coco_pred = {'images': images.copy(),\n'categories': categories.copy(),\n'annotations': coco_pred}\n# Call pycocotools API to calculate the AP\ncoco_eval = self._get_coco_evaluations(dataset_coco_true, dataset_coco_pred)\ndict_ap = self._get_ap_for_classes(coco_eval)\nreturn dict_ap\n@classmethod\ndef _put_bboxes_in_coco_format(self, bboxes: List[List[dict]], inv_dict_classes: dict) -&gt; List[dict]:\n'''Puts a list of list of bboxes (for example from a prediction) in the right format for pycocotools API.\n        Args:\n            bboxes (list&lt;list&lt;dict&gt;&gt;) : A list of list of bboxes. The first level of list corresponds to the images and the second level to the\n            bboxes of this image.\n            inv_dict_classes (dict) : The dictionary of classes in the format {class_name: class_id}\n        Returns:\n            A list of bboxes\n        '''\nannotations = []\nidx_bbox = 1  # WARNING: index begins at 1\nfor idx_img, list_bboxes in enumerate(bboxes):\nfor bbox in list_bboxes:\ndict_bbox = {'id': idx_bbox,\n'image_id': idx_img + 1,  # WARNING : index begins at 1\n'category_id': inv_dict_classes[bbox['class']],\n'bbox': np.array([bbox['x1'], bbox['y1'], bbox['x2'] - bbox['x1'], bbox['y2'] - bbox['y1']]),\n'area': (bbox['y2'] - bbox['y1']) * (bbox['x2'] - bbox['x1']),\n'iscrowd': 0,\n'score': bbox.get('proba', 1)}\nidx_bbox += 1\nannotations.append(dict_bbox.copy())\nreturn annotations\n@classmethod\ndef _get_coco_evaluations(self, dataset_coco_true: dict, dataset_coco_pred: dict) -&gt; COCOeval:\n'''Calculates the AP from true and predicted datasets in the COCO format, the returns COCOeval,\n        the pycocotools API containing all the results.\n        Args:\n            dataset_coco_true (dict) : Ground truth bboxes in COCO format\n            dataset_coco_pred (dict) : Predicted bboxes in COCO format\n        Returns:\n            A COCOeval (pycocotools API) containing the AP\n        '''\n# Everything on mute ! pycocotools library prints too much logs and there are no level settings\nwith utils.HiddenPrints():\n# Put the ground truth bboxes in the pycocotools API\ncoco_ds = COCO()\ncoco_ds.dataset = dataset_coco_true.copy()\ncoco_ds.createIndex()\n# Put the predicted bboxes in the pycocotools API\ncoco_dt = COCO()\ncoco_dt.dataset = dataset_coco_pred.copy()\ncoco_dt.createIndex()\n# Get image IDs\nimgIds = sorted(coco_ds.getImgIds())\n# Set evaluator\ncocoEval = COCOeval(coco_ds, coco_dt, 'bbox')\ncocoEval.params.imgIds = imgIds\ncocoEval.params.useCats = True\ncocoEval.params.iouType = \"bbox\"\n# Evaluate\ncocoEval.evaluate()\ncocoEval.accumulate()\n# Return evaluator\nreturn cocoEval\ndef _get_ap_for_classes(self, coco_eval: COCOeval) -&gt; dict:\n'''Gets the AP per class from cocoEval, the pycocotools API.\n        Args:\n            coco_eval (COCOeval) : A pycocotools COCOeval which calculated the AP.\n                In this function, we just get them, we do not calculate them\n        Returns:\n            The dictionary containing the AP for each class\n        '''\n# Compute per-category AP\n# from https://detectron2.readthedocs.io/en/latest/_modules/detectron2/evaluation/coco_evaluation.html\nprecisions = coco_eval.eval[\"precision\"]\n# precision has dims (iou, recall, cls, area range, max dets)\nassert len(self.dict_classes) == precisions.shape[2]\n# Retrieve APs\ndict_ap = {}\nfor idx, name in self.dict_classes.items():\n# area range index 0: all area ranges\n# max dets index -1: typically 100 per image\nprecision = precisions[:, :, idx, 0, -1]\nprecision = precision[precision &gt; -1]\nap = np.mean(precision) if precision.size else float(\"nan\")\ndict_ap[name] = ap\nreturn dict_ap\ndef save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n        Kwargs:\n            json_data (dict): Additional configurations to be saved\n        '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['list_classes'] = self.list_classes\njson_data['dict_classes'] = self.dict_classes\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/#template_vision.models_training.object_detectors.model_object_detector.ModelObjectDetectorMixin.__init__","title":"<code>__init__(level_save='HIGH', **kwargs)</code>","text":"<p>Initialization of the parent class - Object detector</p> Kwargs <p>level_save (str): Level of saving     LOW: stats + configurations + logger keras - /! The model can't be reused /! -     MEDIUM: LOW + hdf5 + pkl + plots     HIGH: MEDIUM + predictions</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])</p> Source code in <code>template_vision/models_training/object_detectors/model_object_detector.py</code> <pre><code>def __init__(self, level_save: str = 'HIGH', **kwargs) -&gt; None:\n'''Initialization of the parent class - Object detector\n    Kwargs:\n        level_save (str): Level of saving\n            LOW: stats + configurations + logger keras - /!\\\\ The model can't be reused /!\\\\ -\n            MEDIUM: LOW + hdf5 + pkl + plots\n            HIGH: MEDIUM + predictions\n    Raises:\n        ValueError: If the object level_save is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\n    '''\nsuper().__init__(level_save=level_save, **kwargs)  # forwards level_save &amp; all unused arguments\nif level_save not in ['LOW', 'MEDIUM', 'HIGH']:\nraise ValueError(f\"The object level_save ({level_save}) is not a valid option (['LOW', 'MEDIUM', 'HIGH'])\")\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Model type\nself.model_type = 'object_detector'\n# List of classes to consider (set on fit)\nself.list_classes = None\nself.dict_classes = None\n# Other options\nself.level_save = level_save\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/#template_vision.models_training.object_detectors.model_object_detector.ModelObjectDetectorMixin.get_and_save_metrics","title":"<code>get_and_save_metrics(y_true, y_pred, list_files_x=None, type_data='', **kwargs)</code>","text":"<p>Gets and saves the metrics of a model</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <code>list</code> <p>Bboxes list, one entry corresponds to the bboxes of one file - truth format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}</p> required <code>y_pred</code> <code>list</code> <p>Bboxes list, one entry corresponds to the bboxes of one file - predicted format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ..., 'proba': ...}</p> required Kwargs <p>list_files_x (?): List of input files for the prediction type_data (str): Type of the dataset (validation, test, ...)</p> <p>Returns:</p> Type Description <code>pd.DataFrame</code> <p>pd.DataFrame: The dataframe containing statistics</p> Source code in <code>template_vision/models_training/object_detectors/model_object_detector.py</code> <pre><code>def get_and_save_metrics(self, y_true: list, y_pred: list, list_files_x: list = None,\ntype_data: str = '', **kwargs) -&gt; pd.DataFrame:\n'''Gets and saves the metrics of a model\n    Args:\n        y_true (list): Bboxes list, one entry corresponds to the bboxes of one file - truth\n            format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ...}\n        y_pred (list): Bboxes list, one entry corresponds to the bboxes of one file - predicted\n            format bbox : {'class': ..., 'x1': ..., 'y1': ..., 'x2': ..., 'y2': ..., 'proba': ...}\n    Kwargs:\n        list_files_x (?): List of input files for the prediction\n        type_data (str): Type of the dataset (validation, test, ...)\n    Returns:\n        pd.DataFrame: The dataframe containing statistics\n    '''\n# Manage errors\nif len(y_true) != len(y_pred):\nraise ValueError(f\"The size of the two lists (y_true et y_pred) must be equal ({len(y_true)} != {len(y_pred)})\")\nif list_files_x is not None and len(y_true) != len(list_files_x):\nraise ValueError(f\"The size of the two lists (y_true et list_files_x) must be equal ({len(y_true)} != {len(list_files_x)})\")\n# Construction dataframe\nif list_files_x is None:\ndf = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred})\nelse:\ndf = pd.DataFrame({'y_true': y_true, 'y_pred': y_pred, 'file_path': list_files_x})\n# Save a prediction file if wanted\nif self.level_save == 'HIGH':\nfile_path = os.path.join(self.model_dir, f\"predictions{'_' + type_data if len(type_data) &gt; 0 else ''}.csv\")\nif 'file_path' in df.columns:\ndf = df.sort_values('file_path')\ndf.to_csv(file_path, sep=';', index=None, encoding='utf-8')\n# Print info on missing classes and the impact on metrics\ngt_classes = set([bbox['class'] for bboxes in y_true for bbox in bboxes])\ngt_classes_not_in_model = gt_classes.difference(set(self.list_classes))\nmodel_classes_not_in_gt = set(self.list_classes).difference(gt_classes)\n# Prints\nif len(gt_classes_not_in_model):\nself.logger.info(f\"Classes {gt_classes_not_in_model} are not predicted by the model.\")\nself.logger.info(\"We won't take them into account in the calculation of the metrics.\")\nif len(model_classes_not_in_gt):\nself.logger.info(f\"Classes {model_classes_not_in_gt} are not present in the dataset used to calculate the metrics.\")\nself.logger.info(\"Metrics on these classes won't be accurate.\")\n# Get the classes support\ntotal_bbox = sum([1 for image in y_true for bbox in image if bbox['class'] in self.list_classes])\nclasses_support = {}\nif total_bbox == 0:\ntotal_bbox = 1\nfor cl in self.list_classes:\nclasses_support[cl] = sum([bbox['class'] == cl for image in y_true for bbox in image]) / total_bbox\n# Get metrics\n# We use the COCO method to get the Average Precision (AP)\ndict_ap_coco = self._get_coco_ap(y_true, y_pred)\n# Calculate the mean Average Precision (mAP) (weighted or not)\ncoco_map = np.mean([value for value in list(dict_ap_coco.values()) if not np.isnan(value)])\ncoco_wap = sum([dict_ap_coco[cl] * classes_support[cl] for cl in self.list_classes if classes_support[cl] &gt; 0])\n# Global statistics\nself.logger.info('-- * * * * * * * * * * * * * * --')\nself.logger.info(f\"Statistics mAP{' ' + type_data if len(type_data) &gt; 0 else ''}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"mean Average Precision (mAP) - COCO method : {round(coco_map, 4)}\")\nself.logger.info('--------------------------------')\nself.logger.info(f\"weighted Average Precision (wAP) - COCO method : {round(coco_wap, 4)}\")\nself.logger.info('--------------------------------')\n# Statistics per classes\nfor cl in self.list_classes:\nself.logger.info(f\"Class {cl}: AP COCO = {round(dict_ap_coco[cl], 4)} /// Support = {round(classes_support[cl], 4)}\")\nself.logger.info('--------------------------------')\n# Construction df_stats\ndf_stats = pd.DataFrame(columns=['Label', 'AP COCO', 'Support'])\ndf_stats = df_stats.append({'Label': 'All', 'AP COCO': coco_map, 'Support': 1.0}, ignore_index=True)\nfor cl in self.list_classes:\ndf_stats = df_stats.append({'Label': cl, 'AP COCO': dict_ap_coco[cl], 'Support': classes_support[cl]}, ignore_index=True)\n# Save csv\nfile_path = os.path.join(self.model_dir, f\"map_coco{'_' + type_data if len(type_data) &gt; 0 else ''}@{round(coco_map, 4)}.csv\")\ndf_stats.to_csv(file_path, sep=';', index=False, encoding='utf-8')\n# Return df_stats\nreturn df_stats\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/#template_vision.models_training.object_detectors.model_object_detector.ModelObjectDetectorMixin.inverse_transform","title":"<code>inverse_transform(y)</code>","text":"<p>Gets a list of classes from predictions. Useless here, used solely for compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>y</code> <code>?</code> <p>Array-like, shape = [n_samples, n_features], arrays of 0s and 1s</p> required <p>Returns:</p> Type Description <code>list</code> <p>(list)</p> Source code in <code>template_vision/models_training/object_detectors/model_object_detector.py</code> <pre><code>def inverse_transform(self, y) -&gt; list:\n'''Gets a list of classes from predictions.\n    Useless here, used solely for compatibility.\n    Args:\n        y (?): Array-like, shape = [n_samples, n_features], arrays of 0s and 1s\n    Returns:\n        (list)\n    '''\nreturn list(y) if isinstance(y, np.ndarray) else y\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/model_object_detector/#template_vision.models_training.object_detectors.model_object_detector.ModelObjectDetectorMixin.save","title":"<code>save(json_data=None)</code>","text":"<p>Saves the model</p> Kwargs <p>json_data (dict): Additional configurations to be saved</p> Source code in <code>template_vision/models_training/object_detectors/model_object_detector.py</code> <pre><code>def save(self, json_data: Union[dict, None] = None) -&gt; None:\n'''Saves the model\n    Kwargs:\n        json_data (dict): Additional configurations to be saved\n    '''\n# Save model\nif json_data is None:\njson_data = {}\njson_data['list_classes'] = self.list_classes\njson_data['dict_classes'] = self.dict_classes\n# Save\nsuper().save(json_data=json_data)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/","title":"Utils faster rcnn","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer","title":"<code>RoiPoolingLayer</code>","text":"<p>         Bases: <code>Layer</code></p> <p>Layer selecting a zone from a ROI in a features map and resize it</p>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer--input-shape","title":"Input shape","text":"<pre><code>List of two 4D tensors [X_img, X_roi] with shape:\nX_img : list of images\n    (batch_size, cols, rows, channels)\nX_roi : list of ROI with 4 coordinates (x, y, w, h)\n    (batch_size, nb_rois, 4)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer--output-shape","title":"Output shape","text":"<pre><code>5D tensor with shape:\n    (batch_size, nb_rois, pool_size, pool_size, channels)\n    pool_size is a parameter of resizing of the features map\n</code></pre> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>class RoiPoolingLayer(Layer):\n'''Layer selecting a zone from a ROI in a features map and resize it\n    # Input shape\n        List of two 4D tensors [X_img, X_roi] with shape:\n        X_img : list of images\n            (batch_size, cols, rows, channels)\n        X_roi : list of ROI with 4 coordinates (x, y, w, h)\n            (batch_size, nb_rois, 4)\n    # Output shape\n        5D tensor with shape:\n            (batch_size, nb_rois, pool_size, pool_size, channels)\n            pool_size is a parameter of resizing of the features map\n    '''\ndef __init__(self, pool_size: int, **kwargs) -&gt; None:\n'''Initialization of the layer\n        Args:\n            pool_size (int): Output size of the layer\n        '''\nself.pool_size = pool_size\nsuper().__init__(**kwargs)\ndef build(self, input_shape):\nself.nb_channels = input_shape[0][3]\ndef compute_output_shape(self, input_shape):\nreturn None, None, self.pool_size, self.pool_size, self.nb_channels\ndef cut_feature_map(self, feature_map, roi):\n'''Cuts a features map with a ROI\n        Args:\n            feature_map : input features map\n                # Shape : (cols, rows, channels)\n            roi : input ROI\n                # Shape : (4,)\n        '''\nx, y, h, w = roi[0], roi[1], roi[2], roi[3]\nreturn tf.image.resize(feature_map[y: y + h, x: x + w, :], (self.pool_size, self.pool_size))\ndef call(self, x: list, mask=None):\n'''Call to the layer\n        Args:\n            x (list): List of two tensors\n                0 -&gt; features maps # Shape (batch_size, cols, rows, channels)\n                1 -&gt; rois # Shape (batch_size, nb_rois, 4)\n        Returns:\n            tensor: images (features maps) cut with the ROIs\n                # Shape (batch_size, nb_rois, cols, rows, nb_channel)\n        '''\n# Get the tensors\nfeature_maps = x[0]  # Shape (batch_size, cols, rows, channels)\nrois = K.cast(x[1], 'int32')  # Shape (batch_size, nb_rois, 4)\n# We loop on each batch, and then, we loop on each ROI\n# We crop each image with the ROIs of the batch\n# We also resize (cf. pool_size)\n# We format the results and return it\n# TODO: WARNING, WE MUST HAVE THE SAME NUMBER OF ROIS PER IMAGE :'(\noutput = tf.map_fn(lambda batch:\n# IN : fmap (h, w, nb_channel)\n# IN : rois (nb_rois, 4)\ntf.map_fn(\n# IN : fmap (h, w, nb_channel)\n# IN : roi (4,)\nlambda roi: self.cut_feature_map(batch[0], roi)\n, batch[1], fn_output_signature=tf.float32\n# OUT (pool_size, pool_size, nb_channel)\n)\n# OUT : (nb_rois, pool_size, pool_size, nb_channel)\n, (feature_maps, rois), fn_output_signature=tf.float32)\n# OUT : (batch_size, nb_rois, pool_size, pool_size, nb_channel)\nreturn output\ndef get_config(self) -&gt; dict:\nconfig = {'pool_size': self.pool_size}\nbase_config = super().get_config()\nreturn dict(list(base_config.items()) + list(config.items()))\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer.__init__","title":"<code>__init__(pool_size, **kwargs)</code>","text":"<p>Initialization of the layer</p> <p>Parameters:</p> Name Type Description Default <code>pool_size</code> <code>int</code> <p>Output size of the layer</p> required Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def __init__(self, pool_size: int, **kwargs) -&gt; None:\n'''Initialization of the layer\n    Args:\n        pool_size (int): Output size of the layer\n    '''\nself.pool_size = pool_size\nsuper().__init__(**kwargs)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer.call","title":"<code>call(x, mask=None)</code>","text":"<p>Call to the layer</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>list</code> <p>List of two tensors 0 -&gt; features maps # Shape (batch_size, cols, rows, channels) 1 -&gt; rois # Shape (batch_size, nb_rois, 4)</p> required <p>Returns:</p> Name Type Description <code>tensor</code> <p>images (features maps) cut with the ROIs</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def call(self, x: list, mask=None):\n'''Call to the layer\n    Args:\n        x (list): List of two tensors\n            0 -&gt; features maps # Shape (batch_size, cols, rows, channels)\n            1 -&gt; rois # Shape (batch_size, nb_rois, 4)\n    Returns:\n        tensor: images (features maps) cut with the ROIs\n            # Shape (batch_size, nb_rois, cols, rows, nb_channel)\n    '''\n# Get the tensors\nfeature_maps = x[0]  # Shape (batch_size, cols, rows, channels)\nrois = K.cast(x[1], 'int32')  # Shape (batch_size, nb_rois, 4)\n# We loop on each batch, and then, we loop on each ROI\n# We crop each image with the ROIs of the batch\n# We also resize (cf. pool_size)\n# We format the results and return it\n# TODO: WARNING, WE MUST HAVE THE SAME NUMBER OF ROIS PER IMAGE :'(\noutput = tf.map_fn(lambda batch:\n# IN : fmap (h, w, nb_channel)\n# IN : rois (nb_rois, 4)\ntf.map_fn(\n# IN : fmap (h, w, nb_channel)\n# IN : roi (4,)\nlambda roi: self.cut_feature_map(batch[0], roi)\n, batch[1], fn_output_signature=tf.float32\n# OUT (pool_size, pool_size, nb_channel)\n)\n# OUT : (nb_rois, pool_size, pool_size, nb_channel)\n, (feature_maps, rois), fn_output_signature=tf.float32)\n# OUT : (batch_size, nb_rois, pool_size, pool_size, nb_channel)\nreturn output\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer.call--shape-batch_size-nb_rois-cols-rows-nb_channel","title":"Shape (batch_size, nb_rois, cols, rows, nb_channel)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer.cut_feature_map","title":"<code>cut_feature_map(feature_map, roi)</code>","text":"<p>Cuts a features map with a ROI</p> <p>Parameters:</p> Name Type Description Default <code>feature_map</code> <p>input features map</p> required <code>roi</code> <p>input ROI</p> required Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def cut_feature_map(self, feature_map, roi):\n'''Cuts a features map with a ROI\n    Args:\n        feature_map : input features map\n            # Shape : (cols, rows, channels)\n        roi : input ROI\n            # Shape : (4,)\n    '''\nx, y, h, w = roi[0], roi[1], roi[2], roi[3]\nreturn tf.image.resize(feature_map[y: y + h, x: x + w, :], (self.pool_size, self.pool_size))\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer.cut_feature_map--shape-cols-rows-channels","title":"Shape : (cols, rows, channels)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.RoiPoolingLayer.cut_feature_map--shape-4","title":"Shape : (4,)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.class_loss_cls","title":"<code>class_loss_cls(y_true, y_pred)</code>","text":"<p>Calculates the classifier classification loss (Cross entropy)</p> <p>Parameters:</p> Name Type Description Default <code>y_true</code> <p>Model's target. Shape (batch_size, nb_bboxes, nb_classes)</p> required <code>y_pred</code> <p>Outputs of the model. Shape (batch_size, nb_bboxes, nb_classes)</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>Calculated loss</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def class_loss_cls(y_true, y_pred):\n'''Calculates the classifier classification loss (Cross entropy)\n    Args:\n        y_true: Model's target. Shape (batch_size, nb_bboxes, nb_classes)\n        y_pred: Outputs of the model. Shape (batch_size, nb_bboxes, nb_classes)\n    Returns:\n        float: Calculated loss\n    '''\nreturn K.mean(categorical_crossentropy(y_true, y_pred))\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.get_class_loss_regr","title":"<code>get_class_loss_regr(nb_classes)</code>","text":"<p>Gets the classifier regression loss depending on the number of classes of the model</p> <p>Parameters:</p> Name Type Description Default <code>nb_classes</code> <code>int</code> <p>Number of classes of the model</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Classifier regression loss</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def get_class_loss_regr(nb_classes: int) -&gt; Callable:\n'''Gets the classifier regression loss depending on the number of classes of the model\n    Args:\n        nb_classes (int): Number of classes of the model\n    Returns:\n        Callable: Classifier regression loss\n    '''\ndef class_loss_regr(y_true, y_pred):\n'''Calculates the classifier regression loss (Huber loss)\n               0.5*x\u00b2 (if x_abs &lt; 1)\n               x_abs - 0.5 (otherwise)\n        Args:\n            y_true: Model's targets. Shape (batch_size, nb_bboxes, 2*4*num_classes)\n                first part : ROI class\n                                  Example with two classes (without taking 'bg' into account)\n                                    -&gt; [0, 0, 0, 0, 0, 0, 0, 0] : background\n                                    -&gt; [0, 0, 0, 0, 1, 1, 1, 1] : classe n\u00b01\n                                    etc...\n                                    the loss is calculated only on ROIs associated to an object\n                second part : regression\n            y_pred: Outputs of the model. Shape (batch_size, nb_bboxes, 4*num_classes)\n        Returns:\n            float: Calculated loss\n        '''\ny_true = tf.cast(y_true, tf.float32)\ny_pred = tf.cast(y_pred, tf.float32)\n# Evaluate difference\nind_sep = 4 * nb_classes  # Separation index of the two parts of y_true\nx = y_true[:, :, ind_sep:] - y_pred\nx_abs = K.abs(x)\n# If x_abs &lt;= 1.0, x_bool = 1\nx_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\nreturn K.sum(y_true[:, :, :ind_sep] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(1e-4 + y_true[:, :, :ind_sep])\n# Return loss\nreturn class_loss_regr\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.get_custom_objects_faster_rcnn","title":"<code>get_custom_objects_faster_rcnn(nb_anchors, nb_classes)</code>","text":"<p>Gets the keras custom_objects depending of the number of anchors and of classes of a model</p> <p>Parameters:</p> Name Type Description Default <code>nb_anchors</code> <code>int</code> <p>Number of anchors of the model</p> required <code>nb_classes</code> <code>int</code> <p>Number of classes of the model</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Set of customs objects</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def get_custom_objects_faster_rcnn(nb_anchors: int, nb_classes: int) -&gt; dict:\n'''Gets the keras custom_objects depending of the number of anchors and of classes of a model\n    Args:\n        nb_anchors (int): Number of anchors of the model\n        nb_classes (int): Number of classes of the model\n    Returns:\n        dict: Set of customs objects\n    '''\n# /!\\ Important -&gt; This dictionary defines the \"custom\" objets used in our Faster RCNN models\n# /!\\ Important -&gt; They are mandatory in order to serialize and save the model\n# /!\\ Important -&gt; All customs objects must be added to it\ncustom_objects_faster_rcnn = {\n'RoiPoolingLayer': RoiPoolingLayer,\n'rpn_loss_regr': get_rpn_loss_regr(nb_anchors),\n'rpn_loss_cls': get_rpn_loss_cls(nb_anchors),\n'class_loss_cls': class_loss_cls,\n'class_loss_regr': get_class_loss_regr(nb_classes),\n}\nreturn custom_objects_faster_rcnn\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.get_rpn_loss_cls","title":"<code>get_rpn_loss_cls(nb_anchors)</code>","text":"<p>Gets the RPN classification loss depending on the number of anchor of the model</p> <p>Parameters:</p> Name Type Description Default <code>nb_anchors</code> <code>int</code> <p>Number of anchors of the model</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>RPN classification loss</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def get_rpn_loss_cls(nb_anchors: int) -&gt; Callable:\n'''Gets the RPN classification loss depending on the number of anchor of the model\n    Args:\n        nb_anchors (int): Number of anchors of the model\n    Returns:\n        Callable: RPN classification loss\n    '''\ndef rpn_loss_cls(y_true, y_pred) -&gt; float:\n'''Calculates the RPN classification loss (Cross entropy)\n        Args:\n            y_true: Model's targets. Shape (batch_size, height, width, 2*nb_anchors)\n                first part : validity of the anchor box. Valid if positive (object match) or negative (background match),\n                                                         Not valid if neutral (in between object and background)\n                             the loss is calculated only on valid anchor boxes\n                second part : classe of the anchor box\n                                 0  --&gt; background\n                                 1  --&gt; object\n            y_pred: Outputs of the model. Shape (batch_size, height, width, nb_anchors)\n        Returns:\n            float: Calculated loss\n        '''\nind_sep = nb_anchors  # Separation index of the two parts of y_true\nreturn K.sum(y_true[:, :, :, :ind_sep] * K.binary_crossentropy(y_pred[:, :, :, :], y_true[:, :, :, ind_sep:])) / K.sum(1e-4 + y_true[:, :, :, :ind_sep])\n# Return loss\nreturn rpn_loss_cls\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_faster_rcnn/#template_vision.models_training.object_detectors.utils_faster_rcnn.get_rpn_loss_regr","title":"<code>get_rpn_loss_regr(nb_anchors)</code>","text":"<p>Gets the RPN regression loss depending on the number of anchor of the model</p> <p>Parameters:</p> Name Type Description Default <code>nb_anchors</code> <code>int</code> <p>Number of anchors of the model</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>RPN regression loss</p> Source code in <code>template_vision/models_training/object_detectors/utils_faster_rcnn.py</code> <pre><code>def get_rpn_loss_regr(nb_anchors: int) -&gt; Callable:\n'''Gets the RPN regression loss depending on the number of anchor of the model\n    Args:\n        nb_anchors (int): Number of anchors of the model\n    Returns:\n        Callable: RPN regression loss\n    '''\ndef rpn_loss_regr(y_true, y_pred) -&gt; float:\n'''Calculates the RPN regression loss (Huber loss)\n               0.5*x\u00b2 (if x_abs &lt; 1)\n               x_abs - 0.5 (otherwise)\n        Args:\n            y_true: Model's targets. Shape (batch_size, height, width, 2*4*nb_anchors)\n                first part : class of the anchor boxes -&gt; object or background\n                                the loss is calculated only on anchor boxes associated to an object\n                second part : regression\n            y_pred: Outputs of the model. Shape (batch_size, height, width, 4*nb_anchors)\n        Returns:\n            float: Calculated loss\n        '''\ny_true = tf.cast(y_true, tf.float32)\ny_pred = tf.cast(y_pred, tf.float32)\n# Evaluate difference\nind_sep = 4 * nb_anchors  # Separation index of the two parts of y_true\nx = y_true[:, :, :, ind_sep:] - y_pred\nx_abs = K.abs(x)\n# If x_abs &lt;= 1.0, x_bool = 1\nx_bool = K.cast(K.less_equal(x_abs, 1.0), tf.float32)\n# robust loss function (smooth L1)\nreturn K.sum(y_true[:, :, :, :ind_sep] * (x_bool * (0.5 * x * x) + (1 - x_bool) * (x_abs - 0.5))) / K.sum(1e-4 + y_true[:, :, :, :ind_sep])\n# Return loss\nreturn rpn_loss_regr\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/","title":"Utils object detectors","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.add_regression_target_to_pos_valid","title":"<code>add_regression_target_to_pos_valid(anchor_boxes_dict)</code>","text":"<p>Add the regression target for positive and valid anchors Otherwise, keep (0, 0, 0, 0) and won't be used by the loss</p> <p>Parameters:</p> Name Type Description Default <code>anchor_boxes_dict</code> <code>dict</code> <p>Anchor boxes dictionary - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space) - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou - 'anchor_type': anchor type (pos, neg or neutral) - 'anchor_validity': anchor validity - 'best_bbox_index': bbox associated to this anchor</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated anchor boxes withe the regression targets</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def add_regression_target_to_pos_valid(anchor_boxes_dict: dict) -&gt; dict:\n'''Add the regression target for positive and valid anchors\n    Otherwise, keep (0, 0, 0, 0) and won't be used by the loss\n    Args:\n        anchor_boxes_dict (dict): Anchor boxes dictionary\n            - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space)\n            - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou\n            - 'anchor_type': anchor type (pos, neg or neutral)\n            - 'anchor_validity': anchor validity\n            - 'best_bbox_index': bbox associated to this anchor\n    Returns:\n        dict: Updated anchor boxes withe the regression targets\n    '''\n# For each anchor ...\nfor anchor_idx, anchor in anchor_boxes_dict.items():\n# ... and if the anchor is positive and valid ...\nif anchor['anchor_type'] == 'pos' and anchor['anchor_validity'] == 1:\n# ... we get the regression target between this anchor and the closest bbox (best iou)\ncoordinates_anchor = anchor['anchor_img_coordinates']\nbest_bbox_index = anchor['best_bbox_index']\ncoordinates_bbox = anchor['bboxes'][best_bbox_index]['bbox_img_coordinates']\nanchor['regression_target'] = calc_regr(coordinates_bbox, coordinates_anchor)\n# Otherwise, default to 0\nelse:\nanchor['regression_target'] = (0, 0, 0, 0)\nanchor_boxes_dict[anchor_idx] = anchor\n# Return updated dict\nreturn anchor_boxes_dict\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.apply_regression","title":"<code>apply_regression(coordinates_and_regression)</code>","text":"<p>Applies the result of a regression on an anchor box (or a ROI) given in xyhw format.</p> <p>Parameters:</p> Name Type Description Default <code>coordinates_and_regression</code> <code>np.ndarray</code> <p>An array composed of 8 objects x_anc, y_anc, h_anc, w_anc, tx, ty, th, tw. (x_anc, y_anc, h_anc, w_anc) are the coordinates of the anchor (or of a ROI) (tx, ty, th, tw) are the predictions of a regression</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>coordinates after regression applied on the anchor box (or on the ROI) - x coordinate of the upper left corner</p> <code>float</code> <code>float</code> <p>coordinates after regression applied on the anchor box (or on the ROI) - y coordinate of the upper left corner</p> <code>float</code> <code>float</code> <p>coordinates after regression applied on the anchor box (or on the ROI) - height</p> <code>float</code> <code>float</code> <p>coordinates after regression applied on the anchor box (or on the ROI) - width</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def apply_regression(coordinates_and_regression: np.ndarray) -&gt; Tuple[float, float, float, float]:\n'''Applies the result of a regression on an anchor box (or a ROI) given in xyhw format.\n    Args:\n        coordinates_and_regression (np.ndarray): An array composed of 8 objects x_anc, y_anc, h_anc, w_anc, tx, ty, th, tw.\n            (x_anc, y_anc, h_anc, w_anc) are the coordinates of the anchor (or of a ROI)\n            (tx, ty, th, tw) are the predictions of a regression\n            # Shape (8,)\n    Returns:\n        float: coordinates after regression applied on the anchor box (or on the ROI) - x coordinate of the upper left corner\n        float: coordinates after regression applied on the anchor box (or on the ROI) - y coordinate of the upper left corner\n        float: coordinates after regression applied on the anchor box (or on the ROI) - height\n        float: coordinates after regression applied on the anchor box (or on the ROI) - width\n    '''\nx_anc, y_anc, h_anc, w_anc, tx, ty, th, tw = coordinates_and_regression\nw_roi = np.exp(tw) * w_anc  # Take the inverse of the log and get rid of the normalization\nh_roi = np.exp(th) * h_anc  # Take the inverse of the log and get rid of the normalization\nx_roi = (tx * w_anc + (x_anc + w_anc / 2)) - w_roi / 2  # Get rid of the normalization, then add the center of the anchor = center of ROI, then remove half the width to get x1\ny_roi = (ty * h_anc + (y_anc + h_anc / 2)) - h_roi / 2  # Get rid of the normalization, then add the center of the anchor = center of ROI, then remove half the height to get y1\nreturn x_roi, y_roi, h_roi, w_roi\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.apply_regression--shape-8","title":"Shape (8,)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.calc_regr","title":"<code>calc_regr(coordinates_bbox, coordinates_anchor)</code>","text":"<p>Gives the target of a regression given the coordinates of a bbox and of an anchor (or a ROI)</p> <p>Parameters:</p> Name Type Description Default <code>coordinates_bbox</code> <code>tuple</code> <p>The coordinates of a bbox in opposite points format</p> required <code>coordinates_anchor</code> <code>tuple</code> <p>The coordinates of an anchor (or a ROI) in opposite points format</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Gap between the centers (x coordinate) normalized by the width of the anchor</p> <code>float</code> <code>float</code> <p>Gap between the centers (y coordinate) normalized by the height of the anchor</p> <code>float</code> <code>float</code> <p>Height ratio : bbox / anchor (log version)</p> <code>float</code> <code>float</code> <p>Width ratio : bbox / anchor (log version)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def calc_regr(coordinates_bbox: Tuple[float, float, float, float],\ncoordinates_anchor: Tuple[float, float, float, float]) -&gt; Tuple[float, float, float, float]:\n'''Gives the target of a regression given the coordinates of a bbox and of an anchor (or a ROI)\n    Args:\n        coordinates_bbox (tuple): The coordinates of a bbox in opposite points format\n        coordinates_anchor (tuple): The coordinates of an anchor (or a ROI) in opposite points format\n    Returns:\n        float: Gap between the centers (x coordinate) normalized by the width of the anchor\n        float: Gap between the centers (y coordinate) normalized by the height of the anchor\n        float: Height ratio : bbox / anchor (log version)\n        float: Width ratio : bbox / anchor (log version)\n    '''\ncx_bbox, cy_bbox, height_bbox, width_bbox = xyxy_to_cxcyhw(*coordinates_bbox)\ncx_anchor, cy_anchor, height_anchor, width_anchor = xyxy_to_cxcyhw(*coordinates_anchor)\ntx = (cx_bbox - cx_anchor) / width_anchor\nty = (cy_bbox - cy_anchor) / height_anchor\nth = np.log(height_bbox / height_anchor)\ntw = np.log(width_bbox / width_anchor)\nreturn tx, ty, th, tw\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.check_coordinates_validity","title":"<code>check_coordinates_validity(function)</code>","text":"<p>Decorator to make sure that the coordinates are valid</p> <p>Parameters:</p> Name Type Description Default <code>function</code> <code>Callable</code> <p>Function to decorate</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a set of coordinates is impossible</p> <p>Returns:</p> Name Type Description <code>function</code> <code>Callable</code> <p>The decorated function</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def check_coordinates_validity(function: Callable) -&gt; Callable:\n'''Decorator to make sure that the coordinates are valid\n    Args:\n        function (Callable): Function to decorate\n    Raises:\n        ValueError: If a set of coordinates is impossible\n    Returns:\n        function: The decorated function\n    '''\n# Get wrapper\ndef wrapper(*args, **kwargs):\n'''Wrapper'''\n# Get vars.\nf_args = inspect.getfullargspec(function).args\nx1 = kwargs['x1'] if 'x1' in kwargs.keys() else (args[f_args.index('x1')] if 'x1' in f_args else None)\ny1 = kwargs['y1'] if 'y1' in kwargs.keys() else (args[f_args.index('y1')] if 'y1' in f_args else None)\nx2 = kwargs['x2'] if 'x2' in kwargs.keys() else (args[f_args.index('x2')] if 'x2' in f_args else None)\ny2 = kwargs['y2'] if 'y2' in kwargs.keys() else (args[f_args.index('y2')] if 'y2' in f_args else None)\nx = kwargs['x'] if 'x' in kwargs.keys() else (args[f_args.index('x')] if 'x' in f_args else None)\ny = kwargs['y'] if 'y' in kwargs.keys() else (args[f_args.index('y')] if 'y' in f_args else None)\nw = kwargs['w'] if 'w' in kwargs.keys() else (args[f_args.index('w')] if 'w' in f_args else None)\nh = kwargs['h'] if 'h' in kwargs.keys() else (args[f_args.index('h')] if 'h' in f_args else None)\n# Apply checks\nif x1 is not None:\nassert x1 &gt;= 0, 'x1 must be non negative'\nif y1 is not None:\nassert y1 &gt;= 0, 'y1 must be non negative'\nif x2 is not None:\nassert x2 &gt;= 0, 'x2 must be non negative'\nif y2 is not None:\nassert y2 &gt;= 0, 'y2 must be non negative'\nif x is not None:\nassert x &gt;= 0, 'x must be non negative'\nif y is not None:\nassert y &gt;= 0, 'y must be non negative'\nif w is not None:\nassert w &gt; 0, 'w must be positive'\nif h is not None:\nassert h &gt; 0, 'h must be positive'\nif x1 is not None and x2 is not None:\nassert x2 &gt; x1, 'x2 must be bigger than x1'\nif y1 is not None and y2 is not None:\nassert y2 &gt; y1, 'y2 must be bigger than y1'\n# Return\nreturn function(*args, **kwargs)\nreturn wrapper\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.complete_at_least_one_anchor_per_bbox","title":"<code>complete_at_least_one_anchor_per_bbox(anchor_boxes_dict, bboxes_index_with_no_positive)</code>","text":"<p>Completes the dictionary of anchor to have at least one positive anchor per bbox if it is not already the case. If a bbox is not associated to an anchor, we associate it to the anchor with which it has the biggest iou (if this anchor is not already associated with another bbox)</p> <p>Parameters:</p> Name Type Description Default <code>anchor_boxes_dict</code> <code>dict</code> <p>Anchor boxes dictionary - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space) - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou - 'anchor_type': anchor type (pos, neg or neutral) - 'anchor_validity': anchor validity - 'best_bbox_index': bbox associated to this anchor</p> required <code>bboxes_index_with_no_positive</code> <code>list</code> <p>List of bboxes with no positive anchor associated</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated anchor boxes dictionary</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def complete_at_least_one_anchor_per_bbox(anchor_boxes_dict: dict, bboxes_index_with_no_positive: List[dict]) -&gt; dict:\n'''Completes the dictionary of anchor to have at least one positive anchor per bbox if it is not\n    already the case.\n    If a bbox is not associated to an anchor, we associate it to the anchor with which it has\n    the biggest iou (if this anchor is not already associated with another bbox)\n    Args:\n        anchor_boxes_dict (dict): Anchor boxes dictionary\n            - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space)\n            - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou\n            - 'anchor_type': anchor type (pos, neg or neutral)\n            - 'anchor_validity': anchor validity\n            - 'best_bbox_index': bbox associated to this anchor\n        bboxes_index_with_no_positive (list): List of bboxes with no positive anchor associated\n    Returns:\n        dict: Updated anchor boxes dictionary\n    '''\n# For each missing bbox ...\nfor index_bbox in bboxes_index_with_no_positive:\n# ... we look for the anchor box with the best iou ...\nbest_iou = -1  # We could set it to 0 but there exists rare case where all the anchor boxes have a 0 iou.\nbest_anchor_idx = -1\nfor anchor_idx, anchor in anchor_boxes_dict.items():\niou = anchor['bboxes'][index_bbox]['iou']\nif iou &gt; best_iou:\nbest_iou = iou\nbest_anchor_idx = anchor_idx\n# ... and we update this anchor if it is not already positive\nanchor = anchor_boxes_dict[best_anchor_idx]\nif anchor['anchor_type'] != 'pos' and best_iou &gt; 0:\nanchor['anchor_type'] = 'pos'\nanchor['anchor_validity'] = 1\nanchor['best_bbox_index'] = index_bbox\nanchor_boxes_dict[best_anchor_idx] = anchor  # Update\n# Return\nreturn anchor_boxes_dict\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.create_fake_dict_rois_targets","title":"<code>create_fake_dict_rois_targets(img_data, subsampling_ratio, nb_rois_per_img)</code>","text":"<p>Creates fake dict_rois_targets in the rare cases where the function limit_rois_targets gives an empty object (None).</p> <pre><code>Process : we return ROIs on the entire image, considered as background\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>img_data</code> <code>dict</code> <p>Metadata of the image after the preprocessing. In particular, the size of the image</p> required <code>subsampling_ratio</code> <code>int</code> <p>Subsampling of the base model (shared layers)</p> required <code>nb_rois_per_img</code> <code>int</code> <p>Number of fake ROIs to return</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The dictionary of fake \"selected\" ROIs</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def create_fake_dict_rois_targets(img_data: dict, subsampling_ratio: int, nb_rois_per_img: int) -&gt; dict:\n'''Creates fake dict_rois_targets in the rare cases where the function limit_rois_targets gives an empty object (None).\n        Process : we return ROIs on the entire image, considered as background\n    Args:\n        img_data (dict): Metadata of the image after the preprocessing. In particular, the size of the image\n        subsampling_ratio (int): Subsampling of the base model (shared layers)\n        nb_rois_per_img (int): Number of fake ROIs to return\n    Returns:\n        dict: The dictionary of fake \"selected\" ROIs\n    '''\n# Get the size of the image in the features map\nheight_img_in_feature_map, width_img_in_feature_map = get_feature_map_size(img_data['resized_height'], img_data['resized_width'], subsampling_ratio)\n# Create a dictionary with an unique entry : a ROI on the entire image, considered as background\ndict_rois_targets = {\n0: {\n'coordinates': {'x1': 0, 'y1': 0, 'x2': width_img_in_feature_map, 'y2': height_img_in_feature_map,\n'h': height_img_in_feature_map, 'w': width_img_in_feature_map},\n'classifier_regression_target': (0, 0, 0, 0),\n'classifier_class_target': 'bg',\n}\n}\n# We clone this ROI to have as many as wanted\ndict_rois_targets = {i: copy.deepcopy(dict_rois_targets[0]) for i in range(nb_rois_per_img)}\n# Return\nreturn dict_rois_targets\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.draw_bboxes","title":"<code>draw_bboxes(input_img, output_path=None, gt_bboxes=None, predicted_bboxes=None)</code>","text":"<p>Adds bboxes to an image (np.ndarray)</p> <p>Parameters:</p> Name Type Description Default <code>input_img</code> <code>np.ndarray</code> <p>Input image</p> required Kwargs <p>output_path (str): Path to the output file. If None, the result is not saved gt_bboxes (list): List of \"ground truth\" bboxes to display     Each entry must be a dictionary with keys x1, y1, x2, y2 and (optional) class predicted_bboxes (list): List of bboxes coming from a prediction (same format as gt_bboxes)</p> <p>Raises:</p> Type Description <code>FileExistsError</code> <p>If the output file already exists</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>(np.ndarray) : The image with the boxes</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def draw_bboxes(input_img: np.ndarray, output_path: Union[str, None] = None, gt_bboxes: Union[List[dict], None] = None,\npredicted_bboxes: Union[List[dict], None] = None) -&gt; np.ndarray:\n'''Adds bboxes to an image (np.ndarray)\n    Args:\n        input_img (np.ndarray): Input image\n    Kwargs:\n        output_path (str): Path to the output file. If None, the result is not saved\n        gt_bboxes (list): List of \"ground truth\" bboxes to display\n            Each entry must be a dictionary with keys x1, y1, x2, y2 and (optional) class\n        predicted_bboxes (list): List of bboxes coming from a prediction (same format as gt_bboxes)\n    Raises:\n        FileExistsError: If the output file already exists\n    Returns:\n        (np.ndarray) : The image with the boxes\n    '''\nif output_path is not None and os.path.exists(output_path):\nraise FileExistsError(f\"The file {output_path} already exists\")\nif gt_bboxes is None:\ngt_bboxes = []\nif predicted_bboxes is None:\npredicted_bboxes = []\n# Define colors\ngreen = (0, 255, 0, 255)\nred = (255, 0, 0, 255)\n# Create green rectangles for each bbox\nfor bbox in gt_bboxes:\ndraw_rectangle_from_bbox(input_img, bbox, color=green, thickness=5)\n# Create red rectangles for each predicted bbox\nfor bbox in predicted_bboxes:\ndraw_rectangle_from_bbox(input_img, bbox, color=red, thickness=5)\nif output_path is not None:\nio.imsave(output_path, input_img)\nlogger.info(f\"Image saved here : {output_path}\")\nreturn input_img\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.draw_bboxes_from_file","title":"<code>draw_bboxes_from_file(input_path, output_path=None, gt_bboxes=None, predicted_bboxes=None)</code>","text":"<p>Adds bboxes to an image from a file</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input image</p> required Kwargs <p>output_path (str): Path to the output file. If None, the result is not saved gt_bboxes (list): List of \"ground truth\" bboxes to display     Each entry must be a dictionary with keys x1, y1, x2, y2 and (optional) class predicted_bboxes (list): List of bboxes coming from a prediction (same format as gt_bboxes)</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the input file does not exist</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>(np.ndarray) : The image with the boxes</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def draw_bboxes_from_file(input_path: str, output_path: Union[str, None] = None, gt_bboxes: Union[List[dict], None] = None,\npredicted_bboxes: Union[List[dict], None] = None) -&gt; np.ndarray:\n'''Adds bboxes to an image from a file\n    Args:\n        input_path (str): Path to the input image\n    Kwargs:\n        output_path (str): Path to the output file. If None, the result is not saved\n        gt_bboxes (list): List of \"ground truth\" bboxes to display\n            Each entry must be a dictionary with keys x1, y1, x2, y2 and (optional) class\n        predicted_bboxes (list): List of bboxes coming from a prediction (same format as gt_bboxes)\n    Raises:\n        FileNotFoundError: If the input file does not exist\n    Returns:\n        (np.ndarray) : The image with the boxes\n    '''\nif not os.path.exists(input_path):\nraise FileNotFoundError(f\"The file {input_path} does not exist\")\n# Load image\ninput_img = io.imread(input_path)\nreturn draw_bboxes(input_img, output_path, gt_bboxes, predicted_bboxes)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.draw_rectangle_from_bbox","title":"<code>draw_rectangle_from_bbox(img, bbox, color=None, thickness=None, with_center=False)</code>","text":"<p>Draws a rectangle in the image and adds a text (optional)</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>np.ndarray</code> <p>The considered image</p> required <code>bbox</code> <code>dict</code> <p>The dictionary containing the coordinates and the text</p> required <code>color</code> <code>tuple</code> <p>A RGB tuple giving the color of the rectangle</p> <code>None</code> <code>thickness</code> <code>int</code> <p>The thickness of the rectangle</p> <code>None</code> <code>with_center</code> <code>bool</code> <p>If True, also draws the center of the rectangle</p> <code>False</code> <p>Raises:</p> Type Description <code>ValueError</code> <p>If one of the keys 'x1', 'y1', 'x2', 'y2' is missing</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def draw_rectangle_from_bbox(img: np.array, bbox: dict, color: Union[tuple, None] = None,\nthickness: Union[int, None] = None, with_center: bool = False):\n'''Draws a rectangle in the image and adds a text (optional)\n    Args:\n        img (np.ndarray): The considered image\n        bbox (dict): The dictionary containing the coordinates and the text\n        color (tuple): A RGB tuple giving the color of the rectangle\n        thickness (int): The thickness of the rectangle\n        with_center (bool): If True, also draws the center of the rectangle\n    Raises:\n        ValueError: If one of the keys 'x1', 'y1', 'x2', 'y2' is missing\n    '''\n# Check mandatory keys\nif any([key not in bbox.keys() for key in ['x1', 'y1', 'x2', 'y2']]):\nraise ValueError(\"One of the mandatory keys ('x1', 'y1', 'x2', 'y2') is missing in the object bbox.\")\n# Process\nx1, y1, x2, y2 = bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2']\nclass_name = bbox.get('class', None)\ncv2.rectangle(img, (x1, y1), (x2, y2), color, thickness)\nif class_name is not None:\nfont = cv2.FONT_HERSHEY_SIMPLEX\nif 'proba' in bbox:\nproba = format(bbox['proba'], \".2f\")\nclass_name = class_name + f\" ({proba})\"\ncv2.putText(img, class_name, (x1 + 5, y1 + 30), font, 1, color, 2)\nif with_center:\ncenter_x = int((x1 + x2) / 2)\ncenter_y = int((y1 + y2) / 2)\ncv2.circle(img, (center_x, center_y), 3, color, -1)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.format_classifier_inputs_and_targets","title":"<code>format_classifier_inputs_and_targets(dict_rois_targets, dict_classes, classifier_regr_scaling)</code>","text":"<p>Transforms a dictionary of target ROIs into a suitable format for the classifier model</p> <p>Parameters:</p> Name Type Description Default <code>dict_rois</code> <code>dict</code> <p>Dictionary containing the possible inputs / targets of the classifier</p> required <code>dict_classes</code> <code>dict</code> <p>Mapping of the classes of the model (must not contain 'bg'), format :  {idx: label}</p> required <code>classifier_regr_scaling</code> <code>list&lt;float&gt;</code> <p>Regression coefficient to apply to coordinates</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: coordinates of each selected ROIs</p> <code>np.ndarray</code> <p>np.ndarray: Classification target of the classifier (with the background)</p> <code>np.ndarray</code> <p>np.ndarray: Two parts array:</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def format_classifier_inputs_and_targets(dict_rois_targets: dict, dict_classes: dict,\nclassifier_regr_scaling: List[float]) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n'''Transforms a dictionary of target ROIs into a suitable format for the classifier model\n    Args:\n        dict_rois (dict): Dictionary containing the possible inputs / targets of the classifier\n        dict_classes (dict): Mapping of the classes of the model (must not contain 'bg'), format :  {idx: label}\n        classifier_regr_scaling (list&lt;float&gt;): Regression coefficient to apply to coordinates\n    Returns:\n        np.ndarray: coordinates of each selected ROIs\n            # Shape : (1, nb_rois, 4), format x, y, h, w\n        np.ndarray: Classification target of the classifier (with the background)\n            # Shape (1, nb_rois, (nb_classes + 1))\n        np.ndarray: Two parts array:\n            # Shape (1, nb_rois, 2 * nb_classes * 4)\n            -&gt; First half : identification class ground truth to calculate the regression loss for the classifier\n                # Shape (1, nb_rois, nb_classes * 4)\n            -&gt; Second hald : regression target for the classifier (one regression per class)\n                # Shape (1, nb_rois, nb_classes * 4)\n    '''\n# Get the number of selected ROIs (the same for each image) and info on classes\nnb_rois = len(dict_rois_targets)\nnb_classes = len(dict_classes)\nclass_mapping = {name_class: index for index, name_class in dict_classes.items()}\nclass_mapping['bg'] = len(class_mapping)  # Add background to the mapping\n# Init. of output arrays\nX = np.zeros((nb_rois, 4))  # ROIs coordinates\nY1 = np.zeros((nb_rois, (nb_classes + 1)))  # + 1 for background\nY2_1 = np.zeros((nb_rois, 4 * nb_classes))  # OHE class (without background), but repeated 4 times (one for each coordinate), will be used by the loss\nY2_2 = np.zeros((nb_rois, 4 * nb_classes))  # One regression per class (# TODO verify if we can't do only one regression)\n# For each ROI, we fill up the output arrays\nfor i, (roi_index, roi) in enumerate(dict_rois_targets.items()):\n### ROI coordinates\nX[i, :] = (roi['coordinates']['x1'], roi['coordinates']['y1'], roi['coordinates']['h'], roi['coordinates']['w'])  # Format x, y, h, w\n### Targets of the classifier model\n# ROI class\ngt_class = roi['classifier_class_target']\nidx_gt_class = class_mapping[gt_class]\nohe_target = [0] * (nb_classes + 1)\nohe_target[idx_gt_class] = 1  # --&gt; e.g. [0, 1, 0] / 2 classes + 'bg'\nY1[i, :] = ohe_target\n# ROI regression - loss target\nohe_target_no_bg = ohe_target[:nb_classes]  # We get rid of the background, no regression here --&gt; e.g. [0, 1]\nohe_target_no_bg_repeated = np.repeat(ohe_target_no_bg, 4)  # We repeat the OHE targets four times 4 fois (one for each coordinate)\nY2_1[i, :] = ohe_target_no_bg_repeated  # e.g. [0, 0, 0, 0, 1, 1, 1, 1]\n# ROI regression - regression - only if not background\nif gt_class != 'bg':\ntarget_regression = [0.] * nb_classes * 4\nregression_values = [a * b for a, b in zip(roi['classifier_regression_target'], classifier_regr_scaling)]  # Apply a scaling\ntarget_regression[idx_gt_class * 4: (idx_gt_class + 1) * 4] = regression_values  # e.g. [0, 0, 0, 0, 0.2, -0.3, 0.1, 0.9]\nY2_2[i, :] = target_regression\n# Concatenate Y2\nY2 = np.concatenate([Y2_1, Y2_2], axis=1)\n# Add batch dimension\nX = np.expand_dims(X, axis=0)\nY1 = np.expand_dims(Y1, axis=0)\nY2 = np.expand_dims(Y2, axis=0)\n# Returns\nreturn X, Y1, Y2\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.format_classifier_inputs_and_targets--shape-1-nb_rois-4-format-x-y-h-w","title":"Shape : (1, nb_rois, 4), format x, y, h, w","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.format_classifier_inputs_and_targets--shape-1-nb_rois-nb_classes-1","title":"Shape (1, nb_rois, (nb_classes + 1))","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.format_classifier_inputs_and_targets--shape-1-nb_rois-2-nb_classes-4","title":"Shape (1, nb_rois, 2 * nb_classes * 4)","text":"<p>-&gt; First half : identification class ground truth to calculate the regression loss for the classifier     # Shape (1, nb_rois, nb_classes * 4) -&gt; Second hald : regression target for the classifier (one regression per class)     # Shape (1, nb_rois, nb_classes * 4)</p>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_all_viable_anchors_boxes","title":"<code>get_all_viable_anchors_boxes(base_anchors, subsampling_ratio, feature_map_height, feature_map_width, im_resized_height, im_resized_width)</code>","text":"<p>Gets a dictionary of 'viable' anchor boxes.</p> <p>From a list of \"base\" anchors, we will take each point of a features map, get its initial coordinates (input of the model) and build as many anchors as \"base\" anchors with this point as a center. Then we filter out the ones which are outside the image</p> <p>Parameters:</p> Name Type Description Default <code>base_anchors</code> <code>list</code> <p>List of base anchors</p> required <code>subsampling_ratio</code> <code>int</code> <p>Subsampling ratio of the shared model</p> required <code>feature_map_height</code> <code>int</code> <p>Height of the features map</p> required <code>feature_map_width</code> <code>int</code> <p>Width of the features map</p> required <code>im_resized_height</code> <code>int</code> <p>Height of the input image (preprocessed, without padding)</p> required <code>im_resized_width</code> <code>int</code> <p>Width of the input image (preprocessed, without padding)</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>set of 'viable' anchor boxes identified by (y, x, index_anchor)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_all_viable_anchors_boxes(base_anchors: List[tuple], subsampling_ratio: int, feature_map_height: int,\nfeature_map_width: int, im_resized_height: int, im_resized_width: int) -&gt; dict:\n'''Gets a dictionary of 'viable' anchor boxes.\n    From a list of \"base\" anchors, we will take each point of a features map, get its initial coordinates\n    (input of the model) and build as many anchors as \"base\" anchors with this point as a center.\n    Then we filter out the ones which are outside the image\n    Args:\n        base_anchors (list): List of base anchors\n        subsampling_ratio (int): Subsampling ratio of the shared model\n        feature_map_height (int): Height of the features map\n        feature_map_width (int): Width of the features map\n        im_resized_height (int): Height of the input image (preprocessed, without padding)\n        im_resized_width (int): Width of the input image (preprocessed, without padding)\n    Returns:\n        dict : set of 'viable' anchor boxes identified by (y, x, index_anchor)\n    '''\nviable_anchor_boxes = {}\n# For each anchor...\nfor index_anchor, (height_anchor, width_anchor) in enumerate(base_anchors):\n# For each point of the features map...\nfor x_feature_map in range(feature_map_width):\n# x coordinate of the anchor, input format (ie in image space)\nx1_anchor = subsampling_ratio * (x_feature_map + 0.5) - width_anchor / 2  # center - width / 2\nx2_anchor = subsampling_ratio * (x_feature_map + 0.5) + width_anchor / 2  # center + width / 2\n# We do not consider the anchors outside the image (before padding)\nif x1_anchor &lt; 0 or x2_anchor &gt;= im_resized_width:\ncontinue\nfor y_feature_map in range(feature_map_height):\n# y coordinate of the anchor, input format (ie in image space)\ny1_anchor = subsampling_ratio * (y_feature_map + 0.5) - height_anchor / 2  # center - height / 2\ny2_anchor = subsampling_ratio * (y_feature_map + 0.5) + height_anchor / 2  # center + height / 2\n# We do not consider the anchors outside the image (before padding)\nif y1_anchor &lt; 0 or y2_anchor &gt;= im_resized_height:\ncontinue\n# We update the dictionary of the 'viable' anchor boxes (y, x, anchor)\nid_key = (y_feature_map, x_feature_map, index_anchor)\nviable_anchor_boxes[id_key] = {\n'anchor_img_coordinates': (x1_anchor, y1_anchor, x2_anchor, y2_anchor)\n}\n# Check errors\nif len(viable_anchor_boxes) == 0:\nlogger.error(\"No viable bbox for one of the input images.\")\nlogger.error(\"The size of the preprocessed images may be too small when compared to the list of anchors of the model.\")\nraise RuntimeError(\"No viable bbox for one of the input images.\")\n# Return\nreturn viable_anchor_boxes\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_area_from_xyxy","title":"<code>get_area_from_xyxy(x1, y1, x2, y2)</code>","text":"<p>Gives the area (absolute, not relative) of a rectangle in opposite points format</p> Args <p>x1 (float): x coordinate of the upper left point y1 (float): y coordinate of the upper left point x2 (float): x coordinate of the bottom right point y2 (float): y coordinate of the bottom right point</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>The absolute area of the rectangle</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>@check_coordinates_validity\ndef get_area_from_xyxy(x1: float, y1: float, x2: float, y2: float) -&gt; float:\n'''Gives the area (absolute, not relative) of a rectangle in opposite points format\n    Args :\n        x1 (float): x coordinate of the upper left point\n        y1 (float): y coordinate of the upper left point\n        x2 (float): x coordinate of the bottom right point\n        y2 (float): y coordinate of the bottom right point\n    Returns:\n        float : The absolute area of the rectangle\n    '''\nreturn abs((x2 - x1) * (y2 - y1))\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_classifier_test_inputs","title":"<code>get_classifier_test_inputs(rois_coordinates)</code>","text":"<p>Formats the inputs for the classifier from ROIs proposed by the RPN (test case)</p> <p>Process : For each ROI, we simply get the format x, y, h, w</p> <p>Parameters:</p> Name Type Description Default <code>rois_coordinates</code> <code>list&lt;np.ndarray&gt;</code> <p>ROIs to transform rois_coordinates must be a list with only one entry : the ROIs of the current image (for prediction, the batch_size is forced to 1) The unique entry is a numpy array:     # Shape (nb_rois, 4)     # Format x1, y1, x2, y2</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of elements in the list is different from 1</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray : ROIs to use as inputs of the model</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_classifier_test_inputs(rois_coordinates: List[np.ndarray]) -&gt; np.ndarray:\n'''Formats the inputs for the classifier from ROIs proposed by the RPN (test case)\n    Process : For each ROI, we simply get the format x, y, h, w\n    Args:\n        rois_coordinates (list&lt;np.ndarray&gt;): ROIs to transform\n            rois_coordinates must be a list with only one entry : the ROIs of the current image (for prediction, the batch_size is forced to 1)\n            The unique entry is a numpy array:\n                # Shape (nb_rois, 4)\n                # Format x1, y1, x2, y2\n    Raises:\n        ValueError: If the number of elements in the list is different from 1\n    Returns:\n        np.ndarray : ROIs to use as inputs of the model\n            # Shape : (1, nb_rois, 4), format x, y, h, w\n    '''\nif len(rois_coordinates) != 1:\nraise ValueError(\"In prediction mode, the batch_size must be 1.\")\n# Init. of the output array\nnb_rois = rois_coordinates[0].shape[0]\noutput_shape = (1, nb_rois, 4)\nX = np.zeros(output_shape)\n# We process ROIs one at a time\nfor i, roi in enumerate(rois_coordinates[0]):\n# Get format x, y, h, w\nx1, y1, x2, y2 = roi[0], roi[1], roi[2], roi[3]\nx1, y1, h, w = xyxy_to_xyhw(x1, y1, x2, y2)\n# Update output array\nX[0, i, :] = (x1, y1, h, w)\n# Return result\nreturn X\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_classifier_test_inputs--shape-1-nb_rois-4-format-x-y-h-w","title":"Shape : (1, nb_rois, 4), format x, y, h, w","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_classifier_train_inputs_and_targets","title":"<code>get_classifier_train_inputs_and_targets(model, img_data_batch, rois_coordinates)</code>","text":"<p>Gives the regression and classification of the classifier from the ROIs given by the RPN</p> We got the ROIs from the RPN prediction (and transformed them via get_roi_from_rpn_predictions) <p>For each image we will : - Calculate the ious between bboxes and ROIs - Keep, for each ROI, the bbox with the biggest iou (if the iou is bigger than a threshold) - Filter the ROIs to only some of them:     - Allows to keep OOM in check     - We respect to the loss, we will, of course, only take into account the selected ROIs     - We will keep a balance between positive ROIs (match with an object) and negative ROIs (match with 'bg') - Format the inputs and targets of the model</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelKerasFasterRcnnObjectDetector</code> <p>Model used (contains all the necessary configs)</p> required <code>img_data_batch</code> <code>list&lt;dict&gt;</code> <p>List of img_data for the batch Used here to get the bboxes of the images to define the targets of the classifier</p> required <code>rois_coordinates</code> <code>list&lt;np.ndarray&gt;</code> <p>Final list of the ROIs selected for the classifier part. Each element is a numpy array containing the coordinates of the ROIs calculated for an image of the batch</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray : ROIs coordinates in input of the model - Format x, y, h, w</p> <code>np.ndarray</code> <p>np.ndarray : Targets of the classifier - classification</p> <code>np.ndarray</code> <p>np.ndarray : Targets of the classifier - regression</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_classifier_train_inputs_and_targets(model, img_data_batch: List[dict],\nrois_coordinates: List[np.ndarray]) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n'''Gives the regression and classification of the classifier from the ROIs given by the RPN\n    Process : We got the ROIs from the RPN prediction (and transformed them via get_roi_from_rpn_predictions)\n              For each image we will :\n              - Calculate the ious between bboxes and ROIs\n              - Keep, for each ROI, the bbox with the biggest iou (if the iou is bigger than a threshold)\n              - Filter the ROIs to only some of them:\n                  - Allows to keep OOM in check\n                  - We respect to the loss, we will, of course, only take into account the selected ROIs\n                  - We will keep a balance between positive ROIs (match with an object) and negative ROIs (match with 'bg')\n              - Format the inputs and targets of the model\n    Args:\n        model (ModelKerasFasterRcnnObjectDetector): Model used (contains all the necessary configs)\n        img_data_batch (list&lt;dict&gt;): List of img_data for the batch\n            Used here to get the bboxes of the images to define the targets of the classifier\n        rois_coordinates (list&lt;np.ndarray&gt;): Final list of the ROIs selected for the classifier part.\n            Each element is a numpy array containing the coordinates of the ROIs calculated for an image of the batch\n            # Format x1, y1, x2, y2 (opposite points)\n    Returns:\n        np.ndarray : ROIs coordinates in input of the model - Format x, y, h, w\n            # Shape : (batch_size, nb_rois_per_img, 4), format x, y, h, w\n        np.ndarray : Targets of the classifier - classification\n            # Shape (batch_size, nb_rois_per_img, (nb_classes + 1))\n        np.ndarray : Targets of the classifier - regression\n            # Shape (batch_size, nb_rois_per_img, 2 * nb_classes * 4)\n    '''\n# Get model attributes\nsubsampling_ratio = model.shared_model_subsampling\nclassifier_min_overlap = model.classifier_min_overlap\nclassifier_max_overlap = model.classifier_max_overlap\nnb_rois_per_img = model.nb_rois_classifier\nclassifier_regr_scaling = model.classifier_regr_scaling\ndict_classes = model.dict_classes\n# Init. of output arrays\nX, Y1, Y2 = None, None, None\n# Preprocess one image at a time\nfor img_data, rois in zip(img_data_batch, rois_coordinates):\n# Get all the ious betwee, ROIs and bboxes\ndict_rois = get_rois_bboxes_iou(rois, img_data, subsampling_ratio)\n# Find the best bbox for each ROI et the corresponding regression\ndict_rois_targets = get_rois_targets(dict_rois, classifier_min_overlap, classifier_max_overlap)\n# Limit the number of ROI\ndict_rois_targets = limit_rois_targets(dict_rois_targets, nb_rois_per_img)\n# If we have no more targets (very rare !), we consider the entire image as 'bg'\nif dict_rois_targets is None:\nlogger.warning(\"There is an image with no suitable target for the classifier. We consider the entire image as background.\")\ndict_rois_targets = create_fake_dict_rois_targets(img_data, subsampling_ratio, nb_rois_per_img)\n# Format the classification and regression targets\nX_tmp, Y1_tmp, Y2_tmp = format_classifier_inputs_and_targets(dict_rois_targets, dict_classes, classifier_regr_scaling)\n# Increment output\nif X is None:\nX, Y1, Y2 = X_tmp, Y1_tmp, Y2_tmp\nelse:\n# TODO : get rid of the concatenate in the loop and concatenate a list one time at the end instead (much faster)\nX = np.concatenate((X, X_tmp), axis=0)\nY1 = np.concatenate((Y1, Y1_tmp), axis=0)\nY2 = np.concatenate((Y2, Y2_tmp), axis=0)\n# We return the result\nreturn X, Y1, Y2\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_classifier_train_inputs_and_targets--format-x1-y1-x2-y2-opposite-points","title":"Format x1, y1, x2, y2 (opposite points)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_classifier_train_inputs_and_targets--shape-batch_size-nb_rois_per_img-4-format-x-y-h-w","title":"Shape : (batch_size, nb_rois_per_img, 4), format x, y, h, w","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_classifier_train_inputs_and_targets--shape-batch_size-nb_rois_per_img-nb_classes-1","title":"Shape (batch_size, nb_rois_per_img, (nb_classes + 1))","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_classifier_train_inputs_and_targets--shape-batch_size-nb_rois_per_img-2-nb_classes-4","title":"Shape (batch_size, nb_rois_per_img, 2 * nb_classes * 4)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_feature_map_size","title":"<code>get_feature_map_size(input_height, input_width, subsampling_ratio)</code>","text":"<p>Gives the size of the features map given the height and width of the image using the subsampling_ratio of the shared model. For exemple, for VGG16, the subsampling_ratio is 16</p> <p>Parameters:</p> Name Type Description Default <code>input_height</code> <code>int</code> <p>Height of the image</p> required <code>input_width</code> <code>int</code> <p>Width of the image</p> required <code>subsampling_ratio</code> <code>int</code> <p>Subsampling ratio of the shared model</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If incorrect dimension of the image (&lt; 1)</p> <code>ValueError</code> <p>If the subsampling_ratio is incorrect (&lt; 1)</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Height of the features map</p> <code>int</code> <code>int</code> <p>Width of the features map</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_feature_map_size(input_height: int, input_width: int, subsampling_ratio: int) -&gt; Tuple[int, int]:\n'''Gives the size of the features map given the height and width of the image using\n    the subsampling_ratio of the shared model. For exemple, for VGG16, the subsampling_ratio is 16\n    Args:\n        input_height (int): Height of the image\n        input_width (int): Width of the image\n        subsampling_ratio (int): Subsampling ratio of the shared model\n    Raises:\n        ValueError: If incorrect dimension of the image (&lt; 1)\n        ValueError: If the subsampling_ratio is incorrect (&lt; 1)\n    Returns:\n        int: Height of the features map\n        int: Width of the features map\n    '''\n# Manage errors\nif input_height &lt; 1 or input_width &lt; 1:\nraise ValueError(f\"Bad image shape (H : {input_height} / W : {input_width})\")\nif subsampling_ratio &lt; 1:\nraise ValueError(f\"Bad subsampling ratio ({subsampling_ratio})\")\n# Process\nreturn input_height // subsampling_ratio, input_width // subsampling_ratio\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_final_bboxes","title":"<code>get_final_bboxes(final_boxes, img_data)</code>","text":"<p>Resizes the final predicted boxes to image space and formats them.</p> <p>Parameters:</p> Name Type Description Default <code>final_boxes</code> <code>list) </code> <p>list of boxes valid from a probability AND coordinates xyxy points of view and with \"no\" overlap</p> required <code>img_data</code> <code>dict) </code> <p>Metadata associated with the image (used to resize predictions)</p> required <p>Returns:</p> Type Description <code>List[dict]</code> <p>A list of bboxes corresponding to the model predictions</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_final_bboxes(final_boxes: List[tuple], img_data: dict) -&gt; List[dict]:\n'''Resizes the final predicted boxes to image space and formats them.\n    Args:\n        final_boxes (list) : list of boxes valid from a probability AND coordinates xyxy points of view and with \"no\" overlap\n            # Format [(cl, proba, coordinates), (...), ...)\n        img_data (dict) : Metadata associated with the image (used to resize predictions)\n    Returns:\n        A list of bboxes corresponding to the model predictions\n    '''\nlist_bboxes = []\nresized_width = img_data['resized_width']\noriginal_width = img_data['original_width']\nresized_height = img_data['resized_height']\noriginal_height = img_data['original_height']\nfor cl, proba, coordinates in final_boxes:\nbbox = {'class': cl, 'proba': proba, 'x1': coordinates[0],\n'y1': coordinates[1], 'x2': coordinates[2], 'y2': coordinates[3]}\nbbox['x1'] = int(bbox['x1'] * (original_width / resized_width))\nbbox['x2'] = int(bbox['x2'] * (original_width / resized_width))\nbbox['y1'] = int(bbox['y1'] * (original_height / resized_height))\nbbox['y2'] = int(bbox['y2'] * (original_height / resized_height))\nlist_bboxes.append(copy.deepcopy(bbox))\nreturn list_bboxes\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_final_bboxes--format-cl-proba-coordinates","title":"Format [(cl, proba, coordinates), (...), ...)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_iou","title":"<code>get_iou(coordinatesA, coordinatesB)</code>","text":"<p>Gives the intersection over union (iou) from the coordinates of two rectangles (in opposite points format)</p> <p>Parameters:</p> Name Type Description Default <code>coordinatesA</code> <code>tuple&lt;float&gt;</code> <p>The coordinates of the first rectangle in the format (x1, y1, x2, y2)</p> required <code>coordinatesB</code> <code>tuple&lt;float&gt;</code> <p>The coordinates of the second rectangle in the format (x1, y1, x2, y2)</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Intersection over union of the two rectangles</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_iou(coordinatesA: Tuple[float, float, float, float], coordinatesB: Tuple[float, float, float, float]) -&gt; float:\n'''Gives the intersection over union (iou) from the coordinates of two\n    rectangles (in opposite points format)\n    Args:\n        coordinatesA (tuple&lt;float&gt;): The coordinates of the first rectangle in the format (x1, y1, x2, y2)\n        coordinatesB (tuple&lt;float&gt;): The coordinates of the second rectangle in the format (x1, y1, x2, y2)\n    Returns:\n        float: Intersection over union of the two rectangles\n    '''\n# Get areas of A and B\nareaA = get_area_from_xyxy(*coordinatesA)\nareaB = get_area_from_xyxy(*coordinatesB)\n# If any null, iou is equal to 0\nif areaA == 0 or areaB == 0:\nreturn 0\nx1A, y1A, x2A, y2A = coordinatesA\nx1B, y1B, x2B, y2B = coordinatesB\n# Get coordinates of the intersection\nx1_inter = max(x1A, x1B)\ny1_inter = max(y1A, y1B)\nx2_inter = min(x2A, x2B)\ny2_inter = min(y2A, y2B)\n# Get intersection area\nif x2_inter &gt; x1_inter and y2_inter &gt; y1_inter:\narea_inter = get_area_from_xyxy(x1_inter, y1_inter, x2_inter, y2_inter)\nelse:\narea_inter = 0\n# Return IOU\nreturn area_inter / (areaA + areaB - area_inter)\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_iou_anchors_bboxes","title":"<code>get_iou_anchors_bboxes(anchor_boxes_dict, image_bboxes)</code>","text":"<p>Gives the iou for each anchor boxes with all the bboxes of a list (for example, all the bboxes of an image)</p> <p>Parameters:</p> Name Type Description Default <code>anchor_boxes_dict</code> <code>dict</code> <p>Anchor boxes dictionary - 'anchor_img_coordinates': xyxy coordinates of the anchor boxe (input format, ie. image space)</p> required <code>image_bboxes</code> <code>list&lt;dict&gt;</code> <p>List of bboxes</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The input dictionary to which we added a bboxes field containing coordinates and iou</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_iou_anchors_bboxes(anchor_boxes_dict: dict, image_bboxes: List[dict]) -&gt; dict:\n'''Gives the iou for each anchor boxes with all the bboxes of a list (for example, all the\n    bboxes of an image)\n    Args:\n        anchor_boxes_dict (dict): Anchor boxes dictionary\n            - 'anchor_img_coordinates': xyxy coordinates of the anchor boxe (input format, ie. image space)\n        image_bboxes (list&lt;dict&gt;): List of bboxes\n    Returns:\n        dict: The input dictionary to which we added a bboxes field containing coordinates and iou\n    '''\n# For each anchor ...\nfor anchor_idx, anchor in anchor_boxes_dict.items():\nanchor['bboxes'] = {}\nanchor_img_coordinates = anchor['anchor_img_coordinates']\n# ... and for each bbox in the list ...\nfor index_bbox, bbox in enumerate(image_bboxes):\n# ... we calculate the iou and add the info to the dictionary of anchors\nbbox_coordinates = (bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2'])\niou = get_iou(anchor_img_coordinates, bbox_coordinates)\nanchor['bboxes'][index_bbox] = {\n'iou': iou,\n'bbox_img_coordinates': bbox_coordinates,\n}\nanchor_boxes_dict[anchor_idx] = anchor\n# Return\nreturn anchor_boxes_dict\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_new_img_size_from_min_side_size","title":"<code>get_new_img_size_from_min_side_size(height, width, img_min_side_size=300)</code>","text":"<p>Gets the new dimensions of an image so that the smaller dimension is equal to img_min_side_size but keeping the ratio.</p> <p>Parameters:</p> Name Type Description Default <code>height</code> <code>int</code> <p>Height of the base image</p> required <code>width</code> <code>int</code> <p>Width of the base image</p> required Kwargs <p>img_min_side_size (int): Final size of the smaller dimension</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If incorrect dimension of the image (&lt; 1)</p> <code>ValueError</code> <p>If img_min_side_size is incorrect (&lt; 1)</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Resized height</p> <code>int</code> <code>int</code> <p>Resized width</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_new_img_size_from_min_side_size(height: int, width: int, img_min_side_size: int = 300) -&gt; Tuple[int, int]:\n'''Gets the new dimensions of an image so that the smaller dimension is equal to img_min_side_size\n    but keeping the ratio.\n    Args:\n        height (int): Height of the base image\n        width (int): Width of the base image\n    Kwargs:\n        img_min_side_size (int): Final size of the smaller dimension\n    Raises:\n        ValueError: If incorrect dimension of the image (&lt; 1)\n        ValueError: If img_min_side_size is incorrect (&lt; 1)\n    Returns:\n        int: Resized height\n        int: Resized width\n    '''\n# Manage errors\nif height &lt; 1 or width &lt; 1:\nraise ValueError(f\"Incorrect dimension of the image (H : {height} / W : {width})\")\nif img_min_side_size &lt; 1:\nraise ValueError(f\"Minimal size wanted incorrect ({img_min_side_size})\")\n# Width smaller than height, we calculates the new height and set the width to img_min_side_size\nif width &lt;= height:\nf = float(img_min_side_size) / width\nresized_height = int(f * height)\nresized_width = img_min_side_size\n# Height smaller than width, we calculates the new width and set the height to img_min_side_size\nelse:\nf = float(img_min_side_size) / height\nresized_width = int(f * width)\nresized_height = img_min_side_size\n# Return\nreturn resized_height, resized_width\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_roi_from_rpn_predictions","title":"<code>get_roi_from_rpn_predictions(model, img_data_batch, rpn_predictions_cls, rpn_predictions_regr)</code>","text":"<p>Converts the output layers of the RPN (classification and regression) in ROIs</p> We get the prediction results of the RPN and we want to select regions of interest (ROIs) for the <p>classifier part. For each point and each base anchor, we apply the results of the regression. Then we crop the resulting ROIs in order to stay in the limit of the image. Then we delete the unsuitable ROIs (ie. invalid) and finally we apply a Non Max Suppression (NMS) algorithm to remove the ROIs which overlap too much.</p> <p>Note : We work with float coordinates. It is no big deal, we will recast them to int to display them.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelKerasFasterRcnnObjectDetector</code> <p>Model used (contains all the necessary configs)</p> required <code>img_data_batch</code> <code>list&lt;dict&gt;</code> <p>List of img_data of the batch Here, it is used to get the (preprocessed) size of the images in order to remove the ROIs which are outside the image. Each entry must contain 'resized_height' &amp; 'resized_width'</p> required <code>rpn_predictions_cls</code> <code>np.ndarray</code> <p>Classification prediction (output RPN)</p> required <code>rpn_predictions_regr</code> <code>np.ndarray</code> <p>Regression prediction (output RPN)</p> required <p>Returns:</p> Type Description <code>List[np.ndarray]</code> <p>list : Final ROIs list selected for the classifier part (coordinates in features map space) Each element is a numpy array of the ROIs coordinates calculated for an image of the batch (variable number). The coordinates are returned as int (whereas they were float as output of the RPN)"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_roi_from_rpn_predictions--shape-batch_size-height_feature_map-width_feature_map-nb_anchor","title":"shape: (batch_size, height_feature_map, width_feature_map, nb_anchor)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_roi_from_rpn_predictions--shape-batch_size-height_feature_map-width_feature_map-4-nb_anchor","title":"shape: (batch_size, height_feature_map, width_feature_map, 4 * nb_anchor)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_roi_from_rpn_predictions--format-x1-y1-x2-y2","title":"Format x1, y1, x2, y2","text":"<p>Note : We can't return a numpy array because there are not the same number of ROIs for each image,        thus, we return a list</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_roi_from_rpn_predictions(model, img_data_batch: List[dict], rpn_predictions_cls: np.ndarray,\nrpn_predictions_regr: np.ndarray) -&gt; List[np.ndarray]:\n'''Converts the output layers of the RPN (classification and regression) in ROIs\n    Process : We get the prediction results of the RPN and we want to select regions of interest (ROIs) for the\n              classifier part. For each point and each base anchor, we apply the results of the regression. Then\n              we crop the resulting ROIs in order to stay in the limit of the image. Then we delete the unsuitable\n              ROIs (ie. invalid) and finally we apply a Non Max Suppression (NMS) algorithm to remove the ROIs\n              which overlap too much.\n    Note : We work with float coordinates. It is no big deal, we will recast them to int to display them.\n    Args:\n        model (ModelKerasFasterRcnnObjectDetector): Model used (contains all the necessary configs)\n        img_data_batch (list&lt;dict&gt;): List of img_data of the batch\n            Here, it is used to get the (preprocessed) size of the images in order to remove the ROIs which\n            are outside the image.\n            Each entry must contain 'resized_height' &amp; 'resized_width'\n        rpn_predictions_cls (np.ndarray): Classification prediction (output RPN)\n            # shape: (batch_size, height_feature_map, width_feature_map, nb_anchor)\n        rpn_predictions_regr (np.ndarray): Regression prediction (output RPN)\n            # shape: (batch_size, height_feature_map, width_feature_map, 4 * nb_anchor)\n    Returns:\n        list&lt;np.ndarray&gt; : Final ROIs list selected for the classifier part (coordinates in features map space)\n            Each element is a numpy array of the ROIs coordinates calculated for an image of the batch (variable number).\n            The coordinates are returned as int (whereas they were float as output of the RPN)\n            # Format x1, y1, x2, y2\n            Note : We can't return a numpy array because there are not the same number of ROIs for each image,\n                   thus, we return a list\n    '''\n# Get model attributes and info from the input shapes\nrpn_regr_scaling = model.rpn_regr_scaling\nsubsampling_ratio = model.shared_model_subsampling\nbase_anchors = model.list_anchors\nnb_anchors = model.nb_anchors\nroi_nms_overlap_threshold = model.roi_nms_overlap_threshold\nnms_max_boxes = model.nms_max_boxes\nbatch_size, height_feature_map, width_feature_map, _ = rpn_predictions_cls.shape\n# First we unscale the regression prediction (we scaled the target of the RPN)\nrpn_predictions_regr = rpn_predictions_regr / rpn_regr_scaling\n# We get the base anchor base in features map space\nbase_anchors_feature_maps = [(size[0] / subsampling_ratio, size[1] / subsampling_ratio) for size in base_anchors]\n# First we get all the possible anchor boxes on the features map\n# ie., for each point and each base anchor, we get the coordinates of the anchor box centered on the point\n# TODO : check if the + 0.5 are necessary\nanchor_on_feature_maps = np.array([\n[\n[\n[(x + 0.5 - width_anchor / 2, y + 0.5 - height_anchor / 2, height_anchor, width_anchor) for height_anchor, width_anchor in base_anchors_feature_maps]\nfor x in range(width_feature_map)\n]\nfor y in range(height_feature_map)\n]\nfor i in range(batch_size)\n])  # Format (batch_size, height, width, nb_anchors, nb_coords (format xyhw -&gt; 4))\n# Then we apply the regression result to these anchors\n# First we put together the coordinates of the anchor box and the regression next to each other for each point /anchor box /image\n# Format (batch_size, height, width, nb_anchors, 8 (x_anc, y_anc, h_anc, w_anc, tx, ty, th, tw))\n# Note : first we reshape the regression where the results of each anchors were concatenated\nrpn_predictions_regr = rpn_predictions_regr.reshape((batch_size, height_feature_map, width_feature_map, nb_anchors, 4))\nconcatenation_anchor_regr = np.concatenate([anchor_on_feature_maps, rpn_predictions_regr], axis=4)\n# Then we apply the regression for each entry and obtain the candidate ROIs\n# Format (batch_size, height, width, nb_anchors, 4 (x_roi, y_roi, h_roi, w_roi))\nrois_on_feature_maps = np.apply_along_axis(func1d=apply_regression, axis=4, arr=concatenation_anchor_regr)  # Format x, y, h, w\n# Then we crop the ROIs to stay inside the image\n# Problem : in a batch, we padded the images so that they all have the same size,\n#           and we want to crop the ROIs with respect to the initial size (unpadded)\n# Solution : We apply same trick as before, ie. we put the limit size of each image after the coordinates of the associated ROIs\nfeature_map_sizes = np.array([get_feature_map_size(img_data['resized_height'], img_data['resized_width'], subsampling_ratio)\nfor img_data in img_data_batch])\narray_img_size = np.broadcast_to(feature_map_sizes, (height_feature_map, width_feature_map, nb_anchors, batch_size, 2))\narray_img_size = np.transpose(array_img_size, (3, 0, 1, 2, 4))  # Format (batch_size, height, width, nb_anchors, 2)\nrois_on_feature_maps = np.concatenate([rois_on_feature_maps, array_img_size], axis=4)  # We add the sizes to the coordinates\n# We do some work on ROI coordinates\n# Format (batch_size, height, width, nb_anchors, 4 (x_roi, y_roi, h_roi, w_roi))\nrois_on_feature_maps = np.apply_along_axis(func1d=restrict_and_convert_roi_boxes, axis=4, arr=rois_on_feature_maps)   # Format x1, y1, x2, y2\n# Reshape the ROIs in order to have (batch_size, nb_rois, 4), nb_rois = height_feature_map * width_feature_map * nb_anchors\nrois_on_feature_maps = np.reshape(rois_on_feature_maps.transpose((0, 4, 1, 2, 3)), (batch_size, 4, -1)).transpose((0, 2, 1))\n# Same thing with RPN probabilities, ie. shape (batch_size, nb_rois)\nrois_probas = rpn_predictions_cls.reshape((batch_size, -1))\n# Finally we select the final ROIs by deleting the invalid ones and by limiting the overlaps\n# We cast the coordinateds to int in order to use them when cutting the ROIs\n# TODO : Could we try to always get 300? --&gt; Shape consistente\nrois_on_feature_maps = select_final_rois(rois_on_feature_maps, rois_probas, roi_nms_overlap_threshold,\nnms_max_boxes, feature_map_sizes)\nreturn rois_on_feature_maps\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_rois_bboxes_iou","title":"<code>get_rois_bboxes_iou(rois, img_data, subsampling_ratio)</code>","text":"<p>Gives the ious between the ROIs (in rois) and the bboxes (in img_data).</p> <p>Parameters:</p> Name Type Description Default <code>rois</code> <code>np.ndarray</code> <p>ROIs given by the RPN (ie. by the function get_roi_from_rpn_predictions())</p> required <code>img_data</code> <code>dict</code> <p>Metadata of the image after the preprocessing. In particular, the bboxes have been resized and rotated if the image has been resized and rotated. We only use the 'bboxes' field</p> required <code>subsampling_ratio</code> <code>int</code> <p>Subsampling of the base model (shared layers) - to apply to bboxes (which are in image space)</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing all the IOUs betwee, ROIs and bboxes of the image Keys : (index_roi) -&gt; 'coordinates' -&gt; 'x1', 'y1', 'x2', 'y2', 'h', 'w'                    -&gt; (index_bbox)  -&gt; 'coordinates': 'x1', 'y1', 'x2', 'y2'                                     -&gt; 'iou'                                     -&gt; 'class'</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_rois_bboxes_iou(rois: np.ndarray, img_data: dict, subsampling_ratio: int) -&gt; dict:\n'''Gives the ious between the ROIs (in rois) and the bboxes (in img_data).\n    Args:\n        rois (np.ndarray): ROIs given by the RPN (ie. by the function get_roi_from_rpn_predictions())\n            # Shape (N, 4), N corresponds to the number of given ROIs (in general max 300, cf. model.nms_max_boxes)\n        img_data (dict): Metadata of the image after the preprocessing. In particular, the bboxes have been resized\n            and rotated if the image has been resized and rotated. We only use the 'bboxes' field\n        subsampling_ratio (int): Subsampling of the base model (shared layers) - to apply to bboxes (which are in image space)\n    Returns:\n        dict: Dictionary containing all the IOUs betwee, ROIs and bboxes of the image\n            Keys : (index_roi) -&gt; 'coordinates' -&gt; 'x1', 'y1', 'x2', 'y2', 'h', 'w'\n                               -&gt; (index_bbox)  -&gt; 'coordinates': 'x1', 'y1', 'x2', 'y2'\n                                                -&gt; 'iou'\n                                                -&gt; 'class'\n    '''\n# Init. output dictionary\ndict_rois = {}\n# For each ROI, we get its coordinates, and the ious with each bbox of the image\nfor index_roi in range(rois.shape[0]):\n# Get the coordinates of each ROI\nx1_roi, y1_roi, x2_roi, y2_roi = rois[index_roi, :]\n_, _, h_roi, w_roi = xyxy_to_xyhw(*rois[index_roi, :])\ndict_roi = {\n'coordinates': {'x1': x1_roi, 'y1': y1_roi, 'x2': x2_roi, 'y2': y2_roi, 'h': h_roi, 'w': w_roi},\n'bboxes': {}\n}\n# Get the iou of each bbox\nfor index_bbox, bbox in enumerate(img_data['bboxes']):\n# bbox coordinates - input image format\n# Bbox coordinates (input format, ie. image space)\nbbox_coordinates = (bbox['x1'], bbox['y1'], bbox['x2'], bbox['y2'])\n# Coordinates transformation to features map space\nx1_bbox, y1_bbox, x2_bbox, y2_bbox = (coord / subsampling_ratio for coord in bbox_coordinates)\n# Calculus iou\niou = get_iou((x1_bbox, y1_bbox, x2_bbox, y2_bbox), (x1_roi, y1_roi, x2_roi, y2_roi))\ndict_roi['bboxes'][index_bbox] = {\n'coordinates': {'x1': x1_bbox, 'y1': y1_bbox, 'x2': x2_bbox, 'y2': y2_bbox},\n'iou': iou,\n'class': bbox['class']\n}\n# Append results\ndict_rois[index_roi] = dict_roi\n# Returns\nreturn dict_rois\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_rois_bboxes_iou--shape-n-4-n-corresponds-to-the-number-of-given-rois-in-general-max-300-cf-modelnms_max_boxes","title":"Shape (N, 4), N corresponds to the number of given ROIs (in general max 300, cf. model.nms_max_boxes)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_rois_targets","title":"<code>get_rois_targets(dict_rois, classifier_min_overlap, classifier_max_overlap)</code>","text":"<p>Finds the bbox with the biggest iou with an ROI and associate them. Then associates the class of this bbox to the ROI and, if the iou is sufficiently big, gives the associated regression.</p> <p>Parameters:</p> Name Type Description Default <code>dict_rois</code> <code>dict</code> <p>Dictionary containing all the ious between the ROIs and the bboxes of the image</p> required <code>classifier_min_overlap</code> <code>float</code> <p>Minimal threshold to consider a ROI as a target of the classifier (which can still be 'bg')</p> required <code>classifier_max_overlap</code> <code>float</code> <p>Minimal threshold to consider a ROI as matching with a bbox (so with a class which is not 'bg')</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the 'viable' ROIs and their classification and regression targets</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_rois_targets(dict_rois: dict, classifier_min_overlap: float, classifier_max_overlap: float) -&gt; dict:\n'''Finds the bbox with the biggest iou with an ROI and associate them. Then associates the class\n    of this bbox to the ROI and, if the iou is sufficiently big, gives the associated regression.\n    Args:\n        dict_rois (dict): Dictionary containing all the ious between the ROIs and the bboxes of the image\n        classifier_min_overlap (float): Minimal threshold to consider a ROI as a target of the classifier (which can still be 'bg')\n        classifier_max_overlap (float): Minimal threshold to consider a ROI as matching with a bbox (so with a class which is not 'bg')\n    Returns:\n        dict: Dictionary containing the 'viable' ROIs and their classification and regression targets\n    '''\ndict_rois_targets = {}\n# For each ROI ...\nfor roi_index, dict_roi in dict_rois.items():\n# ... get the coordinates ...\ncoords_roi = dict_roi['coordinates']\nx1_roi, y1_roi, x2_roi, y2_roi = (coords_roi['x1'], coords_roi['y1'], coords_roi['x2'], coords_roi['y2'])\n# ... get the best associated bbox (highest iou) ...\ndict_iou = {bbox_index: dict_roi['bboxes'][bbox_index]['iou'] for bbox_index in dict_roi['bboxes']}\nbest_bbox_index = max(dict_iou, key=dict_iou.get)\nbest_iou = dict_iou[best_bbox_index]\n# ... if best_iou lower than a threshold, we ignore this ROI ...\nif best_iou &lt; classifier_min_overlap:\ncontinue\n# ... otherwise, we define the best bbox and we complete the targets\ndict_roi['best_bbox_index'] = best_bbox_index\ndict_roi['best_iou'] = best_iou\ndict_roi['classifier_regression_target'] = (0, 0, 0, 0)\n# ... if best_iou is above a threshold, we consider a match on a class,\n# and get the regression target ...\nif best_iou &gt;= classifier_max_overlap:\n# class\ndict_roi['classifier_class_target'] = dict_roi['bboxes'][best_bbox_index]['class']\n# regression\ncoords_bbox = dict_roi['bboxes'][best_bbox_index]['coordinates']\nx1_bbox, y1_bbox, x2_bbox, y2_bbox = coords_bbox['x1'], coords_bbox['y1'], coords_bbox['x2'], coords_bbox['y2']\ndict_roi['classifier_regression_target'] = calc_regr((x1_bbox, y1_bbox, x2_bbox, y2_bbox), (x1_roi, y1_roi, x2_roi, y2_roi))\n# ... otherwise, we consider the ROI to be background (ie. 'bg')\n# Note : the regression target is not calculated if background\nelse:\ndict_roi['classifier_class_target'] = 'bg'\n# Append results\ndict_rois_targets[roi_index] = dict_roi\n# Returns\nreturn dict_rois_targets\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_rpn_targets","title":"<code>get_rpn_targets(model, img_data_batch)</code>","text":"<p>Gives the classification and regression targets for the RPN</p> We defined a set of possible anchor boxes (def. 9). For each point of the features map, <p>we look at the possible anchor boxes. We get back to the input image space and keep only the anchor boxes which are totally included in the image. Then, for each anchor box, we check if it matches with a bbox (via iou) and we define our target : match bbox vs match background and gap between anchor box and bbox for the regression part (only if there is a match on a bbox). We use this process for each image</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>ModelKerasFasterRcnnObjectDetector</code> <p>Model used (contains all the necessary configs)</p> required <code>img_data_batch</code> <code>list</code> <p>The list of img_data (dict) for the batch. Each entry is a dictionary with the content of an image (already preprocessed) and associated metadata:         - 'img' -&gt; image in the numpy format (h, w, c), preprocessed and ready to be used by the model         - 'bboxes' -&gt; (dict) associated bboxes (preprocessed image format)              'x1', 'x2', 'y1', 'y2'         - 'original_width' -&gt; Original width of the image         - 'original_height' -&gt; Original height of the image         - 'resized_width' -&gt; Resized width of the image (ie. smaller dim set to img_min_side_size px (def 300))         - 'resized_height' -&gt; Resized height of the image (ie. smaller dim set to img_min_side_size px (def 300))         - 'batch_width' -&gt; Width of the images in the batch (max width of the batch, we pad the smaller images with zeroes)         - 'batch_height' -&gt; Height of the images in the batch (max height of the batch, we pad the smaller images with zeroes)</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: Classification targets : [y_is_box_valid] + [y_rpn_overlap] for each image with :         - y_is_box_valid -&gt; if a box is valid (and thus, should enter in the classification loss)         - y_rpn_overlap -&gt; target of the classification ('pos', 'neg' or 'neutral')</p> <code>np.ndarray</code> <p>np.ndarray: Regression targets : [y_rpn_overlap (repeated x 4)] + [y_rpn_regr] for each image with :         - y_rpn_overlap -&gt; if a box is an object (and thus, should enter in the regression loss)             repeated to account for the 4 coordinates         - y_rpn_regr -&gt; regression targets</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_rpn_targets(model, img_data_batch: List[dict]) -&gt; Tuple[np.ndarray, np.ndarray]:\n'''Gives the classification and regression targets for the RPN\n    Process : We defined a set of possible anchor boxes (def. 9). For each point of the features map,\n              we look at the possible anchor boxes. We get back to the input image space and keep only\n              the anchor boxes which are totally included in the image. Then, for each anchor box, we check\n              if it matches with a bbox (via iou) and we define our target : match bbox vs match background\n              and gap between anchor box and bbox for the regression part (only if there is a match on a bbox).\n              We use this process for each image\n    Args:\n        model (ModelKerasFasterRcnnObjectDetector): Model used (contains all the necessary configs)\n        img_data_batch (list): The list of img_data (dict) for the batch.\n            Each entry is a dictionary with the content of an image (already preprocessed) and associated metadata:\n                    - 'img' -&gt; image in the numpy format (h, w, c), preprocessed and ready to be used by the model\n                    - 'bboxes' -&gt; (dict) associated bboxes (preprocessed image format)\n                         'x1', 'x2', 'y1', 'y2'\n                    - 'original_width' -&gt; Original width of the image\n                    - 'original_height' -&gt; Original height of the image\n                    - 'resized_width' -&gt; Resized width of the image (ie. smaller dim set to img_min_side_size px (def 300))\n                    - 'resized_height' -&gt; Resized height of the image (ie. smaller dim set to img_min_side_size px (def 300))\n                    - 'batch_width' -&gt; Width of the images in the batch (max width of the batch, we pad the smaller images with zeroes)\n                    - 'batch_height' -&gt; Height of the images in the batch (max height of the batch, we pad the smaller images with zeroes)\n    Returns:\n        np.ndarray: Classification targets : [y_is_box_valid] + [y_rpn_overlap] for each image with :\n                    - y_is_box_valid -&gt; if a box is valid (and thus, should enter in the classification loss)\n                    - y_rpn_overlap -&gt; target of the classification ('pos', 'neg' or 'neutral')\n            # Shape (batch_size, feature_map_height, feature_map_width, nb_anchors * 2)\n        np.ndarray: Regression targets : [y_rpn_overlap (repeated x 4)] + [y_rpn_regr] for each image with :\n                    - y_rpn_overlap -&gt; if a box is an object (and thus, should enter in the regression loss)\n                        repeated to account for the 4 coordinates\n                    - y_rpn_regr -&gt; regression targets\n            # Shape (batch_size, feature_map_height, feature_map_width, nb_anchors * 2 * 4)\n    '''\n# Extract params from model\nbase_anchors = model.list_anchors\nnb_anchors = model.nb_anchors\nsubsampling_ratio = model.shared_model_subsampling\nrpn_min_overlap = model.rpn_min_overlap\nrpn_max_overlap = model.rpn_max_overlap\nrpn_regr_scaling = model.rpn_regr_scaling\nnum_regions = model.rpn_restrict_num_regions\n# Info batch size\nbatch_size = len(img_data_batch)\n# Get size of the features map of the batch (for example by taking the first image)\nfeature_map_height, feature_map_width = get_feature_map_size(img_data_batch[0]['batch_height'], img_data_batch[0]['batch_width'], subsampling_ratio)\n# Setup target arrays\nY1 = np.zeros((batch_size, feature_map_height, feature_map_width, nb_anchors * 2))\nY2 = np.zeros((batch_size, feature_map_height, feature_map_width, nb_anchors * 2 * 4))\n# We process each image\nfor ind, img_data in enumerate(img_data_batch):\n# Info image data\nim_resized_height, im_resized_width = img_data['resized_height'], img_data['resized_width']\nimage_bboxes = img_data['bboxes']\n# Get the \"viable\" anchor boxes : one for each couple (point features map, base anchor) except if it does not fit\n# in the image\nanchor_boxes_dict = get_all_viable_anchors_boxes(base_anchors, subsampling_ratio, feature_map_height,\nfeature_map_width, im_resized_height, im_resized_width)\n# Get iou for each couple (anchor box / bbox)\nanchor_boxes_dict = get_iou_anchors_bboxes(anchor_boxes_dict, image_bboxes)\n# Set anchor validity &amp; type for each anchor\n# - pos &amp; valid if match on a bbox (ie. an object)\n# - neg &amp; valid if match on background\n# - neutral &amp; invalid otherwise\nanchor_boxes_dict, bboxes_index_with_no_positive = set_anchors_type_validity(anchor_boxes_dict, image_bboxes, rpn_min_overlap, rpn_max_overlap)\n# We add at least one positive anchor box for each bbox which does not have one match\n# (in some cases, it is not possible, in that case : skip)\nanchor_boxes_dict = complete_at_least_one_anchor_per_bbox(anchor_boxes_dict, bboxes_index_with_no_positive)\n# Invalidate some anchors in order not to have too many\nanchor_boxes_dict = restrict_valid_to_n_regions(anchor_boxes_dict, num_regions=num_regions)\n# Add the regression target for the positive and valid anchors\nanchor_boxes_dict = add_regression_target_to_pos_valid(anchor_boxes_dict)\n# We have the anchors, their type and validity and the regression targets for the pos/valid anchors\n# We format the result. Here we initialize\ny_rpn_overlap = np.zeros((feature_map_height, feature_map_width, nb_anchors))  # Target classifier\ny_is_box_valid = np.zeros((feature_map_height, feature_map_width, nb_anchors))  # couples (ix, anchor) which will enter the loss (ie. are not neutral)\ny_rpn_regr = np.zeros((feature_map_height, feature_map_width, nb_anchors * 4))\n# For each anchor, we add data.\n# The deleted anchors (because not 'viable') are not in anchor_boxes_dict, BUT all their characteristics should be zero\n# which is the case thanks to the initialization\nfor anchor_idx, anchor in anchor_boxes_dict.items():\ny_rpn_overlap[anchor_idx[0], anchor_idx[1], anchor_idx[2]] = 1 if anchor['anchor_type'] == 'pos' else 0\ny_is_box_valid[anchor_idx[0], anchor_idx[1], anchor_idx[2]] = anchor['anchor_validity']\nstart_regr_index = 4 * anchor_idx[2]\ny_rpn_regr[anchor_idx[0], anchor_idx[1], start_regr_index: start_regr_index + 4] = anchor['regression_target']\n# We then concat all final arrays\n# For regression part, we add y_rpn_overlap (repeated) to y_rpn_regr in order to identify data that should be used by the regression loss\ny_rpn_cls = np.concatenate([y_is_box_valid, y_rpn_overlap], axis=2)\ny_rpn_regr = np.concatenate([np.repeat(y_rpn_overlap, 4, axis=2), y_rpn_regr], axis=2)\n# We scale the regression target\ny_rpn_regr[:, :, y_rpn_regr.shape[2] // 2:] *= rpn_regr_scaling\n# We finally update the output arrays\nY1[ind, :, :, :] = y_rpn_cls\nY2[ind, :, :, :] = y_rpn_regr\n# Return\nreturn Y1, Y2\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_rpn_targets--shape-batch_size-feature_map_height-feature_map_width-nb_anchors-2","title":"Shape (batch_size, feature_map_height, feature_map_width, nb_anchors * 2)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_rpn_targets--shape-batch_size-feature_map_height-feature_map_width-nb_anchors-2-4","title":"Shape (batch_size, feature_map_height, feature_map_width, nb_anchors * 2 * 4)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_valid_boxes_from_coordinates","title":"<code>get_valid_boxes_from_coordinates(input_img, input_rois, fm_boxes_candidates, regr_coordinates, classifier_regr_scaling, subsampling_ratio, dict_classes)</code>","text":"<p>Calculates the coordinates (in image space) after application of the regression of the boxes (in features map space) whose probability is sufficiently high. Then restricts them to the image and keeps only the valid boxes</p> <p>Parameters:</p> Name Type Description Default <code>input_img</code> <code>np.ndarray</code> <p>Resized image (useful to get the dimensions)</p> required <code>input_rois</code> <code>np.ndarray</code> <p>ROIs given by the RPN</p> required <code>fm_boxes_candidates</code> <code>list</code> <p>The boxes (in features map space) valid with respect to their proba</p> required <code>regr_coordinates</code> <code>np.ndarray</code> <p>Regression prediction for the boxes</p> required <code>classifier_regr_scaling</code> <code>list</code> <p>Scaling to remove from the regression results</p> required <code>subsampling_ratio</code> <code>int</code> <p>Subsampling of the base model (shared layers) - to apply to bboxes (which are in image space)</p> required <code>dict_classes</code> <code>dict</code> <p>Dictionary of the classes of the model</p> required <p>Returns:</p> Type Description <code>List[tuple]</code> <p>A list of boxes valid from a probability AND coordinates xyxy points of view</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_valid_boxes_from_coordinates(input_img: np.ndarray, input_rois: np.ndarray, fm_boxes_candidates: List[tuple],\nregr_coordinates: np.ndarray, classifier_regr_scaling: List[float], subsampling_ratio: int,\ndict_classes: dict) -&gt; List[tuple]:\n'''Calculates the coordinates (in image space) after application of the regression of the boxes (in features map space) whose\n    probability is sufficiently high. Then restricts them to the image and keeps only the valid boxes\n    Args:\n        input_img (np.ndarray): Resized image (useful to get the dimensions)\n        input_rois (np.ndarray): ROIs given by the RPN\n        fm_boxes_candidates (list): The boxes (in features map space) valid with respect to their proba\n        regr_coordinates (np.ndarray): Regression prediction for the boxes\n        classifier_regr_scaling (list): Scaling to remove from the regression results\n        subsampling_ratio (int): Subsampling of the base model (shared layers) - to apply to bboxes (which are in image space)\n        dict_classes (dict): Dictionary of the classes of the model\n    Returns:\n        A list of boxes valid from a probability AND coordinates xyxy points of view\n            # Format [(cl, proba, coordinates), (...), ...)\n    '''\nboxes_candidates = []\n# For each box (in features map space)...\nfor index_box, predicted_class, predicted_proba in fm_boxes_candidates:\nroi_coordinates = input_rois[index_box]  # Get corresponding ROI\nregr_predicted = regr_coordinates[index_box][predicted_class * 4: (predicted_class + 1) * 4]  # Get the regression associated to this class\nregr_predicted = np.array([a / b for a, b in zip(regr_predicted, classifier_regr_scaling)])  # Remove the scaling\n# Apply predicted regression\ncoordinates_after_regr = list(apply_regression(np.concatenate([roi_coordinates, regr_predicted])))\n# Make sure that the upper left point is in the features map\ncoordinates_after_regr[0] = max(0, coordinates_after_regr[0])\ncoordinates_after_regr[1] = max(0, coordinates_after_regr[1])\nbbox_fm_coords = xyhw_to_xyxy(*coordinates_after_regr)\n# Get the coordinates in the input format (ie. in image space)\nx1_bbox, y1_bbox, x2_bbox, y2_bbox = (coord * subsampling_ratio for coord in bbox_fm_coords)\n# Make sure that the point defining the box are in the image\nx1_bbox = max(0, x1_bbox)\ny1_bbox = max(0, y1_bbox)\nx2_bbox = min(input_img.shape[1], x2_bbox)\ny2_bbox = min(input_img.shape[0], y2_bbox)\n# If the box is valid ...\nif x1_bbox &lt; x2_bbox and y1_bbox &lt; y2_bbox:\n# ... we add it to the list\nbox_infos = (dict_classes[predicted_class], predicted_proba, (x1_bbox, y1_bbox, x2_bbox, y2_bbox))\nboxes_candidates.append(box_infos)\nreturn boxes_candidates\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_valid_boxes_from_coordinates--format-cl-proba-coordinates","title":"Format [(cl, proba, coordinates), (...), ...)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_valid_fm_boxes_from_proba","title":"<code>get_valid_fm_boxes_from_proba(probas, proba_threshold, bg_index)</code>","text":"<p>Keeps predicted (in features map space) boxes whose probability is above a threshold. Also deletes all the boxes which matched on background</p> <p>Parameters:</p> Name Type Description Default <code>probas</code> <code>np.ndarray</code> <p>Probabilities of the boxes predicted by the model</p> required <code>proba_threshold</code> <code>float</code> <p>Threshold below which, boxes are eliminated</p> required <p>Returns:</p> Type Description <code>List[tuple]</code> <p>A list of boxes (in features map space) valid from a probability point of view</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def get_valid_fm_boxes_from_proba(probas: np.ndarray, proba_threshold: float, bg_index: int) -&gt; List[tuple]:\n'''Keeps predicted (in features map space) boxes whose probability is above a threshold. Also deletes\n    all the boxes which matched on background\n    Args:\n        probas (np.ndarray): Probabilities of the boxes predicted by the model\n        proba_threshold (float): Threshold below which, boxes are eliminated\n    Returns:\n        A list of boxes (in features map space) valid from a probability point of view\n            # Format [(index, index_cl, proba), (...), ...)\n    '''\nfm_boxes_candidates = []\n# For each box ...\nfor index_box, box_probas in enumerate(probas):\n# ... get the class ...\npredicted_class = np.argmax(box_probas)\n# ... get the corresponding probability ...\npredicted_proba = box_probas[predicted_class]\n# ..., and, if we are above the threshold and the predicted class is not the background...\nif predicted_proba &gt;= proba_threshold and predicted_class != bg_index:\n# ... we add the box to the list\nfm_boxes_candidates.append((index_box, predicted_class, predicted_proba))\nreturn fm_boxes_candidates\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.get_valid_fm_boxes_from_proba--format-index-index_cl-proba","title":"Format [(index, index_cl, proba), (...), ...)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.limit_rois_targets","title":"<code>limit_rois_targets(dict_rois_targets, nb_rois_per_img)</code>","text":"<p>Limits the number of input / output for each image in order not to have OOM</p> <p>Parameters:</p> Name Type Description Default <code>dict_rois</code> <code>dict</code> <p>Dictionary containing the possible inputs / targets of the classifier</p> required <code>nb_rois_per_img</code> <code>int</code> <p>Maximal number of ROIs to return for each image In the rare case where there are not enough ROIs, we clone the ROIs in order to have enough If no ROI, we return None. This case is then handled by the function create_fake_dict_rois_targets</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>Union[dict, None]</code> <p>The dictionary containing the \"selected\" dictionary</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def limit_rois_targets(dict_rois_targets: dict, nb_rois_per_img: int) -&gt; Union[dict, None]:\n'''Limits the number of input / output for each image in order not to have OOM\n    Args:\n        dict_rois (dict): Dictionary containing the possible inputs / targets of the classifier\n        nb_rois_per_img (int): Maximal number of ROIs to return for each image\n            In the rare case where there are not enough ROIs, we clone the ROIs in order to have enough\n            If no ROI, we return None. This case is then handled by the function create_fake_dict_rois_targets\n    Returns:\n        dict: The dictionary containing the \"selected\" dictionary\n    '''\n# Get the positive and negative ROIs\npos_rois_indexes = [roi_index for roi_index, roi in dict_rois_targets.items() if roi['classifier_class_target'] != 'bg']\nneg_rois_indexes = [roi_index for roi_index, roi in dict_rois_targets.items() if roi['classifier_class_target'] == 'bg']\n# Case 1 : no ROI (very rare ?!), return None\nif len(pos_rois_indexes) + len(neg_rois_indexes) == 0:\nlogger.warning(\"Warning, there is an image for which we do not have a target ROI for the classifier.\")\nreturn None\n# Case 2 : not enough ROIs\nelif len(pos_rois_indexes) + len(neg_rois_indexes) &lt; nb_rois_per_img:\nlogger.warning(f\"Warning, there is an image for which we have less than {nb_rois_per_img} target ROIs. We randomly clone some ROIs\")\nselected_indexes = pos_rois_indexes + neg_rois_indexes\nselected_indexes = random.sample(selected_indexes, k=len(selected_indexes))\nselected_indexes = list(np.resize(selected_indexes, nb_rois_per_img))\n# Case 3 : enough ROIs\nelse:\n# Case 3.1 : not enough positive ROIs\nif len(pos_rois_indexes) &lt; nb_rois_per_img // 2:\nselected_neg_indexes = random.sample(neg_rois_indexes, k=(nb_rois_per_img - len(pos_rois_indexes)))\nselected_indexes = pos_rois_indexes + selected_neg_indexes\nselected_indexes = random.sample(selected_indexes, k=len(selected_indexes))\n# Cas 3.2 : not enough negative ROIs\nelif len(neg_rois_indexes) &lt; nb_rois_per_img // 2:\nselected_pos_indexes = random.sample(pos_rois_indexes, k=(nb_rois_per_img - len(neg_rois_indexes)))\nselected_indexes = selected_pos_indexes + neg_rois_indexes\nselected_indexes = random.sample(selected_indexes, k=len(selected_indexes))\n# Cas 3.3 : nominal case, we have everything we need\nelse:\nselected_pos_indexes = random.sample(pos_rois_indexes, k=nb_rois_per_img // 2)\nselected_neg_indexes = random.sample(neg_rois_indexes, k=(nb_rois_per_img - len(selected_pos_indexes)))\nselected_indexes = selected_pos_indexes + selected_neg_indexes\nselected_indexes = random.sample(selected_indexes, k=len(selected_indexes))\n# We return the ROIs whose index are in selected_indexes\n# We are careful to manage \"duplicates\" in the list of selected indices\nreturn {i: copy.deepcopy(dict_rois_targets[roi_index]) for i, roi_index in enumerate(selected_indexes)}\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast","title":"<code>non_max_suppression_fast(img_boxes_coordinates, img_boxes_probas, nms_overlap_threshold, nms_max_boxes, img_boxes_classes=None)</code>","text":"<p>Filters boxes in order to limit overlaps on the same object using a list of boxes (ROIs or final predictions) and the probabilities of matching with an object.</p> <p>Parameters:</p> Name Type Description Default <code>img_boxes_coordinates</code> <code>np.ndarray</code> <p>The coordinates of the boxes (in opposite points format)</p> required <code>img_boxes_probas</code> <code>np.ndarray</code> <p>The probabilities associated to the boxes</p> required <code>nms_overlap_threshold</code> <code>float</code> <p>The iou value above which we assume that two boxes overlap</p> required <code>nms_max_boxes</code> <code>int</code> <p>The maximal number of boxes that this function can return</p> required Kwargs <p>img_boxes_classes (np.ndarray): The classes associated with the boxes (optional)     # shape: (nb_boxes)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If img_boxes_probas is not the same length as img_boxes_coordinates</p> <code>ValueError</code> <p>If nms_overlap_threshold &lt;= 0 or &gt; 1</p> <code>ValueError</code> <p>If nms_max_boxes &lt; 1</p> <code>ValueError</code> <p>If img_boxes_classes is not the same length as img_boxes_coordinates (if != None)</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.ndarray: List of kept boxes</p> <code>np.ndarray</code> <p>np.ndarray: Associated probabilities</p> <code>np.ndarray</code> <p>np.ndarray: Associated classes (if prediction)</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def non_max_suppression_fast(img_boxes_coordinates: np.ndarray, img_boxes_probas: np.ndarray, nms_overlap_threshold: float,\nnms_max_boxes: int, img_boxes_classes: Union[np.ndarray, None] = None) -&gt; np.ndarray:\n'''Filters boxes in order to limit overlaps on the same object using a list of boxes (ROIs or final predictions)\n    and the probabilities of matching with an object.\n    Args:\n        img_boxes_coordinates (np.ndarray): The coordinates of the boxes (in opposite points format)\n            # shape: (nb_boxes, 4)\n        img_boxes_probas (np.ndarray): The probabilities associated to the boxes\n            # shape: (nb_boxes)\n        nms_overlap_threshold (float): The iou value above which we assume that two boxes overlap\n        nms_max_boxes (int): The maximal number of boxes that this function can return\n    Kwargs:\n        img_boxes_classes (np.ndarray): The classes associated with the boxes (optional)\n            # shape: (nb_boxes)\n    Raises:\n        ValueError: If img_boxes_probas is not the same length as img_boxes_coordinates\n        ValueError: If nms_overlap_threshold &lt;= 0 or &gt; 1\n        ValueError: If nms_max_boxes &lt; 1\n        ValueError: If img_boxes_classes is not the same length as img_boxes_coordinates (if != None)\n    Returns:\n        np.ndarray: List of kept boxes\n            # shape: (nb_boxes_kept, 4)\n        np.ndarray: Associated probabilities\n        np.ndarray: Associated classes (if prediction)\n    '''\n# code taken from here: http://www.pyimagesearch.com/2015/02/16/faster-non-maximum-suppression-python/\n# if there are no boxes, returns an empty list\n# Manage errors\nif img_boxes_coordinates.shape[0] != img_boxes_probas.shape[0]:\nraise ValueError(\"The arrays img_boxes_coordinates and img_boxes_probas must have the same length.\")\nif not 0 &lt; nms_overlap_threshold &lt;= 1:\nraise ValueError(\"The value of nms_overlap_threshold must be between 0 and 1 (0 excluded, 1 included)\")\nif nms_max_boxes &lt; 1:\nraise ValueError(\"The argument nms_max_boxes must be positive\")\nif img_boxes_classes is not None and img_boxes_coordinates.shape[0] != img_boxes_classes.shape[0]:\nraise ValueError(\"The arrays img_boxes_coordinates and img_boxes_classes must have the same length.\")\n# Process explanation:\n#   Step 1: Sort the probs list\n#   Step 2: Find the larget prob 'Last' in the list and save it to the pick list\n#   Step 3: Calculate the IoU with 'Last' box and other boxes in the list. If the IoU is larger than overlap_threshold, delete the box from list\n#   Step 4: Repeat step 2 and step 3 until there is no item in the probs list\n# If empty, return an empty array\nif len(img_boxes_coordinates) == 0:\nreturn np.array([]), np.array([]), np.array([])\n# Grab the coordinates of the boxes &amp; calculate the areas\nx1_box, y1_box, x2_box, y2_box = (img_boxes_coordinates[:, i] for i in range(4))\nboxes_areas = (x2_box - x1_box) * (y2_box - y1_box)\n# We now loop over each boxes, sorted by max probas\npicked_index = []\nidxs = np.argsort(img_boxes_probas)\n# Keep looping while some indexes still remain in the list\nwhile len(idxs) &gt; 0:\n# If we have enough boxes, break\nif len(picked_index) &gt;= nms_max_boxes:\nbreak\n# Add highest proba remaining to picked indexes\npicked_index.append(idxs[-1])\n# Find intersection area between picked box &amp; remaining candidates\nxx1_int = np.maximum(x1_box[idxs[-1]], x1_box[idxs[:-1]])\nyy1_int = np.maximum(y1_box[idxs[-1]], y1_box[idxs[:-1]])\nxx2_int = np.minimum(x2_box[idxs[-1]], x2_box[idxs[:-1]])\nyy2_int = np.minimum(y2_box[idxs[-1]], y2_box[idxs[:-1]])\nww_int = np.maximum(0, xx2_int - xx1_int)\nhh_int = np.maximum(0, yy2_int - yy1_int)\narea_int = ww_int * hh_int\n# Get the union\narea_union = boxes_areas[idxs[-1]] + boxes_areas[idxs[:-1]] - area_int\n# Compute the overlap (i.e. iou)\noverlap = area_int / (area_union + 1e-6)\n# Delete last index (selected) &amp; all indexes from the index list that have an IOU higher than a given threhsold\nidxs = np.delete(idxs, np.concatenate(([len(idxs) - 1], np.where(overlap &gt; nms_overlap_threshold)[0])))\n# Return only the boxes that were picked\nimg_boxes_coordinates = img_boxes_coordinates[picked_index, :]\nif img_boxes_classes is not None:\nreturn img_boxes_coordinates, img_boxes_probas[picked_index], img_boxes_classes[picked_index]\nelse:\nreturn img_boxes_coordinates, img_boxes_probas[picked_index], None\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast--shape-nb_boxes-4","title":"shape: (nb_boxes, 4)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast--shape-nb_boxes","title":"shape: (nb_boxes)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast--shape-nb_boxes_kept-4","title":"shape: (nb_boxes_kept, 4)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast_on_preds","title":"<code>non_max_suppression_fast_on_preds(boxes_candidates, nms_overlap_threshold)</code>","text":"<p>Applies the NMS algorithm on the valid predicted boxes to avoid overlaps.</p> <p>Parameters:</p> Name Type Description Default <code>boxes_candidates</code> <code>list</code> <p>Valid predicted boxes</p> required <code>nms_overlap_threshold</code> <code>float</code> <p>Above this threshold for the iou, two boxes are said to be overlapping</p> required <p>Returns:</p> Type Description <code>List[tuple]</code> <p>A list of boxes valid from a probability AND coordinates xyxy points of view and with \"no\" overlap</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def non_max_suppression_fast_on_preds(boxes_candidates: List[tuple], nms_overlap_threshold: float) -&gt; List[tuple]:\n'''Applies the NMS algorithm on the valid predicted boxes to avoid overlaps.\n    Args:\n        boxes_candidates (list): Valid predicted boxes\n            # Format [(cl, proba, coordinates), (...), ...)\n        nms_overlap_threshold (float): Above this threshold for the iou, two boxes are said to be overlapping\n    Returns:\n        A list of boxes valid from a probability AND coordinates xyxy points of view and with \"no\" overlap\n            # Format [(cl, proba, coordinates), (...), ...)\n    '''\n# If there are no valid boxes\nif len(boxes_candidates) == 0:\nreturn []\n# First we format the inputs to the format for the NMS\nimg_boxes_classes = np.array([cl for cl, _, _ in boxes_candidates])\nimg_boxes_probas = np.array([proba for _, proba, _ in boxes_candidates])\nimg_boxes_coordinates = np.array([coordinates for _, _, coordinates in boxes_candidates])\n# Apply NMS\nnms_result = non_max_suppression_fast(img_boxes_coordinates, img_boxes_probas, nms_overlap_threshold, np.inf, img_boxes_classes=img_boxes_classes)\nimg_boxes_coordinates, img_boxes_probas, img_boxes_classes = nms_result\n# Format final result\nfinal_boxes = []\nfor i in range(img_boxes_coordinates.shape[0]):\nfinal_boxes.append((img_boxes_classes[i], img_boxes_probas[i], img_boxes_coordinates[i]))\n# Return\nreturn final_boxes\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast_on_preds--format-cl-proba-coordinates","title":"Format [(cl, proba, coordinates), (...), ...)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.non_max_suppression_fast_on_preds--format-cl-proba-coordinates","title":"Format [(cl, proba, coordinates), (...), ...)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.restrict_and_convert_roi_boxes","title":"<code>restrict_and_convert_roi_boxes(bbox_coordinates)</code>","text":"<p>Resizes the box to have the minimal size and crops it to stay in the features map. Finally, converts it in xyxy coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>bbox_coordinates</code> <code>np.ndarray</code> <p>An array composed of 6 objects : x_roi, y_roi, h_roi, w_roi, height_img_in_feature_map, width_img_in_feature_map. (x_roi, y_roi, h_roi, w_roi) are the coordinates of a ROI (height_img_in_feature_map, width_img_in_feature_map) sont les tailles avant padding de l'image correspondantes, puis downsampled au format feature map</p> required <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Coordinates of the ROI after correction - x coordinate of the upper left point</p> <code>float</code> <code>float</code> <p>Coordinates of the ROI after correction - y coordinate of the upper left point</p> <code>float</code> <code>float</code> <p>Coordinates of the ROI after correction - x coordinate of the bottom right point</p> <code>float</code> <code>float</code> <p>Coordinates of the ROI after correction - y coordinate of the bottom right point</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def restrict_and_convert_roi_boxes(bbox_coordinates: np.ndarray) -&gt; Tuple[float, float, float, float]:\n'''Resizes the box to have the minimal size and crops it to stay in the features map. Finally,\n    converts it in xyxy coordinates.\n    Args:\n        bbox_coordinates (np.ndarray): An array composed of 6 objects : x_roi, y_roi, h_roi, w_roi, height_img_in_feature_map, width_img_in_feature_map.\n            (x_roi, y_roi, h_roi, w_roi) are the coordinates of a ROI\n            (height_img_in_feature_map, width_img_in_feature_map) sont les tailles avant padding de l'image correspondantes, puis downsampled au format feature map\n    Returns:\n        float: Coordinates of the ROI after correction - x coordinate of the upper left point\n        float: Coordinates of the ROI after correction - y coordinate of the upper left point\n        float: Coordinates of the ROI after correction - x coordinate of the bottom right point\n        float: Coordinates of the ROI after correction - y coordinate of the bottom right point\n    '''\nx, y, h, w, height_img_in_feature_map, width_img_in_feature_map = bbox_coordinates\n# We want the box to have a size of at least 1\nh = np.maximum(1, h)\nw = np.maximum(1, w)\n# We want the upper left point to be in the image (projected on the features map)\nx = np.maximum(0, x)\ny = np.maximum(0, y)\n# Convert in xyxy (opposite points format)\nx1, y1, x2, y2 = xyhw_to_xyxy(x, y, h, w)\n# We want the bottom right point to be in the image (projected on the features map)\nx2 = np.minimum(width_img_in_feature_map, x2)\ny2 = np.minimum(height_img_in_feature_map, y2)\n# Return new coordinates\nreturn x1, y1, x2, y2\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.restrict_valid_to_n_regions","title":"<code>restrict_valid_to_n_regions(anchor_boxes_dict, num_regions)</code>","text":"<p>Restricts the number of valid anchor boxes.</p> If there are more positive anchor boxes than hald of num_regions <ul> <li>we invalidate positive anchors until there are less than num_regions / 2</li> <li>Then, we invalidate positive anchors until the number of valid anchors is equal to num_regions</li> </ul> <p>Parameters:</p> Name Type Description Default <code>anchor_boxes_dict</code> <code>dict</code> <p>Anchor boxes dictionary - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space) - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou - 'anchor_type': anchor type (pos, neg or neutral) - 'anchor_validity': anchor validity - 'best_bbox_index': bbox associated to this anchor</p> required <code>num_regions</code> <code>int</code> <p>The number of valid anchors we want to consider</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated anchor boxes dictionary</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def restrict_valid_to_n_regions(anchor_boxes_dict: dict, num_regions: int) -&gt; dict:\n'''Restricts the number of valid anchor boxes.\n    If there are more positive anchor boxes than hald of num_regions :\n        - we invalidate positive anchors until there are less than num_regions / 2\n        - Then, we invalidate positive anchors until the number of valid anchors is equal to num_regions\n    Args:\n        anchor_boxes_dict (dict): Anchor boxes dictionary\n            - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space)\n            - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou\n            - 'anchor_type': anchor type (pos, neg or neutral)\n            - 'anchor_validity': anchor validity\n            - 'best_bbox_index': bbox associated to this anchor\n        num_regions (int): The number of valid anchors we want to consider\n    Returns:\n        dict: Updated anchor boxes dictionary\n    '''\n# We look at both positive and negative anchor boxes\n# No need to test for validity at this point, positive and negative anchor boxes are necessarily valid\npositive_anchor_indexes = [anchor_idx for anchor_idx, anchor in anchor_boxes_dict.items() if anchor['anchor_type'] == 'pos']\nnegative_anchor_indexes = [anchor_idx for anchor_idx, anchor in anchor_boxes_dict.items() if anchor['anchor_type'] == 'neg']\n# First we invalidate the surplus of positive anchors if needed ...\nnb_pos = len(positive_anchor_indexes)\nnb_pos_to_invalidate = max(0, nb_pos - int(num_regions / 2))\nif nb_pos_to_invalidate &gt; 0:\n# Random select\nanchors_indexes_to_unvalid = random.sample(positive_anchor_indexes, nb_pos_to_invalidate)\nfor anchor_idx in anchors_indexes_to_unvalid:\nanchor_boxes_dict[anchor_idx]['anchor_validity'] = 0\nnb_pos = int(num_regions / 2)\n# ... Then we invalidate negative regions until we have num_regions valid anchor boxes\nnb_neg_to_invalidate = len(negative_anchor_indexes) + nb_pos - num_regions\nif nb_neg_to_invalidate &gt; 0:\n# Random select\nanchors_indexes_to_unvalid = random.sample(negative_anchor_indexes, nb_neg_to_invalidate)\nfor anchor_idx in anchors_indexes_to_unvalid:\nanchor_boxes_dict[anchor_idx]['anchor_validity'] = 0\n# Return updated dict\nreturn anchor_boxes_dict\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.select_final_rois","title":"<code>select_final_rois(rois_coordinates, rois_probas, roi_nms_overlap_threshold, nms_max_boxes, feature_map_sizes)</code>","text":"<p>Deletes the invalid ROIs and selects some of them to limit the overlaps</p> <p>Parameters:</p> Name Type Description Default <code>rois_coordinates</code> <code>np.ndarray</code> <p>The set of all selected ROIs for all the images of the batch</p> required <code>rois_probas</code> <code>np.ndarray</code> <p>The probabilities associated to each selected ROIsfor all the images</p> required <code>roi_nms_overlap_threshold</code> <code>float</code> <p>Above this threshold for the iou, we assume that two ROIs overlap</p> required <code>nms_max_boxes</code> <code>int</code> <p>Maximal number of ROIs that this function can return for each image</p> required <code>feature_map_sizes</code> <code>np.ndarray</code> <p>Theoretical heights and widths of the features maps - useful if we have no valid ROI anymore. Allows to manage the fact that, in a batch, we padded the images so that they all have the same size     # shape: (batch_size, 2)</p> required <p>Returns:</p> Type Description <code>List[np.ndarray]</code> <p>list: Final list of the ROIs selected for the classifier part Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def select_final_rois(rois_coordinates: np.ndarray, rois_probas: np.ndarray, roi_nms_overlap_threshold: float,\nnms_max_boxes: int, feature_map_sizes: np.ndarray) -&gt; List[np.ndarray]:\n'''Deletes the invalid ROIs and selects some of them to limit the overlaps\n    Args:\n        rois_coordinates (np.ndarray): The set of all selected ROIs for all the images of the batch\n            # shape: (batch_size, nb_rois, 4)\n        rois_probas (np.ndarray): The probabilities associated to each selected ROIsfor all the images\n            # shape: (batch_size, nb_rois)\n        roi_nms_overlap_threshold (float): Above this threshold for the iou, we assume that two ROIs overlap\n        nms_max_boxes (int): Maximal number of ROIs that this function can return for each image\n        feature_map_sizes (np.ndarray): Theoretical heights and widths of the features maps - useful if we have no valid ROI anymore.\n            Allows to manage the fact that, in a batch, we padded the images so that they all have the same size\n                # shape: (batch_size, 2)\n    Returns:\n        list&lt;np.ndarray&gt;: Final list of the ROIs selected for the classifier part\n    '''\n# We process each image of the batch separately and stocks the results in list_rois\n# We can't return a numpy array because the number of ROIs for each image is not the same -&gt; we return a list\nlist_rois = []\nfor img_index in range(rois_coordinates.shape[0]):\n# Get infos\nimg_rois_coordinates = rois_coordinates[img_index]\nimg_rois_probas = rois_probas[img_index]\nx1, y1, x2, y2 = (img_rois_coordinates[:, i] for i in range(4))\n# Eliminate invalid anchors\nidxs = np.where((x1 - x2 &gt;= 0) | (y1 - y2 &gt;= 0))\nimg_rois_coordinates = np.delete(img_rois_coordinates, idxs, 0)\nimg_rois_probas = np.delete(img_rois_probas, idxs, 0)\n# In the rare cases where there are no more ROIs, we create one artificially (the whole image)\nif img_rois_coordinates.shape[0] == 0:\nlogger.warning(\"Warning, there is an image for which we can't find a valid ROI.\")\nlogger.warning(\"By default, we create an artificial ROI which cover the whole image.\")\nheight_img_in_feature_map, width_img_in_feature_map = feature_map_sizes[img_index, :]\nimg_rois_coordinates = np.array([[0, 0, width_img_in_feature_map, height_img_in_feature_map]])  # x1, y1, x2, y2\n# Otherwise, we continue the process\nelse:\n# We keep the ROIs which do not overlap\nimg_rois_coordinates, _, _ = non_max_suppression_fast(img_rois_coordinates, img_rois_probas, roi_nms_overlap_threshold, nms_max_boxes)\n# Finally, we cast to int (because we will cut the features map per index)\n# Round\nimg_rois_coordinates = np.around(img_rois_coordinates).astype(\"int\")\n# Delete invalid ROIs again (after rounding)\nx1, y1, x2, y2 = (img_rois_coordinates[:, i] for i in range(4))\nidxs = np.where((x1 - x2 &gt;= 0) | (y1 - y2 &gt;= 0))\nimg_rois_coordinates = np.delete(img_rois_coordinates, idxs, 0)\n# Append result\nlist_rois.append(img_rois_coordinates)\n# Return ROIs\nreturn list_rois\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.select_final_rois--shape-batch_size-nb_rois-4","title":"shape: (batch_size, nb_rois, 4)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.select_final_rois--shape-batch_size-nb_rois","title":"shape: (batch_size, nb_rois)","text":""},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.set_anchors_type_validity","title":"<code>set_anchors_type_validity(anchor_boxes_dict, image_bboxes, rpn_min_overlap, rpn_max_overlap)</code>","text":"<p>Defines the type and the validity of each anchor     Type:         - pos -&gt; Match between the anchor and a bbox         - neg -&gt; Match between the anchor and the background         - neutral -&gt; In between the two, won't be used by the model     Validity:         - 1 -&gt; If pos or neg         - 0 -&gt; If neutral</p> <p>Parameters:</p> Name Type Description Default <code>anchor_boxes_dict</code> <code>dict</code> <p>Anchor boxes dictionary - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space) - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou</p> required <code>image_bboxes</code> <code>list&lt;dict&gt;</code> <p>List of bboxes of the image</p> required <code>rpn_min_overlap</code> <code>float</code> <p>Threshold below which a bbox is marked as negative</p> required <code>rpn_max_overlap</code> <code>float</code> <p>Threshold aboce which a bbox is marked as positive</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of the anchors boxes (input dictionary) with type and validity added ('anchor_type', 'anchor_validity')</p> <code>list</code> <code>list</code> <p>Liste of the bboxes with no positive anchor associated</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>def set_anchors_type_validity(anchor_boxes_dict: dict, image_bboxes: List[dict], rpn_min_overlap: float,\nrpn_max_overlap: float) -&gt; Tuple[dict, list]:\n'''Defines the type and the validity of each anchor\n        Type:\n            - pos -&gt; Match between the anchor and a bbox\n            - neg -&gt; Match between the anchor and the background\n            - neutral -&gt; In between the two, won't be used by the model\n        Validity:\n            - 1 -&gt; If pos or neg\n            - 0 -&gt; If neutral\n    Args:\n        anchor_boxes_dict (dict): Anchor boxes dictionary\n            - 'anchor_img_coordinates': Coordinates of the anchor box (input format ie. image space)\n            - 'bboxes': bboxes with their coordinates xyxy (in image space) and iou\n        image_bboxes (list&lt;dict&gt;): List of bboxes of the image\n        rpn_min_overlap (float): Threshold below which a bbox is marked as negative\n        rpn_max_overlap (float): Threshold aboce which a bbox is marked as positive\n    Returns:\n        dict: Dictionary of the anchors boxes (input dictionary) with type and validity added ('anchor_type', 'anchor_validity')\n        list: Liste of the bboxes with no positive anchor associated\n    '''\nbboxes_index_with_positive = set()\n# For each anchor ...\nfor anchor_idx, anchor in anchor_boxes_dict.items():\n# Get the dictionary where the keys are the bboxes and the values, the iou\ndict_iou = {index_bbox: dict_bbox['iou'] for index_bbox, dict_bbox in anchor['bboxes'].items()}\n# Get max iou (if for some reason no bbox, set it to 0 (i.e 'neg'))\nmax_iou = max(dict_iou.values()) if len(dict_iou) &gt; 0 else 0\n# If we are above threshold max, the anchor is positive and valid\nif max_iou &gt; rpn_max_overlap:\nanchor['anchor_type'] = 'pos'\nanchor['anchor_validity'] = 1\nbest_bbox_index = max(dict_iou, key=dict_iou.get)\nanchor['best_bbox_index'] = best_bbox_index\nbboxes_index_with_positive.add(best_bbox_index)\n# If we are below threshold min, the anchor is negative and valid\nelif 0 &lt;= max_iou &lt; rpn_min_overlap:\nanchor['anchor_type'] = 'neg'\nanchor['anchor_validity'] = 1\nanchor['best_bbox_index'] = -1\n# Otherwise, it is invalid (and we set it to neutral)\nelse:\nanchor['anchor_type'] = 'neutral'\nanchor['anchor_validity'] = 0\nanchor['best_bbox_index'] = -1\nanchor_boxes_dict[anchor_idx] = anchor\n# Get list of bboxes index without positive anchor\nbboxes_index_with_no_positive = [index_bbox for index_bbox in range(len(image_bboxes))\nif index_bbox not in bboxes_index_with_positive]\nreturn anchor_boxes_dict, bboxes_index_with_no_positive\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.xyhw_to_xyxy","title":"<code>xyhw_to_xyxy(x, y, h, w)</code>","text":"<p>Changes a rectangle in the format xyhw (x, y, h, w) to the format xyxy (x1, y1, x2, y2)</p> Args <p>x (float): x coordinate of the upper left point y (float): y coordinate of the upper left point h (float): height of the rectangle w (float): width of the rectangle</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>x coordinate of the upper left point</p> <code>float</code> <code>float</code> <p>y coordinate of the upper left point</p> <code>float</code> <code>float</code> <p>x coordinate of the bottom right point</p> <code>float</code> <code>float</code> <p>y coordinate of the bottom right point</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>@check_coordinates_validity\ndef xyhw_to_xyxy(x: float, y: float, h: float, w: float) -&gt; Tuple[float, float, float, float]:\n'''Changes a rectangle in the format xyhw (x, y, h, w) to\n    the format xyxy (x1, y1, x2, y2)\n    Args :\n        x (float): x coordinate of the upper left point\n        y (float): y coordinate of the upper left point\n        h (float): height of the rectangle\n        w (float): width of the rectangle\n    Returns:\n        float: x coordinate of the upper left point\n        float: y coordinate of the upper left point\n        float: x coordinate of the bottom right point\n        float: y coordinate of the bottom right point\n    '''\nx2 = x + w\ny2 = y + h\nreturn x, y, x2, y2\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.xyxy_to_cxcyhw","title":"<code>xyxy_to_cxcyhw(x1, y1, x2, y2)</code>","text":"<p>Changes a rectangle in the format xyxy (x1, y1, x2, y2) to the format cxcyhw (cx, cy, h, w)</p> Args <p>x1 (float): x coordinate of the upper left point y1 (float): y coordinate of the upper left point x2 (float): x coordinate of the bottom right point y2 (float): y coordinate of the bottom right point</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>x coordinate of the center of the rectangle</p> <code>float</code> <code>float</code> <p>y coordinate of the center of the rectangle</p> <code>float</code> <code>float</code> <p>height of the rectangle</p> <code>float</code> <code>float</code> <p>width of the rectangle</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>@check_coordinates_validity\ndef xyxy_to_cxcyhw(x1: float, y1: float, x2: float, y2: float) -&gt; Tuple[float, float, float, float]:\n'''Changes a rectangle in the format xyxy (x1, y1, x2, y2) to\n    the format cxcyhw (cx, cy, h, w)\n    Args :\n        x1 (float): x coordinate of the upper left point\n        y1 (float): y coordinate of the upper left point\n        x2 (float): x coordinate of the bottom right point\n        y2 (float): y coordinate of the bottom right point\n    Returns:\n        float: x coordinate of the center of the rectangle\n        float: y coordinate of the center of the rectangle\n        float: height of the rectangle\n        float: width of the rectangle\n    '''\ncx = (x1 + x2) / 2.0\ncy = (y1 + y2) / 2.0\nwidth = x2 - x1\nheight = y2 - y1\nreturn cx, cy, height, width\n</code></pre>"},{"location":"reference/template_vision/models_training/object_detectors/utils_object_detectors/#template_vision.models_training.object_detectors.utils_object_detectors.xyxy_to_xyhw","title":"<code>xyxy_to_xyhw(x1, y1, x2, y2)</code>","text":"<p>Changes a rectangle in the format xyxy (x1, y1, x2, y2) to the format xyhw (x, y, h, w)</p> Args <p>x1 (float): x coordinate of the upper left point y1 (float): y coordinate of the upper left point x2 (float): x coordinate of the bottom right point y2 (float): y coordinate of the bottom right point</p> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>x coordinate of the upper left point</p> <code>float</code> <code>float</code> <p>y coordinate of the upper left point</p> <code>float</code> <code>float</code> <p>height of the rectangle</p> <code>float</code> <code>float</code> <p>width of the rectangle</p> Source code in <code>template_vision/models_training/object_detectors/utils_object_detectors.py</code> <pre><code>@check_coordinates_validity\ndef xyxy_to_xyhw(x1: float, y1: float, x2: float, y2: float) -&gt; Tuple[float, float, float, float]:\n'''Changes a rectangle in the format xyxy (x1, y1, x2, y2) to\n    the format xyhw (x, y, h, w)\n    Args :\n        x1 (float): x coordinate of the upper left point\n        y1 (float): y coordinate of the upper left point\n        x2 (float): x coordinate of the bottom right point\n        y2 (float): y coordinate of the bottom right point\n    Returns:\n        float: x coordinate of the upper left point\n        float: y coordinate of the upper left point\n        float: height of the rectangle\n        float: width of the rectangle\n    '''\nh = y2 - y1\nw = x2 - x1\nreturn x1, y1, h, w\n</code></pre>"},{"location":"reference/template_vision/monitoring/","title":"Monitoring","text":""},{"location":"reference/template_vision/monitoring/mlflow_logger/","title":"Mlflow logger","text":""},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger","title":"<code>MLflowLogger</code>","text":"<p>Abstracts how MlFlow works</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>class MLflowLogger:\n'''Abstracts how MlFlow works'''\ndef __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n'''Class initialization\n        Args:\n            experiment_name (str):  Name of the experiment to activate\n        Kwargs:\n            tracking_uri (str): URI of the tracking server\n            artifact_uri (str): URI where to store artifacts\n        '''\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Backup to local save if no uri (i.e. empty string)\nif not tracking_uri:\ntracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n# Add \"file\" scheme if no scheme in the tracking_uri\nelif not urlparse(tracking_uri).scheme:\ntracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n# If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n# Otherwise we suppose artifact_uri is configured by the system\nif not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\nartifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n# Set tracking URI &amp; experiment name\nself.tracking_uri = tracking_uri\n# Get the experiment if it exists and check if there is a connection error by doing it\ntry:\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nexcept Exception as e:\nself.logger.error(repr(e))\nraise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n# If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\nif experiment:\nexperiment_id = experiment.experiment_id\nartifact_uri = experiment.artifact_location\n# Otherwise we create a new experiment with the provided artifact_uri\nelse:\nexperiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nartifact_uri = experiment.artifact_location\nmlflow.set_experiment(experiment_id=experiment_id)\nself.__experiment_id = experiment_id\nself.__experiment_name = experiment_name\nself.__artifact_uri = artifact_uri\nself.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n@property\ndef tracking_uri(self) -&gt; str:\n'''Current tracking uri'''\nreturn mlflow.get_tracking_uri()\n@tracking_uri.setter\ndef tracking_uri(self, uri:str) -&gt; None:\n'''Set tracking uri'''\nmlflow.set_tracking_uri(uri)\n@property\ndef experiment_id(self) -&gt; str:\n'''Experiment id. It can not be changed.'''\nreturn self.__experiment_id\n@property\ndef experiment_name(self) -&gt; str:\n'''Experiment name. It can not be changed.'''\nreturn self.__experiment_name\n@property\ndef artifact_uri(self) -&gt; str:\n'''Experiment artifact URI. It can not be changed.'''\nreturn self.__artifact_uri\ndef end_run(self) -&gt; None:\n'''Stops an MLflow run'''\ntry:\nmlflow.end_run()\nexcept Exception:\nself.logger.error(\"Can't stop mlflow run\")\ndef log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n'''Logs a metric on mlflow\n        Args:\n            key (str): Name of the metric\n            value (float, ?): Value of the metric\n        Kwargs:\n            step (int): Step of the metric\n        '''\n# Check for None\nif value is None:\nvalue = math.nan\n# Log metric\nmlflow.log_metric(key, value, step)\ndef log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n'''Logs a set of metrics in mlflow\n        Args:\n            metrics (dict): Metrics to log\n        Kwargs:\n            step (int): Step of the metric\n        '''\n# Check for Nones\nfor k, v in metrics.items():\nif v is None:\nmetrics[k] = math.nan\n# Log metrics\nmlflow.log_metrics(metrics, step)\ndef log_param(self, key: str, value) -&gt; None:\n'''Logs a parameter in mlflow\n        Args:\n            key (str): Name of the parameter\n            value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n        '''\nif value is None:\nvalue = 'None'\n# Log parameter\nmlflow.log_param(key, value)\ndef log_params(self, params: dict) -&gt; None:\n'''Logs a set of parameters in mlflow\n        Args:\n            params (dict): Name and value of each parameter\n        '''\n# Check for Nones\nfor k, v in params.items():\nif v is None:\nparams[k] = 'None'\n# Log parameters\nmlflow.log_params(params)\ndef set_tag(self, key: str, value) -&gt; None:\n'''Logs a tag in mlflow\n        Args:\n            key (str): Name of the tag\n            value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n        Raises:\n            ValueError: If the object value is None\n        '''\nif value is None:\nraise ValueError('value must not be None')\n# Log tag\nmlflow.set_tag(key, value)\ndef set_tags(self, tags: dict) -&gt; None:\n'''Logs a set of tags in mlflow\n        Args:\n            tags (dict): Name and value of each tag\n        '''\n# Log tags\nmlflow.set_tags(tags)\ndef valid_name(self, key: str) -&gt; bool:\n'''Validates key names\n        Args:\n            key (str): Key to check\n        Returns:\n            bool: If key is a valid mlflow key\n        '''\nif mlflow.mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\nreturn True\nelse:\nreturn False\ndef log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n'''Log a dataframe containing metrics from a training\n        Args:\n            df_stats (pd.Dataframe): Dataframe containing metrics from a training\n        Kwargs:\n            label_col (str): default labelc column name\n        '''\nif label_col not in df_stats.columns:\nraise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n# Get metrics columns\nmetrics_columns = [col for col in df_stats.columns if col != label_col]\n# Log labels\nlabels = df_stats[label_col].values\nfor i, label in enumerate(labels):  # type: ignore\nself.log_param(f'Label {i}', label)\n# Log metrics\nml_flow_metrics = {}\nfor i, row in df_stats.iterrows():\nfor j, col in enumerate(metrics_columns):\nmetric_key = f\"{row[label_col]} --- {col}\"\n# Check that mlflow accepts the key, otherwise, replace it\n# TODO: could be improved ...\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- {col}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"{row[label_col]} --- Col {j}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- Col {j}\"\nml_flow_metrics[metric_key] = row[col]\n# Log metrics\nself.log_metrics(ml_flow_metrics)\ndef log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n'''Logs a dictionary as an artifact in MLflow\n        Args:\n            dictionary (dict): A dictionary\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\nmlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\ndef log_text(self, text: str, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n        Args:\n            text (str): A text\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n        '''\nmlflow.log_text(text=text, artifact_file=artifact_file)\ndef log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n        Args:\n            figure (matplotlib.figure.Figure): A matplotlib figure\n            artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n        '''\nmlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.artifact_uri","title":"<code>artifact_uri: str</code>  <code>property</code>","text":"<p>Experiment artifact URI. It can not be changed.</p>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.experiment_id","title":"<code>experiment_id: str</code>  <code>property</code>","text":"<p>Experiment id. It can not be changed.</p>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.experiment_name","title":"<code>experiment_name: str</code>  <code>property</code>","text":"<p>Experiment name. It can not be changed.</p>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.tracking_uri","title":"<code>tracking_uri: str</code>  <code>writable</code> <code>property</code>","text":"<p>Current tracking uri</p>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.__init__","title":"<code>__init__(experiment_name, tracking_uri='', artifact_uri='')</code>","text":"<p>Class initialization</p> <p>Parameters:</p> Name Type Description Default <code>experiment_name</code> <code>str</code> <p>Name of the experiment to activate</p> required Kwargs <p>tracking_uri (str): URI of the tracking server artifact_uri (str): URI where to store artifacts</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def __init__(self, experiment_name: str, tracking_uri: str = '', artifact_uri: str = '') -&gt; None:\n'''Class initialization\n    Args:\n        experiment_name (str):  Name of the experiment to activate\n    Kwargs:\n        tracking_uri (str): URI of the tracking server\n        artifact_uri (str): URI where to store artifacts\n    '''\n# Get logger\nself.logger = logging.getLogger(__name__)\n# Backup to local save if no uri (i.e. empty string)\nif not tracking_uri:\ntracking_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns')).as_uri()\n# Add \"file\" scheme if no scheme in the tracking_uri\nelif not urlparse(tracking_uri).scheme:\ntracking_uri = pathlib.Path(tracking_uri).resolve().as_uri()\n# If no artifact_uri and tracking_uri scheme is \"file\", we set a default artifact_uri in experiments folder\n# Otherwise we suppose artifact_uri is configured by the system\nif not artifact_uri and urlparse(tracking_uri).scheme == \"file\":\nartifact_uri = pathlib.Path(os.path.join(utils.get_data_path(), 'experiments', 'mlruns_artifacts')).as_uri()\n# Set tracking URI &amp; experiment name\nself.tracking_uri = tracking_uri\n# Get the experiment if it exists and check if there is a connection error by doing it\ntry:\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nexcept Exception as e:\nself.logger.error(repr(e))\nraise ConnectionError(f\"Can't reach MLflow at {self.tracking_uri}. Please check the URI.\")\n# If the experiment exists, we recover experiment id and artifact_uri (which is link to the experiment)\nif experiment:\nexperiment_id = experiment.experiment_id\nartifact_uri = experiment.artifact_location\n# Otherwise we create a new experiment with the provided artifact_uri\nelse:\nexperiment_id = mlflow.create_experiment(experiment_name, artifact_location=artifact_uri)\nexperiment = mlflow.get_experiment_by_name(experiment_name)\nartifact_uri = experiment.artifact_location\nmlflow.set_experiment(experiment_id=experiment_id)\nself.__experiment_id = experiment_id\nself.__experiment_name = experiment_name\nself.__artifact_uri = artifact_uri\nself.logger.info(f'MLflow running. Metrics available @ {self.tracking_uri}. Experiment artifacts availaible @ {self.artifact_uri}')\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.end_run","title":"<code>end_run()</code>","text":"<p>Stops an MLflow run</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def end_run(self) -&gt; None:\n'''Stops an MLflow run'''\ntry:\nmlflow.end_run()\nexcept Exception:\nself.logger.error(\"Can't stop mlflow run\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_df_stats","title":"<code>log_df_stats(df_stats, label_col='Label')</code>","text":"<p>Log a dataframe containing metrics from a training</p> <p>Parameters:</p> Name Type Description Default <code>df_stats</code> <code>pd.Dataframe</code> <p>Dataframe containing metrics from a training</p> required Kwargs <p>label_col (str): default labelc column name</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_df_stats(self, df_stats: pd.DataFrame, label_col: str = 'Label') -&gt; None:\n'''Log a dataframe containing metrics from a training\n    Args:\n        df_stats (pd.Dataframe): Dataframe containing metrics from a training\n    Kwargs:\n        label_col (str): default labelc column name\n    '''\nif label_col not in df_stats.columns:\nraise ValueError(f\"The provided label column name ({label_col}) not found in df_stats' columns.\")\n# Get metrics columns\nmetrics_columns = [col for col in df_stats.columns if col != label_col]\n# Log labels\nlabels = df_stats[label_col].values\nfor i, label in enumerate(labels):  # type: ignore\nself.log_param(f'Label {i}', label)\n# Log metrics\nml_flow_metrics = {}\nfor i, row in df_stats.iterrows():\nfor j, col in enumerate(metrics_columns):\nmetric_key = f\"{row[label_col]} --- {col}\"\n# Check that mlflow accepts the key, otherwise, replace it\n# TODO: could be improved ...\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- {col}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"{row[label_col]} --- Col {j}\"\nif not self.valid_name(metric_key):\nmetric_key = f\"Label {i} --- Col {j}\"\nml_flow_metrics[metric_key] = row[col]\n# Log metrics\nself.log_metrics(ml_flow_metrics)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_dict","title":"<code>log_dict(dictionary, artifact_file)</code>","text":"<p>Logs a dictionary as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>dictionary</code> <code>dict</code> <p>A dictionary</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_dict(self, dictionary: dict, artifact_file: str) -&gt; None:\n'''Logs a dictionary as an artifact in MLflow\n    Args:\n        dictionary (dict): A dictionary\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\nmlflow.log_dict(dictionary=dictionary, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_figure","title":"<code>log_figure(figure, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>figure</code> <code>matplotlib.figure.Figure</code> <p>A matplotlib figure</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the figure is saved</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_figure(self, figure: Figure, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n    Args:\n        figure (matplotlib.figure.Figure): A matplotlib figure\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the figure is saved\n    '''\nmlflow.log_figure(figure=figure, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_metric","title":"<code>log_metric(key, value, step=None)</code>","text":"<p>Logs a metric on mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the metric</p> required <code>value</code> <code>float, ?</code> <p>Value of the metric</p> required Kwargs <p>step (int): Step of the metric</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_metric(self, key: str, value, step: Union[int, None] = None) -&gt; None:\n'''Logs a metric on mlflow\n    Args:\n        key (str): Name of the metric\n        value (float, ?): Value of the metric\n    Kwargs:\n        step (int): Step of the metric\n    '''\n# Check for None\nif value is None:\nvalue = math.nan\n# Log metric\nmlflow.log_metric(key, value, step)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_metrics","title":"<code>log_metrics(metrics, step=None)</code>","text":"<p>Logs a set of metrics in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>dict</code> <p>Metrics to log</p> required Kwargs <p>step (int): Step of the metric</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_metrics(self, metrics: dict, step: Union[int, None] = None) -&gt; None:\n'''Logs a set of metrics in mlflow\n    Args:\n        metrics (dict): Metrics to log\n    Kwargs:\n        step (int): Step of the metric\n    '''\n# Check for Nones\nfor k, v in metrics.items():\nif v is None:\nmetrics[k] = math.nan\n# Log metrics\nmlflow.log_metrics(metrics, step)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_param","title":"<code>log_param(key, value)</code>","text":"<p>Logs a parameter in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the parameter</p> required <code>value</code> <code>str, ?</code> <p>Value of the parameter (which will be cast to str if not already of type str)</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_param(self, key: str, value) -&gt; None:\n'''Logs a parameter in mlflow\n    Args:\n        key (str): Name of the parameter\n        value (str, ?): Value of the parameter (which will be cast to str if not already of type str)\n    '''\nif value is None:\nvalue = 'None'\n# Log parameter\nmlflow.log_param(key, value)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_params","title":"<code>log_params(params)</code>","text":"<p>Logs a set of parameters in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Name and value of each parameter</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_params(self, params: dict) -&gt; None:\n'''Logs a set of parameters in mlflow\n    Args:\n        params (dict): Name and value of each parameter\n    '''\n# Check for Nones\nfor k, v in params.items():\nif v is None:\nparams[k] = 'None'\n# Log parameters\nmlflow.log_params(params)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.log_text","title":"<code>log_text(text, artifact_file)</code>","text":"<p>Logs a text as an artifact in MLflow</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>A text</p> required <code>artifact_file</code> <code>str</code> <p>The run-relative artifact file path in posixpath format to which the dictionary is saved</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def log_text(self, text: str, artifact_file: str) -&gt; None:\n'''Logs a text as an artifact in MLflow\n    Args:\n        text (str): A text\n        artifact_file (str): The run-relative artifact file path in posixpath format to which the dictionary is saved\n    '''\nmlflow.log_text(text=text, artifact_file=artifact_file)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.set_tag","title":"<code>set_tag(key, value)</code>","text":"<p>Logs a tag in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Name of the tag</p> required <code>value</code> <code>str, ?</code> <p>Value of the tag (which will be cast to str if not already of type str)</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the object value is None</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def set_tag(self, key: str, value) -&gt; None:\n'''Logs a tag in mlflow\n    Args:\n        key (str): Name of the tag\n        value (str, ?): Value of the tag (which will be cast to str if not already of type str)\n    Raises:\n        ValueError: If the object value is None\n    '''\nif value is None:\nraise ValueError('value must not be None')\n# Log tag\nmlflow.set_tag(key, value)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.set_tags","title":"<code>set_tags(tags)</code>","text":"<p>Logs a set of tags in mlflow</p> <p>Parameters:</p> Name Type Description Default <code>tags</code> <code>dict</code> <p>Name and value of each tag</p> required Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def set_tags(self, tags: dict) -&gt; None:\n'''Logs a set of tags in mlflow\n    Args:\n        tags (dict): Name and value of each tag\n    '''\n# Log tags\nmlflow.set_tags(tags)\n</code></pre>"},{"location":"reference/template_vision/monitoring/mlflow_logger/#template_vision.monitoring.mlflow_logger.MLflowLogger.valid_name","title":"<code>valid_name(key)</code>","text":"<p>Validates key names</p> <p>Parameters:</p> Name Type Description Default <code>key</code> <code>str</code> <p>Key to check</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>If key is a valid mlflow key</p> Source code in <code>template_vision/monitoring/mlflow_logger.py</code> <pre><code>def valid_name(self, key: str) -&gt; bool:\n'''Validates key names\n    Args:\n        key (str): Key to check\n    Returns:\n        bool: If key is a valid mlflow key\n    '''\nif mlflow.mlflow.utils.validation._VALID_PARAM_AND_METRIC_NAMES.match(key):\nreturn True\nelse:\nreturn False\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/","title":"Model explainer","text":""},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.Explainer","title":"<code>Explainer</code>","text":"<p>Parent class for the explainers</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>class Explainer:\n'''Parent class for the explainers'''\ndef __init__(self, *args, **kwargs) -&gt; None:\n'''Initialization of the parent class'''\nself.logger = logging.getLogger(__name__)\ndef explain_instance(self, content: Image.Image, **kwargs) -&gt; Any:\n'''Explains a prediction\n        Args:\n            content (Image.Image): Image to be explained\n        Returns:\n            (?): An explanation object\n        '''\nraise NotImplementedError(\"'explain_instance' needs to be overridden\")\ndef explain_instance_as_html(self, content: Image.Image, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n        Args:\n            content (Image.Image): Image to be explained\n        Returns:\n            str: An HTML code with the explanation\n        '''\nraise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\ndef explain_instance_as_json(self, content: Image.Image, **kwargs) -&gt; Union[dict, list]:\n'''Explains a prediction - returns an JSON serializable object\n        Args:\n            content (str): Text to be explained\n        Returns:\n            str: A JSON serializable object with the explanation\n        '''\nraise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.Explainer.__init__","title":"<code>__init__(*args, **kwargs)</code>","text":"<p>Initialization of the parent class</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def __init__(self, *args, **kwargs) -&gt; None:\n'''Initialization of the parent class'''\nself.logger = logging.getLogger(__name__)\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.Explainer.explain_instance","title":"<code>explain_instance(content, **kwargs)</code>","text":"<p>Explains a prediction</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Image.Image</code> <p>Image to be explained</p> required <p>Returns:</p> Type Description <code>?</code> <p>An explanation object</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: Image.Image, **kwargs) -&gt; Any:\n'''Explains a prediction\n    Args:\n        content (Image.Image): Image to be explained\n    Returns:\n        (?): An explanation object\n    '''\nraise NotImplementedError(\"'explain_instance' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.Explainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>Image.Image</code> <p>Image to be explained</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>An HTML code with the explanation</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: Image.Image, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n    Args:\n        content (Image.Image): Image to be explained\n    Returns:\n        str: An HTML code with the explanation\n    '''\nraise NotImplementedError(\"'explain_instance_as_html' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.Explainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an JSON serializable object</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text to be explained</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>Union[dict, list]</code> <p>A JSON serializable object with the explanation</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: Image.Image, **kwargs) -&gt; Union[dict, list]:\n'''Explains a prediction - returns an JSON serializable object\n    Args:\n        content (str): Text to be explained\n    Returns:\n        str: A JSON serializable object with the explanation\n    '''\nraise NotImplementedError(\"'explain_instance_as_json' needs to be overridden\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer","title":"<code>LimeExplainer</code>","text":"<p>         Bases: <code>Explainer</code></p> <p>Lime Explainer wrapper class</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>class LimeExplainer(Explainer):\n'''Lime Explainer wrapper class'''\ndef __init__(self, model: Type[ModelClassifierMixin], model_conf: dict) -&gt; None:\n''' Initialization\n        Args:\n            model: A model instance with predict &amp; predict_proba functions, and list_classes attribute\n            model_conf (dict): The model's configuration\n        Raises:\n            ValueError: If the provided model is not a classifier\n            TypeError: If the provided model does not implement a `predict_proba` function\n            TypeError: If the provided model does not have a `list_classes` attribute\n        '''\nsuper().__init__()\npred_proba_op = getattr(model, \"predict_proba\", None)\n# Check classifier\nif not model.model_type == 'classifier':\nraise ValueError(\"LimeExplainer only supported with classifier models\")\n# Check needed methods\nif pred_proba_op is None or not callable(pred_proba_op):\nraise TypeError(\"The supplied model must implement a predict_proba() function\")\nif getattr(model, \"list_classes\", None) is None:\nraise TypeError(\"The supplied model must have a list_classes attribute\")\nself.model = model\nself.model_conf = model_conf\nself.class_names = self.model.list_classes\n# Our explainers will explain a prediction for a given class / label\n# These atributes are set on the fly\nself.current_class_index = 0\n# Create the explainer\nself.explainer = LimeImageExplainer()\ndef classifier_fn(self, content_arrays: np.ndarray) -&gt; np.ndarray:\n'''Function to get probabilities from a list of (not preprocessed) images\n        Args:\n            content_arrays (np.ndarray): images to be considered\n        Returns:\n            np.array: probabilities\n        '''\n# Get preprocessor\nif 'preprocess_str' in self.model_conf.keys():\npreprocess_str = self.model_conf['preprocess_str']\nelse:\npreprocess_str = \"no_preprocess\"\npreprocessor = preprocess.get_preprocessor(preprocess_str)\n# Preprocess images\nimages = [Image.fromarray(img, 'RGB') for img in content_arrays]\nimages_preprocessed = preprocessor(images)\n# Temporary folder\nwith tempfile.TemporaryDirectory(dir=utils.get_data_path()) as tmp_folder:\n# Save images\nimages_path = [os.path.join(tmp_folder, f'image_{i}.png') for i in range(len(images_preprocessed))]\nfor i, img_preprocessed in enumerate(images_preprocessed):\nimg_preprocessed.save(images_path[i], format='PNG')\n# Get predictions\ndf = pd.DataFrame({'file_path': images_path})\nprobas = self.model.predict_proba(df)\n# Return probas\nreturn probas\ndef explain_instance(self, content: Image.Image, class_index: Union[int, None] = None,\nnum_samples: int = 100, batch_size: int = 100, hide_color=0, **kwargs):\n'''Explains a prediction\n        This function calls the Lime module. It generates neighborhood data by randomly perturbing features from the instance.\n        Then, it learns locally weighted linear models on this neighborhood data to explain each of the classes in an interpretable way.\n        Args:\n            img (Image.Image): Image to be explained\n        Kwargs:\n            class_index (int): for classification only. Class or label index to be considered.\n            num_samples (int): size of the neighborhood to learn the linear model (cf. Lime documentation)\n            batch_size (int): classifier_fn will be called on batches of this size (cf. Lime documentation)\n            hide_color (?): TODO\n        Returns:\n            (?): An explanation object\n        '''\n# Set index\nif class_index is not None:\nself.current_class_index = class_index\nelse:\nself.current_class_index = 1  # Def to 1\n# Get explanations (images must be convert into rgb, then into np array)\nreturn self.explainer.explain_instance(np.array(content.convert('RGB')), self.classifier_fn,\nlabels=(self.current_class_index,),\nnum_samples=num_samples, batch_size=batch_size,\nhide_color=hide_color, top_labels=None)\ndef explain_instance_as_html(self, content: Image.Image, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n        ** NOT IMPLEMENTED **\n        '''\nraise NotImplementedError(\"'explain_instance_as_html' is not defined for LimeExplainer\")\ndef explain_instance_as_json(self, content: Image.Image, **kwargs) -&gt; Union[dict, list]:\n'''Explains a prediction - returns an JSON serializable object\n        ** NOT IMPLEMENTED **\n        '''\nraise NotImplementedError(\"'explain_instance_as_json' is not defined for LimeExplainer\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer.__init__","title":"<code>__init__(model, model_conf)</code>","text":"<p>Initialization</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Type[ModelClassifierMixin]</code> <p>A model instance with predict &amp; predict_proba functions, and list_classes attribute</p> required <code>model_conf</code> <code>dict</code> <p>The model's configuration</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the provided model is not a classifier</p> <code>TypeError</code> <p>If the provided model does not implement a <code>predict_proba</code> function</p> <code>TypeError</code> <p>If the provided model does not have a <code>list_classes</code> attribute</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def __init__(self, model: Type[ModelClassifierMixin], model_conf: dict) -&gt; None:\n''' Initialization\n    Args:\n        model: A model instance with predict &amp; predict_proba functions, and list_classes attribute\n        model_conf (dict): The model's configuration\n    Raises:\n        ValueError: If the provided model is not a classifier\n        TypeError: If the provided model does not implement a `predict_proba` function\n        TypeError: If the provided model does not have a `list_classes` attribute\n    '''\nsuper().__init__()\npred_proba_op = getattr(model, \"predict_proba\", None)\n# Check classifier\nif not model.model_type == 'classifier':\nraise ValueError(\"LimeExplainer only supported with classifier models\")\n# Check needed methods\nif pred_proba_op is None or not callable(pred_proba_op):\nraise TypeError(\"The supplied model must implement a predict_proba() function\")\nif getattr(model, \"list_classes\", None) is None:\nraise TypeError(\"The supplied model must have a list_classes attribute\")\nself.model = model\nself.model_conf = model_conf\nself.class_names = self.model.list_classes\n# Our explainers will explain a prediction for a given class / label\n# These atributes are set on the fly\nself.current_class_index = 0\n# Create the explainer\nself.explainer = LimeImageExplainer()\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer.classifier_fn","title":"<code>classifier_fn(content_arrays)</code>","text":"<p>Function to get probabilities from a list of (not preprocessed) images</p> <p>Parameters:</p> Name Type Description Default <code>content_arrays</code> <code>np.ndarray</code> <p>images to be considered</p> required <p>Returns:</p> Type Description <code>np.ndarray</code> <p>np.array: probabilities</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def classifier_fn(self, content_arrays: np.ndarray) -&gt; np.ndarray:\n'''Function to get probabilities from a list of (not preprocessed) images\n    Args:\n        content_arrays (np.ndarray): images to be considered\n    Returns:\n        np.array: probabilities\n    '''\n# Get preprocessor\nif 'preprocess_str' in self.model_conf.keys():\npreprocess_str = self.model_conf['preprocess_str']\nelse:\npreprocess_str = \"no_preprocess\"\npreprocessor = preprocess.get_preprocessor(preprocess_str)\n# Preprocess images\nimages = [Image.fromarray(img, 'RGB') for img in content_arrays]\nimages_preprocessed = preprocessor(images)\n# Temporary folder\nwith tempfile.TemporaryDirectory(dir=utils.get_data_path()) as tmp_folder:\n# Save images\nimages_path = [os.path.join(tmp_folder, f'image_{i}.png') for i in range(len(images_preprocessed))]\nfor i, img_preprocessed in enumerate(images_preprocessed):\nimg_preprocessed.save(images_path[i], format='PNG')\n# Get predictions\ndf = pd.DataFrame({'file_path': images_path})\nprobas = self.model.predict_proba(df)\n# Return probas\nreturn probas\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer.explain_instance","title":"<code>explain_instance(content, class_index=None, num_samples=100, batch_size=100, hide_color=0, **kwargs)</code>","text":"<p>Explains a prediction</p> <p>This function calls the Lime module. It generates neighborhood data by randomly perturbing features from the instance. Then, it learns locally weighted linear models on this neighborhood data to explain each of the classes in an interpretable way.</p> <p>Parameters:</p> Name Type Description Default <code>img</code> <code>Image.Image</code> <p>Image to be explained</p> required Kwargs <p>class_index (int): for classification only. Class or label index to be considered. num_samples (int): size of the neighborhood to learn the linear model (cf. Lime documentation) batch_size (int): classifier_fn will be called on batches of this size (cf. Lime documentation) hide_color (?): TODO</p> <p>Returns:</p> Type Description <code>?</code> <p>An explanation object</p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance(self, content: Image.Image, class_index: Union[int, None] = None,\nnum_samples: int = 100, batch_size: int = 100, hide_color=0, **kwargs):\n'''Explains a prediction\n    This function calls the Lime module. It generates neighborhood data by randomly perturbing features from the instance.\n    Then, it learns locally weighted linear models on this neighborhood data to explain each of the classes in an interpretable way.\n    Args:\n        img (Image.Image): Image to be explained\n    Kwargs:\n        class_index (int): for classification only. Class or label index to be considered.\n        num_samples (int): size of the neighborhood to learn the linear model (cf. Lime documentation)\n        batch_size (int): classifier_fn will be called on batches of this size (cf. Lime documentation)\n        hide_color (?): TODO\n    Returns:\n        (?): An explanation object\n    '''\n# Set index\nif class_index is not None:\nself.current_class_index = class_index\nelse:\nself.current_class_index = 1  # Def to 1\n# Get explanations (images must be convert into rgb, then into np array)\nreturn self.explainer.explain_instance(np.array(content.convert('RGB')), self.classifier_fn,\nlabels=(self.current_class_index,),\nnum_samples=num_samples, batch_size=batch_size,\nhide_color=hide_color, top_labels=None)\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer.explain_instance_as_html","title":"<code>explain_instance_as_html(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an HTML object  NOT IMPLEMENTED </p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_html(self, content: Image.Image, **kwargs) -&gt; str:\n'''Explains a prediction - returns an HTML object\n    ** NOT IMPLEMENTED **\n    '''\nraise NotImplementedError(\"'explain_instance_as_html' is not defined for LimeExplainer\")\n</code></pre>"},{"location":"reference/template_vision/monitoring/model_explainer/#template_vision.monitoring.model_explainer.LimeExplainer.explain_instance_as_json","title":"<code>explain_instance_as_json(content, **kwargs)</code>","text":"<p>Explains a prediction - returns an JSON serializable object  NOT IMPLEMENTED </p> Source code in <code>template_vision/monitoring/model_explainer.py</code> <pre><code>def explain_instance_as_json(self, content: Image.Image, **kwargs) -&gt; Union[dict, list]:\n'''Explains a prediction - returns an JSON serializable object\n    ** NOT IMPLEMENTED **\n    '''\nraise NotImplementedError(\"'explain_instance_as_json' is not defined for LimeExplainer\")\n</code></pre>"},{"location":"reference/template_vision/preprocessing/","title":"Preprocessing","text":""},{"location":"reference/template_vision/preprocessing/manage_white_borders/","title":"Manage white borders","text":""},{"location":"reference/template_vision/preprocessing/manage_white_borders/#template_vision.preprocessing.manage_white_borders.fill_with_white","title":"<code>fill_with_white(im, image_ratio)</code>","text":"<p>Fills an image with white such that it respects a wanted ratio</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Image</code> <p>Image to be processed</p> required Kwargs <p>image_ratio (float): Wanted image ratio</p> <p>Returns:</p> Name Type Description <code>Image</code> <code>Image</code> <p>Transformed image</p> Source code in <code>template_vision/preprocessing/manage_white_borders.py</code> <pre><code>def fill_with_white(im: Image, image_ratio: float) -&gt; Image:\n'''Fills an image with white such that it respects a wanted ratio\n    Args:\n        im (Image): Image to be processed\n    Kwargs:\n        image_ratio (float): Wanted image ratio\n    Returns:\n        Image: Transformed image\n    '''\nwidth, height = im.size\nratio = width / height\nif ratio &gt; image_ratio:\n# Increase height\nwanted_height = round(width / image_ratio)\nnew_size = (width, wanted_height)\nold_size = (width, height)\n# Set new image\nnew_im = Image.new(\"RGB\", new_size, (255, 255, 255))\n# Fill it with the old image, centered\n# (use floor to ensure the old image fits into the new one)\nx_pos = 0\ny_pos = floor((new_size[1] - old_size[1]) / 2)\nnew_im.paste(im, (x_pos, y_pos))\nelif ratio &lt; image_ratio:\n# Increase width\nwanted_width = round(height * image_ratio)\nnew_size = (wanted_width, height)\nold_size = (width, height)\n# Set new image\nnew_im = Image.new(\"RGB\", new_size, (255, 255, 255))\n# Fill it with the old image, centered\n# (use floor to ensure the old image fits into the new one)\nx_pos = floor((new_size[0] - old_size[0]) / 2)\ny_pos = 0\nnew_im.paste(im, (x_pos, y_pos))\nelse:  # Already correct ratio\nnew_im = im\nreturn new_im\n</code></pre>"},{"location":"reference/template_vision/preprocessing/manage_white_borders/#template_vision.preprocessing.manage_white_borders.remove_white_borders","title":"<code>remove_white_borders(images, image_ratio_strategy=None, image_ratio=0.75, with_rotation=True)</code>","text":"<p>Removes white border Also change the image ratio and rotate (if wanted) along largest dim. (i.e. portrait mode)</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>Images to be processed</p> required Kwargs <p>image_ratio_strategy (str): wanted strategy to apply new image_ratio     - None: no change in image ratio     - 'fill': add white borders on the smallest dimension     - 'stretch': stretch the images such that they have the wanted ratio image_ratio (float): Wanted final image ratio (unused if image_ratio_strategy is None) with_rotation (bool): If the images must be rotated along largest dim. (i.e. portrait mode)</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If image_ratio_strategy value is not a valid option ([None, 'fill', 'stretch'])</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Images sans les marges blanches</p> Source code in <code>template_vision/preprocessing/manage_white_borders.py</code> <pre><code>def remove_white_borders(images: list, image_ratio_strategy: Union[str, None] = None, image_ratio: float = 0.75, with_rotation: bool = True) -&gt; list:\n'''Removes white border\n    Also change the image ratio and rotate (if wanted) along largest dim. (i.e. portrait mode)\n    Args:\n        images (list): Images to be processed\n    Kwargs:\n        image_ratio_strategy (str): wanted strategy to apply new image_ratio\n            - None: no change in image ratio\n            - 'fill': add white borders on the smallest dimension\n            - 'stretch': stretch the images such that they have the wanted ratio\n        image_ratio (float): Wanted final image ratio (unused if image_ratio_strategy is None)\n        with_rotation (bool): If the images must be rotated along largest dim. (i.e. portrait mode)\n    Raises:\n        ValueError: If image_ratio_strategy value is not a valid option ([None, 'fill', 'stretch'])\n    Returns:\n        list: Images sans les marges blanches\n    '''\nif image_ratio_strategy not in [None, 'fill', 'stretch']:\nraise ValueError(f\"image ratio strategy (image_ratio_strategy) '{image_ratio_strategy}' is not a valid option ([None, 'fill', 'stretch'])\")\n# Get 'True' white\n# TODO : to be improved !\ntrue_white = _rgb2gray(np.array([255, 255, 255]))\n# Process each image, one by one\nresults = []\nfor i, im in enumerate(tqdm.tqdm(images)):\n# Remove white borders\npixels = _rgb2gray(np.array(im))\n# x : horizontal\n# y : vertical\nfirst_x = _get_first_x(pixels, true_white)  # Left\nfirst_y = _get_first_y(pixels, true_white)  # Upper\nlast_x = _get_last_x(pixels, true_white)  # Right\nlast_y = _get_last_y(pixels, true_white)  # Lower\n# If first_x -1 -&gt; no 'non-white' pixel, do nothing\nif first_x == -1:\ncontinue\nelse:\n# Crop image (left, upper, right, lower)\nim = im.crop((first_x, first_y, last_x + 1, last_y + 1))\n# Rotate image if wanted\nif with_rotation:\nim = rotate_image_largest_dim(im)\n# Manage new ratio strategy\nif image_ratio_strategy == 'fill':  # fill with white borders to get correct format\nim = fill_with_white(im, image_ratio)\nelif image_ratio_strategy == 'stretch':\nim = stretch_image(im, image_ratio)\n# If None, do nothign\n# Update list\nresults.append(im)\n# Return\nreturn results\n</code></pre>"},{"location":"reference/template_vision/preprocessing/manage_white_borders/#template_vision.preprocessing.manage_white_borders.rotate_image_largest_dim","title":"<code>rotate_image_largest_dim(im)</code>","text":"<p>Rotates an image along largest dim. (i.e. portrait mode)</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Image</code> <p>Image to be processed</p> required <p>Returns:</p> Name Type Description <code>Image</code> <code>Image</code> <p>Rotated image</p> Source code in <code>template_vision/preprocessing/manage_white_borders.py</code> <pre><code>def rotate_image_largest_dim(im: Image) -&gt; Image:\n'''Rotates an image along largest dim. (i.e. portrait mode)\n    Args:\n        im (Image): Image to be processed\n    Returns:\n        Image: Rotated image\n    '''\norientation = _get_orientation(im)\nif orientation != 0:\nim = im.rotate(orientation, expand=True)\nreturn im\n</code></pre>"},{"location":"reference/template_vision/preprocessing/manage_white_borders/#template_vision.preprocessing.manage_white_borders.stretch_image","title":"<code>stretch_image(im, image_ratio)</code>","text":"<p>Stretch an image such that it respects a wanted ratio</p> <p>Parameters:</p> Name Type Description Default <code>im</code> <code>Image</code> <p>Image to be processed</p> required Kwargs <p>image_ratio (float): Wanted image ratio</p> <p>Returns:</p> Name Type Description <code>Image</code> <code>Image</code> <p>Transformed image</p> Source code in <code>template_vision/preprocessing/manage_white_borders.py</code> <pre><code>def stretch_image(im: Image, image_ratio: float) -&gt; Image:\n'''Stretch an image such that it respects a wanted ratio\n    Args:\n        im (Image): Image to be processed\n    Kwargs:\n        image_ratio (float): Wanted image ratio\n    Returns:\n        Image: Transformed image\n    '''\nwidth, height = im.size\nratio = width / height\nif ratio &gt; image_ratio:\n# Increase height\nwanted_height = round(width / image_ratio)\nnew_size = (width, wanted_height)\nnew_im = im.resize(new_size)\nelif ratio &lt; image_ratio:\n# Increase width\nwanted_width = round(height * image_ratio)\nnew_size = (wanted_width, height)\nnew_im = im.resize(new_size)\nelse:  # Already correct ratio\nnew_im = im\nreturn new_im\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/","title":"Preprocess","text":""},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.apply_pipeline","title":"<code>apply_pipeline(images, pipeline)</code>","text":"<p>Applies a pipeline (i.e. list of transformations)</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be transformed</p> required <code>pipeline</code> <code>list</code> <p>List of transformation to be applied</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Preprocessed images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def apply_pipeline(images: list, pipeline: list) -&gt; list:\n'''Applies a pipeline (i.e. list of transformations)\n    Args:\n        images (list): List of images to be transformed\n        pipeline (list): List of transformation to be applied\n    Returns:\n        list: Preprocessed images\n    '''\n# Process\nresults = None\nfor transformer in pipeline:\nif results is None:\nresults = transformer(images)\nelse:\nresults = transformer(results)\nreturn results\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.convert_rgb","title":"<code>convert_rgb(images)</code>","text":"<p>Converts a list of image into RGB images</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be converted</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>RGB images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def convert_rgb(images: list) -&gt; list:\n'''Converts a list of image into RGB images\n    Args:\n        images (list): List of images to be converted\n    Returns:\n        list: RGB images\n    '''\nreturn [im.convert('RGB') for im in images]\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.get_preprocessor","title":"<code>get_preprocessor(preprocess_str)</code>","text":"<p>Gets a preprocessing (function) from its name</p> <p>Parameters:</p> Name Type Description Default <code>preprocess_str</code> <code>str</code> <p>Name of the preprocess</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the name of the preprocess is not known</p> <p>Returns:</p> Name Type Description <code>Callable</code> <code>Callable</code> <p>Function to be used for the preprocessing</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def get_preprocessor(preprocess_str: str) -&gt; Callable:\n'''Gets a preprocessing (function) from its name\n    Args:\n        preprocess_str (str): Name of the preprocess\n    Raises:\n        ValueError: If the name of the preprocess is not known\n    Returns:\n        Callable: Function to be used for the preprocessing\n    '''\n# Process\npreprocessors_dict = get_preprocessors_dict()\nif preprocess_str not in preprocessors_dict.keys():\nraise ValueError(f\"The preprocess {preprocess_str} is not known.\")\n# Get preprocessor\npreprocessor = preprocessors_dict[preprocess_str]\n# Return\nreturn preprocessor\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.get_preprocessors_dict","title":"<code>get_preprocessors_dict()</code>","text":"<p>Gets a dictionary of available preprocessing</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of preprocessing</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def get_preprocessors_dict() -&gt; dict:\n'''Gets a dictionary of available preprocessing\n    Returns:\n        dict: Dictionary of preprocessing\n    '''\npreprocessors_dict = {\n'no_preprocess': lambda x: x,  # - /!\\ DO NOT DELETE -&gt; necessary for compatibility /!\\ -\n'preprocess_convert_rgb': preprocess_convert_rgb,  # Simple RGB converter\n'preprocess_docs': preprocess_docs,  # Example pipeline with documents (remove white borders, 3/4 ratio, resize 224x224)\n}\nreturn preprocessors_dict\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.jpeg_compression","title":"<code>jpeg_compression(images, quality=75)</code>","text":"<p>Simulates a JPEG compression Might be useful for prediction if a model is trained with JPEG compressed images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be compressed</p> required Kwargs <p>quality (int): Wanted quality</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Compressed images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def jpeg_compression(images: list, quality: int = 75) -&gt; list:\n'''Simulates a JPEG compression\n    Might be useful for prediction if a model is trained with JPEG compressed images.\n    Args:\n        images (list): List of images to be compressed\n    Kwargs:\n        quality (int): Wanted quality\n    Returns:\n        list: Compressed images\n    '''\n# Process images one by one\nresults = []\nfor i, im in enumerate(tqdm.tqdm(images)):\nout = BytesIO()\nim.save(out, format='JPEG', quality=quality)\nout.seek(0)\ntmp_im = Image.open(out)\ntmp_im.load()  # Cumpulsory to avoid \"Too many open files\" issue\nresults.append(tmp_im)\nreturn results\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.preprocess_convert_rgb","title":"<code>preprocess_convert_rgb(images)</code>","text":"<p>Applies a simple RGB conversion</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be transformed</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Preprocessed images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def preprocess_convert_rgb(images: list) -&gt; list:\n'''Applies a simple RGB conversion\n    Args:\n        images (list): List of images to be transformed\n    Returns:\n        list: Preprocessed images\n    '''\n# Process\npipeline = [convert_rgb]\nreturn apply_pipeline(images, pipeline=pipeline)\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.preprocess_docs","title":"<code>preprocess_docs(images)</code>","text":"<p>Applies a list of usual transformations with scanned documents</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be transformed</p> required <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Preprocessed images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def preprocess_docs(images: list) -&gt; list:\n'''Applies a list of usual transformations with scanned documents\n    Args:\n        images (list): List of images to be transformed\n    Returns:\n        list: Preprocessed images\n    '''\n# Process\npipeline = [convert_rgb, functools.partial(manage_white_borders.remove_white_borders, image_ratio_strategy='fill', image_ratio=0.75, with_rotation=True), resize]\nreturn apply_pipeline(images, pipeline=pipeline)\n</code></pre>"},{"location":"reference/template_vision/preprocessing/preprocess/#template_vision.preprocessing.preprocess.resize","title":"<code>resize(images, width=224, height=224)</code>","text":"<p>Resizes images</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of images to be resized</p> required Kwargs <p>width (int): Wanted width height (int): Wanted height</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If width &lt; 1</p> <code>ValueError</code> <p>If height &lt; 1</p> <p>Returns:</p> Name Type Description <code>list</code> <code>list</code> <p>Resized images</p> Source code in <code>template_vision/preprocessing/preprocess.py</code> <pre><code>def resize(images: list, width: int = 224, height: int = 224) -&gt; list:\n'''Resizes images\n    Args:\n        images (list): List of images to be resized\n    Kwargs:\n        width (int): Wanted width\n        height (int): Wanted height\n    Raises:\n        ValueError: If width &lt; 1\n        ValueError: If height &lt; 1\n    Returns:\n        list: Resized images\n    '''\nif width &lt; 1:\nraise ValueError('Width must be strictly positive.')\nif height &lt; 1:\nraise ValueError('Height must be strictly positive.')\n# Process images one by one\nresults = []\nfor i, im in enumerate(tqdm.tqdm(images)):\nresults.append(im.resize((width, height)))\nreturn results\n</code></pre>"}]}